1
00:00:00,000 --> 00:00:10,160
안녕하세요 여러분, 제 이름은 Daniel Yu입니다. 21번째 강의입니다. 이번 프레젠테이션에서는

2
00:00:10,160 --> 00:00:16,240
대규모 언어 모델과 도구 확장에 대해 이야기합니다.

3
00:00:16,240 --> 00:00:21,880
그래서 우리에게 약간의 동기를 부여하기 위해 우리는 대규모 언어 모델이 자연어 처리에 탁월하다는 것을 보았습니다.

4
00:00:21,880 --> 00:00:27,720
그러나 언어 모델은 또한 많은 본질적인 한계가 있음을 보여줍니다.

5
00:00:28,160 --> 00:00:35,240
먼저, LLM은 특정 시점까지의 관련 텍스트 코퍼스로 훈련을 받았기 때문에

6
00:00:35,240 --> 00:00:41,080
최신 정보에 접근할 수 없습니다. 또한 이를 보여주는 많은 연구 결과가 있습니다.

7
00:00:41,080 --> 00:00:49,560
큰 언어 모델은 뚱뚱한 환각을 일으키기 쉽습니다. 그들은 또한 다단계에 매우 나쁘다

8
00:00:49,560 --> 00:00:57,720
추론 능력이 부족하고 정확한 계산을 위한 수학적 능력이 부족합니다. 게다가 그들은

9
00:00:57,720 --> 00:01:03,560
시간의 진행을 인식하지 못하므로 시간에 민감한 작업을 수행할 수 없습니다.

10
00:01:05,640 --> 00:01:11,080
물론 이러한 제한 사항 중 일부는 모델을 더욱 확장하여 부분적으로 해결될 수 있습니다.

11
00:01:11,640 --> 00:01:20,280
하지만 훨씬 더 간단하고 훨씬 더 대중적인 솔루션은 LLM의 한계를 극복하는 것입니다.

12
00:01:20,280 --> 00:01:28,520
외부 도구를 사용하여. 이 경우 외부 도구는 다음과 같이 제한된 사용 도구를 나타냅니다.

13
00:01:28,520 --> 00:01:36,440
계산기, 달력, 심지어 프로그램 통역사까지. 검색 엔진도 포함될 수 있습니다.

14
00:01:36,520 --> 00:01:43,880
전문 지식을 위한 API 또는 외부 데이터베이스를 검색할 수 있습니다.

15
00:01:48,760 --> 00:01:53,240
이제 현대에는 이러한 도구에 편리하게 액세스할 수 있습니다.

16
00:01:53,800 --> 00:02:00,200
LamaIndex와 같은 데이터 프레임워크를 사용합니다. LamaIndex는 LLM 간의 인터페이스를 제공합니다

17
00:02:00,280 --> 00:02:08,040
외부 도구 또는 문서. 이 경우 LLM 인덱스에는 실제로 인덱싱 기능이 있습니다.

18
00:02:08,040 --> 00:02:16,920
이를 통해 외부 라이브러리에서 문서나 API를 쉽게 검색할 수 있습니다. 특징이 있습니다

19
00:02:16,920 --> 00:02:23,480
이를 통해 구조화된 데이터베이스나 원시 파일과 같은 구조화되지 않은 문서에 액세스할 수 있습니다.

20
00:02:23,480 --> 00:02:30,920
또는 벡터 저장소와 같은 반구조화된 데이터베이스 또는 애플리케이션과 같은 프로그래밍 데이터일 수도 있습니다.

21
00:02:30,920 --> 00:02:43,000
아피스. 그리고 이 프레젠테이션의 개요를 위해 우리는 6개의 연구 논문을 다룰 것입니다. 나는 될 것이다

22
00:02:43,000 --> 00:02:49,320
처음 세 개, 전직 아트 및 에이전트 벤치 두 개를 다루고 Mohamed가 마지막 세 개를 다룰 예정입니다.

23
00:02:49,480 --> 00:02:57,240
그리고 이 6개 논문은 모두 우리가 미세 조정을 사용할 수 있는 접근 방식을 나타냅니다.

24
00:02:57,800 --> 00:03:02,120
외부 도구로 강화된 대규모 언어 모델을 프롬프트하거나 벤치마킹합니다.

25
00:03:06,760 --> 00:03:14,680
우리가 이야기하는 첫 번째 논문은 ToolFormer입니다. 미세 조정을 논의한 최초의 논문 중 하나입니다.

26
00:03:14,680 --> 00:03:24,760
도구를 사용하여 모델을 만듭니다. 우선, 도구 사용의 표준 접근 방식은 특정 도구를 사용하는 것입니다.

27
00:03:24,760 --> 00:03:30,680
특수 언어 모델. 그러나 일반적으로 이 접근 방식은 많은 양의 정보를 제공해야 합니다.

28
00:03:30,680 --> 00:03:37,320
도구 사용을 시연하거나 특정 도메인에 대해 제한된 양의 도구만 사용할 수 있습니다.

29
00:03:37,320 --> 00:03:46,440
언어 모델. 이제 분명히 이러한 유형의 도구 사용에는 높은 비용 문제가 발생합니다.

30
00:03:46,440 --> 00:03:52,920
대량의 인간 주석의 경우. 제한된 양의 도구만 사용한다면,

31
00:03:52,920 --> 00:03:59,800
언어 모델에는 일반성이 부족합니다. 그리고 이 경우 프롬프트는 특정 작업에 맞게 조정되어야 합니다.

32
00:03:59,800 --> 00:04:09,320
특정 도구. 이 문제를 해결하기 위해 본 논문의 저자는 ToolFormer의 방법을 개발했습니다.

33
00:04:10,440 --> 00:04:17,160
우선 ToolFormer는 외부 데이터베이스에 액세스할 수 있는 API를 사용할 수 있는 기능을 모델에 제공합니다.

34
00:04:17,960 --> 00:04:24,920
그러나 더 중요한 것은 ToolFormer가 언어 모델을 미세 조정하는 방법이라는 것입니다.

35
00:04:24,920 --> 00:04:32,440
자체적으로 특정 도구를 사용할지 여부와 사용 가능한 도구를 언제, 어떻게 사용할지 결정할 수 있습니다.

36
00:04:34,120 --> 00:04:41,560
이제 이는 대규모 언어 모델뿐만 아니라 대규모 언어 모델에도 도움이 됩니다.

37
00:04:41,560 --> 00:04:47,720
사용할 수 있는 도구가 더 많아졌지만 인간의 직관이 항상 정확하지는 않을 수도 있기 때문입니다.

38
00:04:48,440 --> 00:04:54,920
그리고 인간이 특정 시나리오에서 유용하다고 생각하는 도구는 사용자가 사용하는 도구가 아닐 수도 있습니다.

39
00:04:54,920 --> 00:05:02,440
모델이 유용하다고 생각합니다. 따라서 ToolFormer에서는 도구 사용법을 자기주도적으로 학습해야 합니다.

40
00:05:02,440 --> 00:05:08,440
무엇을 해야 할지 보여주는 대량의 인간 주석을 사용하는 방식입니다. ToolFormer의 접근 방식

41
00:05:08,440 --> 00:05:12,120
또한 모델의 핵심 언어 능력을 손상시키지 않아야 합니다.

42
00:05:12,520 --> 00:05:18,920
ToolFormer의 접근 방식을 위해 언어 모델 M에 다음을 사용할 수 있는 기능을 갖추려고 한다고 가정해 보겠습니다.

43
00:05:18,920 --> 00:05:28,120
API 비용과 다른 도구. 여기서 아이디어는 인간이 어떻게 행동하는지에 대한 소수의 예를 통해

44
00:05:28,120 --> 00:05:35,560
API를 사용할 수 있고 상황 내 학습을 사용하여 언어 모델 M을 일반화하고

45
00:05:36,200 --> 00:05:43,800
API 비용을 사용할 가능성이 있는 대규모 데이터 세트에 주석을 답니다. 그런 다음 우리는 어느 것을 결정할 수 있습니다

46
00:05:43,800 --> 00:05:49,560
이러한 API 비용 중 실제로는 모델이 미래 토큰을 예측하고 모델을 미세 조정하는 데 도움이 됩니다.

47
00:05:49,560 --> 00:06:00,040
유용하다고 판단되는 API 비용에 대해 따라서 이 특별한 경우에는 API 호출이 표시됩니다.

48
00:06:00,040 --> 00:06:10,440
쌍으로 C는 AC, IC입니다. AC는 API의 이름이고 IC는 API에 대한 입력입니다.

49
00:06:11,960 --> 00:06:17,240
이제 API 호출의 입력과 출력은 선형화된 텍스트 시퀀스로 표시됩니다.

50
00:06:18,120 --> 00:06:26,440
EC 또는 ECR. API 호출 생성기가 응답하는 경우 응답은 단일 텍스트 시퀀스여야 합니다.

51
00:06:27,400 --> 00:06:33,400
응답은 텍스트 시퀀스로 연결됩니다. 그리고 이 텍스트 시퀀스에서는

52
00:06:34,040 --> 00:06:39,320
API, NAPI 및 기호 매핑은 모두 특수 토큰일 뿐입니다.

53
00:06:43,240 --> 00:06:49,560
이제 일반 텍스트 데이터세트를 확장하기 위해 일반 텍스트 데이터세트 C가 주어졌다고 가정해 보겠습니다.

54
00:06:49,560 --> 00:06:56,920
API 비용으로 이를 C 스타로 늘리려고 하면 다음을 수행할 수 있습니다. 먼저 우리는 큰 것을 사용합니다.

55
00:06:56,920 --> 00:07:04,040
대량의 잠재적 API 비용을 샘플링하기 위한 언어 모델 M. 그런 다음 이 API를 실행합니다.

56
00:07:04,040 --> 00:07:09,320
비용을 절감하고 언어 모델이 미래 토큰을 예측하는 데 도움이 되지 않는 결과를 필터링합니다.

57
00:07:11,240 --> 00:07:16,760
마지막으로 다양한 유형의 도구에 남아 있는 API 비용을 병합합니다.

58
00:07:17,480 --> 00:07:28,440
다음은 필터링 프로세스의 예입니다. 그럼 데이터 세트의 예부터 시작한다고 가정해 보겠습니다.

59
00:07:29,080 --> 00:07:34,680
텍스트가 피츠버그인 곳은 스틸 시티(Still City)로 알려져 있습니다. 이는 데이터 세트의 원본 텍스트입니다.

60
00:07:35,240 --> 00:07:43,000
그리고 여기에서 언어 모델에 잠재적으로 도움이 될 수 있는 여러 API 비용을 샘플링하도록 요청합니다.

61
00:07:43,320 --> 00:07:51,240
센터를 생성하는 언어 모델. 여기 그들은 그들이 할 수 있는 많은 질문의 샘플을 보여줍니다.

62
00:07:51,960 --> 00:07:59,160
질문 및 답변 API에 사용합니다. 그런 다음 실제 답변을 얻기 위해 이러한 모든 API 비용을 실행합니다.

63
00:08:00,440 --> 00:08:08,520
그런 다음 plaintext라는 API와 관련된 손실 함수를 확인합니다. 그리고 우리는 일부를 사용합니다

64
00:08:08,520 --> 00:08:14,440
실제로 이 API 비용을 유지하고 싶은지 여부를 확인하는 자체 감독 기준

65
00:08:14,440 --> 00:08:21,240
언어 모델이 이 텍스트를 생성하는 데 도움이 됩니다. 그리고 남은 API 비용에 대해서는

66
00:08:21,240 --> 00:08:28,280
API 비용의 선형화된 텍스트 시퀀스를 원본 텍스트로 확대하는 방법을 살펴보겠습니다.

67
00:08:28,360 --> 00:08:30,120
따라서 이 예를 보강합니다.

68
00:08:35,000 --> 00:08:41,480
이제 구체적으로 말하자면, 각 API에 대해 언어 모델을 장려하는 프롬프트 PX를 작성합니다.

69
00:08:41,480 --> 00:08:47,560
API 비용으로 예시 X에 주석을 추가합니다. 오른쪽의 예는 그러한 프롬프트 중 하나입니다.

70
00:08:48,200 --> 00:08:57,000
처음에는 많은 예제 데모가 있습니다. 그런 다음 X를 연결하면 됩니다.

71
00:08:58,280 --> 00:09:08,520
프롬프트에 대한 입력으로. 이제 이를 고려하여 API 수행을 위한 K개의 후보자 위치를 샘플링하겠습니다.

72
00:09:08,520 --> 00:09:15,000
다음 토큰이 특수 API 토큰일 확률을 계산하여 비용을 계산합니다. 그렇기 때문에 우리는

73
00:09:15,000 --> 00:09:24,520
텍스트 예제의 특정 위치 I에 대해 이 확률 PI를 계산했습니다. 그리고 말하자

74
00:09:24,600 --> 00:09:32,680
샘플링 임계값 tau가 제공됩니다. 이는 양수입니다. 그리고 우리는 모든 위치를 유지

75
00:09:32,680 --> 00:09:38,520
API 토큰을 생성할 확률이 임계값보다 큰 I입니다.

76
00:09:39,160 --> 00:09:46,840
이것이 I가 API 비용에 대한 후보 위치 집합입니다. 그러므로 나는 후보자로서 모든 사람에 대해

77
00:09:46,840 --> 00:09:56,200
위치에 따라 최대 M API 비용까지 샘플링하고 C1 I부터 CMI까지 확인합니다. 이러한 API 비용은

78
00:09:56,200 --> 00:10:07,080
원본 언어 모델 M에 의해 생성되었습니다. 이제 모든 샘플 API 비용을 실행한 후,

79
00:10:07,080 --> 00:10:13,800
혜택을 제공하지 않거나 혜택보다 해로움이 더 많은 API 비용을 필터링하고 싶습니다.

80
00:10:14,760 --> 00:10:20,520
세대에. 이를 수행하는 방법은 Z의 자체 감독 손실 Li를 정의하는 것입니다.

81
00:10:21,560 --> 00:10:29,160
여기서 Z는 API 비용의 선형화된 순서입니다. 따라서 이 확률은

82
00:10:30,040 --> 00:10:39,480
J에서 1을 뺀 원래 접두사 X1이 주어지면 XJ를 생성할 확률, 그리고 잠재적인

83
00:10:39,480 --> 00:10:47,640
Z라는 API. 이제 일련의 가중치가 주어졌다고 가정해 보겠습니다. 이 Li of Z는 가중치를 계산합니다.

84
00:10:47,640 --> 00:10:57,880
M에 대한 교차 엔트로피 손실. 그리고 가중치 함수의 경우 이 논문의 저자는 이 가중치를 사용합니다.

85
00:10:57,880 --> 00:11:08,680
기능. 이는 조각별 선형 함수입니다. 그러나 더 중요한 것은 WT, 즉 WT의 질량입니다.

86
00:11:08,680 --> 00:11:16,040
T가 0에 가까울수록 더 집중됩니다. 그리고 T가 더 멀어지면 감소할 것입니다.

87
00:11:16,680 --> 00:11:22,760
실제로 T가 5일 때 이 함수는 0이 됩니다. 이렇게 하면 API 비용이 발생합니다.

88
00:11:22,760 --> 00:11:26,680
API 출력이 실제로 생성에 도움이 되는 위치에 가깝습니다.

89
00:11:32,120 --> 00:11:37,160
이제 API 비용을 필터링하기 위해 보유 손실의 가중 손실을 계산합니다.

90
00:11:37,160 --> 00:11:43,880
API 비용 결과가 접두사로 표시되지 않습니다. 우리가 계산하는 것 중 하나는

91
00:11:44,680 --> 00:11:52,200
L 플러스 i. 특정 샘플 API 비용을 포함하면 이는 손실일 뿐입니다.

92
00:11:53,640 --> 00:12:02,520
L 마이너스 i는 엡실론 손실과 의 손실의 최소값으로 계산됩니다.

93
00:12:03,480 --> 00:12:10,600
CI API 호출을 사용하지만 엡실론 결과를 반환합니다. 여기서 엡실론은 단지 빈 문자열입니다.

94
00:12:12,120 --> 00:12:20,600
따라서 이 두 가지는 샘플 API 비용을 포함할 때 또는

95
00:12:20,600 --> 00:12:25,560
샘플 API 비용은 포함되지 않습니다. 이제 F의 필터링 임계값 tau가 주어지면,

96
00:12:25,560 --> 00:12:29,320
우리는 최소한 한도만큼 손실을 줄이는 API 비용만 유지합니다.

97
00:12:33,000 --> 00:12:40,680
필터링 후 나머지 API 비용을 병합하여 원본과 인터리브합니다.

98
00:12:40,680 --> 00:12:50,760
입력. 따라서 오른쪽의 예를 통해 이를 확인할 수 있습니다. API 비용이 확인되면

99
00:12:50,760 --> 00:12:57,560
생성에 도움이 될 경우 원본 텍스트에 삽입하여 보강할 수 있습니다.

100
00:12:58,520 --> 00:13:05,960
그리고 이 방법을 사용하여 API 비용이 증가하는 데이터 세트 C 스타를 만들었습니다.

101
00:13:06,760 --> 00:13:12,360
마지막 단계는 증강 데이터 세트 C star를 사용하여 언어 모델 M을 미세 조정하는 것입니다.

102
00:13:13,240 --> 00:13:18,760
이렇게 하면 삽입된 추가 API 비용이 M이 미래 토큰을 예측하는 데 도움이 됩니다.

103
00:13:19,480 --> 00:13:26,920
이제 추론을 위해 기능할 맵을 만날 때마다

104
00:13:27,720 --> 00:13:35,400
API 비용의 일부 응답이 예상됨을 나타냅니다. 따라서 LLM은 생성을 일시 중지하고,

105
00:13:37,000 --> 00:13:42,680
응답에 대해 적절한 API 비용을 실행한 다음 해당 응답을

106
00:13:42,680 --> 00:13:46,520
그런 다음 생성된 응답으로 디코딩을 계속합니다.

107
00:13:48,680 --> 00:13:53,160
이것이 접근 방식이며 두 가지 전자의 구성을 말합니다.

108
00:13:55,640 --> 00:14:03,160
이제 이 논문의 저자는 효율성을 벤치마킹하기 위해 몇 가지 실험을 수행했습니다.

109
00:14:03,160 --> 00:14:08,920
이 접근 방식의. 이를 위해 그들은 다음과 같은 다섯 가지 도구를 탐색했습니다.

110
00:14:09,480 --> 00:14:15,880
언어 모델의 다양한 단점을 해결합니다. 첫 번째는 질문 답변 도구입니다.

111
00:14:17,080 --> 00:14:25,160
이는 검색 증강 생성인 보조 언어 모델인 Atlas에 의해 제공됩니다.

112
00:14:25,160 --> 00:14:31,800
간단한 사실 질문에 답하는 데 사용되는 모델입니다. 두 번째 도구는 Wikipedia입니다.

113
00:14:31,800 --> 00:14:46,040
찾다. 이것은 검색어가 주어지면 Wikipedia의 짧은 텍스트 조각을 반환하는 엔진입니다.

114
00:14:47,880 --> 00:14:52,520
그리고 매우 간단한 계산기는 간단한 산술 연산을 수행하는 것입니다.

115
00:14:53,480 --> 00:15:01,880
입력 없이 API를 쿼리하면 달력은 현재 날짜만 반환합니다.

116
00:15:03,080 --> 00:15:09,400
그리고 마지막으로 기계 번역 시스템도 보조 언어 모델로 제공됩니다.

117
00:15:10,120 --> 00:15:16,440
이를 LLB라고 합니다. 6억 개의 매개변수 언어 모델이며 번역이 가능합니다.

118
00:15:16,440 --> 00:15:20,120
200개 언어 중 하나에서 영어까지 다시 사용할 수 있습니다.

119
00:15:23,080 --> 00:15:31,400
실험 설정을 위해 CC net의 하위 집합을 대규모 언어 모델링 데이터 세트 C로 사용합니다.

120
00:15:32,280 --> 00:15:41,640
그리고 우리는 GPDJ를 언어 모델 M으로 사용합니다. 계산 비용을 전자 2개로 줄이기 위해,

121
00:15:41,640 --> 00:15:46,680
우리는 C의 하위 집합을 얻기 위해 일부 API에 대해 수동 경험적 방법을 사용합니다.

122
00:15:47,080 --> 00:15:55,000
예를 들어, 창에 세 개의 숫자가 있다고 가정하면 계산기 도구를 사용하겠습니다.

123
00:15:55,000 --> 00:16:00,280
토큰 100개. 그렇지 않으면 이 도구를 사용하지 못하게 됩니다.

124
00:16:02,280 --> 00:16:10,680
또한 모델이 추론 중에 API 도구를 더 자주 호출하도록 하기 위해 API가 호출될 때마다 API 호출을 시작합니다.

125
00:16:10,680 --> 00:16:17,000
토큰은 가장 가능성이 높은 토큰 중 하나입니다. 이제 우리가 비교할 기본 모델이 있습니다.

126
00:16:17,000 --> 00:16:22,840
먼저, 미세 조정 없이 오리지널 베이스 GPDJ 모델인 GPDJ입니다.

127
00:16:22,840 --> 00:16:32,200
GPDJ plus CC는 API 호출 없이 CC net의 하위 집합에서 미세 조정된 GPDJ입니다.

128
00:16:32,920 --> 00:16:38,920
GPDJ는 API 호출 없이 CC 네트의 하위 집합을 미세 조정했습니다.

129
00:16:40,120 --> 00:16:44,600
두 번째는 증강 데이터 세트 C star에서 미세 조정된 GPDJ입니다.

130
00:16:44,600 --> 00:16:49,160
그리고 마지막으로 두 명의 이전 장애인이 C 스타에 대해 미세 조정된 GPDJ입니다.

131
00:16:49,160 --> 00:16:53,880
그러나 추론 중에는 API 호출이 비활성화됩니다.

132
00:16:54,680 --> 00:16:59,000
이는 API 토큰의 확률을 0으로 수동으로 설정하여 수행됩니다.

133
00:17:02,440 --> 00:17:08,680
이제 벤치마크 실험을 위해 여러 다운스트림 작업을 사용하고

134
00:17:08,680 --> 00:17:15,720
제로샷 성능. 여기서 제로 샷 설정은 알려주고 싶지 않기 때문에 사용됩니다.

135
00:17:15,720 --> 00:17:21,000
언어 모델이 결정할 수 있도록 이벤트에서 사용할 도구를 대규모 언어 모델

136
00:17:21,000 --> 00:17:29,000
그 자체. 첫 번째 벤치마크 세트는 Lama입니다. 이는 원래 대량 언어 모델링에 사용되었습니다.

137
00:17:29,880 --> 00:17:37,320
그리고 데이터 세트 팀인 Google RE 및 REX에서 여기서 작업은 완료하는 것입니다.

138
00:17:37,320 --> 00:17:44,040
사실이 누락된 짧은 진술. 이제 여기 전 두 사람은 위키피디아를 사용할 수 없습니다.

139
00:17:44,040 --> 00:17:51,320
검색 API가 필요한 이유는 이러한 데이터 세트의 많은 내용이 Wikipedia에서 직접 얻은 것이기 때문입니다.

140
00:17:53,320 --> 00:17:57,560
그러나 이 경우에도 전자 두 개는 여전히 기본 모델보다 성능이 뛰어납니다.

141
00:17:57,560 --> 00:18:01,160
표와 밑줄 친 정확도에서 볼 수 있듯이.

142
00:18:05,000 --> 00:18:15,480
이제 수학 데이터 세트의 경우 ASDIV, SV, AMP 및 MAWPS를 사용합니다. 이것들은 모두 수학일 뿐이야

143
00:18:15,480 --> 00:18:24,120
단어 문제. 그리고 다시 한 번 말씀드리지만, 반드시 사용하지 않을 수도 있기 때문에 제로 샷 설정을 사용합니다.

144
00:18:24,120 --> 00:18:29,320
수학 단어 문제를 위한 계산기만 있습니다. 우리도 그럴 수 있습니다. 언어 모델은

145
00:18:29,320 --> 00:18:38,920
질문 응답 API를 사용하기로 결정했습니다. 이제 모델이 API를 호출할 수 있도록 허용합니다.

146
00:18:40,280 --> 00:18:45,720
모델이 API 호출을 할 수 있도록 허용하면 성능이 두 배 이상 향상됩니다. 이는 다음을 의미합니다.

147
00:18:45,720 --> 00:18:53,400
두 전자는 정확한 계산 및 추론과 관련된 이러한 문제 중 일부를 실제로 해결할 수 있습니다.

148
00:18:54,840 --> 00:19:02,760
그리고 우리가 살펴보는 다음 데이터 세트는 웹 질문, 자연 질문, 사소한 QA입니다.

149
00:19:03,400 --> 00:19:08,840
여기에는 상식적인 질문이 포함되어 있습니다. 여부를 확인하여 성과를 평가합니다.

150
00:19:08,840 --> 00:19:14,040
모델이 예측한 처음 20개 단어에는 정답이 포함됩니다. 그리고 여기에서는,

151
00:19:14,040 --> 00:19:22,200
이전 두 가지에 대해서는 질문 응답 API가 비활성화되었습니다. 다시 말하지만, 이 API를 사용하면

152
00:19:22,840 --> 00:19:26,840
매우 직접적인 접근 방식입니다. 그리고 우리는 언어 모델의

153
00:19:27,960 --> 00:19:32,920
2차 언어 모델 아틀라스의 능력이 아닌 AMP의 능력입니다.

154
00:19:36,200 --> 00:19:40,920
이것이 그 성능입니다. 보시다시피 약간의 증가도 있습니다

155
00:19:41,960 --> 00:19:49,720
세 가지 벤치마크 모두에서 전자 두 개가 포함되었습니다. 아, 좀 더 어려운 작업에 대해서는 평가해 보겠습니다.

156
00:19:49,720 --> 00:19:58,280
다국어 QA 데이터 세트. 이러한 데이터 세트는 스페인어, 독일어,

157
00:19:58,280 --> 00:20:07,160
힌디어, 베트남어, 중국어, 아랍어. 우리는 두 가지 이전 모델과 모든 기본 모델을 평가합니다.

158
00:20:07,160 --> 00:20:12,920
이 작업에 대해 모델 생성에 올바른 내용이 포함된 횟수의 비율을 평가하세요.

159
00:20:12,920 --> 00:20:20,280
답변. 이제 이 표에서는 흥미로운 현상이 발생합니다. API 호출을 사용하면 실제로 개선됩니다.

160
00:20:20,280 --> 00:20:26,840
전자 두 개는 성능이 우수하지만 전자 두 개는 기본 GPTJ 모델보다 일관되게 성능이 뛰어나지는 않습니다.

161
00:20:27,480 --> 00:20:35,880
그리고 주된 이유 또는 설명 중 하나는 CCNet의 하위 집합에 대한 미세 조정입니다.

162
00:20:36,760 --> 00:20:47,560
실제로 성능이 저하됩니다. 이제 다음과 같은 시간 데이터 세트를 살펴보겠습니다.

163
00:20:47,560 --> 00:20:54,120
tempLama 및 데이터 세트. 두 가지 모두 질문에 대한 답이 시간에 따라 변하는 데이터 세트입니다.

164
00:20:54,840 --> 00:21:00,680
좀 더 구체적으로 tempLama는 누가 우두머리인지 등 사실에 대한 질문을 담고 있습니다.

165
00:21:00,680 --> 00:21:04,600
뉴욕시 정부의. 반면, 요일 세트 데이터 세트는

166
00:21:05,400 --> 00:21:10,840
10일 전이 무슨 요일인지 등 날짜와 기간에 관한 질문이 포함되어 있습니다.

167
00:21:12,280 --> 00:21:19,160
흥미롭게도 두 데이터 세트 모두 이전 두 데이터 세트에서 성능이 향상되었지만 더 가까워지면

168
00:21:19,160 --> 00:21:26,840
평가 결과, tempLama의 성능 향상은 달력에 의한 것이 아니라 실제로는

169
00:21:26,920 --> 00:21:35,880
시간 정보가 어떻게 관련되어 있는지에 따라 Wikipedia 검색과 QA에 귀속됩니다.

170
00:21:35,880 --> 00:21:43,240
일반적인 세계에 관한 사실. 반면에 요일 설정이 크게 개선되었습니다.

171
00:21:43,800 --> 00:21:47,320
이는 달력 도구를 사용했기 때문입니다.

172
00:21:48,280 --> 00:21:57,800
우리는 또한 이전 두 가지를 미세 조정하면 실제로 언어가 저하되는지 확인하고 싶었습니다.

173
00:21:57,800 --> 00:22:07,000
원본, 언어 모델의 원래 언어 능력입니다. 그래서 실험이 있었습니다

174
00:22:07,000 --> 00:22:15,480
위키 텍스트와 다른 하위 집합에 대한 다양한 기본 모델의 복잡성을 비교하도록 만들어졌습니다.

175
00:22:15,480 --> 00:22:21,320
원래 미세 조정에 사용된 하위 집합과 다른 ccinet의 집합입니다.

176
00:22:22,440 --> 00:22:27,000
그래서 우리는 언어 모델의 성능이 저하되지 않도록 하고 싶습니다.

177
00:22:27,560 --> 00:22:32,840
표에서 볼 수 있듯이, 복잡성은 그다지 변하지 않습니다.

178
00:22:35,480 --> 00:22:40,840
마지막으로 성능이 크기에 따라 어떻게 확장되는지 테스트하는 실험이 있습니다.

179
00:22:40,920 --> 00:22:48,040
언어 모델. 따라서 이 실험에서는 동일한 실험 설정을 사용했습니다.

180
00:22:48,040 --> 00:22:55,560
GPT-2 제품군의 다양한 매개변수 모델. 그리고 이전과 이전의 두 곡선을 비교해 보면

181
00:22:55,560 --> 00:23:03,640
아래 세 다이어그램에서 이전 두 개가 비활성화되어 도구를 사용할 수 있는 기능만 나타나는 것을 알 수 있습니다.

182
00:23:03,640 --> 00:23:15,480
약 7억 7,500만 개의 매개변수 모델이 있습니다. 이것이 이전의 두 가지에 관한 것입니다. 다음은 다음에 대해 이야기하겠습니다.

183
00:23:15,480 --> 00:23:19,960
자동 추론과 사용을 의미하는 예술.

184
00:23:24,280 --> 00:23:30,680
예술의 동기를 위해 우리는 맥락 학습에서 대규모 언어 모델이 가능하다는 것을 확인했습니다.

185
00:23:30,680 --> 00:23:35,880
자연어 지침과 몇 가지 데모를 사용하여 새로운 작업에 빠르게 적응합니다.

186
00:23:37,240 --> 00:23:42,680
그러나 다단계 추론에는 심각한 성능 제한이 있습니다.

187
00:23:42,680 --> 00:23:45,480
수학적 문제 해결 및 기타 복잡한 작업.

188
00:23:46,920 --> 00:23:52,600
생각의 자동 칭과 같은 다양한 프롬프트 방법이 있지만 대부분의 경우

189
00:23:52,600 --> 00:23:57,560
우리가 본 것처럼 이는 다단계 추론과 관련된 문제를 완화할 수 있습니다. 이 아니라면

190
00:23:57,560 --> 00:24:04,360
이번 프레젠테이션에서는 도구 사용을 모델에 통합하려고 합니다. 그렇기 때문에 소개합니다

191
00:24:04,360 --> 00:24:11,640
art는 자동으로 생성하여 문제를 해결하려고 하는 프레임워크입니다.

192
00:24:12,440 --> 00:24:22,440
새로운 작업의 인스턴스에 대한 중간 추론 단계의 분해. 하면서 자동으로

193
00:24:22,440 --> 00:24:25,880
각 중간 단계에서 적절한 도구를 사용하십시오.

194
00:24:29,320 --> 00:24:34,760
아트 프레임워크에서 우리는 대규모 언어 모델에 방법을 보여주는 데모를 제공합니다.

195
00:24:34,760 --> 00:24:42,920
여러 관련 작업과 도구 라이브러리의 도구를 사용하는 방법을 분해합니다. 나중에 보자

196
00:24:42,920 --> 00:24:49,080
작업 및 관련 작업의 구성이 구조화된 쿼리 언어에 의해 제공된다는 것

197
00:24:49,720 --> 00:24:56,520
이를 사용하여 중간 단계를 구문 분석할 수 있습니다. 그러나 전반적으로 예술은 다음을 장려하는 프레임워크입니다.

198
00:24:56,520 --> 00:25:04,280
분해된 새로운 작업을 일반화하고, 도구를 선택하고 사용하는 모델입니다. 오른쪽에는

199
00:25:04,280 --> 00:25:11,320
우리는 그러한 과정의 예를 볼 수 있습니다. 그리고 무대가 끝나면 우리는 그것을 볼 수 있습니다

200
00:25:12,280 --> 00:25:18,200
추론 분해에 실수가 있는 경우 사용자가 이를 수정할 수도 있습니다.

201
00:25:18,200 --> 00:25:25,160
새 도구를 추가하거나 태스크 라이브러리를 업데이트하여 실수를 저지르십시오. 이에 대해서는 나중에 더 자세히 살펴보겠습니다.

202
00:25:30,600 --> 00:25:36,760
좋아요, 예술은 사실 사고의 사슬과 자동차라는 아이디어를 바탕으로 구축된 프레임워크입니다.

203
00:25:36,760 --> 00:25:45,000
생각의 사슬. 생각의 자동 연쇄를 모르면 생각의 연쇄와 같습니다. 하지만

204
00:25:45,000 --> 00:25:51,720
데모는 다양한 샘플링을 통해 대규모 언어 모델에 의해 자동으로 구성됩니다.

205
00:25:51,720 --> 00:25:56,280
질문을 던지고 각각의 추론 체인을 생성하여 예시로 사용합니다.

206
00:25:58,280 --> 00:26:04,440
예술은 교차 작업 시연이 가능하고 향상된다는 장점이 있습니다.

207
00:26:04,440 --> 00:26:11,240
중간 추론 단계의 정확성. 예술에는 추가 교육이 필요하지 않습니다.

208
00:26:11,240 --> 00:26:17,880
또는 두 개의 특정 프롬프트. 사용자는 기본 대규모 언어 모델을 교체하거나 새 도구를 추가할 수 있습니다.

209
00:26:17,880 --> 00:26:25,960
유용하다고 판단되면. 오른쪽에는 각 도구 또는 기능의 종류에 대한 표가 있습니다.

210
00:26:25,960 --> 00:26:37,880
접근 방식을 사용합니다. 보시다시피 COT에는 다단계 추론 기능이 있습니다. 그리고 자동 COT

211
00:26:37,880 --> 00:26:43,480
데모를 자동으로 생성하므로 감독이 덜 필요합니다. 두 전

212
00:26:43,480 --> 00:26:48,120
6가지 기능을 모두 갖춘 도구 및 예술 주장을 사용할 수 있습니다.

213
00:26:54,760 --> 00:27:00,760
이제 우리는 이 프레임워크의 핵심 개요를 살펴보겠습니다. 예술은 냉동을 사용합니다

214
00:27:00,760 --> 00:27:06,200
감독 없이 새로운 작업의 인스턴스를 여러 단계로 분해하는 대규모 언어 모델입니다.

215
00:27:07,000 --> 00:27:14,760
첫 번째 단계에서는 신속한 구축을 수행합니다. 따라서 신속한 구축에서 예술은 유사한 작업을 검색합니다.

216
00:27:14,760 --> 00:27:19,960
작업 라이브러리에서 해당 작업의 인스턴스를 프롬프트에 데모로 추가합니다.

217
00:27:22,440 --> 00:27:29,160
태스크 라이브러리의 데모는 사용자 정의 형식으로 작성됩니다. 이 사용자 정의 형식을

218
00:27:29,160 --> 00:27:36,520
문법 표현을 파싱합니다. 이 문법은 모든 작업 인스턴스를 일련의 작업으로 분류하는 데 도움이 됩니다.

219
00:27:36,520 --> 00:27:42,760
하위 단계. 이러한 하위 단계 중 일부에는 도구 사용에 해당하는 기호가 포함되어 있습니다.

220
00:27:44,120 --> 00:27:51,080
본 논문에서 저자는 작곡을 프로그램이라고도 부른다. 이 때문입니다

221
00:27:51,960 --> 00:27:58,440
순차적 추론 단계와 기호 호출은 기존 프로그램과 유사합니다.

222
00:27:58,520 --> 00:28:08,440
기능 비용. 다음 단계는 세대이다. 세대 시간 동안 큰 언어는

223
00:28:08,440 --> 00:28:16,360
모델은 자체 프로그램을 작성합니다. 그리고 예술은 도구 호출이 발생하면 이 세대를 일시 중지합니다.

224
00:28:16,360 --> 00:28:21,640
생성된 텍스트에서 도구를 호출하고 도구 출력을 다시 프로그램에 통합합니다.

225
00:28:21,640 --> 00:28:28,840
LLM은 코딩을 계속합니다. 마지막 선택 단계는 사람의 피드백을 위한 것입니다.

226
00:28:29,480 --> 00:28:36,920
인간은 추론 단계에서 실수나 오류를 발견하고 이를 추가할 수 있습니다.

227
00:28:36,920 --> 00:28:42,840
작업 라이브러리에 구성 데모를 표시하거나 도구 라이브러리에서 도구를 편집하여

228
00:28:42,840 --> 00:28:46,760
문제를 수정하고 관심 있는 특정 작업의 성능을 향상시킵니다.

229
00:28:51,240 --> 00:28:54,520
다음은 이전 개요에서 이야기한 모든 내용의 예입니다.

230
00:28:55,800 --> 00:29:01,640
왼쪽에는 태스크 라이브러리의 태스크에 대한 몇 가지 예가 표시됩니다.

231
00:29:02,760 --> 00:29:11,640
이러한 예 또는 이러한 작업은 잠재적인 교차 작업 일반화를 위해 LLM에 사용됩니다.

232
00:29:11,640 --> 00:29:21,400
작곡의. 이 예는 산술 문제이며 다음을 보여줍니다.

233
00:29:22,360 --> 00:29:29,000
이와 같은 질문의 계산을 해결하기 위해 분해를 생성하는 방법. 바닥

234
00:29:29,000 --> 00:29:41,560
예는 아나크로미즘의 과제입니다. 사물이나 사건을 확인한다는 뜻이다.

235
00:29:41,560 --> 00:29:48,600
각자의 시대에 일어난다. 오른쪽에는 작업의 예가 있습니다.

236
00:29:48,600 --> 00:29:56,680
물리학 질문 답변 데이터 세트. 고전적인 기계공학에 대한 질문을 하고 여기서 LLM을 묻습니다.

237
00:29:56,680 --> 00:30:03,560
먼저 검색 엔진을 사용하여 적절한 물리적 공식을 찾아 질문을 분해하고,

238
00:30:03,560 --> 00:30:10,840
그런 다음 코드 생성 도구에 이 수식을 실행하기 위한 코드를 생성하도록 요청합니다.

239
00:30:10,840 --> 00:30:16,120
마지막으로 올바른 매개변수를 대체하고 질문에 답하는 Python 프로그램입니다.

240
00:30:16,120 --> 00:30:25,640
결국. 예술 작업 라이브러리에는 Big의 작은 작업 세트에 대한 프로그램이 있습니다.

241
00:30:25,640 --> 00:30:32,440
벤치. Big Bench는 언어 모델의 기능과 한계를 측정하는 벤치마크입니다.

242
00:30:32,440 --> 00:30:38,600
전통적인 자연어 처리 문제, 수학,

243
00:30:38,680 --> 00:30:45,400
상식 추론, 질문 답변. 이러한 벤치마크를 평가하기 위해,

244
00:30:46,040 --> 00:30:52,120
우리는 이 벤치마크에서 가장 많이 사용되는 5가지 기술을 그룹화했습니다. 첫 번째는 산술이고,

245
00:30:52,120 --> 00:31:01,080
이는 단순히 대수 문제, 코드 생성 및 실행 기술을 해결하는 것입니다.

246
00:31:01,080 --> 00:31:08,200
주로 Python 프로그램입니다. 다음 기술은 검색 및 질문 분해입니다.

247
00:31:09,240 --> 00:31:14,840
그런 다음 자연어로 추론을 단계별로 설명하는 자유 형식 추론이 있습니다.

248
00:31:15,480 --> 00:31:22,280
마지막으로 문자열 작업, 즉 문자열 형식을 다시 지정하고 편집하는 기술 등이 있습니다.

249
00:31:24,040 --> 00:31:30,120
작업 라이브러리에서는 모든 그룹에서 2~4개의 작업을 선택합니다. 총 15개의 과제가 있었습니다

250
00:31:30,120 --> 00:31:36,920
이 다섯 그룹이 선택했습니다. 예를 들어 각 작업의 몇 가지 예를 위한 프로그램을 작성합니다.

251
00:31:40,440 --> 00:31:44,040
프로그램에는 외부 도구에 대한 잠재적인 원인이 포함되어 있습니다.

252
00:31:48,440 --> 00:31:54,120
이제 표현식 문법을 구문 분석하는 방법에 대해 이야기합니다. 이는 표현하는 데 사용되는 사용자 정의 쿼리 언어입니다.

253
00:31:54,760 --> 00:32:00,200
분해된 추론 단계와 외부 도구에 대한 함수 호출을 통합하려고 시도합니다.

254
00:32:02,040 --> 00:32:08,280
각 프로그램은 일련의 노드로 구성됩니다. 첫 번째 노드는 항상 입력 노드입니다.

255
00:32:08,280 --> 00:32:13,400
입력 노드에는 작업 이름, 작업을 설명하는 명령,

256
00:32:13,400 --> 00:32:20,440
작업의 특정 인스턴스에 대한 입력입니다. 그런 다음 입력 노드 뒤에는 여러 개의 노드가 옵니다.

257
00:32:20,440 --> 00:32:26,520
하위 단계 노드. 하위 단계 노드는 모두 다음과 같은 쿼리 응답 쌍으로 레이블이 지정됩니다.

258
00:32:27,960 --> 00:32:33,240
쿼리 응답 쌍에서 쿼리 항목에는 우선 작업 이름,

259
00:32:33,880 --> 00:32:43,480
그런 다음 작업에 대한 입력이 이어졌습니다. 답변 항목인 AI에는 다음 중 하나가 포함됩니다.

260
00:32:43,480 --> 00:32:49,240
작업에 의해 생성된 출력 또는 외부 도구를 사용한 결과입니다.

261
00:32:52,040 --> 00:32:58,520
그런 다음 프로그램은 더미 하위 작업으로 끝납니다. 여기서 쿼리는 쿼리의 특수 기호 끝입니다.

262
00:32:59,160 --> 00:33:02,200
답변 필드에는 최종 답변이 포함됩니다.

263
00:33:04,920 --> 00:33:09,000
작업 검색을 위해 새로운 작업이 주어졌다고 가정해 보겠습니다.

264
00:33:09,960 --> 00:33:14,760
그런 다음 Art는 작업 라이브러리에서 여러 작업을 검색합니다.

265
00:33:14,760 --> 00:33:19,800
데모에 사용된 동적 멀티태스킹 프롬프트를 구성합니다.

266
00:33:21,240 --> 00:33:27,560
이제 여기에서 논문은 유사한 작업을 검색하는 방법에 대한 두 가지 전략을 탐색했습니다.

267
00:33:28,680 --> 00:33:34,600
첫 번째 전략은 작업 클러스터 기반입니다. 이 방법은 라벨이 많이 붙은 경우에 작동합니다.

268
00:33:34,600 --> 00:33:41,320
새로운 작업에 대한 예. 따라서 수행하는 작업은 5개 그룹 모두에 대해 반복하고 필드를 선택하는 것입니다.

269
00:33:41,320 --> 00:33:47,960
각 그룹의 작업 프로그램을 사용하여 프롬프트를 구성합니다. 그런 다음 가장 높은 작업 클러스터

270
00:33:47,960 --> 00:33:57,720
레이블이 지정된 예의 성능이 선택됩니다. 두 번째 접근 방식은 LLM 유사성 기반입니다.

271
00:33:58,520 --> 00:34:05,480
이 접근 방식은 조금 더 복잡합니다. 그것이 하는 일은 현장 촬영 프롬프트를 만드는 것입니다.

272
00:34:05,480 --> 00:34:13,320
각 기술 그룹 간의 작업 쌍이 있습니다. 각 작업에는 이름, 지침,

273
00:34:13,320 --> 00:34:22,040
그리고 몇 가지 IO 예시입니다. 그런 다음 모든 쌍에 대해 LLM의 접촉 학습 기능을 사용하여

274
00:34:22,040 --> 00:34:31,160
유사하거나 유사하지 않은 라벨을 제공합니다. 이제 런타임에 우리가 할 일은

275
00:34:31,160 --> 00:34:38,280
테스트 작업을 수행하고 이를 작업 라이브러리의 모든 작업과 연결합니다. 그러면 인택트를 활용해서

276
00:34:38,280 --> 00:34:46,200
원래 LLM의 학습 능력을 고려하여 LLM이 유사한 라벨을 적용하고 그렇지 않은 라벨도 적용하기를 원합니다.

277
00:34:46,200 --> 00:34:54,280
이러한 새로운 작업 쌍과 유사합니다. 그럼 확률에 따라 순위를 매겨보겠습니다.

278
00:34:54,280 --> 00:35:06,520
비슷한 라벨을 선택하고 새 프롬프트 내에서 사용하려면 가장 높은 순위의 작업을 선택하세요.

279
00:35:06,520 --> 00:35:20,200
작업. R2 라이브러리에서는 구문 분석 표현식 문법에서 작업 이름이

280
00:35:20,200 --> 00:35:28,600
도구 이름과 일치하면 이 외부 도구의 결과를 이 중간 단계에 통합합니다.

281
00:35:28,600 --> 00:35:34,920
단계. 예를 들어, 이 문서에서는 두 라이브러리에 다음 예제 도구가 포함되어 있습니다.

282
00:35:36,520 --> 00:35:43,320
첫 번째는 검색 도구입니다. 이 도구는 SERP API를 사용하며 다음에 대한 API를 제공합니다.

283
00:35:43,320 --> 00:35:51,800
구글 검색. 해당 쿼리 항목에는 검색을 통해 제공된 작업 이름이 있고

284
00:35:51,800 --> 00:36:00,520
인수는 검색어입니다. 코드 생성 도구에서는 코덱스 모델을 사용합니다. 그만큼

285
00:36:00,520 --> 00:36:08,360
해당 항목은 작업 이름이고 Python 코드 생성이고 인수는 생성입니다.

286
00:36:08,360 --> 00:36:18,360
자연어로 작성된 지시. 세 번째 도구는 Python 코드를 실행하는 코드 실행입니다.

287
00:36:18,360 --> 00:36:24,680
가상 환경. 이 가상 환경은 산술 기호 및 과학 기능을 갖습니다.

288
00:36:24,680 --> 00:36:34,840
컴퓨팅 패키지가 이미 사전 설치되어 있습니다. 쿼리는 작업 이름으로 코드를 실행한 다음

289
00:36:34,840 --> 00:36:42,200
코드 본문을 인수로 사용하거나 대부분의 경우 코드 본문이 답일 뿐입니다.

290
00:36:43,080 --> 00:36:46,280
코드 생성 도구의 결과일 뿐입니다.

291
00:36:47,000 --> 00:36:56,520
마지막으로 인간이 피드백을 제공하는 것이 선택 사항인 단계에 대해 이야기합니다.

292
00:36:57,160 --> 00:37:03,400
추론의 흔적과 예술의 결과물에. 글쎄요, 우선 예술은 필요하지 않기 때문에

293
00:37:03,400 --> 00:37:10,760
추가적인 미세 조정을 통해 사용자는 작업이나 작업을 편집하여 프레임워크에 피드백을 통합할 수 있습니다.

294
00:37:10,760 --> 00:37:19,240
도구 라이브러리. 여기 다이어그램은 사용자가 특정 프로그램을 수정하는 예를 보여줍니다.

295
00:37:19,240 --> 00:37:26,920
추가 단계를 포함시킨 다음 이 프로그램을 태스크 라이브러리에 추가합니다. 파트 A에서는 사용자

296
00:37:27,880 --> 00:37:34,360
두 개의 하위 단계를 추가하여 프로그램을 편집합니다. 첫 번째 단계는 답을 가장 가까운 값으로 반올림하는 것입니다.

297
00:37:34,360 --> 00:37:43,320
정수를 선택하고 답에 측정 단위를 추가합니다. 이는 다음에 대한 답변을 수정하거나 개선합니다.

298
00:37:43,320 --> 00:37:52,120
질문. 파트 B에서는 사용자가 사전에서 일반적인 단어를 검색하는 도구를 구현합니다.

299
00:37:55,160 --> 00:38:01,240
그리고 여기 이 예에서는 사용자가 작업 구성을 방해합니다.

300
00:38:02,200 --> 00:38:08,520
그러나 모든 단계에서 수행된 생성에서 수행된 생성은 여전히 ​​에 의해 수행됩니다.

301
00:38:08,520 --> 00:38:14,760
대규모 언어 모델. 이 예에서는 인간의 피드백이 디버깅의 한 형태로 간주됩니다.

302
00:38:15,560 --> 00:38:23,160
여기서 사용자는 프로그램을 생성하는 대신 프로그램을 편집합니다. 또한 인간의 전문지식이 있기 때문에

303
00:38:23,160 --> 00:38:30,360
이러한 간단한 작업을 사용하면 대상 작업의 성능을 매우 쉽게 향상시킬 수 있습니다.

304
00:38:31,480 --> 00:38:39,640
이제 예술의 성능을 테스트하기 위해 여기에 실험 설정이 있습니다.

305
00:38:40,680 --> 00:38:46,760
이 경우 작업 라이브러리의 15개 작업 외에도 아트도 평가합니다.

306
00:38:46,760 --> 00:38:55,000
빅벤치 벤치마크의 19개 추가 작업에 대해 알아보세요. 우리는 또한 MMLU의 예술을 평가합니다.

307
00:38:55,000 --> 00:39:04,200
기준. 여기서 MMLU는 대규모 멀티태스킹 언어 이해를 의미합니다. 그리고 우리는 추가로 평가합니다.

308
00:39:05,800 --> 00:39:12,520
이전 두 가지 작업에서 사용된 작업의 하위 집합입니다. 이는 도구를 사용하여 접근 방식을 비교하는 데 도움이 됩니다.

309
00:39:12,520 --> 00:39:19,240
미세 조정과 접촉 학습을 통해. LLMU의 첫 번째는 구조체 GPT를 사용하고

310
00:39:19,240 --> 00:39:27,400
열병합 발전 도구는 OpenAI 코덱입니다. 여기 예제에서는 두 가지 데모 프로그램을 사용합니다.

311
00:39:27,400 --> 00:39:31,800
각 작업에서 5회 실행에 대한 성능 평균을 보고합니다.

312
00:39:35,080 --> 00:39:38,280
여기서는 이 실험에 대한 몇 가지 기본 접근 방식도 소개합니다.

313
00:39:39,560 --> 00:39:46,280
몇 번의 샷 기준선에서는 중간 없이 입력 출력 쌍으로 LLM에 메시지를 표시합니다.

314
00:39:46,280 --> 00:39:51,880
단계. 그리고 어떤 벤치마크가 사용되는지에 따라 3~5개의 예를 사용하겠습니다.

315
00:39:53,160 --> 00:39:58,920
생각의 자동 연쇄도 비교되는데, 자동으로 생성되는 생각의 자동 연쇄

316
00:39:58,920 --> 00:40:04,520
자연어의 다단계 추론. 이 기준에는 사용자 도구가 포함되지 않습니다.

317
00:40:05,880 --> 00:40:14,040
다음은 미술 도구입니다. 이것은 예술이지만 도구 라이브러리가 꺼져 있습니다. 그럼 예술 마이너스가 되어야지

318
00:40:14,040 --> 00:40:23,320
여기 두 개. 그리고 마지막으로 가장 잘 게시된 GPT-3 및 코덱의 결과를 보여주는 GPT-3 best입니다.

319
00:40:23,320 --> 00:40:29,800
다단계 분해 및 도구 사용. 여기에 구성은 몇 가지에 의해 제공됩니다

320
00:40:29,800 --> 00:40:41,560
추가적인 인간 감독. 예술과 모든 기본 모델에 대한 결과는 다음과 같습니다.

321
00:40:41,560 --> 00:40:49,480
작업 라이브러리에 있습니다. 오른쪽에서 볼 수 있듯이 다섯 가지 색상으로 구분된 그룹은

322
00:40:50,280 --> 00:40:55,480
5가지 기술과 각 기술 내의 과제를 나타냅니다.

323
00:40:56,600 --> 00:41:02,600
보시다시피 아트는 태스크 라이브러리를 사용하기 때문에 자연스럽게 잘 수행되어야 합니다.

324
00:41:02,600 --> 00:41:12,920
그리고 이 표에서 볼 수 있듯이 그들은 그렇게 합니다. 그리고 아트 행이 있는 이러한 델타는 실제로

325
00:41:14,120 --> 00:41:21,640
다른 기본 모델과 비교하여 아트를 사용할 때 정확도가 일부 향상되었음을 보여줍니다.

326
00:41:22,120 --> 00:41:34,360
다음은 빅 벤치 작업에 대한 결과입니다. 그리고 이것들은 유사한 작업 그룹입니다.

327
00:41:34,920 --> 00:41:47,000
5가지 스킬 카테고리로 나뉜다. MMLU 작업의 결과는 다음과 같습니다. 그리고 이번에는 과제

328
00:41:47,000 --> 00:41:53,080
두 가지 기술 그룹만 포함되어 있지만 여전히 성능이 매우 간단하게 향상됩니다.

329
00:41:55,880 --> 00:42:01,880
여기서는 추가 벤치마크에 대한 성능도 평가합니다. 우선, 여기에는

330
00:42:03,240 --> 00:42:08,520
이전 두 사람과 비교하여 예술의 성과. 대부분의 벤치마크에서 볼 수 있듯이

331
00:42:09,480 --> 00:42:17,160
예술의 정확도가 향상되었습니다. 여기에서의 비교는 실제로 정확하지는 않지만

332
00:42:17,160 --> 00:42:24,440
두 전자는 더 작은 GBTJ 모델을 사용하고 여기서는 더 성능이 뛰어난 고정 LN을 사용합니다.

333
00:42:26,440 --> 00:42:33,080
우리는 또한 자기 일관성을 가지고 예술을 향상시키기 위해 노력합니다. 여기서 자기 일관성

334
00:42:33,720 --> 00:42:39,640
15세대에 걸친 다수결이다. 보시다시피 너무 많지는 않지만

335
00:42:40,520 --> 00:42:49,560
자기 일관성은 이러한 모든 잠재적인 작업에 비해 예술의 성능을 약간 향상시킵니다.

336
00:42:54,600 --> 00:42:59,160
마지막으로 저자는 두 가지 모두에 인간 피드백을 추가하는 방법도 검토했습니다.

337
00:42:59,880 --> 00:43:06,040
사고방식 추론과 예술 추론의 사슬. 두 경우 모두 프로그램을 편집하면

338
00:43:06,040 --> 00:43:13,880
인간의 결단력이 있기 때문에 작업 성능이 크게 향상될 것으로 예상됩니다.

339
00:43:15,320 --> 00:43:22,280
오른쪽에는 수정된 오류의 몇 가지 예가 나와 있습니다. 여기서 제 생각에는 다음 중 하나가 될 것 같습니다.

340
00:43:22,280 --> 00:43:29,480
저자는 오류를 발생시키는 모델 생성기 프로그램의 5개 인스턴스를 무작위로 선택했습니다.

341
00:43:29,480 --> 00:43:38,680
그런 다음 각각 수정합니다. 여기서 C는 하위 단계의 올바른 오류일 뿐입니다. 수단

342
00:43:38,680 --> 00:43:46,280
누락된 하위 단계를 추가하면 T는 피드백으로 새 도구나 새 데모로 정의됩니다.

343
00:43:46,920 --> 00:43:57,400
자, 이것으로 아트페이퍼를 마칩니다. 그리고 마지막으로, 다음 논문에서는 조금 우회하겠습니다.

344
00:43:57,400 --> 00:44:04,200
대규모 언어 모델을 에이전트로 간주합니다. 그리고 우리는 이 에이전트가 다음을 요구한다는 것을 알게 될 것입니다.

345
00:44:04,200 --> 00:44:13,800
두 가지를 능력으로 사용합니다. LLM은 자연어 작업 이상의 일을 할 수 있기 때문에

346
00:44:13,800 --> 00:44:19,000
그들은 또한 결정을 내리고 지시를 실행할 수도 있습니다.

347
00:44:19,640 --> 00:44:24,920
따라서 대화형 환경에서 LLM을 에이전트로 평가하는 것이 시급히 필요했습니다.

348
00:44:26,440 --> 00:44:35,160
LLM에 대한 대부분의 에이전트 벤치마크는 단일 환경에만 중점을 둡니다. 그리고 거기에는

349
00:44:35,160 --> 00:44:42,200
에이전트로서 LLM의 능력을 반영하는 체계적이고 표준적인 벤치마크가 부족함

350
00:44:42,200 --> 00:44:50,040
대부분의 실제 사용 사례에서. 따라서 본 논문의 저자는 에이전트 벤치를 제안했습니다.

351
00:44:50,760 --> 00:44:57,640
8개의 서로 다른 환경에서 LLM을 에이전트로 평가하는 포괄적인 벤치마크입니다.

352
00:45:02,600 --> 00:45:08,360
많은 개선된 전략이 제안되었지만 본 논문에서는 LLM을 가장 많이 평가합니다.

353
00:45:08,360 --> 00:45:15,560
원시적인 사고 사슬을 자극합니다. 생각의 연쇄가 가장 쉽기 때문에 가장 저렴하고,

354
00:45:15,560 --> 00:45:22,760
이는 또한 사람들이 LLM을 에이전트로 배포하는 가장 일반적인 방법이기도 합니다. 이 논문은 또한 제공

355
00:45:22,760 --> 00:45:31,960
모델 평가를 위한 벤치마크의 사용자 정의를 가능하게 하는 통합 툴킷입니다.

356
00:45:31,960 --> 00:45:35,960
이 툴킷은 현재 연구 커뮤니티에서 공개적으로 사용할 수 있습니다.

357
00:45:36,840 --> 00:45:46,840
이제 에이전트 벤치 내의 8가지 서로 다른 환경을 살펴보기 위해 다음 사항에 주목하겠습니다.

358
00:45:46,840 --> 00:45:51,640
환경은 코드 게임과 웹이라는 세 가지 유형의 접지로 분류됩니다.

359
00:45:53,640 --> 00:45:59,800
먼저 코드 기반 환경을 살펴보겠습니다. 이제 LLM이 생성할 수 있으므로

360
00:45:59,800 --> 00:46:05,480
고품질 코드를 사용하여 인간과 컴퓨터의 상호 작용을 지원하는 능력을 평가하고 싶습니다.

361
00:46:05,480 --> 00:46:14,360
상호 작용. 이러한 벤치마크 중 첫 번째는 운영 체제 벤치마크입니다. 이제 우리에게 주어진 것은

362
00:46:15,080 --> 00:46:20,600
LLM은 가상 머신 터미널에서 운영 체제에 액세스하고 조작하는 기능입니다.

363
00:46:21,400 --> 00:46:28,200
LLM에는 특정 파일 내의 총 파일 수를 찾는 것과 같은 작업이 제공됩니다.

364
00:46:28,200 --> 00:46:36,040
예배 규칙서. 그런 다음 LLM은 텍스트 기반 지침을 생성합니다. 이 특별한 경우에는

365
00:46:36,040 --> 00:46:41,640
시스템과 상호 작용하는 데 유효한 배치 명령이 될 것입니다. 그리고 마지막 관찰은

366
00:46:41,640 --> 00:46:50,360
시스템 표준 출력이 됩니다. 따라서 이 작업에서는 성공률을 성능 지표로 사용합니다.

367
00:46:51,320 --> 00:46:59,640
우리는 또한 LLM의 운영 능력을 검사하고

368
00:46:59,640 --> 00:47:06,680
SQL을 통해 실제 데이터베이스를 검사합니다. 예를 들어 여기에서는 LLM에게 작업이 주어집니다.

369
00:47:06,680 --> 00:47:13,400
예를 들어, 테이블이 주어졌을 때 미국이 획득한 금속의 총 개수는 얼마입니까?

370
00:47:13,400 --> 00:47:21,720
올림픽 메달 집계. 그런 다음 LLM은 최종 결과를 얻기 위해 SQL 명령을 생성합니다.

371
00:47:22,920 --> 00:47:30,440
지식 그래프 환경도 있었습니다. 여기서 문제는 에이전트에게 다음을 요구하는 것입니다.

372
00:47:30,440 --> 00:47:38,120
불완전한 정보로 의사결정을 내리고 불확실성을 관리합니다. LLM은 오직

373
00:47:38,120 --> 00:47:46,200
이 그래프의 부분적인 하위 집합에서 작동합니다. 따라서 언어 모델의 능력을 테스트합니다.

374
00:47:46,200 --> 00:47:53,480
지식 그래프 인터페이스와의 상호 작용 등 계획 및 사용을 위해. 지식을 위해

375
00:47:53,480 --> 00:48:00,760
그래프 추론에서는 F1 제곱 정리를 채택하겠습니다. 다음으로 살펴볼 접지는

376
00:48:00,760 --> 00:48:07,480
게임 기반 환경. 여기서는 에이전트의 전략 설계 능력을 평가하겠습니다.

377
00:48:07,480 --> 00:48:15,560
게임을 하면서 합리적인 선택을 하고 지시를 따르세요. 여기 네 번째 환경이 있습니다.

378
00:48:15,560 --> 00:48:22,840
디지털 카드 게임. 디지털 카드 게임에는 많은 카드가 있고

379
00:48:22,840 --> 00:48:30,200
카드에 대한 풍부한 텍스트 설명. 따라서 모델은 이러한 맥락을 활용해야 하며 우리는 평가할 것입니다.

380
00:48:30,200 --> 00:48:39,400
게임 규칙을 이해하고, 전략을 수립하는 능력을 운영하고 평가하는 모델의 능력

381
00:48:39,400 --> 00:48:49,720
결정. 다음 게임 환경은 측면적 사고 퍼즐입니다. 이 작업은 LLM을 평가합니다.

382
00:48:49,720 --> 00:48:57,320
관습에 얽매이지 않는 관점에서 사실을 추론하고 새로운 아이디어를 탐구하는 능력. 그래서 특히,

383
00:48:57,880 --> 00:49:03,800
측면적 사고 퍼즐에는 진행자가 장면에 대해 모호하게 설명하고,

384
00:49:04,440 --> 00:49:10,840
예를 들어, 불이 꺼진 채 미끄러졌다가 다음날 아침 창문을 열고 자살한 남자 등이 있습니다.

385
00:49:10,840 --> 00:49:20,760
왜 그런데? 그런 다음 LLM 에이전트는 다음과 같은 이진 답변으로 질문을 시도합니다.

386
00:49:20,760 --> 00:49:30,520
죄책감으로 자살한 걸까요, 아니면 그가 들어간 방의 창문이 열린 걸까요? 그럼 호스트는 그럼

387
00:49:30,520 --> 00:49:37,880
예, 아니오 또는 관련이 없다고 대답하고 이 정보를 사용하여 LLM은

388
00:49:37,880 --> 00:49:46,440
이 모호한 설명 아래의 실제 플롯입니다. 이제 성능을 평가하기 위해 실제 플롯을 나눕니다.

389
00:49:46,440 --> 00:49:52,680
몇 가지 주요 항목으로 구분되며 LLM의 성능은 게임 진행에 따라 평가됩니다.

390
00:49:54,680 --> 00:50:01,320
다음 환경은 집안일이다. 이 벤치마크는 ALF 세계에서 이루어집니다. 이것은 대화식입니다

391
00:50:01,320 --> 00:50:06,600
가정 내에서 발생하는 환경에 대한 설명이며 완전히 텍스트 기반 지침입니다.

392
00:50:07,800 --> 00:50:14,920
예를 들어, 우리는 LLM에게 비누를 청소하고 비누를 욕조 위에 놓는 것과 같은 작업을 수행하도록 요청할 것입니다.

393
00:50:14,920 --> 00:50:24,520
수조. 그런 다음 LLM은 상담원이 탐색할 수 있도록 안내하는 일련의 텍스트 지침을 생성합니다.

394
00:50:24,520 --> 00:50:34,840
가구를 관리하고 다음 작업을 수행합니다. 이번 공연에서는 그 성능이 평가된다.

395
00:50:34,840 --> 00:50:46,360
성공률로. 다음으로 최종 접지는 우리가 평가하는 웹 기반 환경입니다.

396
00:50:46,360 --> 00:50:53,720
웹 페이지를 탐색하는 LLM의 능력. 이 카테고리의 작업은

397
00:50:53,720 --> 00:51:02,360
GUI 웹 페이지이지만 LLM은 텍스트 기반이므로 HTML 요소가 제공됩니다. 그래서 사용 가능한

398
00:51:02,360 --> 00:51:09,480
이러한 작업에서 LLM의 작업은 웹 페이지의 HTML 요소 중 하나를 선택하는 것입니다.

399
00:51:09,480 --> 00:51:18,440
단어를 클릭하거나 입력하고, 답변을 검색하고, 특정 옵션을 선택하세요. 여기서 두 가지 작업은 웹 쇼핑입니다.

400
00:51:19,400 --> 00:51:24,680
특정 설명이 포함된 특정 제품을 찾는 작업입니다.

401
00:51:25,560 --> 00:51:32,680
목표는 이 제품이 포함된 웹페이지로 이동하는 것입니다. 그리고 마지막으로 웹 브라우징 작업입니다.

402
00:51:33,720 --> 00:51:37,560
특정 목표를 가지고 웹페이지를 탐색하는 것입니다.

403
00:51:42,200 --> 00:51:50,440
이제 이러한 모든 벤치마크를 테스트하기 위해 저자는 27개의 API 기반 LLM 및 오픈 소스를 평가합니다.

404
00:51:50,440 --> 00:51:57,160
이러한 벤치마크를 갖춘 LLM. LLM과 환경 간의 상호 작용 기록은 다음과 같습니다.

405
00:51:57,160 --> 00:52:08,680
프롬프트로 제공됩니다. 다음은 모든 벤치마크와 모든 LLM에서 얻은 점수 표입니다.

406
00:52:10,760 --> 00:52:16,840
이러한 까다로운 벤치마크 중 일부에서 최고의 언어 모델이 가능하다는 것을 알 수 있습니다.

407
00:52:16,840 --> 00:52:24,840
실제 세계의 상호작용을 다루는 것입니다. 예를 들어 GPT-4의 성능이 가장 좋습니다.

408
00:52:26,120 --> 00:52:32,120
Agent Bench의 데이터 세트 중 8분의 6 중 6개입니다. GPT-4에 이어,

409
00:52:34,120 --> 00:52:46,440
Claude2, Claude 및 GPT-3.5 Turbo도 괜찮은 점수를 받았습니다. 다른 API 기반 LLM은 품질이 좋지 않습니다.

410
00:52:46,440 --> 00:52:53,480
성능을 발휘하지만 대부분은 각 영역 내에서 꽤 몇 퍼센트의 문제를 해결할 수 있습니다.

411
00:52:53,480 --> 00:53:01,320
환경. 반면 오픈 소스 LLM은 일반적으로 일부 문제를 해결하지 못합니다.

412
00:53:03,240 --> 00:53:11,880
지식 그래프와 같은 것입니다. 그들 대부분은 임무에 실패했습니다. 디지털 카드 게임 및

413
00:53:11,880 --> 00:53:21,800
그들이 어려움을 겪는 가정 문제. 이는 대부분의 오픈 소스 언어 모델을 다음과 같이 식별합니다.

414
00:53:22,520 --> 00:53:26,680
실제 환경에서 에이전트가 될 수 없습니다.

415
00:53:29,720 --> 00:53:39,960
다음은 27개의 LLM에 대한 LLM의 성과를 나타내는 두 개의 그래프입니다.

416
00:53:42,600 --> 00:53:50,040
그리고 오른쪽 그래프에는 실제로 LLM의 전체 점수가 다음과 같이 표시됩니다.

417
00:53:50,040 --> 00:53:59,400
Agent Bench 벤치마크의 에이전트. 이제 일부 실패의 원인을 더 자세히 파악하기 위해

418
00:53:59,400 --> 00:54:06,440
이러한 LLM 중에서 우리는 LLM 상담원이 완료 이유를 5가지 유형으로 분류합니다. 가장 먼저,

419
00:54:07,000 --> 00:54:13,000
완료되면 완료로 기록됩니다. 하지만 실패 이유도 네 가지가 있습니다.

420
00:54:13,720 --> 00:54:19,240
우선 연락 한도를 초과했습니다. 상호작용 기록이 표시될 때 발생합니다.

421
00:54:19,800 --> 00:54:27,000
LLM의 최대 컨텍스트 링크를 초과합니다. 그래서 상호작용 기록이 너무 길었어요

422
00:54:27,720 --> 00:54:34,360
입력으로 사용됩니다. 형식도 잘못되었습니다. 상담원이 할 때 이런 일이 발생합니다.

423
00:54:34,360 --> 00:54:42,760
형식 지침을 따르지 마십시오. 형식을 따르면 잘못된 작업이 발생합니다.

424
00:54:42,760 --> 00:54:52,200
지침이 있지만 선택한 작업이 잘못되었습니다. 마지막으로 작업 제한 초과는 다음과 같은 경우에 발생합니다.

425
00:54:52,840 --> 00:54:59,800
에이전트는 미리 정의된 유한한 양의 최대 상호 작용 이후에는 문제를 해결할 수 없습니다.

426
00:55:00,760 --> 00:55:07,640
이는 LLM 생성이 무한 루프에 들어갈 때 발생했습니다. 그래서 그런 일이 일어날 수 있었어

427
00:55:07,640 --> 00:55:12,760
에이전트가 아무데도 도달하지 못하는 동일한 작업을 계속 반복하는 경우입니다.

428
00:55:16,040 --> 00:55:25,880
마지막으로 LLM이 모든 분야에서 실패하는 가장 일반적인 이유를 보여주는 표는 다음과 같습니다.

429
00:55:25,880 --> 00:55:32,600
8개의 서로 다른 환경. 대부분의 LLM 에이전트는 주어진 상황에서 어려운 작업을 해결하지 못합니다.

430
00:55:32,600 --> 00:55:40,520
시간이 지나거나 반복세대에 빠지게 됩니다. 예를 들어, 지식 그래프 벤치마크와

431
00:55:41,320 --> 00:55:48,200
측면적 사고 퍼즐 벤치마크에서 대부분의 LLM이 작업 제한을 초과하여 실패했습니다.

432
00:55:49,080 --> 00:55:58,920
데이터베이스와 디지털 카드 게임의 경우 많은 LLM이 지침을 제공하기 때문에 실패합니다.

433
00:55:58,920 --> 00:56:06,200
형식이 잘못되었습니다. 잘못된 형식이나 잘못된 작업으로 인해 실패하는 LLM은 일반적으로

434
00:56:06,200 --> 00:56:14,680
LLM이 지침을 따르지 못해서 발생합니다. 그리고 작업 제한을 초과하여 LLM이 많이 실패하면

435
00:56:14,760 --> 00:56:19,320
이는 LLM의 다항적 결정 능력이 약하다는 것을 나타냅니다.

436
00:56:20,600 --> 00:56:26,760
이것으로 Agent Bench의 프레젠테이션을 마칩니다.