1
00:00:00,000 --> 00:00:03,360
안녕하세요, 저는 콜 와이어트입니다.

2
00:00:03,360 --> 00:00:06,560
그리고 저는 카터 블레어입니다.

3
00:00:06,560 --> 00:00:13,320
그리고 우리는 스케일링 법칙에 대해 발표할 것입니다.

4
00:00:13,320 --> 00:00:19,320
따라서 우리는 스케일링 법칙에 대한 몇 가지 정보를 제공하는 것부터 시작하겠습니다.

5
00:00:19,320 --> 00:00:24,240
그것을 소개한 논문과 후속 결과.

6
00:00:24,240 --> 00:00:29,160
그런 다음 약간의 의문을 불러일으키는 몰입 능력에 대해 이야기하겠습니다.

7
00:00:29,160 --> 00:00:32,760
부드러운 스케일링 법칙에 대한 아이디어입니다.

8
00:00:32,760 --> 00:00:38,840
그런 다음 몰입에 대한 이야기를 복잡하게 만드는 다른 결과에 대해서도 이야기하겠습니다.

9
00:00:38,840 --> 00:00:42,640
능력.

10
00:00:42,640 --> 00:00:46,800
그래서 이것에 관심을 갖는 몇 가지 동기가 있습니다.

11
00:00:46,800 --> 00:00:51,720
첫 번째는 모델 성능이 실제로 어떻게 나타나는지 연구하는 과학적 문제입니다.

12
00:00:51,720 --> 00:00:53,360
규모가 커지면서 변화합니다.

13
00:00:53,360 --> 00:00:59,160
그래서 이것은 우리가 실험적으로 다룰 수 있는 것이고, 우리는 근본적인 몇 가지를 추출하려고 노력할 수 있습니다.

14
00:00:59,160 --> 00:01:04,040
추세 및 테스트 가설.

15
00:01:04,040 --> 00:01:08,640
그렇다면 분명히 중요한 엔지니어링 중요성이 있습니다.

16
00:01:08,640 --> 00:01:14,680
따라서 프론티어 모델을 훈련하는 데 점점 더 많은 비용이 들고 때로는 수백만 달러가 소요되기도 합니다.

17
00:01:14,680 --> 00:01:16,280
단일 훈련 실행을 위해.

18
00:01:16,280 --> 00:01:25,800
따라서 크기와 개수를 설정하는 최적의 방법을 예측할 수 있는 것이 매우 중요합니다.

19
00:01:25,800 --> 00:01:32,800
동일한 성능을 위해 비용을 절약하기 위해 모델에 대한 훈련 토큰을 사용합니다.

20
00:01:32,800 --> 00:01:36,680
마지막으로 AI 안전의 중요성이 있습니다.

21
00:01:36,680 --> 00:01:43,480
따라서 모델은 이러한 예상치 못한 창발 능력을 종종 보여주었습니다.

22
00:01:44,000 --> 00:01:49,160
우리는 다음에 어떤 수준의 기능을 기대해야 하는지 알려주는 몇 가지 도구를 갖고 싶습니다.

23
00:01:49,160 --> 00:01:52,240
프론티어 모델 세대가 될 것입니다.

24
00:01:52,240 --> 00:01:57,000
그리고 이것은 우리에게 속도를 늦추거나

25
00:01:57,000 --> 00:02:02,480
특정 유형의 위험에 대비하십시오.

26
00:02:02,480 --> 00:02:09,960
자, 그럼 Kaplan 2020에 대한 이야기부터 시작하겠습니다.

27
00:02:09,960 --> 00:02:14,160
그리고 이것은 스케일링 법칙을 소개한 논문이었습니다.

28
00:02:14,160 --> 00:02:19,960
그리고 그들이 보려고 했던 가장 중요한 질문은 '관계가 있는가'였습니다.

29
00:02:19,960 --> 00:02:26,520
입력 종류에 따라 데이터 또는 매개변수 수와 성능을 계산합니다.

30
00:02:26,520 --> 00:02:28,560
모델의.

31
00:02:28,560 --> 00:02:34,760
그래서 여기서 주요 아이디어는 그들이 매우 다양한 규모의 모델을 훈련한다는 것입니다.

32
00:02:34,760 --> 00:02:38,760
데이터 계산과 매개변수 수 측면에서 모두 그렇습니다.

33
00:02:38,760 --> 00:02:44,480
그리고 그들은 최종 손실이 예측 가능하다는 것을 알게 됩니다.

34
00:02:44,480 --> 00:02:48,760
그리고 이는 각 기본 입력의 규모와 관련이 있습니다.

35
00:02:48,760 --> 00:02:55,400
그리고 이러한 입력을 바탕으로 최종 손실을 예측하기 위한 예측 프레임워크를 제공합니다.

36
00:02:55,400 --> 00:03:03,840
그리고 그들은 네트워크 규모가 얼마나 되어야 하는지에 대한 실질적인 권장 사항을 제시합니다.

37
00:03:03,840 --> 00:03:05,320
특정 양의 데이터.

38
00:03:05,880 --> 00:03:08,960
그들은 또한 수렴에 대해 훈련해서는 안 된다는 점을 발견하고 중요한 배치를 찾습니다.

39
00:03:08,960 --> 00:03:11,800
크기.

40
00:03:11,800 --> 00:03:16,160
따라서 그들의 주요 결과는 성능이 규모에 크게 의존하고 모델에 약하게 의존한다는 것입니다.

41
00:03:16,160 --> 00:03:17,160
모양.

42
00:03:17,160 --> 00:03:22,120
이 내용은 빠르게 살펴보고 나중에 더 자세히 살펴보겠습니다.

43
00:03:22,120 --> 00:03:27,000
병목 현상이 발생하지 않을 때 각 컴퓨팅 데이터와 매개변수 수에 따라 성능이 확장됩니다.

44
00:03:27,000 --> 00:03:28,960
다른 두 사람에 의해.

45
00:03:28,960 --> 00:03:34,200
매개변수 수와

46
00:03:34,200 --> 00:03:35,560
동시에 데이터의 양.

47
00:03:35,560 --> 00:03:44,080
하지만 그 중 하나를 늘리는 것을 중단하면 수익이 감소하게 될 것입니다.

48
00:03:44,080 --> 00:03:45,520
또한 훈련 곡선도 예측 가능합니다.

49
00:03:45,520 --> 00:03:50,200
따라서 훈련 곡선의 초기 부분을 보면 최종 곡선이 무엇인지 대략적으로 예측할 수 있습니다.

50
00:03:50,200 --> 00:03:55,080
데이터 손실이 얼마나 되는지 파악하려는 경우 매우 유용할 수 있습니다.

51
00:03:55,080 --> 00:04:00,840
특정 최종 손실량을 얻기 위해 사용하려는 매개변수의 수.

52
00:04:00,840 --> 00:04:05,920
따라서 우리는 분포 데이터가 아닌 데이터에서만 모델이 얼마나 잘 수행될지 예측할 수 있습니다.

53
00:04:05,920 --> 00:04:09,240
훈련 데이터의 검증 정확도를 살펴봅니다.

54
00:04:09,240 --> 00:04:14,880
따라서 이러한 종류의 안정적인 전송이 있으며 대규모 모델이 더 효율적입니다.

55
00:04:14,880 --> 00:04:19,000
작은 것보다 수렴이 매우 비효율적입니다.

56
00:04:19,000 --> 00:04:26,960
그들은 또한 이 중요한 배치 크기를 발견했으며 이는 대략 손실만의 힘입니다.

57
00:04:26,960 --> 00:04:30,560
따라서 모델 성능을 시작하려면 대부분 규모에 따라 달라집니다.

58
00:04:30,560 --> 00:04:37,120
여기 하단 그림에서 다양한 종류의 모델을 볼 수 있습니다.

59
00:04:37,120 --> 00:04:44,560
매개변수를 입력하면 손실과 관련된 일종의 관계를 볼 수 있습니다.

60
00:04:44,560 --> 00:04:50,560
그리고 전반적으로 특히 주의 머리 차원과 마찬가지로 실제로는 중요하지 않습니다.

61
00:04:50,560 --> 00:04:57,320
모든 주의 머리 차원에 대해 거의 동일한 양의 손실을 얻게 될 것입니다.

62
00:04:57,320 --> 00:05:02,520
종횡비와 피드포워드 비율에 관해서는 좀 더 많은 관계가 있습니다.

63
00:05:02,520 --> 00:05:10,440
그와 최종 손실 사이에 있지만 관계에 비해 여전히 전반적으로 매우 약합니다.

64
00:05:10,440 --> 00:05:13,720
규모로.

65
00:05:13,720 --> 00:05:18,640
따라서 앞으로 나아가는 메모와 마찬가지로 많은 그래프가 선형 추세를 보이는 것처럼 보입니다.

66
00:05:18,640 --> 00:05:24,640
하지만 멱함수 법칙이 로그에 선형으로 표시되기 때문에 그렇게 보일 뿐입니다.

67
00:05:24,640 --> 00:05:27,280
스케일 그래프.

68
00:05:27,280 --> 00:05:33,640
이는 일종의 주요 수치 중 하나이며 손실은 예상대로 진행된다는 사실을 발견했습니다.

69
00:05:33,640 --> 00:05:39,200
컴퓨팅 데이터 세트 크기와 매개변수 수를 확장하면 축소됩니다.

70
00:05:39,200 --> 00:05:45,200
그러나 중요한 것은 이 그래프는 모두 계산을 다양하게 할 때

71
00:05:45,200 --> 00:05:48,680
데이터 크기나 매개변수 수로 인해 병목 현상이 발생합니다.

72
00:05:48,680 --> 00:05:59,680
따라서 이러한 테스트 손실은 병목 현상이 발생하지 않을 때 이러한 요소 중 하나의 측면에서 원활하게 감소합니다.

73
00:05:59,680 --> 00:06:02,080
다른 두 사람에 의해.

74
00:06:02,080 --> 00:06:08,440
그래서 그들은 또한 보시다시피 훈련 곡선이 이러한 예측 가능한 패턴을 따른다는 것을 발견했습니다.

75
00:06:08,440 --> 00:06:13,200
매개변수는 모델 크기와 거의 독립적입니다.

76
00:06:13,200 --> 00:06:20,480
따라서 상대적으로 작은 모델의 경우 첫 번째 보라색 선의 모양은 대략 유사합니다.

77
00:06:20,480 --> 00:06:26,480
마지막 노란색 선은 10억 개의 매개변수가 있는 모델입니다.

78
00:06:26,480 --> 00:06:31,040
따라서 훈련 곡선의 초기 부분을 추정함으로써 우리는 대략적으로 무엇을 예측할 수 있습니다.

79
00:06:31,040 --> 00:06:35,480
훨씬 더 오랫동안 훈련함으로써 얻을 수 있는 손실입니다.

80
00:06:35,480 --> 00:06:40,800
그들은 또한 모델의 이전이 상당히 예측 가능하다는 것을 발견했습니다.

81
00:06:40,800 --> 00:06:46,160
여기에서는 x축의 훈련 분포에 대한 테스트 손실과 x축의 손실을 볼 수 있습니다.

82
00:06:46,160 --> 00:06:49,000
y축의 다른 분포.

83
00:06:49,000 --> 00:06:53,480
그리고 대략적으로 이들 사이에는 선형 관계가 있습니다.

84
00:06:53,480 --> 00:07:02,120
따라서 훈련 분포의 손실을 줄이면 다른 분포의 손실도 줄어들 것으로 기대할 수 있습니다.

85
00:07:02,120 --> 00:07:03,120
또한.

86
00:07:03,120 --> 00:07:09,280
또한 그들은 큰 모델이 작은 모델보다 표본 효율성이 훨씬 더 높다는 것을 발견했습니다.

87
00:07:09,280 --> 00:07:13,960
따라서 더 적은 수의 최적화 단계로 동일한 수준의 성능에 도달할 수 있습니다.

88
00:07:13,960 --> 00:07:20,720
여기서 우리는 고정된 데이터 세트 크기(예: 데이터에 10억 개의 토큰)가 있는지 확인할 수 있습니다.

89
00:07:20,720 --> 00:07:32,640
설정된 경우, 더 큰 모델은 이러한 작은 모델보다 훨씬 더 낮은 손실을 달성합니다.

90
00:07:32,960 --> 00:07:36,440
응.

91
00:07:36,440 --> 00:07:41,400
그 이유는 이러한 대형 모델이 아마도 같은 것을 사용하고 있기 때문일 수 있습니다.

92
00:07:41,400 --> 00:07:45,040
동일한 양의 토큰이 표시되고 더 많은 매개변수가 있습니다.

93
00:07:45,040 --> 00:07:52,200
따라서 동일한 양의 데이터를 보려면 훨씬 더 많은 컴퓨팅 비용이 소요됩니다.

94
00:07:52,200 --> 00:07:55,640
따라서 그들은 더 많은 것을 배웁니다.

95
00:07:55,640 --> 00:07:57,920
그들은 또한 수렴이 비효율적이라는 것을 보여줍니다.

96
00:07:57,920 --> 00:08:02,520
따라서 고정된 컴퓨팅 예산 내에서 작업할 때 다른 제한 사항은 없습니다.

97
00:08:02,520 --> 00:08:06,760
모델 크기나 사용 가능한 데이터가 있으므로 무한한 저장 공간과

98
00:08:06,760 --> 00:08:12,400
이 거대한 무한 데이터 세트 또는 병목 현상을 일으키지 않을 만큼 충분히 큰 데이터 세트

99
00:08:12,400 --> 00:08:17,640
매우 큰 모델을 훈련하고 중지함으로써 최적의 성능을 얻을 수 있습니다.

100
00:08:17,640 --> 00:08:20,000
수렴이 현저히 부족합니다.

101
00:08:20,000 --> 00:08:25,640
예를 들어, 이 주황색 선을 보고 이렇게 말하면

102
00:08:25,640 --> 00:08:32,320
계산하면 훈련 곡선 중 하나에서 이 선의 가장 낮은 절편이 나옵니다.

103
00:08:32,320 --> 00:08:36,320
수렴과는 거리가 매우 먼 모델입니다.

104
00:08:36,320 --> 00:08:42,080
따라서 가장 작은 모델을 수렴하도록 훈련한다면 우리는 6개 정도의 결과를 얻을 수 있습니다.

105
00:08:42,080 --> 00:08:43,080
손실.

106
00:08:43,080 --> 00:08:48,120
반면에 우리가 거대한 모델을 훈련하거나 같은 양의 데이터로 훨씬 더 큰 모델을 훈련한다면

107
00:08:48,120 --> 00:08:56,320
계산하면 약 5단위의 손실을 달성할 수 있습니다.

108
00:08:56,320 --> 00:09:02,520
그래서 그들은 숫자를 늘리면 예측적으로 성능이 향상된다는 사실도 발견했습니다.

109
00:09:02,520 --> 00:09:07,920
매개변수와 데이터 양이 동시에 증가하지만 다음 중 하나에 해당하면 수익이 감소합니다.

110
00:09:07,920 --> 00:09:11,680
그것들은 고정되어 있고 다른 것들은 증가합니다.

111
00:09:11,680 --> 00:09:16,880
그리고 성능 저하는 매개변수 수의 비율에 따라 예측적으로 달라집니다.

112
00:09:16,880 --> 00:09:22,040
데이터 세트의 토큰 양에 대해 0.74의 거듭제곱입니다.

113
00:09:22,040 --> 00:09:27,720
그리고 모델 크기를 8배 늘릴 때마다

114
00:09:27,720 --> 00:09:31,440
페널티를 피하기 위해 데이터 크기를 약 5배로 늘립니다.

115
00:09:31,440 --> 00:09:37,320
그러나 이와 같은 특정한 관계는 나중에 의문의 여지가 있습니다.

116
00:09:37,320 --> 00:09:41,240
내 생각에 우리는 제시된 다음 논문에서 보게 될 것입니다.

117
00:09:41,240 --> 00:09:45,120
네, 지금부터 두 편의 논문이요.

118
00:09:46,040 --> 00:09:51,960
그리고 그들은 모델 훈련을 위한 이상적인 배치 크기도 찾았습니다. 대략 다음과 같습니다.

119
00:09:51,960 --> 00:09:59,720
손실의 거듭제곱은 기울기를 측정하여 계속해서 결정 가능합니다.

120
00:09:59,720 --> 00:10:05,400
노이즈 규모이며 가장 큰 수렴에서는 대략 100만 ~ 200만 개의 토큰입니다.

121
00:10:05,400 --> 00:10:07,840
우리가 훈련할 수 있는 모델.

122
00:10:07,840 --> 00:10:14,920
예, 그래디언트에 노이즈가 너무 많으면 다음이 필요할 수도 있다는 생각입니다.

123
00:10:14,920 --> 00:10:23,080
더 신뢰할 수 있는 방향을 얻기 위해 더 큰 배치를 원할 수도 있습니다.

124
00:10:23,080 --> 00:10:28,880
따라서 Kaplan의 주요 시사점은 손실이 모델 규모에 따라 예측 가능하게 확장된다는 것입니다.

125
00:10:28,880 --> 00:10:33,280
따라서 모델의 규모만 봐도 손실이 얼마나 될지 예측할 수 있습니다.

126
00:10:33,280 --> 00:10:35,080
그리고 모델 모양.

127
00:10:35,080 --> 00:10:40,360
따라서 피드 포워드 비율, 종횡비 또는 주의 헤드의 크기는 단지

128
00:10:40,360 --> 00:10:41,680
별로 중요하지 않습니다.

129
00:10:41,680 --> 00:10:48,920
어느 정도 중요하지만 성능 향상의 주요 동인은 규모입니다.

130
00:10:48,920 --> 00:10:51,880
그리고 임베딩 매개변수의 양은 실제로 중요하지 않습니다.

131
00:10:51,880 --> 00:10:56,600
그리고 훈련의 초기 부분만을 기반으로 모델의 최종 손실을 예측할 수 있습니다.

132
00:10:56,600 --> 00:10:57,600
곡선.

133
00:10:57,600 --> 00:11:01,280
Cole이 엔지니어링 관점에서 언급한 것처럼 이것은 좋은 것입니다.

134
00:11:01,520 --> 00:11:07,760
우리는 특정 손실을 달성하기 위해 모델의 크기가 얼마나 커야 하는지 파악하고 싶습니다.

135
00:11:07,760 --> 00:11:13,920
그리고 여러 번의 작은 훈련을 실행하여 결과가 무엇인지 예측할 수 있습니다.

136
00:11:13,920 --> 00:11:18,600
최종 손실은 최종 전체 훈련 실행에 대한 것입니다.

137
00:11:18,600 --> 00:11:23,400
그리고 중요한 것은 이러한 모델을 수렴하도록 교육하는 것이 비효율적이라는 사실도 발견했습니다.

138
00:11:23,400 --> 00:11:28,920
수렴이 아닌 더 큰 모델을 훈련하는 것만으로도 훨씬 더 나은 결과를 얻을 수 있습니다.

139
00:11:28,960 --> 00:11:33,160
그리고 그들은 손실의 힘만 있고 의존하지 않는 이상적인 배치 크기를 찾았습니다.

140
00:11:33,160 --> 00:11:34,160
모델 사이즈에 대해

141
00:11:37,160 --> 00:11:45,400
좋습니다. 이 작품이 나온 후 사람들은 비슷한 것을 찾을 수 있는지에 관심을 갖습니다.

142
00:11:45,400 --> 00:11:51,720
일반적으로 대규모 기초 모델은 미세 조정되기 때문에 전송에 대한 확장 법칙이 적용됩니다.

143
00:11:51,720 --> 00:11:54,080
일종의 다운스트림 작업입니다.

144
00:11:54,080 --> 00:11:57,560
그래서 이것은 실용적인 중요성입니다.

145
00:11:58,200 --> 00:12:07,080
Kaplan이 2021년에 발견한 것은 사전 교육이 특히 가치 있을 수 있다는 것입니다.

146
00:12:07,080 --> 00:12:14,800
미세 조정 데이터 세트가 그리 크지 않은 경우, 이는 자연스러운 현상입니다.

147
00:12:14,800 --> 00:12:19,360
미세 조정 데이터 세트가 사전 훈련 데이터 세트보다 크면 사전 훈련 데이터 세트는 다음과 같습니다.

148
00:12:19,360 --> 00:12:22,160
단지 일종의 배포 소음을 주입하는 것뿐입니다.

149
00:12:23,160 --> 00:12:31,480
그들은 또한 다른 사전 훈련을 통해 이 효과적인 데이터 전송을 모델링할 수 있었습니다.

150
00:12:31,480 --> 00:12:43,720
거듭제곱법칙을 사용했으며, 그들은 이 측정값을 사용하여

151
00:12:43,720 --> 00:12:49,600
멱함수 법칙의 매개변수 중 하나에서 나오는 데이터 세트입니다.

152
00:12:50,600 --> 00:12:56,200
따라서 한 데이터 세트가 다른 데이터 세트와 얼마나 가까운지 측정할 수 있다는 것이 일종의 아이디어입니다.

153
00:12:56,200 --> 00:13:00,200
사전 훈련이 얼마나 유용한지에 따라 결정됩니다.

154
00:13:02,200 --> 00:13:05,840
그리고 마지막으로 사전 훈련을 통해 골화 현상을 살펴보았습니다.

155
00:13:05,840 --> 00:13:07,600
실제로 성능이 저하됩니다.

156
00:13:08,600 --> 00:13:13,280
좋아요, 이 거듭제곱 법칙은 약간 혼란스럽기 때문에 자세히 살펴볼 가치가 있습니다.

157
00:13:13,280 --> 00:13:17,200
이 그래프에서는 비디오를 일시중지할 수도 있습니다.

158
00:13:17,200 --> 00:13:29,360
하지만 아이디어는 일정량의 미세 조정 데이터를 사용하여 사전 훈련된 사용자에게 제공되는 DF입니다.

159
00:13:29,360 --> 00:13:35,520
모델을 사용하면 사전 훈련된 모델이 훈련된 모델과 얼마나 앞서 있는지 확인할 수 있습니다.

160
00:13:35,520 --> 00:13:36,600
기스로부터.

161
00:13:37,600 --> 00:13:51,360
따라서 두 모델 모두 DF 크기의 미세 조정 데이터 세트를 얻지만, 하나는 사전 훈련된 모델입니다.

162
00:13:51,360 --> 00:13:53,800
또한 일부 사전 훈련 데이터에도 액세스할 수 있습니다.

163
00:13:53,800 --> 00:14:04,720
그리고 미세 조정 데이터의 추가 토큰이 얼마나 필요한지 측정함으로써

164
00:14:04,720 --> 00:14:12,880
이 경우 캐릭터는 두 모델 모두에서 훈련을 받아야 합니다.

165
00:14:12,880 --> 00:14:19,600
스크래치 모델이 사전 훈련된 모델을 따라잡아 수평선이 되도록 한 다음,

166
00:14:19,600 --> 00:14:26,400
사전 훈련된 모델에 효과적인 데이터가 얼마나 전송되었는지.

167
00:14:26,400 --> 00:14:30,120
이것이 바로 사전 훈련을 통해 얻는 장점이 바로 DT입니다.

168
00:14:31,120 --> 00:14:38,280
따라서 여기서 N은 비임베딩 모델 매개변수를 나타냅니다.

169
00:14:38,280 --> 00:14:43,280
앞서 임베딩 모델 매개변수의 수가 그다지 중요하지 않다는 것을 확인했습니다.

170
00:14:43,280 --> 00:14:44,920
그리고 마지막 논문에서.

171
00:14:44,920 --> 00:14:51,280
그리고 우리는 이것을 볼 수 있습니다. 전송된 유효 데이터가 다음과 같은 확장 법칙이 있습니다.

172
00:14:51,280 --> 00:15:00,320
멱법칙에 따라 알파에 대한 미세 조정 데이터의 양을 일정하게 곱한 것과 관련됩니다.

173
00:15:00,320 --> 00:15:03,640
매개변수 수를 베타에 곱합니다.

174
00:15:03,640 --> 00:15:11,240
따라서 베타는 데이터 세트에 의존하지 않고 모델 아키텍처에만 의존합니다.

175
00:15:11,240 --> 00:15:16,160
이는 일종의 제외될 수 있는 유용한 결과 중 하나입니다.

176
00:15:16,160 --> 00:15:36,360
그런 다음 K와 알파는 데이터 세트에 따라 달라지며 실제로는 더 작은 것을 볼 수 있습니다.

177
00:15:36,360 --> 00:15:45,000
알파는 더 밀접하게 관련된 데이터 세트를 나타냅니다. 왜냐하면 알파가 더 작을수록

178
00:15:45,000 --> 00:15:51,280
더 많은 것을 얻었으므로 사전 훈련을 통해 더 많은 이점을 이미 고려했습니다.

179
00:15:51,280 --> 00:15:57,000
따라서 알파가 작을수록 K가 커집니다. 이는 더 중요한 의미가 있음을 의미합니다.

180
00:15:57,000 --> 00:16:01,000
미세 조정 데이터를 보기도 전에 먼저 시작하세요.

181
00:16:01,000 --> 00:16:08,520
따라서 더 다른 미세 조정 데이터 세트를 사용하면 더 빠른 이득이 있습니다.

182
00:16:08,520 --> 00:16:15,560
미세 조정 데이터는 사전 훈련 데이터와 다르게 보이기 때문입니다.

183
00:16:15,560 --> 00:16:18,060
데이터.

184
00:16:18,060 --> 00:16:24,800
그런 다음 그들은 텍스트의 유사성을 Python과 50% Python 코드, 50% 텍스트를 Python과 비교합니다.

185
00:16:24,800 --> 00:16:31,560
Python을 사용하여 이 알파를 유사성 측정으로 제안하면 이것이 매우 많다는 것을 알 수 있습니다.

186
00:16:31,560 --> 00:16:37,360
K가 훨씬 더 큰 두 번째 경우에는 더 작습니다.

187
00:16:38,240 --> 00:16:45,880
마지막으로 골화에 대한 우려는 때때로 사전 훈련이

188
00:16:45,880 --> 00:16:50,720
아프기는 하지만, 량에 비해 모델의 크기가 상대적으로 작은 경우에만 해당됩니다.

189
00:16:50,720 --> 00:16:56,520
훈련 데이터.

190
00:16:56,520 --> 00:17:02,240
따라서 시사점은 스케일링 법칙이 계속해서 널리 퍼져 있고 모든 곳에 나타난다는 것입니다.

191
00:17:02,320 --> 00:17:11,320
전이 학습과 사전 훈련에서 사전 훈련이 수행될 수 있는 장소

192
00:17:11,320 --> 00:17:16,040
특히 미세 조정 데이터를 찾기 어려울 때 상당한 이점이 있습니다.

193
00:17:16,040 --> 00:17:23,560
사전 훈련이 해로울 수 있지만 사용 가능한 데이터가 거의 없는 경우에만 가능합니다.

194
00:17:23,560 --> 00:17:27,560
데이터의 양에 비해 모델의 크기가 매우 작은 경우.

195
00:17:32,560 --> 00:17:33,560
좋아요.

196
00:17:37,560 --> 00:17:45,040
따라서 이러한 결과가 나온 후 Hutter는 다음과 같은 이론적 조사를 실시했습니다.

197
00:17:45,040 --> 00:17:51,040
관찰된 스케일링 법칙이 수학적으로 어떻게 설명될 수 있는지.

198
00:17:51,040 --> 00:17:59,040
이는 일반적으로 통계 학습 이론에서 다음을 기대한다는 사실에서 비롯됩니다.

199
00:17:59,040 --> 00:18:05,040
1/2의 지수를 갖는 거듭제곱 법칙에서 지수를 갖는 학습 척도

200
00:18:05,040 --> 00:18:15,040
아니면 하나, 그래, 특히 보통, 그래, 일반적으로 여러 개를 볼 때

201
00:18:15,040 --> 00:18:22,560
예를 들어, 표준 편차는 n의 제곱근에 대한 1로 축소됩니다.

202
00:18:22,560 --> 00:18:32,040
이것은 일반적으로 예상하는 것이지만 Kaplan의 거듭제곱 법칙은 다음과 같은 값을 가졌습니다.

203
00:18:32,040 --> 00:18:37,360
1/2보다 낮은 지수, 그리고 Hutter는 이것을 무한히 설명하려고 했습니다.

204
00:18:37,360 --> 00:18:45,280
매개변수화된 모델을 통해 그는 이 동작과 일치하는 일부 모델을 찾을 수 있었습니다.

205
00:18:45,280 --> 00:18:49,200
그와 아마도 그들이 언급했지만 추구하지 않은 한 가지는

206
00:18:49,200 --> 00:18:53,880
데이터 양에 따라 모델 크기가 커집니다. 이는 실제로 일어나는 일과 더 일치합니다.

207
00:18:53,880 --> 00:18:58,000
실제로는 분석적으로 다루기가 덜해 보입니다.

208
00:18:58,000 --> 00:19:04,600
어쨌든 이 논문이 나온 후 다음 논문에서는 실제로

209
00:19:04,600 --> 00:19:13,200
보통지수 1/2 스케일링 법칙이 다시 돌아왔고, 더 낮은 값이 친절하다는 것

210
00:19:13,200 --> 00:19:19,520
아마도 예상했어야 할 방법론적 오류가 있을 수도 있습니다.

211
00:19:19,520 --> 00:19:25,840
아마도 이것이 학습 이론의 정확한 일반적인 상황에 맞지 않을 수도 있다는 점은 주목할 가치가 있습니다.

212
00:19:25,840 --> 00:19:32,920
모델 사이즈 변경으로 인해

213
00:19:32,920 --> 00:19:39,280
Kaplan의 논문에 대한 답변으로 이 논문 교육 계산은

214
00:19:39,280 --> 00:19:46,120
최적의 대형 언어 모델은 Hoffman et al.에서 나왔습니다. 2022년에 그들은 노력하고 있었습니다

215
00:19:46,120 --> 00:19:50,880
거의 똑같은 질문에, 어쩌면 약간 다른 초점을 두고 대답하려고 합니다.

216
00:19:50,880 --> 00:19:54,640
고정된 모델 크기와 토큰 수 사이의 최적의 절충점은 무엇입니까?

217
00:19:54,640 --> 00:19:55,640
예산을 계산합니다.

218
00:19:55,640 --> 00:20:01,320
그래서 그들은 좀 더 공학적인 관점에서 이 문제에 접근하려고 했으나,

219
00:20:01,320 --> 00:20:08,240
똑같은 문제, 여전히 컴퓨팅 소비 방법을 알려주는 확장 법칙을 찾고 있습니다.

220
00:20:08,240 --> 00:20:12,840
토큰과 매개변수 사이.

221
00:20:12,840 --> 00:20:17,120
그리고 그들이 발견한 것은 모델이 실제로 상당히 부족한 훈련을 받았다는 것입니다.

222
00:20:17,120 --> 00:20:22,000
사람들은 Kaplan이 제안한 확장 전략을 따르고 있었고 그것이 더 나을 것입니다.

223
00:20:22,000 --> 00:20:25,320
융합에 더 가깝게 훈련할 수 있습니다.

224
00:20:25,320 --> 00:20:29,640
그리고 실제로 이 모델 크기와 훈련된 토큰의 수는 동일하게 확장되어야 합니다.

225
00:20:29,640 --> 00:20:39,040
그래서 이것은 서로 같은 비율로 확장되어야 합니다.

226
00:20:39,040 --> 00:20:42,760
특히 이것은 이것에 대한 실제적인 시연입니다.

227
00:20:42,760 --> 00:20:50,560
그들은 훈련을 통해 더 큰 Gopher 모델보다 성능이 뛰어난 Chinchill 모델을 도입했습니다.

228
00:20:50,560 --> 00:20:56,160
훨씬 작은 모델임에도 불구하고 더 많은 토큰을 사용했습니다.

229
00:20:56,160 --> 00:21:04,600
그래서 그들이 수정한 주요 방법론적 세부 사항은 학습 속도 일정이 다음과 같아야 한다는 것이었습니다.

230
00:21:04,600 --> 00:21:06,400
토큰 수에 따라 다릅니다.

231
00:21:06,400 --> 00:21:12,160
그래서 이것은 Kaplan이 회전하는 것을 거의 잊었을 것 같은 손잡이였습니다.

232
00:21:12,160 --> 00:21:20,000
하이퍼파라미터를 다르게 설정하려고 했는데, 이는 일종의 자연스러운 오류일 수도 있습니다.

233
00:21:20,000 --> 00:21:25,760
그러나 일반적으로 훈련할 토큰 수를 알면 아마도

234
00:21:25,760 --> 00:21:30,840
이를 기반으로 학습률 일정을 선택할 가능성이 높습니다.

235
00:21:30,840 --> 00:21:40,680
그래서 이것을 적절하게 수행함으로써 그들은 훈련에서 더 많은 가치를 얻을 수 있었습니다.

236
00:21:40,680 --> 00:21:46,800
토큰을 더 많이 훈련하고 규모를 축소할 것을 권장하게 되었습니다.

237
00:21:46,800 --> 00:21:51,560
모델 크기.

238
00:21:51,560 --> 00:21:55,080
그리고 이 규칙은 실제로 모델 크기에 의존하지 않습니다.

239
00:21:55,080 --> 00:22:01,320
따라서 그들의 목표는 주어진 조건에서 최적의 매개변수 수와 토큰 수를 찾는 것이었습니다.

240
00:22:01,320 --> 00:22:04,480
고정된 컴퓨팅 예산을 최적화 문제로 표현했습니다.

241
00:22:04,480 --> 00:22:09,760
그리고 그들은 그 값을 추정하기 위해 상당히 유사한 세 가지 접근 방식을 시도했습니다.

242
00:22:09,760 --> 00:22:14,000
그래서 먼저 각 매개변수 개수에 대해 다양한 학습률 일정을 시도했습니다.

243
00:22:14,000 --> 00:22:20,800
이것이 그들이 Kaplan의 결과에서 어떻게 벗어났는가 하는 것입니다.

244
00:22:20,800 --> 00:22:30,920
따라서 첫 번째 접근 방식에서는 서로 다른 매개변수 크기의 여러 모델을 훈련했습니다.

245
00:22:30,920 --> 00:22:37,000
그리고 특정 컴퓨팅 수준에 대해 매개변수 크기와 숫자의 조합을 확인했습니다.

246
00:22:37,000 --> 00:22:39,320
토큰의 수가 최적이었습니다.

247
00:22:39,320 --> 00:22:43,760
그런 다음 두 가지를 모두 전력 손실로 별도로 맞춥니다.

248
00:22:43,760 --> 00:22:49,200
예를 들어, 가장 왼쪽 차트를 보면 점을 찾는다고 상상해 보세요.

249
00:22:49,200 --> 00:22:58,240
는 해당 곡선에서 가장 낮고 그로부터 최적의 값을 추출했습니다.

250
00:22:58,240 --> 00:23:13,280
훈련 토큰과 색상에 해당하는 매개변수, 예, 그런 것 같아요.

251
00:23:13,280 --> 00:23:21,080
이 경우에는 하나만 표시되는 것일 수도 있습니다. 예, 하나의 값만 표시되는 것입니다.

252
00:23:21,080 --> 00:23:22,080
매개변수.

253
00:23:22,080 --> 00:23:28,240
그러나 어쨌든 그것은 일종의 단순한 추정 문제입니다.

254
00:23:28,240 --> 00:23:34,720
그래서 두 번째 접근 방식에서는 거의 똑같은 작업을 수행했습니다.

255
00:23:34,720 --> 00:23:43,520
특정 고정된 컴퓨팅 양을 사용하여 각 모델을 완료할 때까지 훈련했습니다.

256
00:23:43,520 --> 00:23:47,520
그런 다음 어느 것이 가장 좋은지 비교했습니다.

257
00:23:47,520 --> 00:23:52,640
그리고 다시, 그들은 이 값들 각각에 적합했고 거의 정확히 같은 결과를 얻었습니다.

258
00:23:52,640 --> 00:23:54,680
접근 방식 1과 마찬가지로 접근 방식 2를 사용하는 지수.

259
00:23:54,680 --> 00:24:01,720
따라서 둘 다 매개변수 수의 지수에 대해 거의 정확히 절반이었습니다.

260
00:24:01,720 --> 00:24:04,760
그리고 데이터의 양.

261
00:24:04,760 --> 00:24:09,800
그리고 마지막으로 그들은 최소화하려고 노력했고, 다음을 고려한 모델을 맞추려고 노력했습니다.

262
00:24:09,800 --> 00:24:16,640
매개변수와 토큰 수는 동시에 계산되며 함께 사용됩니다.

263
00:24:16,640 --> 00:24:18,240
그리고 이것은 약간 다른 접근 방식입니다.

264
00:24:18,240 --> 00:24:24,080
지수에 대해 약간 다른 값을 얻었지만 여전히 절반에 가깝습니다.

265
00:24:24,080 --> 00:24:30,160
좋습니다. 이에 대한 결론은 GoFers 컴퓨팅 예산을 사용하면 더 작은 모델이라는 것입니다.

266
00:24:30,160 --> 00:24:32,920
더 많은 토큰에 대한 교육을 받았어야 했는데 그들은 입이 있던 곳에 돈을 넣었습니다.

267
00:24:32,920 --> 00:24:38,400
실제로 그렇게 해서 700억 개의 매개변수 모델인 친칠라를 훈련시켰습니다.

268
00:24:38,400 --> 00:24:41,760
1조 4천억 개의 토큰에 대해 훈련했는데, 내 생각엔 그 양이 너무 많아진 것 같아요.

269
00:24:41,760 --> 00:24:46,320
이전에 누구나 사용해 본 훈련 데이터입니다.

270
00:24:46,320 --> 00:24:53,880
그리고 이것의 몇 가지 장점은 무엇보다도 더 나은 성능을 가지고 있다는 것이 밝혀졌습니다.

271
00:24:53,880 --> 00:24:59,720
하지만 더 작은 모델이기 때문에 추론 시간과 미세 조정 시간도 단축되었습니다.

272
00:25:00,480 --> 00:25:06,840
따라서 이는 더 가벼운 접근 방식이기도 합니다. 예, 교육 비용이

273
00:25:06,840 --> 00:25:15,840
고정된 추론 시간 비용의 종류는 추론 시간 비용을 절약했습니다.

274
00:25:15,840 --> 00:25:20,680
그리고 하이퍼파라미터 측면에서 이러한 모델을 직접 비교할 수 있습니다.

275
00:25:20,680 --> 00:25:29,360
특히 여기서 GoFers 모델은 훨씬 더 큽니다.

276
00:25:29,360 --> 00:25:34,760
좋아요, 그리고 여기에 둘 사이의 다양한 비교가 있습니다.

277
00:25:34,760 --> 00:25:41,480
파란색으로 표시된 부분은 모두 Chinchilla가 GoFers를 능가하는 작업입니다.

278
00:25:41,480 --> 00:25:47,520
떠났고 Chinchilla가 GoFers보다 성능이 뛰어난 몇 가지 샷 작업도 수행했습니다.

279
00:25:47,520 --> 00:25:51,280
또한.

280
00:25:51,280 --> 00:25:55,240
따라서 중요한 점은 데이터와 매개변수를 함께 확장해야 한다는 것입니다.

281
00:25:55,240 --> 00:25:58,320
그들은 여전히 ​​수렴까지 모델 훈련을 권장하지 않습니다.

282
00:25:58,320 --> 00:26:05,320
그들은 Kaplan이 제안한 것보다 더 길고 수렴에 더 가까운 훈련을 권장합니다.

283
00:26:05,320 --> 00:26:10,720
그리고 네, 마지막 요점은 학습 속도 일정에 주의를 기울이는 것입니다.

284
00:26:10,720 --> 00:26:11,720
좋아요.

285
00:26:11,720 --> 00:26:16,080
좋습니다. 이제 기어를 바꿔보겠습니다.

286
00:26:16,080 --> 00:26:21,760
그래서 우리는 성능을 예측할 수 있도록 멋지고 부드러운 확장 곡선에 대해 이야기했습니다.

287
00:26:21,760 --> 00:26:28,960
계산량과 모델의 크기에서 모델의 양을 매우 명확하게

288
00:26:28,960 --> 00:26:34,320
데이터가 필요하고 새로운 능력이 나타나서 렌치를 던지게 됩니다.

289
00:26:34,320 --> 00:26:35,320
그 안에.

290
00:26:36,200 --> 00:26:43,920
따라서 2022년에는 창발 능력을 살펴보고 주요 연구 질문은 기본적으로

291
00:26:43,920 --> 00:26:52,760
다양한 창발 능력에 대한 조사를 하고 그들은 어떤 잠재력에 대해 이야기합니다.

292
00:26:52,760 --> 00:26:56,960
이러한 출현 능력의 원인.

293
00:26:56,960 --> 00:27:02,240
네, 원래 스케일링 법칙 문서에는 왼쪽 그림이 있습니다.

294
00:27:02,720 --> 00:27:06,520
컴퓨팅이 증가하고 손실이 감소하며 매우 원활한 관계입니다.

295
00:27:06,520 --> 00:27:11,520
그리고 오른쪽에는 긴급 능력이 표시됩니다.

296
00:27:11,520 --> 00:27:19,280
따라서 여기서는 모듈식 산술 수행의 계산과 정확성을 높이는 것과 같습니다.

297
00:27:19,280 --> 00:27:25,520
처럼 플랫하고, 이 마법을 맞기 전까지는 0입니다. 10 대 22 플롭이니 뭐니.

298
00:27:25,520 --> 00:27:27,760
갑자기 성능이 급상승합니다.

299
00:27:27,760 --> 00:27:29,560
그러니 전혀 원활한 관계가 아닙니다.

300
00:27:29,720 --> 00:27:36,440
기능이 정말 좋아지는 특정 지점과 비슷합니다.

301
00:27:36,440 --> 00:27:38,640
빠르게 증가합니다.

302
00:27:38,640 --> 00:27:43,720
그리고 그들은 이런 종류의 새로운 능력을 많이 발견합니다.

303
00:27:43,720 --> 00:27:51,960
따라서 일부 작업은 Big Bench에서 가져온 것입니다.

304
00:27:51,960 --> 00:27:59,120
그래서 그들은 이 문제의 원인이 무엇인지 자세히 살펴보고 다음과 같이 이야기합니다.

305
00:27:59,160 --> 00:28:04,440
이러한 작업 중 일부는 본질적으로 불연속적인 것과 같습니다.

306
00:28:04,440 --> 00:28:12,800
예를 들어, 정확한 문자열 일치에서 모델은 다음과 같은 출력을 제공할 수 있습니다.

307
00:28:12,800 --> 00:28:15,360
목표 출력과 같은 것은 없습니다.

308
00:28:15,360 --> 00:28:20,400
따라서 모델이 인사하도록 하려고 하는데 무작위 문자열처럼 출력된다면,

309
00:28:20,400 --> 00:28:22,440
점수는 0이 될 것입니다.

310
00:28:22,440 --> 00:28:26,400
그리고 여러분 중 일부는 더 큰 모델을 좋아하고 그것을 출력하려고 합니다.

311
00:28:26,400 --> 00:28:30,000
hello 하면 HELLU가 출력됩니다.

312
00:28:30,000 --> 00:28:31,120
꽤 가까운 것 같습니다.

313
00:28:31,120 --> 00:28:36,000
원하는 출력과 거의 비슷하지만 약간 벗어난 것 같습니다.

314
00:28:36,000 --> 00:28:37,120
여전히 0점을 받습니다.

315
00:28:37,120 --> 00:28:40,680
따라서 이 두 출력 사이에는 실제로 차이가 없습니다.

316
00:28:40,680 --> 00:28:45,120
질적으로 두 번째는 훨씬 더 좋고 완벽한 반응은 다음과 같습니다.

317
00:28:45,120 --> 00:28:46,120
일.

318
00:28:46,120 --> 00:28:52,000
그래서 당신은 매우 불연속적인 측정항목이 있는 것처럼 보입니다.

319
00:28:52,000 --> 00:28:58,400
그래서 그들은 이러한 작업 중 일부가 교차 엔트로피를 유발하는 정보를 숨기고 있다고 말할 수도 있습니다.

320
00:28:58,400 --> 00:29:06,760
손실이 포착되고 있으며 아마도 이것이 금액 간의 원활한 관계를 볼 수 있는 이유일 것입니다.

321
00:29:06,760 --> 00:29:12,760
계산과 교차 엔트로피 손실은 매우 불연속적이지만

322
00:29:12,760 --> 00:29:16,000
컴퓨팅 양과 이러한 작업 중 일부.

323
00:29:16,000 --> 00:29:20,800
하지만 그들은 응급 능력이 관찰되기 때문에 그렇지 않을 수도 있다고 말합니다.

324
00:29:20,800 --> 00:29:25,760
분류 작업에 대해서도 마찬가지입니다.

325
00:29:25,760 --> 00:29:31,160
하지만 내 생각엔, 네, 나중에 이 내용을 다루겠지만, 이러한 분류 작업은

326
00:29:31,160 --> 00:29:35,960
또한 어느 정도의 정보를 숨깁니다.

327
00:29:35,960 --> 00:29:41,160
그래서 그들은 또한 당혹감과 창발적 능력 사이의 관계를 살펴봅니다.

328
00:29:41,160 --> 00:29:46,080
그들은 아마도 거기에 어떤 관계가 있을 것이라고 생각합니다.

329
00:29:46,080 --> 00:29:51,360
따라서 빠른 소개와 마찬가지로 당혹감은 기본적으로 모델이 텍스트에 얼마나 놀랐는지를 나타냅니다.

330
00:29:51,360 --> 00:30:01,200
예를 들어, 우리는 이 높은 복잡성 행에서 높은 복잡성을 봅니다.

331
00:30:01,200 --> 00:30:08,040
모델은 이미 퀵 브라운 컬러를 본 적이 있는데 여전히 예쁘네요.

332
00:30:08,040 --> 00:30:09,680
폭스에 놀랐다.

333
00:30:09,680 --> 00:30:15,200
내 생각에 Fox가 다음에 올 것이라고 생각한 것은 확률이 0.05라고 했습니다.

334
00:30:15,200 --> 00:30:19,720
그래서 Fox가 다음에 올 것이라고는 실제로 기대하지 않았습니다.

335
00:30:19,720 --> 00:30:22,680
실제 발언이자 매우 일반적인 발언입니다.

336
00:30:22,680 --> 00:30:28,080
그러나 모델의 당혹감이 낮다면 빠른 갈색 여우를 볼 때쯤에는

337
00:30:28,080 --> 00:30:34,480
거의 확실하거나 빠른 갈색이 보일 때쯤에는 Fox가

338
00:30:34,480 --> 00:30:35,480
다음은 될 것입니다.

339
00:30:35,480 --> 00:30:39,520
그래서 Fox가 나타나도 놀라지 않습니다.

340
00:30:39,520 --> 00:30:49,600
그리고 그들은 음모를 꾸민다.

341
00:30:49,600 --> 00:30:57,600
여기 플롯의 가장 왼쪽에 컴퓨팅과 성능이 표시됩니다.

342
00:30:57,600 --> 00:31:04,280
MLU에서 우리는 그것이 약 10~22플롭에서 이러한 긴급 능력을 가지고 있다는 것을 알 수 있습니다.

343
00:31:04,280 --> 00:31:08,800
그리고 매개변수 수가 약 100억 개에 달하는 중간 그림에서도 이를 볼 수 있습니다.

344
00:31:08,800 --> 00:31:13,560
매개변수, 성능이 갑자기 향상되고 실망스럽게도 똑같을 것 같습니다.

345
00:31:13,560 --> 00:31:17,760
당혹감과 정확성 사이의 관계에 대해서도 마찬가지입니다.

346
00:31:17,760 --> 00:31:24,960
마치 당혹감이 특정 수준으로 줄어들거나, 그래, 특정 수준으로 줄어들면

347
00:31:24,960 --> 00:31:30,560
수준에서는 MLU의 성능이 크게 향상되는 것처럼 보입니다.

348
00:31:30,560 --> 00:31:39,440
다시 말하지만, 당혹감과 성과 사이에는 다소 매끄럽지 못한 관계가 있습니다.

349
00:31:39,440 --> 00:31:41,840
MLU입니다.

350
00:31:41,840 --> 00:31:48,800
따라서 전반적으로 몰입 능력은 많은 관련 변수의 함수로 보아야 합니다.

351
00:31:48,800 --> 00:31:54,440
그것은 아닙니다. 우리가 본 것처럼 단지 얼마나 많은 계산을 던지느냐에 따라 결정되는 것이 아닙니다.

352
00:31:54,760 --> 00:31:57,200
그것에.

353
00:31:57,200 --> 00:32:06,760
그리고 그들이 하는 분석은 실제로 이러한 능력이 나타나는 이유에 대한 근본 원인을 파악하지 못합니다.

354
00:32:06,760 --> 00:32:13,320
그리고 모형 규모나 금액 등을 이용하여 손실을 예측할 수 있음에도 불구하고

355
00:32:13,320 --> 00:32:20,440
데이터와 컴퓨팅이 부족하기 때문에 손실을 사용하여 모델이 가질 수 있는 능력을 예측할 수 없습니다.

356
00:32:20,440 --> 00:32:28,120
이는 안전에 중요한 영향을 미칠 수 있습니다. 할 수만 있다면 전혀 알 수 없습니다.

357
00:32:28,120 --> 00:32:34,440
모델이 달성하는 손실만 고려하면 모델이 무엇을 할 수 있게 될까요?

358
00:32:34,440 --> 00:32:37,240
좋아요.

359
00:32:37,240 --> 00:32:46,240
그렇다면 Schaefer et al.의 이 논문은 다음과 같습니다. 253에서는 몰입 능력이 신기루인지 물었습니다.

360
00:32:46,240 --> 00:32:52,160
그래서 그들은 대규모 언어에서 새로운 능력이 갑작스럽게 출현하는 것이 분명하다고 주장하려고 합니다.

361
00:32:52,160 --> 00:32:57,360
모델은 실제로 모델을 측정하기 위해 선택한 측정항목의 결과입니다.

362
00:32:57,360 --> 00:33:01,040
즉, 실제 불연속성과 능력이 없습니다.

363
00:33:01,040 --> 00:33:06,320
연구자가 선택한 측정값의 불연속성일 뿐입니다.

364
00:33:06,320 --> 00:33:16,480
따라서 이것을 명시적으로 주장하는 방식으로 비선형 불연속 측정법을 선택하면

365
00:33:16,480 --> 00:33:24,880
예를 들어 Carter가 논의한 것처럼 전체 단어 또는 완전한 응답의 정확성은 다음과 같습니다.

366
00:33:24,880 --> 00:33:33,280
선형 연속 메트릭이 점프를 표시하지 않을 때 이러한 갑작스러운 점프를 표시할 수 있습니다.

367
00:33:33,520 --> 00:33:40,960
그들은 이것이 대다수의 공개된 벤치마크에 출현했다는 주장을 설명한다고 주장합니다.

368
00:33:40,960 --> 00:33:47,280
예를 들어 그들은 이 점을 잘 이해하려고 노력합니다.

369
00:33:47,280 --> 00:33:51,520
그런 다음 그들은 선택을 변경하여 명백히 자신만의 창발적인 능력을 만들어내려고 노력합니다.

370
00:33:51,520 --> 00:33:57,200
불연속적이거나 비선형적인 것에 대한 미터법입니다.

371
00:33:57,760 --> 00:34:04,880
그래서 특히 그들은 대규모 작업을 살펴보면 몰입도가 92% 이상이라는 것을 발견했습니다.

372
00:34:04,880 --> 00:34:11,680
큰 벤치에서의 능력은 이 두 가지 지표 중 하나인 객관식 등급에서 나옵니다.

373
00:34:12,880 --> 00:34:19,360
이는 가장 높은 확률 질량이 올바른 옵션 또는 정확한 문자열 일치에 있어야 함을 의미합니다.

374
00:34:19,360 --> 00:34:23,280
이는 출력 문자열이 대상 문자열과 정확히 일치하는 경우입니다.

375
00:34:23,280 --> 00:34:27,920
따라서 객관식 등급은 분명히 불연속적입니다.

376
00:34:27,920 --> 00:34:33,440
두 옵션의 확률 질량이 같을 때 불연속성이 있습니다.

377
00:34:34,480 --> 00:34:41,920
정확한 문자열 일치를 통해 토큰별 모델 정확도에 따라 이것이 어떻게 변하는지 조사합니다.

378
00:34:45,600 --> 00:34:53,040
일종의 이론적 모델로서 그들은 교차 엔트로피에 대한 표준 스케일링 법칙을 가정합니다.

379
00:34:53,040 --> 00:35:00,560
토큰 단위의 손실을 보면 이런 불쾌한 지수 관계가 있음을 알 수 있습니다.

380
00:35:00,560 --> 00:35:08,720
정확성과 토큰별 정확성 사이에 있습니다.

381
00:35:11,600 --> 00:35:20,480
그래서 그들은 열악한 모델에서 많은 샘플을 취하면

382
00:35:21,280 --> 00:35:29,040
완벽한 일치를 결코 볼 수 없을 수도 있습니다. 그래서 당신은 그보다 더 많은 샘플을 채취해야 할 것입니다.

383
00:35:31,280 --> 00:35:37,520
일반적으로 올바른 행동의 사례를 관찰하기 위해 취해질 수 있습니다.

384
00:35:37,520 --> 00:35:44,160
모델에 능력이 전혀 없음을 나타냅니다. 실제로는 모델이 만들고 있는 것일 수도 있습니다.

385
00:35:44,160 --> 00:35:47,440
해당 능력을 갖추는 방향으로 발전하지만 모든 토큰을 올바르게 얻을 수는 없습니다.

386
00:35:48,320 --> 00:35:55,040
따라서 그들이 제안하는 것은 훨씬 덜 불쾌한 관계를 갖는 토큰 편집 거리를 대신 사용하는 것입니다.

387
00:36:03,920 --> 00:36:14,480
네, 그래서 토큰 수의 이런 종류의 힘으로 확장하는 대신

388
00:36:15,200 --> 00:36:20,720
토큰 수에 곱하면 됩니다.

389
00:36:24,160 --> 00:36:35,040
이 위치는 기하급수적입니다. 그래서 그들은 몇 가지 예를 살펴봅니다. 예를 들어,

390
00:36:35,040 --> 00:36:39,920
InstructGPT 또는 GPT-3 제품군은 신흥 지구 특성을 가지고 있다고 가정됩니다.

391
00:36:40,800 --> 00:36:45,040
하지만 그들이 제안한 대로 정확한 문자열 일치 토큰 편집 거리에서 전환하면

392
00:36:45,840 --> 00:36:48,640
이러한 능력은 갑작스러운 것이 아니라 점진적으로 나타납니다.

393
00:36:49,040 --> 00:37:12,320
그렇죠. 그래서 그들은, 그래서 이것으로부터 얻은 교훈은 대부분의 응급 능력 사례가

394
00:37:12,320 --> 00:37:16,000
측정항목에 따라 달라질 뿐입니다. 그리고 난 가지도 않았어, 이렇게 많이 들어가지도 않았어,

395
00:37:16,000 --> 00:37:23,120
하지만 이것은 일종의 장난감 모델이지만 비전에 대한 핵심 측정항목을 의도적으로 선택함으로써

396
00:37:23,120 --> 00:37:31,120
작업을 수행하면 이러한 명백히 긴급한 행동을 유도할 수 있습니다. 그렇습니다. 그러나 그것은 또한 가치가 있습니다

397
00:37:31,120 --> 00:37:36,400
일부 후보의 긴급 능력이 남아 있다는 점에 주목합니다. 예를 들어 파란색과 정규화된

398
00:37:36,400 --> 00:37:42,960
눈에 띄게 불연속적이거나 비선형적이지 않은 집계 점수 측정항목은 여전히 ​​표시됩니다.

399
00:37:42,960 --> 00:37:49,040
성능이 갑자기 급격히 증가합니다. 좋아요.

400
00:37:50,720 --> 00:37:57,600
좋아요. 그러면 역스케일링이 언제 U자 모양이 될 수 있는지 살펴보겠습니다. 그래서

401
00:37:58,400 --> 00:38:02,720
먼저 역스케일링과 그것이 무엇인지부터 살펴보겠습니다. 그런 다음

402
00:38:04,240 --> 00:38:09,840
U자형 스케일링에 대해 말씀드리겠습니다. 여기서 주요 질문은 저울이 무엇을 하는지에 관한 것입니다.

403
00:38:09,840 --> 00:38:18,080
특정 작업에 대한 역 스케일링이 보류됩니까? 그래서 Mackenzie 2022는 이번 대회를 개최했습니다.

404
00:38:18,080 --> 00:38:25,760
더 큰 모델의 성능이 더 나빴던 작업을 찾아보세요. 그리고 그들은 11개의 작업을 찾을 수 있었습니다

405
00:38:25,760 --> 00:38:32,720
작은 모델의 경우 주문과 같이 훨씬 큰 모델보다 훨씬 더 나은 성능을 발휘했습니다.

406
00:38:32,720 --> 00:38:40,080
크기가 더 큰 모델. 그리고 이러한 작업 중 하나는 Modus Tolens 분류였습니다. 그래서 모드

407
00:38:40,080 --> 00:38:48,560
tolens는 이것입니다. 구조는 P 다음 Q인 경우이고, Q가 아닌 경우이므로 P가 아닙니다. 그리고 현재, 현재

408
00:38:48,560 --> 00:38:55,840
틀이 있는 모델은 논리적인 주장뿐만 아니라 방식의 tolens 주장으로 모델을 제시합니다.

409
00:38:55,840 --> 00:39:03,520
구조이지만 실제 예와 같으며 유효한지 물어보십시오. 그래서 인간은 종종

410
00:39:03,520 --> 00:39:12,240
이것에 대한 실수. 그들은 그것이 실제로 유효하면서도 유효하지 않다고 생각합니다. 그래서 그들이 찾은 것은

411
00:39:12,240 --> 00:39:18,640
그들의 가설은 이 작은 모델이 인간을 모방하는 법을 배운다는 것입니다.

412
00:39:19,600 --> 00:39:35,600
그러나 더 큰 모델은 오류를 수정하는 방법을 배웁니다. 그럼 2022년에는 이 11가지를 살펴보았습니다.

413
00:39:35,600 --> 00:39:44,800
작업. 그리고 그들은 계속해서 규모를 더욱 늘리면 결국 성능이 향상된다는 사실을 발견했습니다.

414
00:39:45,360 --> 00:39:52,720
다시 올라갈 것입니다. 그리고 이는 11개 작업 모두에 대해 거의 전반적으로 적용되었습니다. 그래서 당신은

415
00:39:52,720 --> 00:40:01,920
Modus Tolens가 작은 녹색으로 강조 표시된 상자에 표시되는 것을 볼 수 있습니다. 그래서 그들이 이것을 훈련시켰을 때

416
00:40:01,920 --> 00:40:11,200
손바닥 모델은 훈련을 많이 시키거나 크기를 늘리면 처음에는 아래로 내려가서 시작하고

417
00:40:11,200 --> 00:40:21,920
그러다가 결국 올라가기 시작해요. 그래서 그들은 이것을 설명합니다. 여기에 논문을 인용하겠습니다. 그들은 말한다

418
00:40:21,920 --> 00:40:27,760
우리는 U자형 스케일링이 어떻게, 왜 발생하는지 실험적으로 조사하지는 않지만,

419
00:40:27,760 --> 00:40:34,160
작업에 방해가 되는 작업이 포함되어 있을 때 발생할 수 있습니다. 그래서 이 주의를 산만하게 하는 작업은 앞서 언급했듯이

420
00:40:34,480 --> 00:40:42,880
이 방식의 Tolens를 사용하면 산만한 작업은 인간의 결함 있는 추론을 모방하는 것과 같습니다. 그리고

421
00:40:42,880 --> 00:40:49,280
그러면 이것이 모델이 수행하기를 원했던 실제 작업은 모드가 허용되는지 여부를 알려주는 것입니다.

422
00:40:49,280 --> 00:40:55,040
정확합니다. 하지만 이렇게 작은 모델은 모방 작업에 주의가 산만해집니다.

423
00:40:55,040 --> 00:41:09,120
인간의 오류. 따라서 이러한 작업 중 일부에 대한 대규모 모델도 무작위로 수행될 수 있습니다. 그래서 반대

424
00:41:09,120 --> 00:41:15,680
성능이 무작위로 시작된 다음 무작위보다 나빠지고 U가 되는 경우 스케일링이 발생합니다.

425
00:41:15,680 --> 00:41:20,800
모양 스케일링은 성능을 결국 무작위로 되돌릴 수 있습니다. 하지만 무작위는 여전히

426
00:41:20,800 --> 00:41:28,320
대단한 성능은 아닙니다. 정말 나쁘다. 따라서 이러한 역 스케일링 작업은 해결되지 않았지만 제 생각에는

427
00:41:28,320 --> 00:41:34,560
여기서 설명하는 바는 계속 확장하면 성능이 향상될 것이라는 희망이 있다는 것입니다.

428
00:41:34,560 --> 00:41:43,200
계속 올라가세요. 그들은 이러한 역에 대한 성능을 향상시키는 몇 가지 방법을 살펴봅니다.

429
00:41:43,200 --> 00:41:49,600
확장 작업. 특히 그들은 모델을 제공하면서 상황에 맞는 학습을 사용하는 방법을 살펴봅니다.

430
00:41:49,920 --> 00:41:54,480
작업을 올바르게 수행하는 예입니다. 그리고 모델이 수행하도록 유도하는 방법도 살펴봅니다.

431
00:41:54,480 --> 00:42:03,280
사고 추론의 사슬. 그리고 그들은 전반적으로 당신이 사용하면, 기부하면

432
00:42:03,280 --> 00:42:13,280
모델은 하나의 예라고 생각합니다. 예, 성능에 약간 도움이 되는 것 같습니다. 하지만,

433
00:42:13,360 --> 00:42:19,440
예를 들어 모델이 일련의 사고 추론에 참여하도록 유도하면

434
00:42:20,080 --> 00:42:25,200
성능에 꽤 도움이 됩니다. 그리고 거의 전반적으로 모델을 전반적으로 가져옵니다.

435
00:42:25,200 --> 00:42:33,920
예, 모델을 평균 이상으로 만듭니다. 여기서 중요한 점은 규모가 커지는 것입니다.

436
00:42:34,960 --> 00:42:42,000
이러한 역 스케일링, 이러한 역 스케일링 인스턴스를 U자형 스케일링으로 바꿀 수 있습니다.

437
00:42:43,280 --> 00:42:49,840
그리고 역 스케일링은 다음과 같은 많은 작업에 있어서 큰 문제가 아닐 수도 있습니다.

438
00:42:49,840 --> 00:42:54,960
결국 성능이 향상되기 시작했습니다. 하지만 아직 개선의 여지가 있습니다.

439
00:42:56,320 --> 00:43:03,520
방금 언급한 것처럼 매개변수를 5,400억 개까지 확장해도 여전히 얻을 수 있는 것은 다음과 같습니다.

440
00:43:03,520 --> 00:43:10,560
이러한 작업 중 일부를 무작위로 수행합니다. 어쩌면 원샷 같은 기술을 사용하는 것일 수도 있습니다.

441
00:43:10,560 --> 00:43:17,360
촉구하고 일련의 생각을 하는 것이 도움이 될 수 있습니다. 그래서 다음 논문에서는

442
00:43:19,200 --> 00:43:27,600
다음을 사용하여 확장 법칙을 초월하거나 예상 성능보다 더 나은 성능을 얻을 수 있다면

443
00:43:27,600 --> 00:43:34,640
새로운 훈련 기술. 그래서 그들은 처음으로 수강하는 새로운 훈련 절차를 제안합니다.

444
00:43:35,440 --> 00:43:42,560
손바닥 기반 모델을 사용하고 동일한 데이터에 대해 추가로 0.1% 컴퓨팅을 사용하여 학습합니다.

445
00:43:42,560 --> 00:43:48,240
접두사 언어 모델링과 연안 범위 손상을 결합한 목표입니다. 그래서

446
00:43:48,240 --> 00:43:56,640
채우기 작업과 채우기 작업은 기본적으로 당신이 얻지 못하는 곳입니다.

447
00:43:57,360 --> 00:44:04,720
문자열 중간에서 단어나 시퀀스를 꺼내어 모델을 다음과 같이 가져옵니다.

448
00:44:04,720 --> 00:44:09,920
그것을 채워라. 따라서 접두사 언어 모델링과 달리 양쪽에 컨텍스트가 있습니다.

449
00:44:09,920 --> 00:44:15,120
컨텍스트는 처음에만 제공됩니다. 특히 그들은 다음과 같은 조합을 사용합니다.

450
00:44:15,120 --> 00:44:24,240
이 채우기 작업을 위한 디노이저입니다. 여기에는 정기적인 노이즈 제거가 포함됩니다. 그래서 소음은 그냥

451
00:44:24,320 --> 00:44:28,880
범위 또는 시퀀스로 샘플링되고 센티넬 토큰으로 대체됩니다. 그리고 이러한 범위는

452
00:44:29,680 --> 00:44:34,160
평균 3, 평균 길이 3 및 손상으로 균일하게 샘플링되었습니다.

453
00:44:34,720 --> 00:44:41,840
전체 비율은 15%. 그리고 그들은 또한 부패율이 최대인 곳에서는 극도의 노이즈 제거를 사용합니다.

454
00:44:41,840 --> 00:44:50,400
50%이며 누락된 토큰의 범위는 최대 32개까지 가능합니다.

455
00:44:50,400 --> 00:44:58,000
노이즈 제거: 텍스트 시작 부분부터 무작위로 샘플링된 지점까지 항상 노이즈가 샘플링됩니다.

456
00:44:58,000 --> 00:45:08,320
텍스트에서. 그리고 그들은 이 방법이 접두사만 사용하는 것보다 훨씬 낫다는 것을 발견했습니다.

457
00:45:08,320 --> 00:45:16,720
언어 모델링. 따라서 컴퓨팅 비용을 크게 절약할 수 있습니다. 따라서 5,400억 개의 매개변수 모델의 경우

458
00:45:16,720 --> 00:45:24,800
절감액은 약 2배입니다. 따라서 이것은 훈련과 동일하거나 다음과 같습니다.

459
00:45:25,920 --> 00:45:32,160
약 450만 TPU 시간을 절약합니다. 그래서 여러분은 이 그래프를 볼 수 있습니다

460
00:45:32,160 --> 00:45:38,480
두 가지 방법으로. 왼쪽 그래프에서 고정된 컴퓨팅 예산이 다음과 같이 있다고 말할 수 있습니다.

461
00:45:39,280 --> 00:45:51,840
셋째, 23번의 훈련이 실패했습니다. Tay et al이 제안한 이런 종류의 훈련 방법을 사용하면,

462
00:45:52,720 --> 00:46:02,160
예를 들어, 점수가 꽤 많이 나간다는 것을 알 수 있습니다. 그래서 이것에 대한 당신의 성과

463
00:46:03,040 --> 00:46:09,840
NLP 작업에서는 눈에 띄는 개선이 있는 것을 볼 수 있습니다. 하지만 당신은 또한 볼 수 있습니다

464
00:46:09,840 --> 00:46:15,200
다른 방법으로 접근하세요. 그리고 우리가 특정 수준의 성능을 얻고 싶다고 말할 수 있습니다.

465
00:46:15,200 --> 00:46:20,320
그 수준의 성과를 얻으려면 얼마나 많은 훈련이 필요합니까? 그리고 여기가 당신이 있는 곳이에요

466
00:46:20,320 --> 00:46:30,880
그 2x 숫자를 얻으세요. 그래서 당신이 66을 얻고 싶다고 말했듯이 이러한 NLP 작업에 대한 이 측정항목은 66입니다.

467
00:46:30,880 --> 00:46:38,560
그리고 UPOM을 사용하면 훈련의 절반도 안 되는 훈련으로 절반 정도의 훈련으로 할 수 있습니다.

468
00:46:38,560 --> 00:46:45,600
이런 종류의 추가 입력 없이 일반적인 POM 교육에만 필요한 것처럼

469
00:46:46,240 --> 00:46:53,760
훈련. 따라서 여기에서 확장 법칙이 자동 회귀 훈련에만 적용될 수 있다는 주요 내용은 다음과 같습니다.

470
00:46:53,760 --> 00:47:01,040
또는 접두사 언어 모델링, 어쩌면 그럴 수도 있습니다. 어쩌면 우리가 새로운 훈련 계획을 생각해낸다면 말이죠.

471
00:47:01,040 --> 00:47:09,520
UL2처럼 예상치 못한 추가 성능을 얻을 수도 있습니다. 그래서 그것은 일종의, 예, 아마도 모델 성능일 것입니다.

472
00:47:09,520 --> 00:47:20,160
우리가 생각했던 것만큼 예측할 수는 없습니다. 그게 전부입니다. 그리고 이건,

473
00:47:20,960 --> 00:47:22,800
난 단지, 당신은 질문을 할 수 없습니다.

474
00:47:22,800 --> 00:47:26,640
아, 5분 안에 질문이요. 시청 해주셔서 감사합니다.

475
00:47:27,200 --> 00:47:28,000
네, 고마워요.

476
00:47:28,000 --> 00:47:32,080
이것이 사람들에게 도움이 되었기를 바랍니다.