1
00:00:00,000 --> 00:00:11,520
안녕하세요 여러분. 제 이름은 Hossein입니다. 우리 수업에서 발표할 내용에 대해 이야기하고 싶습니다.

2
00:00:11,520 --> 00:00:20,080
이는 Mamba 모델을 소개하는 상태 공간 모델입니다.

3
00:00:20,080 --> 00:00:26,080
따라서 우리가 이야기하고 있는 문제의 종류와 배경 지식을 조금 알려 드리고자 합니다.

4
00:00:26,080 --> 00:00:33,040
여기는 기본적으로 시퀀스 대 시퀀스 모델링이므로 입력이 다음과 같다고 상상할 수 있습니다.

5
00:00:33,040 --> 00:00:38,160
일련의 토큰, 일련의 이미지 또는 기타 양식이 될 수 있습니다.

6
00:00:38,160 --> 00:00:48,680
이 입력을 원하는 출력으로 변환하고 그 사이에 다른 것을 사용할 수 있습니다

7
00:00:48,680 --> 00:00:53,680
일종의 아키텍처이므로 예를 들어 실제로 이 작업을 수행하기 위해 RNN을 생각할 수 있습니다.

8
00:00:53,680 --> 00:01:02,800
당신이나 컨볼루셔널 네트워크가 당신이나 변환기와 함께 이 작업을 수행할 수 있도록

9
00:01:02,800 --> 00:01:14,800
하지만 이 데이터 공간 모델을 위해 Mamba는 이 작업을 보다 효율적으로 수행하려고 합니다.

10
00:01:14,800 --> 00:01:26,840
그리고 그것은 기본적으로 제가 말했듯이 더 효율적이려고 노력하는 새로운 아키텍처입니다.

11
00:01:26,840 --> 00:01:35,320
세부 사항을 설명하기 전에 약간의 배경 지식을 제공하기 위해 계산 과정에서

12
00:01:35,320 --> 00:01:43,080
아키텍처 및 이러한 모델에 대한 일부 이론, 특히 기초 모델

13
00:01:43,080 --> 00:01:48,400
대규모 언어 모델은 대규모 데이터에 대해 사전 훈련된 다음 미세 조정됩니다.

14
00:01:48,400 --> 00:01:56,880
사람들이 일반적으로 하는 다운스트림 작업과 이러한 기반의 핵심 요소

15
00:01:56,880 --> 00:02:04,360
모델은 거의 모든 모델에서 사용되는 주요 아키텍처라는 것입니다.

16
00:02:04,360 --> 00:02:10,320
사람들이 변압기를 사용하는 주된 이유는 변압기에 대한 핵심적인 관심 때문입니다.

17
00:02:10,320 --> 00:02:19,640
실제로 이전 토큰을 회수하고 일부 작업을 수행할 수 있는 기능이 있는 레이어

18
00:02:19,640 --> 00:02:29,880
콘텐츠 학습에서는 비용이 많이 들지만 예를 들어 숫자에서 볼 수 있듯이

19
00:02:29,880 --> 00:02:34,880
실제로 이러한 대규모 언어 모델에 전달할 수 있는 토큰은 제한되어 있습니다.

20
00:02:34,880 --> 00:02:45,280
1024 또는 그보다 낮은 값으로, 이는 주의 메커니즘이 매우 비용이 많이 들기 때문에 2차식이므로

21
00:02:45,280 --> 00:02:53,200
주의 매트릭스를 얻기 위해 시퀀스 길이 n을 입력하면 수행해야 할 작업

22
00:02:53,200 --> 00:03:04,640
n 제곱 곱셈은 매우 비용이 많이 드는 작업이고 우리 모두가 컴퓨터 과학에 종사하고 있기 때문에

23
00:03:04,640 --> 00:03:10,760
우리는 런타임 분석에 있어 n 제곱이 그다지 바람직한 숫자가 아니라는 것을 알고 있습니다.

24
00:03:10,760 --> 00:03:20,600
기본적으로 이러한 문제를 해결하기 위해 상태 공간 시퀀스 모델을 구조화하고

25
00:03:20,600 --> 00:03:25,920
비슷한 장비로 비슷한 작업을 실제로 수행할 수 있는 대체 아키텍처를 생각해 보세요.

26
00:03:26,000 --> 00:03:38,880
정확도는 높지만 하위가 많다고 말했듯이 비용은 더 저렴합니다. 이차 시간 미만의 시간도 많습니다.

27
00:03:38,880 --> 00:03:46,000
선형 주의 게이트 컨볼루션 및 순환 모델과 같은 아키텍처를 사용하여

28
00:03:46,000 --> 00:03:55,440
실제로 변환기를 대체하여 계산상의 비효율성과 구조에 대해 무엇을 말할 수 있습니까?

29
00:03:55,920 --> 00:04:03,680
상태 공간 모델은 그것들이 어떻게든 순환의 조합으로 해석될 수 있다는 것입니다.

30
00:04:03,680 --> 00:04:10,400
신경망과 컨볼루셔널 신경망이 있으며 이 강연 내내 우리가 어떻게 할 수 있는지 살펴보겠습니다.

31
00:04:10,400 --> 00:04:17,440
실제로 이런 종류의 해석이 도출되고 그 도출에 대한 세부 사항을 살펴보겠습니다.

32
00:04:17,760 --> 00:04:27,920
하지만 제가 말했듯이 구조 공간 모델의 주요 동기는 다음과 같습니다.

33
00:04:30,480 --> 00:04:37,360
예를 들어 수백만 개의 시퀀스를 실제로 처리할 수 있도록 더 긴 범위의 시퀀스에 대한 작업을 수행합니다.

34
00:04:37,360 --> 00:04:45,440
토큰을 입력으로 사용하므로 우리가 볼 수 있는 최대 시퀀스의 크기는 이러한 대규모

35
00:04:45,440 --> 00:04:53,280
언어 모델은 백만 개로 확장될 수 있으며, 캡처할 수 있는 용량도 있어야 합니다.

36
00:04:53,280 --> 00:05:00,000
수만 단계에 걸쳐 종속성이 있지만 우리가 보게 되듯이 많은 어려움이 있습니다.

37
00:05:00,000 --> 00:05:08,480
동일한 종류의 정확도를 충족하려는 경우 특히 이러한 s4 모델과 함께 제공됩니다.

38
00:05:08,480 --> 00:05:14,720
트랜스포머 모델이 가지고 있는 성능과 제가 다룰 다음 모델은

39
00:05:14,720 --> 00:05:26,480
mamba는 실제로 s4의 특정 문제를 해결하려고 노력하고 있습니다.

40
00:05:26,480 --> 00:05:34,240
공간 모델은 긴 시퀀스를 효율적으로 모델링하는 것이며 기본적으로 솔루션이 구축됩니다.

41
00:05:34,240 --> 00:05:43,360
상태 공간 모델을 기반으로 하는 신경망 계층이므로 실제로 rnn을 사용할 수 있습니다.

42
00:05:45,600 --> 00:05:54,000
그리고 아시다시피 rnn에는 내부 상태가 있고 이 내부 상태가 있기 때문에 긴 시퀀스를 모델링합니다.

43
00:05:54,000 --> 00:06:06,800
상태는 실제로 일정하므로 rnn의 추론 시간은 실제로 선형입니다.

44
00:06:06,800 --> 00:06:13,360
주의 계산으로 인해 n 제곱된 변환기이지만 rnn의 문제

45
00:06:13,360 --> 00:06:20,800
훈련 중에는 작업할 수 있는 방식처럼 병렬화할 수 없다는 것입니다.

46
00:06:20,800 --> 00:06:29,600
트랜스포머이므로 훈련에 적합한 시퀀스를 이미 알고 있다면

47
00:06:29,600 --> 00:06:38,400
rnn의 구조 때문에 실제로는 순차적으로 수행해야 합니다.

48
00:06:38,960 --> 00:06:48,400
Rnn 사람들에게는 훈련 비용이 정말 높기 때문에 기능을 구성할 수 있습니다.

49
00:06:48,400 --> 00:06:59,600
트랜스포머로 옮겼으므로 s4는 실제로 이 격차를 해소하려고 시도하므로 좋은 실행 시간을 보내십시오.

50
00:06:59,600 --> 00:07:08,560
추론 시간과 일부 선형에는 마비되는 기능이 있습니다.

51
00:07:08,560 --> 00:07:14,880
훈련 시간이 걸리므로 상태 공간 모델을 사용하여 이 질문에 대답하려고 합니다.

52
00:07:16,960 --> 00:07:23,680
그래서 질문은 상태 공간 모델이 무엇인지입니다. 기본적으로 공간 공간 모델을 고려할 수 있습니다.

53
00:07:24,400 --> 00:07:34,240
매핑하는 일종의 매퍼 또는 기본적으로 하나를 매핑하는 함수라고 말하는 것이 더 좋습니다.

54
00:07:34,240 --> 00:07:46,960
t의 차원 입력 신호 u를 t의 n차원 잠재 상태 x로 변환하므로 x는 n의 차원을 갖습니다.

55
00:07:46,960 --> 00:07:55,680
u는 1차원을 갖고 x를 t의 1차원 출력 신호 y에 투영합니다.

56
00:07:55,680 --> 00:08:07,760
실제로는 제어 시스템에서 나오는 것이므로 이 아이디어는 심지어 수년 전부터 존재했습니다.

57
00:08:07,840 --> 00:08:11,920
딥러닝이 등장했지만

58
00:08:15,600 --> 00:08:19,680
보시다시피 이는 기본적으로 미분 방정식을 기반으로 합니다.

59
00:08:21,440 --> 00:08:32,080
오른쪽의 이 특정 방정식에서 x의 기울기가 다음과 같다는 것을 볼 수 있습니다.

60
00:08:32,800 --> 00:08:46,480
차원 x는 차원 n에 속하며 기본적으로 x와 입력 u, 그리고 출력의 함수입니다.

61
00:08:48,160 --> 00:08:59,440
y는 x에 u의 일부 d를 더한 투영입니다. 가끔 저는 특히 이 논문에서 이렇게 생각합니다.

62
00:09:00,400 --> 00:09:08,080
그들은 실제로 d를 무시하고 그들의 공식화에서 ab와 c에 대해서만 이야기합니다.

63
00:09:09,280 --> 00:09:15,360
방정식을 1차원으로 단순화했지만 다음 사항도 고려해야 합니다.

64
00:09:15,360 --> 00:09:22,800
입력 및 1차원 출력 신호를 일반화할 수도 있으므로 이를 고려할 수 있습니다.

65
00:09:22,880 --> 00:09:27,680
여기서 우리가 말하는 것은 다차원 입력 및 출력에 대한 것입니다.

66
00:09:31,120 --> 00:09:42,400
딥러닝의 관점에서 이 전체 방정식을 보려면 abc와 d를 보면 됩니다.

67
00:09:42,400 --> 00:09:48,000
경사하강법을 통해 훈련 중에 학습하게 될 매개변수로서 제가 말했듯이

68
00:09:48,080 --> 00:09:55,520
단순함은 기본적으로 나머지 대화에서 d를 버릴 수 있습니다.

69
00:09:58,720 --> 00:10:06,480
따라서 이 그림은 실제로 관계를 보여주고 행렬의 중요성을 보여줍니다.

70
00:10:07,040 --> 00:10:16,080
실제로 어떻게든 그것은 기본적으로 이전 주의 역사를 보존했습니다.

71
00:10:16,080 --> 00:10:29,680
그리고 내가 말했듯이 ssm은 1960년대에 제어 모델에 처음 소개되었고 내가 언급했듯이 x는 많은 기능을 가지고 있습니다.

72
00:10:29,680 --> 00:10:38,640
우리의 입력과 출력인 u와 y보다 더 높은 차원이고 여기서 논의하는 모든 것은

73
00:10:39,040 --> 00:10:47,360
uy와 x는 여기서 연속이고 시간의 함수이며 이를 어떻게 이산화할 수 있는지 살펴보겠습니다.

74
00:10:47,360 --> 00:10:53,360
왜냐하면 우리가 딥러닝에서 시퀀스 모델링을 다루고 있는 것은 우리가 말하는 것이 아니기 때문입니다.

75
00:10:53,440 --> 00:10:57,360
연속함수 이산함수

76
00:11:02,240 --> 00:11:12,160
네, 기본적으로 연속 작업과 같은 다양한 종류의 작업에 ssm을 사용할 수 있습니다.

77
00:11:12,720 --> 00:11:18,800
연속 표현, 반복 표현, 컨볼루셔널 표현

78
00:11:18,800 --> 00:11:22,400
각 항목을 자세히 살펴보겠습니다.

79
00:11:26,800 --> 00:11:37,120
이제 연속 SSM 공식을 실제로 이산화하여 실제로 다음을 수행할 수 있는 방법에 대해 이야기해 보겠습니다.

80
00:11:38,640 --> 00:11:42,240
그걸 딥러닝에도 활용하는 거죠

81
00:11:42,800 --> 00:11:50,000
그 후에 몇 가지 추가 작업을 수행해야 하지만 이것이 첫 번째 단계입니다.

82
00:11:52,960 --> 00:12:00,080
이제 문제는 t의 연속 함수 u를 갖는 대신에 있다는 것입니다.

83
00:12:00,080 --> 00:12:07,920
이제 우리는 u1 u2가 없으므로 기본적으로 몇 가지 개별 입력이 있습니다.

84
00:12:07,920 --> 00:12:17,200
그리고 우리가 하는 방식은 이 연속 함수를 다음과 같이 이산화해야 합니다.

85
00:12:17,200 --> 00:12:25,520
단계 크기 델타이며 기본적으로 입력 해상도를 나타내는 델타를 볼 수 있습니다.

86
00:12:25,520 --> 00:12:33,760
예를 들어 이미지에서 u sub i는 t의 함수 u의 샘플로 볼 수 있습니다.

87
00:12:37,600 --> 00:12:47,520
기본적으로 우리는 이 이산화를 어떻게 유도할 수 있는지 볼 수 있습니다. 제가 방금 제시한 공식이 주어지면

88
00:12:47,520 --> 00:12:56,480
실제로 오일러 이산화를 사용할 수 있다는 것을 보여주었으므로 xt에 단계 크기인 델타를 더한 값은 같습니다.

89
00:12:56,480 --> 00:13:08,320
xt 더하기 델타 곱하기 x 소수는 x의 기울기이므로 기본적으로 다음과 같이 다시 쓸 수 있습니다.

90
00:13:08,400 --> 00:13:19,840
xt 더하기 델타 곱하기 a의 xt 더하기 b 곱하기 u의 t 왜냐하면 여기 x 소수 t는 이것을 갖기 때문이죠

91
00:13:21,040 --> 00:13:32,400
특정 공식이므로 이 우변을 x 프라임 t로 대체하여 이를 얻습니다.

92
00:13:32,400 --> 00:13:41,920
이제 다음 단계는 이전 방정식을 단순화하여 i에 델타 시간을 더하는 것입니다.

93
00:13:43,360 --> 00:13:53,280
a 곱하기 xt 더하기 델타 하지만 기본적으로 이름을 지정할 수 있습니다.

94
00:13:53,680 --> 00:14:05,760
승수를 막대로 지정하고 이름을 b 막대로 지정하면 xt plus에서 시작하는 것을 볼 수 있습니다.

95
00:14:05,760 --> 00:14:14,640
델타는 t 함수의 연속 u를 이산화하는 데 사용하려는 단계 크기입니다.

96
00:14:15,280 --> 00:14:26,160
우리는 실제로 x를 개별 형식으로 표현하기 위해 이 멋진 형식을 도출할 수 있습니다.

97
00:14:32,320 --> 00:14:43,680
그래서 기본적으로 우리는 이 이산화를 일종의 반복적인 것으로 볼 수 있습니다.

98
00:14:43,680 --> 00:14:48,880
순환 신경망에서 우리가 가지고 있는 표현

99
00:14:52,320 --> 00:15:00,800
그래서 기본적으로 여기서 일어난 일 대신 이 이산화로 인해

100
00:15:00,800 --> 00:15:08,800
t의 연속 함수 u와 t의 y로 작업하는 중 이제 u sub k와 y sub k로 작업합니다.

101
00:15:08,800 --> 00:15:17,040
이는 k가 시간 단계 또는 입력 시퀀스의 각 토큰을 나타냄을 나타냅니다.

102
00:15:19,360 --> 00:15:26,640
이제 볼 수 있듯이 방정식은 다음과 매우 유사하게 x sub k에서 반복됩니다.

103
00:15:26,640 --> 00:15:34,800
RNN에서 사용하는 것과 동일한 용어를 사용하려는 경우

104
00:15:34,800 --> 00:15:42,560
기본적으로 RNN에서 수행되는 숨겨진 상태로 x sub k를 볼 수 있습니다.

105
00:15:44,720 --> 00:15:50,240
제가 말했듯이 RNN의 가장 중요한 점은 더 효율적이고 추론 시간이 있다는 것입니다.

106
00:15:50,240 --> 00:15:55,680
그들은 2차 곱셈을 갖고 있지 않으며 단지 이전 상태만 필요합니다.

107
00:15:55,680 --> 00:16:02,000
이전 상태에서는 출력을 얻을 수 있지만 문제는 출력을 얻을 수 없다는 것입니다.

108
00:16:02,000 --> 00:16:09,760
마비시키므로 훈련에서는 순차적인 모델을 훈련할 수 없습니다.

109
00:16:09,760 --> 00:16:16,480
GPU를 최대한 활용하려면 GPU에 일종의 병렬 계산이 있어야 합니다.

110
00:16:16,480 --> 00:16:25,760
그건 정말 중요한 일이니까 우리가 이 관계를 이해했다는 점을 분명히 하려고요.

111
00:16:25,760 --> 00:16:34,880
SSM과 RNN 사이의 방정식을 쓰면 x sub k는 실제로

112
00:16:36,880 --> 00:16:45,600
막대와 입력을 곱한 이전 상태의 함수

113
00:16:45,920 --> 00:16:59,120
b 막대를 곱한 다음 출력은 행렬 c 막대에 의한 다음 상태의 투영일 뿐입니다.

114
00:17:00,400 --> 00:17:08,960
순환 신경망과 매우 유사합니다. 이제 더 나은 기계적 예를 들어 보겠습니다.

115
00:17:08,960 --> 00:17:16,880
이러한 것들이 실제로 어떻게 작동하는지 이해하려면 스프링으로 벽에 부착된 질량을 고려하세요.

116
00:17:16,880 --> 00:17:27,920
전방 위치 yt를 사용하면 yt는 시간 t에서의 위치를 ​​표시하고 입력인 힘을 갖게 됩니다.

117
00:17:27,920 --> 00:17:35,360
u의 t는 질량에 적용되며 스프링 상수 질량과 같은 다른 매개변수도 있습니다.

118
00:17:35,920 --> 00:17:42,160
현재 존재하는 마찰은 다음 미분 방정식과 모든 것을 연관시킬 수 있습니다.

119
00:17:42,880 --> 00:17:51,360
이는 2차 기울기의 두 번째 기울기인 가속도의 m배입니다.

120
00:17:52,800 --> 00:18:02,320
위치는 t의 힘 u에서 y의 기울기인 속도 b를 뺀 값과 같습니다.

121
00:18:02,320 --> 00:18:10,480
마이너스 kyt이므로 이는 단지 물리적인 공식일 뿐이지만 좋은 점은

122
00:18:10,480 --> 00:18:22,320
실제로 SSM에서 방금 본 ab 및 c 행렬을 사용하여 이 물리적 공식을 다시 작성합니다.

123
00:18:23,280 --> 00:18:35,040
일종의 매개변수를 입력하고 여기에 y와 u, x를 곱하면 됩니다.

124
00:18:35,040 --> 00:18:44,800
동일한 방정식을 구하고 기본적으로 보시다시피 a는 이 모든 용어를 함께 연결하므로

125
00:18:44,880 --> 00:18:56,080
실제로 방정식을 함께 유지하는 데 매우 중요한 역할을 하며 기본적으로 여기서 c는 다음을 보여줍니다.

126
00:18:56,080 --> 00:19:05,840
숨겨진 상태의 첫 번째 차원과 제가 말했듯이 두 번째 차원은 속도입니다.

127
00:19:05,840 --> 00:19:11,920
a는 이 모든 용어를 함께 연결하고 다음은 단지 코드입니다.

128
00:19:12,880 --> 00:19:19,200
SSM을 사용하여 이 전체 방정식을 계산하므로 간단합니다.

129
00:19:22,240 --> 00:19:23,840
실제로 볼 수 있는 코드

130
00:19:26,240 --> 00:19:32,880
내가 참고로 올려놓은 작가 페이지에서 나는 이것에 너무 많은 돈을 쓰고 싶지 않다

131
00:19:34,480 --> 00:19:41,120
이는 SSM이 실제로 이것을 어떻게 모델링하는지에 대한 예일 뿐이므로 k의 힘 u가 주어졌을 때

132
00:19:41,200 --> 00:19:48,240
당신은 얻을 수 있고 위치 yk는 실제로 볼 수 있습니다

133
00:19:50,880 --> 00:19:58,640
SSM이 수행한 예측을 기반으로 개체가 위치할 위치를 확인하세요. 위치가 아쉽습니다.

134
00:19:58,640 --> 00:20:07,120
실제로 SSM에 의해 예측되므로 이 예에서는 숨겨진 두 개가 있는 하나의 SSM만 사용합니다.

135
00:20:07,120 --> 00:20:14,160
상태이지만 딥 러닝 모델에서는 많은 SSM을 함께 쌓을 수 있습니다.

136
00:20:16,880 --> 00:20:22,960
내가 말했듯이 현재 우리가 가지고 있는 공식의 문제점은 다음과 같습니다.

137
00:20:24,480 --> 00:20:33,280
훈련 시간은 기본적으로 여기에서 가장 중요한 문제는 좋아요, 우리는 이 정도가 좋다는 것입니다.

138
00:20:34,000 --> 00:20:43,040
지금까지는 공식을 사용했지만 순차적 모델인 경우 실제로 어떻게 모델을 훈련할 수 있습니까?

139
00:20:43,040 --> 00:20:52,400
실제로 RNN을 일종의 표현으로 변환하려면 여기에서 조금 더 깊이 파고들어 보십시오.

140
00:20:52,400 --> 00:21:04,640
마비될 수 있으며 실제로 우리는 이를 완전히 활용할 수 있는 기회를 갖고 있습니다.

141
00:21:04,640 --> 00:21:11,840
이를 수행하는 한 가지 방법은 실제로 RNN 표현을 바꿀 수 있다는 것입니다.

142
00:21:11,840 --> 00:21:20,880
실제로 그것을 펼쳐서 CNN 표현으로 변환하고 왜 CNN 표현이 더 나은지

143
00:21:20,880 --> 00:21:27,920
RNN 표현은 컨볼루셔널 신경망의 훈련을 실제로 할 수 있기 때문입니다.

144
00:21:27,920 --> 00:21:36,480
동시에 차단된 RNN의 교육 문제가 발생하지 않도록 할 수 있습니다.

145
00:21:39,120 --> 00:21:47,040
순환 SSM은 실제로 이산 컨볼루션으로 작성될 수 있으며 방정식을 살펴보겠습니다.

146
00:21:47,040 --> 00:22:00,000
x sub ka bar + b bar로 시작하여 기본적으로 이것은

147
00:22:01,440 --> 00:22:08,160
우리가 가지고 있는 일반 공식은 y sub k가 c bar kc bar 곱하기 x sub k입니다.

148
00:22:09,040 --> 00:22:19,680
그래서 x naught는 실제로 b bar 곱하기 u naught이고 왼쪽 방정식에 기초하면

149
00:22:20,240 --> 00:22:34,480
x 이하 1을 얻기 위해 막대에 x naught를 곱하고 그렇게 계속하면 실제로 다음을 볼 수 있습니다.

150
00:22:34,480 --> 00:22:42,800
여기에 실제로 나타나는 몇 가지 패턴이 있는데 실제로 매트릭스의 영향을 볼 수 있습니다.

151
00:22:44,160 --> 00:22:47,680
실제로는 실제로 어떤 바

152
00:22:50,400 --> 00:22:57,440
이전 상태의 정보를 새로운 상태로 전파하는 것에 대해 이야기하겠습니다.

153
00:22:57,440 --> 00:23:02,160
하마 행렬에 관해 좀 더 자세히 설명하고 이를 실제로 만드는 방법에 대해 설명합니다.

154
00:23:04,080 --> 00:23:13,360
훨씬 더 효율적으로 기억할 수 있으며 y naught는 실제로 c bar x b bar u naught입니다.

155
00:23:14,720 --> 00:23:24,560
그리고 다시 출력에서도 볼 수 있듯이 막대가 실제로 나타나므로 막대는 매우

156
00:23:24,560 --> 00:23:29,840
이전 상태를 기억하고 회상하는 데 중요한 요소

157
00:23:34,160 --> 00:23:42,960
이제 SSM 모델 교육에 대해 말했듯이 실제로 더 나은 아키텍처를 사용해야 합니다.

158
00:23:42,960 --> 00:23:50,400
훈련이 병렬로 수행될 수 있는지 확인하고 실제로 할 수 있다는 것을 확인했습니다.

159
00:23:50,400 --> 00:24:04,000
전체 방정식을 풀어 이 최종 출력을 얻으세요. 보시다시피 실제로는 다음과 같습니다.

160
00:24:04,000 --> 00:24:16,080
일종의 커널 곱셈은 다음과 같이 정의된 k bar의 커널 곱셈입니다.

161
00:24:16,080 --> 00:24:25,760
입력 입력 토큰에 대해 이와 같이 y는 yk까지 없음

162
00:24:27,760 --> 00:24:36,800
따라서 k bar는 SSM 컨볼루션 커널 또는 필터라고 하며 기본적으로 k bar를 거대한 것으로 볼 수 있습니다.

163
00:24:36,800 --> 00:24:47,920
입력 u에 필터가 적용되었으므로 여기서 가장 큰 문제는 이제 다음과 같은 문제가 있다는 것입니다.

164
00:24:47,920 --> 00:24:56,640
k bar는 거대한 필터이므로 k bar의 계산이 실제로 다음과 같은지 확인해야 합니다.

165
00:24:57,280 --> 00:25:06,800
효율적으로 수행되었지만 여전히 할 수 없는 문제를 해결했습니다.

166
00:25:08,560 --> 00:25:11,840
학습 시간에 병렬 계산을 수행합니다.

167
00:25:17,200 --> 00:25:24,640
따라서 기본적으로 이것으로부터의 주요 시사점은 이제 출력을 계산하지 않고도 계산할 수 있다는 것입니다.

168
00:25:24,640 --> 00:25:33,920
내부 상태이므로 rnn에서 기억하신 것처럼 원하는 경우 출력이 발생하는 문제가 있었습니다.

169
00:25:34,720 --> 00:25:41,920
실제로 시간 단계 t에서 1을 뺀 내부 상태를 갖는 데 필요한 출력을 계산합니다.

170
00:25:42,800 --> 00:25:48,560
따라서 t 마이너스 1 상태의 경우 t 마이너스 1의 내부 상태가 필요합니다.

171
00:25:48,880 --> 00:25:58,080
시간 단계 t에서 2를 뺀 내부 상태이므로 이 순차 계산을 수행해야 했습니다.

172
00:25:58,080 --> 00:26:02,080
하지만 이제 우리는 컨볼루션을 사용하여 이 문제를 해결했습니다.

173
00:26:18,560 --> 00:26:20,020
너

174
00:26:48,560 --> 00:26:50,020
너

175
00:27:18,560 --> 00:27:20,020
너

176
00:27:48,880 --> 00:27:49,380
너

177
00:28:05,440 --> 00:28:09,680
하지만 컨볼루션 정리에 대한 약간의 배경 지식을 제공하기 위해

178
00:28:10,640 --> 00:28:17,440
그것이 어떻게 작동하는지에 대한 광범위한 정의를 제공하기 위해 푸리에 변환이 다음과 같이 말합니다.

179
00:28:17,440 --> 00:28:22,400
두 신호의 컨볼루션이므로 여기서는 연속 함수에 대해 이야기하고 있습니다.

180
00:28:22,400 --> 00:28:28,800
실제로 이산 변환으로 되돌리려면 이 역 1차 푸리에 변환을 수행해야 합니다.

181
00:28:28,800 --> 00:28:34,720
는 푸리에 변환의 산물이므로 두 컨볼루션의 푸리에 변환은

182
00:28:34,720 --> 00:28:43,920
신호는 푸리에 변환과 두 개의 연속 신호 f의 컨볼루션의 산물입니다.

183
00:28:43,920 --> 00:28:47,520
그리고 g는 실제로 이렇게 할 수 있습니다

184
00:28:50,480 --> 00:28:56,560
따라서 두 신호의 곱에 대한 푸리에 변환은 두 신호의 컨볼루션입니다.

185
00:28:57,440 --> 00:29:02,560
푸리에 변환은 우리가 실제로 다음을 만드는 데 사용하는 주요 트릭입니다.

186
00:29:03,440 --> 00:29:05,280
더 효율적인 계산

187
00:29:05,280 --> 00:29:18,080
이것은 실제로 빠른 푸리에 변환을 수행하는 데 사용할 수 있는 원시 코드이므로

188
00:29:20,160 --> 00:29:27,360
따라서 기본적으로 FFT는 직접 회선과 고속 푸리에 중에서 선택하는 부울 플래그가 아닙니다.

189
00:29:27,360 --> 00:29:34,160
변환 기반 컨볼루션이므로 활성화되면 먼저 다음을 수행합니다.

190
00:29:36,480 --> 00:29:41,840
NumPy 라이브러리의 빠른 푸리에 변환

191
00:29:44,240 --> 00:29:54,080
마지막으로 고속 푸리에 변환을 역으로 수행하여 최종 결과를 얻습니다.

192
00:29:58,240 --> 00:30:07,520
지금까지 우리는 사용할 수 있는 일종의 아키텍처에 대해 논의했습니다.

193
00:30:09,120 --> 00:30:15,680
실제로 계산을 더 효율적으로 만들기 위해

194
00:30:17,440 --> 00:30:24,720
긴 시퀀스 장거리 시퀀스 하지만 문제는 이 새로운 종류의 시퀀스입니다.

195
00:30:25,280 --> 00:30:34,640
아키텍처는 장거리 종속성을 해결할 수 있으며 실제로는 다음과 같은 것으로 나타났습니다.

196
00:30:34,640 --> 00:30:43,840
기본 SSM은 실제로 이전에 본 공식 때문에 성능이 매우 좋지 않습니다.

197
00:30:43,840 --> 00:30:49,600
많은 곱셈이 포함되어 있으며 RNN에서 발생하는 일입니다.

198
00:30:50,480 --> 00:30:58,640
행렬 A 막대를 본 것처럼 중첩된 곱셈

199
00:31:00,880 --> 00:31:07,680
시퀀스 길이가 실제로 다음과 같다면 실제로 그 자체가 많이 곱해집니다.

200
00:31:07,680 --> 00:31:13,600
높으면 실제로 그 순서와 시퀀스 길이의 순서가 곱해집니다.

201
00:31:14,560 --> 00:31:19,280
따라서 그라데이션이 사라지거나 폭발하는 문제가 발생할 수 있으며

202
00:31:21,520 --> 00:31:25,680
이는 실제로 장기적인 종속성 문제를 일으킬 수 있습니다.

203
00:31:27,600 --> 00:31:33,520
이 문제를 해결하기 위해 우리는 실제로 이 문제를 해결하기 위해 hippo 행렬을 사용할 수 있습니다.

204
00:31:36,400 --> 00:31:43,120
그래서 기본적으로 이제 우리는 s4의 정의에 도달했다고 생각합니다. 그래서 s4는 무엇입니까?

205
00:31:43,920 --> 00:31:51,920
s4는 SSM 상태 기반 모델에 Hippo와 구조화된 행렬을 더한 것이므로 Hippo가 수행하는 작업은 다음과 같습니다.

206
00:31:51,920 --> 00:31:59,920
매우 간단하므로 SSM의 A 및 B 행렬에 대한 특수 공식을 사용하므로 이는 A입니다.

207
00:31:59,920 --> 00:32:07,120
이는 상태 기반 모델에서 본 공식의 B입니다.

208
00:32:07,120 --> 00:32:15,760
이산 공간에 대한 상태 기반 모델 계산 전개

209
00:32:16,640 --> 00:32:24,080
A는 기본적으로 이전 상태를 캡처한 매트릭스로 볼 수 있으므로 실제로 결정합니다.

210
00:32:24,080 --> 00:32:30,560
정보가 어떻게 다른 상태를 통해 전달되는지는 기본적으로 매우 중요한 매트릭스입니다.

211
00:32:31,120 --> 00:32:46,240
그래서 하마가 하는 일은 실제로 A의 행렬의 특정 클래스를 지정하는 것입니다.

212
00:32:46,960 --> 00:32:54,960
모델이 이전 상태를 기억하는 능력을 유지할 수 있도록 상태를 보존할 수 있습니다.

213
00:32:55,040 --> 00:33:02,720
보시다시피 하마 행렬은 아무것도 아닙니다. 여기 왼쪽에 정의되어 있습니다.

214
00:33:03,360 --> 00:33:05,920
삼각행렬 이외의

215
00:33:10,000 --> 00:33:18,400
여기서 행렬의 이쪽은 0이고 행렬의 이쪽에만 값이 있습니다.

216
00:33:19,280 --> 00:33:29,680
그래서 이런 식으로 하마가 실제로 이전의 정보를 더 잘 보존할 수 있다는 것이 밝혀졌습니다.

217
00:33:29,680 --> 00:33:41,840
기본적으로 다음과 같은 행렬을 보존하는 이런 종류의 행렬을 사용하는 한 가지 예가 있습니다.

218
00:33:41,840 --> 00:33:51,280
이전 상태에서는 순차적 MNIST 분류의 성능을 실제로 향상시킬 수 있습니다.

219
00:33:52,240 --> 00:34:02,400
60%에서 98%로 최대 38%까지 개선되고 하마 매트릭스는 기본적으로

220
00:34:02,400 --> 00:34:12,400
과거 이력 상태를 압축하고 기본적으로 이런 종류를 사용하여 이력을 대략적으로 재구성합니다.

221
00:34:12,400 --> 00:34:25,520
공식이므로 기본적으로 여기서는 함수를 재구성하는 하마의 예를 보고 싶습니다.

222
00:34:26,480 --> 00:34:34,800
여기서 함수 입력 함수는 검은색 선으로 표시된 u이고 여기서 x는 파란색입니다.

223
00:34:34,800 --> 00:34:40,080
보시다시피 더 높은 차원을 가진 선에는 여러 개의 선이 있고 여기의 빨간색 선은

224
00:34:40,080 --> 00:34:49,840
재구성 함수인 yt를 출력하므로 yt가 xt와 동일해지기를 원합니다.

225
00:34:49,840 --> 00:34:59,840
10,000단계 후에 우리는 hippo 행렬을 사용하는 이 SSM 동적이 얼마나 완벽하게 수행될 수 있는지 확인하고 싶습니다.

226
00:35:01,360 --> 00:35:06,160
각 단계에서 볼 수 있도록 함수를 재구성합니다.

227
00:35:08,640 --> 00:35:15,520
ssm이 업데이트되므로 각 단계에서 0부터 10,000까지 공식에 따라 업데이트됩니다.

228
00:35:15,520 --> 00:35:24,160
우리가 본 것과 다른 것들에 대해 여기의 녹색 선은 어떻게 든 일종의 것입니다.

229
00:35:24,160 --> 00:35:34,480
기본적으로 최근 기록이 더 중요하다는 것을 SSM에 알려주는 가중치 매트릭스

230
00:35:34,480 --> 00:35:43,280
나에게는 오래된 것보다 그것이 동일하고 여기서 구체적으로 볼 수 있듯이 유일한 것입니다.

231
00:35:46,480 --> 00:35:53,040
끝쪽으로 10,000걸음에서 찍은 사진입니다. 이것은 10,000걸음에서 찍은 사진입니다.

232
00:35:55,600 --> 00:36:05,120
SSM은 실제로 기능을 완벽하게 재구성할 수 있지만

233
00:36:05,200 --> 00:36:16,000
함수가 시작되면 정확도가 떨어지고 여기서는 64개만 사용했다고 말해야 합니다.

234
00:36:16,720 --> 00:36:25,200
여기 ssm에 대한 숨겨진 상태가 있는데 이는 10,000보다 훨씬 낮기 때문에 정말 놀랍습니다.

235
00:36:25,200 --> 00:36:33,360
모델의 정확도는 이 정도이며 기본적으로 이 테스트는 온라인에서 사용할 수 있습니다.

236
00:36:34,240 --> 00:36:45,440
함수 재구성을 통해 여기에서 더 많은 정보를 볼 수 있지만 문제는

237
00:36:45,440 --> 00:36:53,920
하마 매트릭스가 어떻게 역사를 추적하는지, 내가 말했듯이 특정한 정보를 지닌 숨겨진 상태를 만들어냅니다.

238
00:36:53,920 --> 00:37:01,840
실제로 기록을 기억하고 기본적으로 실제로 유지하는 행렬 A에 대한 정의

239
00:37:01,840 --> 00:37:09,600
범례 다항식의 계수를 추적하고 이러한 계수를 사용하면 실제로 근사할 수 있습니다.

240
00:37:09,600 --> 00:37:19,040
모든 이전 역사 및 범례 다항식은 완전하고 직교하는 다항식 시스템입니다.

241
00:37:19,040 --> 00:37:26,160
여기 오른쪽에는 Wikipedia Wikipedia에서 가져온 첫 번째 6개의 범례 다항식을 볼 수 있습니다.

242
00:37:28,000 --> 00:37:33,440
여기에 또 다른 예가 있으므로 빨간색 선은

243
00:37:38,320 --> 00:37:48,080
입력이고 이것은 다시 우리가 실제로 추적하고 싶은 것과 동일한 재구성입니다.

244
00:37:48,080 --> 00:37:57,120
파란색 선은 실제로 범례 계수이고 하마 행렬은 실제로 이러한 계수를 업데이트합니다.

245
00:37:57,120 --> 00:38:06,000
각 단계에서 실제로 이러한 계수를 합산하면 실제로 다음을 수행할 수 있습니다.

246
00:38:09,040 --> 00:38:17,040
이 파란색 선을 합산하면 실제로 빨간색의 근사치를 얻을 수 있습니다.

247
00:38:18,560 --> 00:38:22,640
그것은 역사의 재구성이다.

248
00:38:26,480 --> 00:38:35,840
그래서 SSM 신경망은 우리가 말한 모든 것들은 실제로 그것을 모델링하는 데 사용할 수 있다는 것입니다.

249
00:38:35,840 --> 00:38:44,720
신경망에는 이제 기본 SSM을 구축하는 데 기본적으로 필요한 모든 구성 요소가 있으므로 가정합니다.

250
00:38:44,720 --> 00:38:52,080
우리는 b 행렬 c 행렬 단계 크기 델타와 스칼라 d를 배울 것입니다.

251
00:38:53,440 --> 00:39:02,080
그리고 하마 행렬은 우리가 정의에서 본 전환 a에 사용됩니다.

252
00:39:03,040 --> 00:39:09,040
오른쪽 코드는 기본적으로 파이토치 구현입니다.

253
00:39:10,000 --> 00:39:19,440
음, 먼저 음 매개변수를 정의합니다. 먼저 이러한 행렬의 모양을 정의한 다음

254
00:39:22,320 --> 00:39:33,200
이것들은 어 먼저 이산화하고 음 그리고 주어진 단계를 기반으로 한 다음 다음을 수행합니다.

255
00:39:33,280 --> 00:39:41,040
어 컨볼루션 어 최종 출력을 얻으려면

256
00:39:48,240 --> 00:39:55,760
보시다시피 여기에 드롭아웃과 레이어 표준을 추가할 수도 있습니다.

257
00:39:55,760 --> 00:39:58,320
이것을 덮어 코드를 사용할 수 있으므로

258
00:40:02,800 --> 00:40:09,840
시간 제한 때문에 음, 제가 말했던 엠니스트 실험이 여기에 있습니다

259
00:40:09,840 --> 00:40:18,960
흰색으로 표시된 초기 몇 개의 픽셀이 주어지면 SSM을 사용하여 실제로 예측합니다.

260
00:40:18,960 --> 00:40:25,360
남은 픽셀은 녹색 선이 실제 실제값을 나타내고 빨간색 선은

261
00:40:25,360 --> 00:40:34,880
SSM의 예측입니다. 보시다시피 빨간색 선이 실제로는 예측입니다. 죄송합니다.

262
00:40:34,880 --> 00:40:42,240
s4에서는 보시다시피 빨간색 선과 녹색 선이 실제로 잘 일치합니다.

263
00:40:42,400 --> 00:40:51,040
하지만 어 또 몇 가지 제한 사항이 있습니다.

264
00:40:52,160 --> 00:41:00,720
SSM 모델 s4 모델은 특히 우리가 가지고 있는 것과 동일한 성능을 발휘하지 못했습니다.

265
00:41:00,720 --> 00:41:08,880
어 트랜스포머에서 볼 수 있는 주요 약점은 콘텐츠 기반 추론을 수행할 수 없다는 것입니다.

266
00:41:08,880 --> 00:41:18,000
그래서 그들은 실제로 이 시퀀스를 제공했는데 실제로 모델은 불가능합니다.

267
00:41:18,000 --> 00:41:25,920
내용과 토큰 간의 관계를 이해하는 것은 어 왜냐하면 당신이 본 것처럼

268
00:41:25,920 --> 00:41:32,640
행렬 ab와 c는 시간 불변이므로 일정 시간 동안 변하지 않습니다.

269
00:41:33,200 --> 00:41:38,400
그리고 mamba는 실제로 이 약점을 구체적으로 해결하려고 노력하고 있습니다.

270
00:41:41,760 --> 00:41:48,320
음, 이제 mamba에 대해 이야기해보겠습니다. mamba는 이전 모델에 비해 몇 가지 개선이 되었습니다.

271
00:41:48,320 --> 00:41:55,040
모델이 선택적으로 정보를 전파하거나 잊을 수 있도록 하는 개별 양식을 도입합니다.

272
00:41:55,040 --> 00:42:02,960
시퀀스 길이에 따라 정보를 전파할 때 기본적으로 선택됩니다.

273
00:42:04,960 --> 00:42:11,040
또한 여기서는 다루지 않을 병렬 알고리즘의 일부 하드웨어를 소개합니다.

274
00:42:11,040 --> 00:42:19,760
시간 제한으로 인해 매우 단순화된 아키텍처를 가지고 있으므로 마지막에 보게 될 것입니다.

275
00:42:19,920 --> 00:42:26,080
어, Mamba의 좋은 점은 추론 속도가 훨씬 빠르다는 것입니다.

276
00:42:26,800 --> 00:42:33,440
변압기에 비해 처리량이 5배 더 높으며 선형 스케일링이 있습니다.

277
00:42:33,440 --> 00:42:42,640
우리가 찾고 있는 것은 기본적으로 이러한 기능을 통해 mamba를 더 큰 용도로 사용할 수 있습니다.

278
00:42:42,640 --> 00:42:51,440
시퀀스 길이를 최대 100만 개로 늘리는 것이 우리의 최종 목표이며 실제로 일부 목표를 달성합니다.

279
00:42:51,440 --> 00:42:58,480
다양한 양식에 걸쳐 최첨단 성능을 제공하며 기본적으로 다음과 비교할 수 있습니다.

280
00:42:58,480 --> 00:43:11,440
트랜스포머 그리고 여기서 주요 디자인 변경 사항이 무엇인지 살펴보겠습니다. 기본적으로 맘바가 핵심입니다.

281
00:43:11,440 --> 00:43:17,600
차이점은 선택 메커니즘이 있어서 효율적으로

282
00:43:17,600 --> 00:43:24,000
입력에 따라 데이터를 선택하므로 s4 디자인에서 본 것

283
00:43:26,800 --> 00:43:34,240
그것은 시간 불변이었고 실제로 특정 입력을 선택하거나 무시하지 않았다는 것입니다.

284
00:43:34,240 --> 00:43:45,040
입력에서 특정 토큰을 가져오므로 mamba는 다음을 허용하는 간단한 선택 메커니즘을 도입합니다.

285
00:43:45,040 --> 00:43:53,440
관련 없는 정보를 필터링하고 관련 정보를 기억하는 모델입니다.

286
00:43:54,240 --> 00:44:03,280
lstm과 매우 유사하므로 lstm에는 게이팅 메커니즘이라는 것이 있습니다.

287
00:44:03,280 --> 00:44:12,560
실제로 모델이 이전 상태 정보를 잊어버리거나 유지할 수 있도록 허용합니다.

288
00:44:13,440 --> 00:44:20,080
그래서 그것은 같은 것입니다. 우리가 볼 때 이것이 이 작품의 주요 핵심입니다.

289
00:44:21,760 --> 00:44:30,400
lstm과 같은 잊혀진 모델이 실제로 다시 살아나고 있으므로 이는 매우 놀라운 일입니다.

290
00:44:30,400 --> 00:44:37,360
이전 세대에 존재했던 이러한 종류의 참신함을 보는 것은 흥미 롭습니다.

291
00:44:38,400 --> 00:44:48,480
신경망 아키텍처는 실제로 mamba 및 s4 모델에서 어느 정도 부활했습니다.

292
00:44:48,480 --> 00:45:00,720
나 자신에게 매우 흥미로운 또 다른 중요한 점은 변압기의 mlp 블록이

293
00:45:00,720 --> 00:45:04,880
mamba는 실제로 단일 블록으로 압축되어 있습니다.

294
00:45:05,440 --> 00:45:15,440
이제 mamba에 존재하는 선택적 상태 공간 블록에 대해 이야기해 보겠습니다.

295
00:45:16,720 --> 00:45:29,440
기본적으로 여기 ht가 우리의 상태이므로 여기에서 표기법이 변경된 것을 볼 수 있을 것 같습니다.

296
00:45:29,440 --> 00:45:39,200
이전에는 이를 x라고 불렀지만 여기서는 입력 x를 호출하고 출력은 여전히 ​​y입니다.

297
00:45:39,920 --> 00:45:47,520
여전히 여기에 있는 주요 매트릭스는 여기에서 기록을 추적합니다.

298
00:45:49,920 --> 00:45:58,640
하지만 한 가지 중요한 점은 s4와 달리 여기에 투영 레이어가 있다는 것입니다.

299
00:45:58,640 --> 00:46:10,080
입력 x가 주어지면 실제로 투영된 다음 단계인 b 델타와 실제로 c가 됩니다.

300
00:46:10,080 --> 00:46:18,720
입력 x의 투영 함수이므로 더 이상 시간 불변이 없습니다.

301
00:46:18,800 --> 00:46:25,760
시간은 변하지만 A는 동일하게 유지됩니다.

302
00:46:31,520 --> 00:46:38,800
그럼 선택적 상태 공간 동기에 대해 이야기해 보겠습니다. 왜 이것이 필요한가요?

303
00:46:38,800 --> 00:46:46,320
시퀀스 모델링의 문제는 컨텍스트를 더 작은 상태로 압축하는 것입니다.

304
00:46:46,320 --> 00:46:53,760
왜냐하면 우리는 이전 토큰의 컨텍스트를 기억하고 회상하고 싶기 때문입니다.

305
00:46:53,760 --> 00:47:02,640
시퀀스 길이가 정말 길어요. 제가 말씀드린 대로 트랜스포머의 문제점은 무엇인가요?

306
00:47:02,640 --> 00:47:13,360
매우 효과적이지만 동시에 n 제곱이고 동시에 매우 비효율적입니다.

307
00:47:13,920 --> 00:47:20,880
주의와 관심 모두에 존재하는 자기회귀적 추론 때문에

308
00:47:23,120 --> 00:47:29,200
Rnn 전체 컨텍스트를 어떻게든 명시적으로 저장해야 합니다.

309
00:47:29,600 --> 00:47:33,360
그래서

310
00:47:35,680 --> 00:47:44,880
반면에 순환 모델은 내가 말했듯이 유한한 상태를 갖기 때문에 효율적입니다.

311
00:47:47,120 --> 00:47:54,400
하지만 동시에 실제로 컨텍스트를 얼마나 잘 압축할 수 있는지에 따라 매우 제한됩니다.

312
00:47:54,400 --> 00:48:00,240
여기의 맥락은 다음과 같기 때문에 정말 중요합니다.

313
00:48:02,560 --> 00:48:12,400
모델의 중요한 성능을 향상시키는 가장 중요한 것은 기본적으로 우리가 원하는 것은

314
00:48:12,400 --> 00:48:19,280
작은 상태를 갖는 동시에 모델이 효과적이어야 하는 효율적인 모델

315
00:48:20,000 --> 00:48:26,400
상태가 실제로 컨텍스트에서 필요한 모든 정보를 저장하는 방식으로

316
00:48:27,440 --> 00:48:32,880
나는 필요한 정보를 모두 말한 것입니다. 일부 정보는 쓸모가 없기 때문입니다.

317
00:48:32,880 --> 00:48:40,080
일부 정보를 버릴 수 없습니다. 모든 것을 기억하기 위해 무한한 메모리를 가질 필요가 없습니다.

318
00:48:40,080 --> 00:48:50,880
그래서 인간으로서 우리도 기억력이 제한되어 있으므로 맘바의 주요 참신함은 이것을 달성하는 것입니다.

319
00:48:50,880 --> 00:49:01,120
선택을 사용하여 균형을 유지하므로 실제로 이전 입력 토큰을 선택적으로 불러올 수 있습니다.

320
00:49:01,120 --> 00:49:09,440
이전 슬라이드에서 행렬 b 델타와 c는 입력 투영의 함수입니다.

321
00:49:09,520 --> 00:49:17,760
실제로 이 결과를 달성하면 이것이 왜 중요한지 더 잘 이해할 수 있습니다.

322
00:49:17,760 --> 00:49:25,680
두 가지 간단한 작업 중 하나가 복사이므로 모델은 기본적으로 여기에 있어야 합니다.

323
00:49:26,240 --> 00:49:33,680
입력 토큰의 색상이 다르고 흰색이 일부 있는 경우

324
00:49:34,480 --> 00:49:40,800
중간이므로 첫 번째 작업의 모델은 실제로 입력 토큰을 어느 정도 이동해야 합니다.

325
00:49:44,080 --> 00:49:51,600
여기서는 s4로 쉽게 수행할 수 있지만 이를 조정하면 다른 문제가 발생합니다.

326
00:49:52,320 --> 00:50:01,360
이는 선택적 복사에서 실제로 관련없는 입력 토큰을 추가하는 것입니다.

327
00:50:01,360 --> 00:50:09,920
여기 흰색 상자로 표시되어 있으므로 기본적으로 이것은 약간의 소음일 뿐입니다.

328
00:50:10,960 --> 00:50:17,760
모델은 실제로 동일한 시퀀스를 보존할 수 있어야 합니다.

329
00:50:18,960 --> 00:50:27,680
출력에서 컬러 토큰에 대한 입력이므로 입력이 파란색 주황색 빨간색 녹색이면 출력이 되어야 합니다.

330
00:50:27,680 --> 00:50:35,840
파란색 주황색 빨간색 녹색이 되고 이 흰색 토큰을 모두 버리세요. 흰색 상자는 기본적으로

331
00:50:37,040 --> 00:50:42,240
우리가 주목하는 것 중 하나는 이 특정 항목에 대한 그래프입니다.

332
00:50:42,240 --> 00:50:50,240
예를 들어 s4 모델은 모두 실패했지만 mamba는 매우 놀라운 성능을 가지고 있습니다.

333
00:50:50,880 --> 00:50:57,920
거의 100%의 정확도를 달성하는 것은 주로 우리가 수행할 선택 메커니즘 때문입니다.

334
00:50:57,920 --> 00:51:07,200
또한 mamba 알고리즘을 자세히 살펴보고 그것이 어떻게 달성되는지 확인하여 mamba가 관련 없는 입력을 무시하도록 하십시오.

335
00:51:07,840 --> 00:51:16,080
선택적 복사로 인해 s4는 실제로 실패합니다. 내용이 아니기 때문입니다.

336
00:51:16,080 --> 00:51:26,400
aber는 abc를 보셨고 s4를 정의하는 것은 실제로는 불가능합니다.

337
00:51:29,520 --> 00:51:36,560
시간이 지남에 따라 변경됩니다. 실제로 입력 기능은 아니지만 그게 제가 말하는 것입니다.

338
00:51:36,960 --> 00:51:46,400
또 다른 작업은 모델이 실제로 호출해야 하는 유도 헤드입니다.

339
00:51:47,120 --> 00:51:53,600
이전 입력의 정보를 사용하여 다음 입력을 작성합니다. 예를 들어 그렇다면 여기에 있습니다.

340
00:51:53,600 --> 00:52:02,000
각 검정색 상자 뒤에는 파란색 상자가 있어야 함을 이해해야 합니다.

341
00:52:02,000 --> 00:52:14,640
예를 들어 테스트 세트에서

342
00:52:14,640 --> 00:52:22,640
실제로 약간의 노이즈를 넣습니다. 백색 소음은 기본적으로 모델이 수행할 수 없는 백색 상자 소음입니다.

343
00:52:22,640 --> 00:52:25,520
주어진 것을 이해

344
00:52:31,920 --> 00:52:39,360
기본적으로 이 특정 입력이 주어지면 다음 항목은 사용할 수 없는 파란색 상자여야 합니다.

345
00:52:39,360 --> 00:52:50,880
s4이지만 mamba는 실제로 달성할 수 있으며 내가 말했듯이 mamba와 s4의 주요 차이점은 다음과 같습니다.

346
00:52:50,880 --> 00:52:59,360
mamba는 입력 시퀀스 간의 시간 전환 아이디어를 완화하거나 기본적으로 독립적이라는 것입니다.

347
00:52:59,360 --> 00:53:01,840
맘바에는 없어

348
00:53:08,720 --> 00:53:18,640
자, 이제 기본적으로 s4 알고리즘과 mamba 알고리즘의 차이점을 살펴보겠습니다.

349
00:53:19,600 --> 00:53:28,400
보시다시피 s4부터 시작하겠습니다. x는 한입 크기 모양입니다.

350
00:53:29,040 --> 00:53:30,880
시퀀스 길이 및 토큰 차원

351
00:53:33,440 --> 00:53:38,640
y는 실제로 입력 x와 동일한 차원을 갖습니다.

352
00:53:39,600 --> 00:53:49,360
a에는 토큰 차원 또는 기본적으로 임베딩인 차원 d가 있습니다.

353
00:53:49,360 --> 00:53:58,560
대규모 언어 모델에 포함되며 n은 기본적으로 여기서 숨겨진 자산의 차원입니다.

354
00:53:59,520 --> 00:54:09,120
내가 말했듯이 a는 역사를 포착하고 당신이 bc를 볼 수 있듯이 또한 같은 것을 가지고 있습니다

355
00:54:10,640 --> 00:54:19,280
a와 delta의 차원은 d의 차원을 가지며, 우리는 delta a와 b를 이산화하여 막대를 얻습니다.

356
00:54:19,280 --> 00:54:28,640
그리고 b bar를 사용하면 ssm 아키텍처를 사용하여 y를 얻을 수 있으므로 시간이 변하지 않습니다.

357
00:54:29,520 --> 00:54:34,800
abc와 delta는 시간이 지나도 변하지 않습니다. 동일하며 동일하게 유지됩니다.

358
00:54:35,760 --> 00:54:45,200
하지만 선택 s6의 경우 실제로는 기본적으로 완전히 다르지는 않지만 완전히 다릅니다.

359
00:54:45,200 --> 00:54:53,040
입력 출력과 a 행렬은 동일하지만 행렬 bc와 delta는 동일하지 않습니다.

360
00:54:54,480 --> 00:55:00,800
더 이상 상수는 보시다시피 실제로 x의 함수입니다.

361
00:55:03,680 --> 00:55:11,920
이것이 그들이 여기에 의존하는 추가 차원 l을 갖는 이유입니다.

362
00:55:12,160 --> 00:55:26,720
기본적으로 입력 시퀀스의 시간 단계는 기본적으로 내가 말했듯이

363
00:55:29,360 --> 00:55:33,280
이것은 mamba가 시간에 따라 변할 수 있게 해준다.

364
00:55:33,360 --> 00:55:46,880
그리고 예, 보시다시피 델타 b와 c의 값은 주어진 입력에 따라 실제로 변경될 수 있습니다.

365
00:55:46,880 --> 00:55:58,640
입력하고 내가 말했듯이 내 프레젠테이션의 핵심은 mamba가 s4를 조금 더 가깝게 이동한다는 것입니다.

366
00:55:58,720 --> 00:56:06,000
입력 처리 시 시간 단계에서 시간 단계로의 시간 종속 전환을 도입하여 lstm

367
00:56:06,000 --> 00:56:15,120
순서이지만 동시에 시간 효율성 측면에서 여전히 s4의 품질을 유지합니다.

368
00:56:18,720 --> 00:56:25,600
이제 mamba 아키텍처를 살펴보겠습니다. 여기서는 ssm을 볼 수 있습니다.

369
00:56:26,400 --> 00:56:36,080
우리가 보여드린 선택적 공간 모델은 실제로 여기 맘바 내부에 있고 여러분은

370
00:56:36,080 --> 00:56:43,120
컨볼루션이 뒤따르는 투영 레이어에는 이 게이팅이 있습니다.

371
00:56:43,280 --> 00:56:48,640
lstm과 매우 유사함

372
00:56:54,080 --> 00:57:00,960
실증적 평가에 대해 조금 이야기해 보겠습니다.

373
00:57:00,960 --> 00:57:08,320
입력 시퀀스에 일부 내용이 포함된 선택적 복사 작업

374
00:57:09,280 --> 00:57:17,360
백색 소음은 모델에서 제거할 수 있어야 하며 맘바를 볼 수 있습니다.

375
00:57:19,440 --> 00:57:27,920
실제로 이 선택적 복사 작업에서 가장 높은 정확도를 달성합니다. 여기서 h3은 실제로 일종의 종류입니다.

376
00:57:27,920 --> 00:57:38,160
s4의 또 다른 작업인 유도 헤드를 설계하려면 예를 들어 다음과 같이 이해해야 합니다.

377
00:57:38,240 --> 00:57:43,440
검은색 토큰 다음에는 우리가 논의한 파란색 토큰이 있는데 그 맘바를 볼 수 있습니다.

378
00:57:45,120 --> 00:57:51,440
테스트 시퀀스 길이에 관계없이 최고의 성능을 달성할 수 있습니다.

379
00:57:51,440 --> 00:58:00,480
테스트 길이 시퀀스가 ​​증가함에 따라 다른 모든 s4가 실패하고 이는 다음과 같습니다.

380
00:58:00,480 --> 00:58:12,800
다운스트림 평가 DNA의 경우 mamba가 실제로 가장 높은 점수를 획득한 것을 볼 수 있습니다.

381
00:58:12,800 --> 00:58:22,560
평균 정확도 점수는 이것으로 제 강연은 끝났습니다. 재미있게 즐기셨기를 바랍니다.

382
00:58:22,560 --> 00:58:33,920
음, 다른 질문이 있으면 저에게 연락하시면 됩니다. 감사합니다. 좋은 하루 보내세요