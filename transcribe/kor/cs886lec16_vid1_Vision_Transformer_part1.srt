1
00:00:00,000 --> 00:00:06,760
안녕하세요 여러분, 제 이름은 Nusia이고 Vision Transformer Part 1을 소개하겠습니다.

2
00:00:06,760 --> 00:00:12,200
이번 프레젠테이션에서는 먼저 아키텍처를 기억하기 위해 Transformer에 대한 리뷰를 하겠습니다.

3
00:00:12,200 --> 00:00:18,180
그런 다음 비전 영역에서 Transformer를 활용하는 일련의 방법을 살펴보겠습니다.

4
00:00:18,180 --> 00:00:23,600
제가 이야기할 논문은 Image GPT, 이제는 Vision Transformer, DIT 그리고

5
00:00:23,600 --> 00:00:24,600
스윈 트랜스포머.

6
00:00:24,600 --> 00:00:31,320
나는 이 논문들을 출판된 연도를 기준으로 정리하려고 노력했습니다.

7
00:00:31,320 --> 00:00:37,560
각 모델은 그 이전 모델과 관련이 있습니다.

8
00:00:37,560 --> 00:00:42,000
우리는 이전에 이 과정에서 Transformer 아키텍처를 여러 번 보았습니다.

9
00:00:42,000 --> 00:00:44,440
인코더와 디코더가 포함되어 있습니다.

10
00:00:44,440 --> 00:00:50,760
이러한 각 구성 요소는 여러 번 반복될 수 있습니다.

11
00:00:50,760 --> 00:00:55,720
이 전체 아키텍처에서 가장 중요한 구성 요소는 다각적 관심입니다.

12
00:00:55,720 --> 00:00:56,720
MSA로.

13
00:00:56,720 --> 00:01:08,080
MSA는 값, 키 및 쿼리로 구성된 세 가지 벡터로 작동하며 이것이 계산 방법입니다.

14
00:01:08,080 --> 00:01:13,800
이 세 가지 벡터와 입력을 기반으로 주의를 기울입니다.

15
00:01:13,800 --> 00:01:18,520
이 아키텍처의 메커니즘은 이전 과정에서 완전히 논의되었으므로 이에 대해서는 다루지 않습니다.

16
00:01:18,520 --> 00:01:22,480
더 자세한 내용을 다루겠습니다. 그냥 넘어가겠습니다.

17
00:01:22,480 --> 00:01:30,840
Vision Transformer의 경우 인코더만 사용할 것이라는 점을 말씀드리고 싶습니다.

18
00:01:30,840 --> 00:01:31,960
디코더는 필요하지 않습니다.

19
00:01:31,960 --> 00:01:38,240
예를 들어 NLP 도메인에서 문장을 번역하려면 다음을 전달합니다.

20
00:01:38,240 --> 00:01:42,320
영어 문장을 인코더에 전달하고 임베딩을 얻은 다음 임베딩을 전달합니다.

21
00:01:42,320 --> 00:01:46,280
디코더에 전달한 다음 다른 문장으로 번역합니다.

22
00:01:46,280 --> 00:01:51,400
하지만 Vision Domain에서는 이미지에서 임베딩을 얻고 싶습니다.

23
00:01:51,400 --> 00:02:00,280
분류와 같은 다운스트림 작업에 이러한 임베딩을 사용할 수 있습니다.

24
00:02:00,280 --> 00:02:06,000
Transformers는 NLP 영역에서 매우 성공적이며 이것이 사람들이 NLP를 사용하도록 영감을 준 것입니다.

25
00:02:06,000 --> 00:02:09,120
Vision Domain의 트랜스포머 아이디어.

26
00:02:09,120 --> 00:02:17,800
Image GPT는 OpenAI가 제안한 아키텍처를 활용한 최초의 논문 중 하나였습니다.

27
00:02:17,800 --> 00:02:24,440
이미지를 위한 Transformer, 그리고 얼마 지나지 않아 Vision Transformer가 제안되었습니다.

28
00:02:24,440 --> 00:02:25,440
Google.

29
00:02:25,440 --> 00:02:31,160
이 접근 방식은 정말 성공적이었으며 Transformer 사용에 대한 관심도 많이 높아졌습니다.

30
00:02:31,160 --> 00:02:32,440
비전에서.

31
00:02:33,200 --> 00:02:40,480
사실 제가 이야기할 다른 논문들은 이 VIT에서 영감을 받은 것입니다.

32
00:02:40,480 --> 00:02:41,960
비전 트랜스포머.

33
00:02:41,960 --> 00:02:49,360
이제 상상할 수 있듯이 Vision에서 Transformer를 사용하는 데 따른 근본적인 과제 중 하나는

34
00:02:49,360 --> 00:02:54,520
도메인은 이미지에서 시퀀스를 생성하는 방법입니다.

35
00:02:54,520 --> 00:03:00,280
우리는 Transformer가 이미지를 입력으로 받아들일 수 없다는 것을 알고 있습니다.

36
00:03:00,280 --> 00:03:03,120
이미지를 시퀀스로 변환합니다.

37
00:03:03,120 --> 00:03:07,440
내가 이야기할 각 논문은 이러한 어려움을 극복할 것입니다.

38
00:03:07,440 --> 00:03:08,440
자신의 방식.

39
00:03:11,440 --> 00:03:22,320
내가 말했듯이, Vision에서 Transformer를 활용한 최초의 논문 중 하나는 생성 증명입니다.

40
00:03:22,320 --> 00:03:26,920
이미지 GPT라고도 알려진 픽셀 학습입니다.

41
00:03:26,920 --> 00:03:30,480
보시다시피 이 아키텍처에는 세 가지 구성 요소가 포함되어 있습니다.

42
00:03:30,480 --> 00:03:37,280
첫 번째 구성 요소는 이미지를 시퀀스로 전송하는 전처리입니다.

43
00:03:37,280 --> 00:03:43,480
두 번째 구성요소는 두 모델을 훈련시키는 사전 훈련입니다.

44
00:03:43,480 --> 00:03:49,520
이 모델의 백본은 GPT2이며 동일하지만 이 두 모델은

45
00:03:49,520 --> 00:03:53,760
단지 두 가지 다른 목표로 훈련받았을 뿐입니다.

46
00:03:53,760 --> 00:04:00,280
마지막 구성 요소이기도 한 세 번째 구성 요소는 이러한 모델을 평가합니다.

47
00:04:00,280 --> 00:04:06,120
각 구성 요소에 대해 자세히 설명하겠습니다.

48
00:04:06,120 --> 00:04:11,960
전처리인 첫 번째 구성 요소는 다음을 통해 이미지를 시퀀스로 변환합니다.

49
00:04:11,960 --> 00:04:14,960
픽셀을 재정렬합니다.

50
00:04:14,960 --> 00:04:25,360
이제 ImageNet Dataset의 이미지 크기는 224 x 224 x 3입니다. 이는 숫자와 같습니다.

51
00:04:25,360 --> 00:04:30,520
채널 수는 150,000픽셀 이상입니다.

52
00:04:30,520 --> 00:04:32,440
이것은 엄청난 순서입니다.

53
00:04:32,440 --> 00:04:38,520
시퀀스를 생성하기 위해 이미지 순서를 바꾸려면 정말 긴 시퀀스가 ​​됩니다.

54
00:04:38,520 --> 00:04:44,960
이것이 ImageGPT가 두 단계로 구성된 ContextReduction을 사용하는 이유입니다.

55
00:04:44,960 --> 00:04:48,240
첫 번째 단계에서는 이미지 크기를 더 낮은 해상도로 조정합니다.

56
00:04:48,240 --> 00:04:55,760
32 x 32 x 3으로 변환한 다음 사용자 정의된 방법을 사용하는 또 다른 단계가 있습니다.

57
00:04:55,760 --> 00:04:58,880
RGB를 변경하려면 색상 팔레트를 사용하세요.

58
00:04:58,880 --> 00:05:07,600
3개였던 채널을 1개로 바꾸더니 차원을 줄였습니다.

59
00:05:07,600 --> 00:05:17,120
150,000픽셀부터 150,000픽셀 이상, 1,000픽셀까지.

60
00:05:17,120 --> 00:05:21,200
어떻게 작동하는지 보기 위해 이것은 ImageNet의 이미지입니다.

61
00:05:21,200 --> 00:05:28,720
여기에서 볼 수 있듯이 해상도를 낮추고 RGB를 변경합니다.

62
00:05:28,720 --> 00:05:34,000
화살표가 표시하는 방식으로 픽셀을 재정렬합니다.

63
00:05:34,000 --> 00:05:41,760
그들은 그것들을 하나로 모아 시퀀스를 만듭니다.

64
00:05:41,760 --> 00:05:46,200
시퀀스를 생성한 후 사전 학습을 위한 구성요소가 있습니다.

65
00:05:46,200 --> 00:05:51,720
다시 말하지만 그들은 두 가지 목표 중 하나를 위해 GPT 튜브 아키텍처를 사용하고 있습니다.

66
00:05:51,720 --> 00:05:58,480
첫 번째 목표는 다음 픽셀 예측이라고도 알려진 자동 회귀입니다.

67
00:05:58,480 --> 00:06:05,720
다른 하나는 마스크 픽셀 예측인 BERT입니다.

68
00:06:05,720 --> 00:06:09,560
먼저 자기회귀에 대해 이야기해 보겠습니다.

69
00:06:09,560 --> 00:06:10,560
이것이 작동하는 방식입니다.

70
00:06:10,560 --> 00:06:17,320
이미지 데이터 세트가 될 데이터 세트가 있다고 가정해 보세요.

71
00:06:17,320 --> 00:06:22,000
이미지 샘플이 될 이 데이터 세트의 데이터를 고려해보세요.

72
00:06:22,000 --> 00:06:29,640
X라고 부르는 이 이미지에는 일련의 픽셀이 포함됩니다.

73
00:06:29,640 --> 00:06:36,800
이 이미지 또는 이 샘플의 확률은 다음의 확률에 기초합니다.

74
00:06:36,800 --> 00:06:40,080
이 샘플을 만든 픽셀입니다.

75
00:06:40,080 --> 00:06:51,800
각 픽셀의 확률은 주어진 픽셀 또는 관찰된 픽셀을 기반으로 합니다.

76
00:06:51,800 --> 00:06:59,080
이 GPT는 항등 순열을 사용했습니다. 즉, 관찰된 픽셀을 재정렬하지 않았습니다.

77
00:06:59,080 --> 00:07:09,120
즉, 픽셀 i의 확률은 픽셀의 확률에 기초합니다.

78
00:07:09,120 --> 00:07:17,680
1에서 i - 1, 또는 이 픽셀에 대해 관찰된 픽셀은 1에서 i - 1이 됩니다.

79
00:07:17,680 --> 00:07:20,360
여기는 재정렬이 없는 것 같아요.

80
00:07:20,360 --> 00:07:23,400
자동회귀 및 NLP는 재정렬을 사용할 수 있습니다.

81
00:07:23,400 --> 00:07:30,880
이미지 GPT에만 있으며 재정렬을 사용하지 않습니다.

82
00:07:30,880 --> 00:07:34,480
이제 확률을 계산하는 방법을 알았습니다.

83
00:07:34,480 --> 00:07:39,800
부정적인 가능성을 최소화하여 모델을 훈련할 수 있습니다.

84
00:07:39,800 --> 00:07:46,840
이 손실을 보면 모델이 확률이 다음과 같은지 확인하려고 노력하고 있음을 알 수 있습니다.

85
00:07:46,840 --> 00:07:55,840
이미지 샘플의 비율이 최대한 높으므로 손실은 0에 가깝습니다.

86
00:07:55,840 --> 00:08:00,760
또 다른 목표는 Masked Pixel Prediction인 BERT입니다.

87
00:08:00,760 --> 00:08:06,920
BERT의 경우 먼저 모든 픽셀의 하위 시퀀스를 고려합니다.

88
00:08:06,920 --> 00:08:16,480
M입니다. M에 각 픽셀이 나타날 확률은 0.15이며, M에 나타나는 픽셀은

89
00:08:16,560 --> 00:08:20,840
M에 나타나는 것은 훈련 중에 마스크됩니다.

90
00:08:20,840 --> 00:08:24,360
이것이 우리가 M을 BERT 마스크라고 부르는 이유입니다.

91
00:08:24,360 --> 00:08:35,720
훈련 과정에서 우리는 이 손실, 즉 손실 함수를 최소화하려고 노력할 것입니다.

92
00:08:35,720 --> 00:08:42,680
마스크되지 않은 요소를 기반으로 마스크 요소를 예측합니다.

93
00:08:42,720 --> 00:08:51,480
예를 들어 극도로 회귀적인 경우 이러한 두 가지 목표를 모두 요약하고 시각화하려면 다음과 같습니다.

94
00:08:51,480 --> 00:08:58,720
입력에 대해 우리는 이 픽셀을 시퀀스로 예측하려고 하며 이는 관찰된 것입니다.

95
00:08:58,720 --> 00:09:00,720
순서.

96
00:09:00,720 --> 00:09:08,200
각 단계에서 각 픽셀은 이전에 관찰된 픽셀을 기반으로 예측됩니다.

97
00:09:08,200 --> 00:09:10,280
예를 들어, 이 파란색을 보세요.

98
00:09:10,280 --> 00:09:16,840
이전에 관찰한 것을 기반으로 한 것인지, 아니면 이 빨간색이 기반으로 한 것인지 알 수 있습니다.

99
00:09:16,840 --> 00:09:20,600
이전에 관찰된 것 등등.

100
00:09:20,600 --> 00:09:31,920
마지막에는 관찰된 다른 모든 픽셀을 사용하여 픽셀을 예측합니다.

101
00:09:31,920 --> 00:09:35,680
이것이 바로 초회귀 예측 픽셀입니다.

102
00:09:35,680 --> 00:09:42,400
우리가 지금 예측한 마지막 픽셀인 이 픽셀의 확률은 다음에 따라 달라집니다.

103
00:09:42,400 --> 00:09:46,000
다른 모든 관찰된 것들.

104
00:09:46,000 --> 00:09:50,240
BERT의 경우 이것이 입력 시퀀스입니다.

105
00:09:50,240 --> 00:09:53,440
두 픽셀이 마스크되어 있다고 상상해 보세요.

106
00:09:53,440 --> 00:09:58,760
이제 그들은 관찰된 다른 모든 것들을 기반으로 예측할 것입니다.

107
00:09:58,760 --> 00:10:02,400
여기 이 파란색을 보세요.

108
00:10:02,400 --> 00:10:08,600
이 파란색 확률은 관찰된 다른 모든 항목을 기반으로 계산됩니다.

109
00:10:08,600 --> 00:10:09,600
픽셀.

110
00:10:09,600 --> 00:10:20,080
보시다시피, 그것은 픽셀 앞의 사람뿐만 아니라 모든 사람과 같습니다.

111
00:10:20,080 --> 00:10:29,240
여기 빨간색과 같이 이 두 픽셀이 생성됩니다.

112
00:10:29,240 --> 00:10:38,640
따라서 이 두 픽셀의 확률은 서로 독립적으로 계산됩니다.

113
00:10:38,640 --> 00:10:43,920
Ultra-regressive 프로세스에서 각 픽셀은 실제로 다음을 기반으로 예측되었습니다.

114
00:10:43,920 --> 00:10:46,920
이전 것들.

115
00:10:46,920 --> 00:10:54,240
좋습니다. 평가를 위해 미세 조정이나 선형 문제를 수행합니다.

116
00:10:54,240 --> 00:11:03,120
미세 조정을 위해 표현의 끝에 분류 헤드를 추가합니다.

117
00:11:03,120 --> 00:11:05,320
마지막 층.

118
00:11:05,320 --> 00:11:08,600
여기 이 이미지에 있을 겁니다.

119
00:11:08,600 --> 00:11:13,400
그런 다음 변환기와 분류기를 함께 훈련합니다.

120
00:11:13,400 --> 00:11:19,400
따라서 손실은 분류기의 손실과 모델의 손실을 기반으로 하며 둘 중 하나일 수 있습니다.

121
00:11:19,400 --> 00:11:23,040
극도로 퇴행적 또는 BERT.

122
00:11:23,040 --> 00:11:30,160
선형 문제의 경우 표현에 분류기를 추가하지만 훈련만 합니다.

123
00:11:30,160 --> 00:11:33,200
분류기를 사용하고 변환기 모델을 고정합니다.

124
00:11:33,200 --> 00:11:38,720
따라서 이것은 분류기 손실을 기반으로만 훈련됩니다.

125
00:11:38,720 --> 00:11:43,600
이제 이 프로세스에서는 변환기를 훈련시키고 싶지 않으므로 다음을 추가할 수 있습니다.

126
00:11:43,600 --> 00:11:45,920
원하는 각 레이어에 분류기를 추가합니다.

127
00:11:45,920 --> 00:11:55,560
마지막 레이어나 중간 레이어 또는 첫 번째 레이어에 추가할 수 있습니다.

128
00:11:55,560 --> 00:12:01,200
그리고 결과적으로 그들은 정확히 그렇게 했습니다.

129
00:12:01,200 --> 00:12:04,960
Ultra-regressive의 경우 ImageNet에서 사전 훈련되었습니다.

130
00:12:04,960 --> 00:12:11,600
그들은 선형 프로브를 적용했거나 분류기를 다른 레이어에 적용하고 보고했습니다.

131
00:12:11,600 --> 00:12:17,600
정확성.

132
00:12:17,600 --> 00:12:22,960
그래서 중간층의 결과가 실제로 더 좋다는 것이 놀랍다는 것을 알 수 있습니다.

133
00:12:22,960 --> 00:12:26,960
마지막 층보다

134
00:12:26,960 --> 00:12:33,520
그 이유는 변환기에서 첫 번째 레이어에는 많은 정보가 포함되지 않기 때문입니다.

135
00:12:33,520 --> 00:12:39,760
마지막 레이어는 모델이 예측하려는 해당 픽셀에 매우 구체적입니다.

136
00:12:39,760 --> 00:12:46,640
그렇기 때문에 중간 계층에는 전역적이고 보다 전역적인 정보와 결과가 포함될 수 있습니다.

137
00:12:46,640 --> 00:12:52,840
예를 들어 이러한 레이어의 분류에서는 더 좋습니다.

138
00:12:52,840 --> 00:12:59,320
미세 조정 평가에서는 C410과 C410의 정확도가 비교적 좋은 것을 알 수 있습니다.

139
00:12:59,320 --> 00:13:07,040
C4100도 마찬가지지만, 예를 들어 C4100에서 CNN 기반의 EfficientNet을 본다면

140
00:13:07,040 --> 00:13:12,480
모델이 실제로 더 나은 성능을 발휘합니다.

141
00:13:12,480 --> 00:13:19,520
이는 또한 두 가지 목표와 두 가지 데이터 세트에 대한 일반적인 결과입니다.

142
00:13:19,520 --> 00:13:23,440
따라서 이것은 C410에 대해 BERT이며 자동 공격적입니다.

143
00:13:23,440 --> 00:13:28,600
이 모델은 ImageNet에서 사전 훈련되었으며 ImageNet 및

144
00:13:28,600 --> 00:13:32,160
그들은 Solvep 이미지에 대해 사전 훈련을 받았습니다.

145
00:13:32,160 --> 00:13:37,240
그래서 여기까지는 둘 다 사전 훈련을 받았습니다.

146
00:13:37,240 --> 00:13:41,960
예를 들어 이 그림에서는 ImageNet에서 사전 훈련되었습니다.

147
00:13:41,960 --> 00:13:50,240
그런 다음 정확도를 높이기 위해 선형 프로펠러를 적용한 다음 미세 조정을 적용합니다.

148
00:13:50,240 --> 00:13:57,120
정확도를 다시 높이려면

149
00:13:57,120 --> 00:14:04,680
보시다시피 C410의 경우 99%에 도달하지만 ImageNet의 경우 최선을 다하는 것은

150
00:14:04,680 --> 00:14:09,640
거의 68% 정도.

151
00:14:09,640 --> 00:14:17,360
그리고 BERT에서 볼 수 있는 이 어두운 부분은 마치 조립 공정을 적용한 것과 같습니다.

152
00:14:17,360 --> 00:14:30,320
정확도를 더욱 향상시키기 위해.

153
00:14:30,320 --> 00:14:36,240
다음은 ImageGPT에 대한 몇 가지 시각적 결과이며 매우 흥미롭다고 생각합니다.

154
00:14:36,240 --> 00:14:44,120
여기 마지막 열은 실제 입력, 즉 목표 입력과 같습니다.

155
00:14:44,120 --> 00:14:49,960
그리고 여기 첫 번째는 마스크를 쓴 사진과 같습니다. 예를 들어, 이것을 먼저 보세요

156
00:14:49,960 --> 00:14:53,640
고양이를 이미지한 것입니다.

157
00:14:53,640 --> 00:15:00,120
이 사진의 절반은 가려져 있지만 그래도 볼 수 있는 중간 사진은,

158
00:15:00,120 --> 00:15:04,960
모델이 예측한 것이 있습니다.

159
00:15:04,960 --> 00:15:10,960
그들은 꽤 괜찮아요. 제 말은, 이걸 보세요, 좀 웃기네요.

160
00:15:10,960 --> 00:15:17,920
예를 들어 이러한 BERT를 통해 모델이 실제로 어떻게 형성되었는지 확인할 수 있습니다.

161
00:15:17,920 --> 00:15:25,560
따라서 이 모델이 실제로 매우 훌륭하게 작동하는 방식은 매우 인상적입니다.

162
00:15:25,560 --> 00:15:35,320
좋습니다. ImageGPT가 어떻게 작동하는지 살펴보았는데 이 모델에는 주요 문제가 있습니다.

163
00:15:35,320 --> 00:15:42,600
이미지 크기를 32x32로 조정하므로 정보에서 많은 세부 정보가 손실됩니다.

164
00:15:42,600 --> 00:15:50,520
그들은 많은 세부 묘사가 포함된 거대한 이미지를 아주 작은 저해상도 이미지로 만들고 있을 뿐입니다.

165
00:15:50,520 --> 00:15:51,520
영상.

166
00:15:51,520 --> 00:15:58,080
VIT는 전체 그림 대신 패치에서 시퀀스를 생성하여 이 문제를 해결했습니다.

167
00:15:58,840 --> 00:16:07,840
다시 말하지만, 이 논문은 매우 성공적이었으며 다른 많은 논문에서도 이 아이디어를 연구에 사용했습니다.

168
00:16:09,840 --> 00:16:13,440
VIT의 전반적인 아키텍처는 다음과 같습니다.

169
00:16:13,440 --> 00:16:20,080
주요 아이디어는 큰 이미지 대신 패치를 사용하는 것입니다.

170
00:16:20,080 --> 00:16:22,400
그 과정을 설명하겠습니다.

171
00:16:23,400 --> 00:16:30,400
여기서는 사전 프로세스에 대해 설명하겠습니다. 하지만 이 슬라이드에서는 그 점만 언급하고 싶습니다.

172
00:16:30,400 --> 00:16:33,720
Transformer 인코더는 Transformer와 동일합니다.

173
00:16:33,720 --> 00:16:37,560
아키텍처에는 아무런 변화도 주지 않았습니다.

174
00:16:37,560 --> 00:16:45,560
그들은 단지 이미지의 클래스를 찾기 위해 분류를 위해 여기에 MLP 헤드를 추가합니다.

175
00:16:45,560 --> 00:16:52,120
좋아요, 이것이 VIT가 작동하는 방식입니다.

176
00:16:52,120 --> 00:16:57,960
먼저 이미지를 패치로 분할합니다. 따라서 이 이미지가 있다고 상상해 보세요.

177
00:16:57,960 --> 00:17:06,560
그들은 이것을 채널 수 때문에 P x P x 3 패치로 나눕니다.

178
00:17:06,560 --> 00:17:11,880
평소 사용하는 패치 사이즈는 16이라고 하더군요.

179
00:17:11,880 --> 00:17:16,080
그런 다음 이 패치를 서로 옆에 배치합니다.

180
00:17:16,080 --> 00:17:24,400
보시다시피 다음과 같이 보입니다. 그런 다음 이 패치의 위치를 ​​옆에 두었습니다.

181
00:17:24,400 --> 00:17:25,400
그들도 마찬가지다.

182
00:17:25,400 --> 00:17:32,440
이는 패치 1, 2, 3, 4, 5, 6 등입니다.

183
00:17:32,440 --> 00:17:38,280
이러한 각 패치는 해당 픽셀을 기반으로 시퀀스로 전환됩니다.

184
00:17:38,280 --> 00:17:43,600
이 시퀀스의 길이는 P x P x 3입니다.

185
00:17:43,600 --> 00:17:48,520
그런 다음 이 시퀀스는 선형 투영을 거칩니다.

186
00:17:48,520 --> 00:18:00,160
이 선형 투영은 차원을 P x P x C에서 D로 변경합니다.

187
00:18:00,160 --> 00:18:05,440
반면에 포지션 임베딩(Position Embedding)이 있습니다.

188
00:18:05,440 --> 00:18:12,360
패치의 이 위치는 위치 임베딩(Position Embedding)을 거칩니다.

189
00:18:12,360 --> 00:18:17,280
그러면 정수가 D 차원 벡터로 변환됩니다.

190
00:18:17,280 --> 00:18:25,400
이 두 벡터는 함께 추가되어 다음을 위한 전처리 단계를 생성합니다.

191
00:18:25,400 --> 00:18:26,960
이번 패치.

192
00:18:26,960 --> 00:18:32,480
이 프로세스는 모든 패치에 대해 계속됩니다.

193
00:18:32,480 --> 00:18:40,160
이것을 변환기에 전달하여 훈련 가능한 벡터를 시작 부분에 추가합니다.

194
00:18:40,160 --> 00:18:41,160
이 시퀀스.

195
00:18:41,160 --> 00:18:43,560
그들은 이것을 CLS 토큰이라고 부릅니다.

196
00:18:43,560 --> 00:18:49,240
CLS 토큰, 여기에 보이는 분홍색 토큰은 훈련 가능한 벡터입니다.

197
00:18:49,240 --> 00:18:55,800
이 보라색은 실제로 패치가 1에서 시작되었기 때문에 0에 대한 위치 임베딩입니다.

198
00:18:55,800 --> 00:18:58,320
위치는 0이다.

199
00:18:58,320 --> 00:19:02,440
이 전체 시퀀스는 변환기를 통과합니다.

200
00:19:03,440 --> 00:19:09,600
선형 투영을 거친 패치인 CLS 토큰의 입력입니다.

201
00:19:09,600 --> 00:19:13,480
그들은 위치와 함께 추가되었습니다.

202
00:19:13,480 --> 00:19:23,240
변환기에는 레이어가 포함되어 있고 그 다음에는 변환기의 마지막 출력인 출력이 포함되어 있습니다.

203
00:19:23,240 --> 00:19:29,040
이 출력 중 우리가 호출할 CLS 토큰과 관련된 부분

204
00:19:29,120 --> 00:19:37,520
Z0L이 이 MLB 헤드를 거쳐 라벨이 생성됩니다.

205
00:19:41,520 --> 00:19:44,200
좋습니다. 결과는 다음과 같습니다.

206
00:19:44,200 --> 00:19:49,280
VIT는 실제로 CNN 기반 모델보다 성능이 뛰어납니다.

207
00:19:49,280 --> 00:19:56,480
여기에서 실제로 GFD라고 불리는 데이터 세트에서 모델을 훈련하는 것을 볼 수 있습니다.

208
00:19:56,480 --> 00:20:02,560
이 데이터 세트에는 훈련용 데이터 3억 개가 포함되어 있으며 공개적으로 사용할 수 없습니다.

209
00:20:02,560 --> 00:20:06,640
Google의 비공개 데이터세트와 같습니다.

210
00:20:06,640 --> 00:20:09,440
그들은 그것에 대해 모델을 훈련합니다.

211
00:20:09,440 --> 00:20:15,840
이는 14패치, 16패치로 훈련하는 것과 같고 또 다른 패치로 훈련하는 것과 같습니다.

212
00:20:15,840 --> 00:20:18,760
데이터세트.

213
00:20:18,760 --> 00:20:24,560
따라서 ResNet 및 EfficientNet과 같은 CNN 기반 모델과 비교하면 정확도가 높다는 것을 알 수 있습니다.

214
00:20:24,560 --> 00:20:26,720
더 나은.

215
00:20:26,720 --> 00:20:36,320
반면 VIT는 여기에서 볼 수 있는 CNN 기반 모델보다 훨씬 빠릅니다.

216
00:20:36,320 --> 00:20:43,080
이것은 또한 당신이 볼 수 있는 또 다른 결과이다.

217
00:20:43,080 --> 00:20:45,760
VTAB라는 데이터세트가 있습니다.

218
00:20:45,760 --> 00:20:48,040
다양한 작업이 있습니다.

219
00:20:48,040 --> 00:20:54,320
따라서 이 그림을 보면 VIT가 실제로 다른 최신 기술보다 성능이 뛰어나다는 것을 알 수 있습니다.

220
00:20:54,320 --> 00:21:01,000
이러한 각 작업에 대한 모델을 제공합니다.

221
00:21:01,000 --> 00:21:04,240
시각화된 결과도 있습니다.

222
00:21:04,240 --> 00:21:08,360
우리는 이 사진들을 통해 VIT의 힘을 볼 수 있습니다.

223
00:21:08,360 --> 00:21:13,200
여기 첫 번째 그림은 선형 투영의 출력에 PCA를 적용한 것입니다.

224
00:21:13,200 --> 00:21:21,320
그래서 그들은 차원을 D에서 28로 변경한 다음 이 결과를 28장의 사진으로 보여주었습니다.

225
00:21:22,320 --> 00:21:24,400
여기에 이미지가 있습니다.

226
00:21:24,400 --> 00:21:32,160
따라서 이 사진은 생성된 특징 추출 사진과 매우 유사합니다.

227
00:21:32,160 --> 00:21:36,360
CNN 모델에 의해.

228
00:21:36,360 --> 00:21:43,240
두 번째 그림은 위치 임베딩의 성능을 보여줍니다.

229
00:21:43,240 --> 00:21:49,960
그래서 그들은 실제로 각 위치 임베딩과 나머지 위치 사이의 코사인 유사성을 계산합니다.

230
00:21:49,960 --> 00:21:51,360
위치의.

231
00:21:51,360 --> 00:21:58,360
각 위치별로 동일한 행과 열에서 유사도가 더 높은 것을 확인할 수 있습니다.

232
00:21:58,360 --> 00:22:06,120
그리고 여기 마지막 사진은 실제로 각 주의에 대한 평균 주의 거리를 보여줍니다.

233
00:22:06,120 --> 00:22:08,120
각 층의 머리.

234
00:22:08,120 --> 00:22:17,680
따라서 이러한 각 점은 평균 주의 거리 또는 전방을 보여줍니다.

235
00:22:17,680 --> 00:22:19,200
이제 이것이 레이어입니다.

236
00:22:19,200 --> 00:22:28,400
이 그림은 레이어로 갈수록 이 거리가 증가한다는 것을 보여줍니다.

237
00:22:28,400 --> 00:22:37,760
즉, 마지막 머리에서 더 전역적인 특징이 추출된다는 의미입니다.

238
00:22:37,760 --> 00:22:44,120
레이어.

239
00:22:44,120 --> 00:22:47,560
그래서 다음 논문은 DIT입니다.

240
00:22:47,560 --> 00:22:53,960
VIT는 매우 잘 작동하지만 큰 문제가 있습니다.

241
00:22:53,960 --> 00:22:59,280
이 모델을 훈련하는 데 사용하는 데이터 세트는 공개적으로 사용할 수 없습니다.

242
00:22:59,280 --> 00:23:00,480
또한 그것은 거대한 데이터 세트입니다.

243
00:23:00,480 --> 00:23:07,160
여기에는 Google이 학습할 수 있는 학습용 데이터 3억 개가 포함되어 있습니다.

244
00:23:07,160 --> 00:23:09,160
그들의 GPU.

245
00:23:09,160 --> 00:23:17,600
페이스북이 제안한 DIT는 증류 과정을 이용해 실제로 문제를 해결한다.

246
00:23:17,600 --> 00:23:20,040
이 문제.

247
00:23:20,040 --> 00:23:24,280
그리고 그들은 10배 더 작은 ImageNet을 사용합니다.

248
00:23:24,280 --> 00:23:33,680
좋습니다. DIT의 아키텍처를 이해하기 전에 먼저 다음 아이디어를 살펴보겠습니다.

249
00:23:33,680 --> 00:23:36,120
지식 증류.

250
00:23:36,120 --> 00:23:40,680
지식 증류에는 사전 훈련된 교사 네트워크가 있고,

251
00:23:40,680 --> 00:23:46,400
일반적으로 교사보다 작은 학생 네트워크가 있으며, 이를 모방하려고 합니다.

252
00:23:46,400 --> 00:23:48,640
교사 네트워크의 행동.

253
00:23:48,640 --> 00:23:55,080
즉, 훈련에서는 교사의 지식을 정제하여

254
00:23:55,080 --> 00:23:58,560
학생.

255
00:23:58,560 --> 00:24:05,080
이 증류는 연질 증류와 경질 증류의 두 가지 방법으로 수행할 수 있습니다.

256
00:24:05,080 --> 00:24:11,480
연증류를 통해 학생은 교사가 예측한 분포를 재현하려고 시도합니다.

257
00:24:11,480 --> 00:24:19,480
그러나 경질 증류의 경우 학생은 교사가 예측한 라벨을 재현하려고 시도합니다.

258
00:24:19,480 --> 00:24:27,120
여기 이 연질 증류를 보면 첫 번째 항은 다음을 기반으로 하는 교차 엔트로피입니다.

259
00:24:27,120 --> 00:24:33,360
학생의 예측과 진정한 라벨.

260
00:24:33,360 --> 00:24:40,360
여기서 두 번째 항은 학생과 교사의 예측 사이의 KL 발산입니다.

261
00:24:40,360 --> 00:24:44,920
그러면 이 두 네트워크 간의 유사성이 높아집니다.

262
00:24:44,920 --> 00:24:54,960
즉, 학생이 교사와 비슷하게 행동하도록 노력할 것입니다.

263
00:24:54,960 --> 00:25:03,600
경질 증류의 경우 첫 번째 용어는 실제 라벨을 기준으로 동일합니다.

264
00:25:03,600 --> 00:25:06,600
학생 네트워크의 출력과 실제 레이블.

265
00:25:06,600 --> 00:25:12,440
여기서 두 번째 항은 학생 네트워크와 교사 레이블의 출력입니다.

266
00:25:12,440 --> 00:25:23,720
내가 말했듯이, 학생은 교사가 예측한 라벨을 재현하려고 노력할 것입니다.

267
00:25:23,720 --> 00:25:32,400
따라서 EIT를 이해하기 위해 EIT와 매우 유사하다고 가정해 보겠습니다.

268
00:25:32,400 --> 00:25:36,680
EIT 아키텍처를 기억하시면 동일한 프로세스입니다.

269
00:25:36,680 --> 00:25:39,880
그래서 우리는 이미지를 패치로 바꿀 것입니다.

270
00:25:39,880 --> 00:25:42,320
이 패치는 선형 투영을 거칩니다.

271
00:25:42,320 --> 00:25:50,720
이러한 패치의 위치를 ​​포함하는 위치 포함 레이어도 있습니다.

272
00:25:50,720 --> 00:25:53,680
그리고 이 시퀀스도 동일하게 생성됩니다.

273
00:25:53,680 --> 00:25:55,560
CLS 토큰은 동일합니다.

274
00:25:55,560 --> 00:25:57,920
이는 초반에 추가될 예정입니다.

275
00:25:57,920 --> 00:26:01,360
이제 첫 번째 차이점이 있습니다.

276
00:26:01,360 --> 00:26:06,840
EIT에는 증류 토큰이라는 또 다른 토큰도 있습니다.

277
00:26:06,840 --> 00:26:11,600
시퀀스 마지막에 추가되지만 CLS와 동일합니다.

278
00:26:11,600 --> 00:26:14,120
훈련 가능한 토큰입니다.

279
00:26:14,120 --> 00:26:15,640
훈련 가능한 벡터입니다.

280
00:26:15,640 --> 00:26:22,600
자, 이 시퀀스는 변환기 인코더를 통과하고 두 개의 MLP 헤드가 있게 됩니다.

281
00:26:22,600 --> 00:26:26,440
첫 번째는 EIT의 것과 동일합니다.

282
00:26:26,440 --> 00:26:28,960
CLS 토큰과 실제 레이블이 필요합니다.

283
00:26:28,960 --> 00:26:32,800
교차 엔트로피 손실을 계산합니다.

284
00:26:32,800 --> 00:26:40,720
다른 MLP는 실제로 또 다른 손실을 생성하며 이 변환기 모델은 훈련됩니다.

285
00:26:40,720 --> 00:26:42,920
이 두 가지 손실을 모두 기반으로 합니다.

286
00:26:42,920 --> 00:26:47,840
좋아요, 여기 교사 모델이 있습니다.

287
00:26:47,840 --> 00:26:51,520
그들이 사용하는 교사 모델은 CNN 기반 모델입니다.

288
00:26:51,520 --> 00:26:58,120
따라서 이 그림은 전체적으로 CNN 기반 모델을 거치게 됩니다.

289
00:26:58,120 --> 00:27:01,480
이 순서는 필요하지 않습니다.

290
00:27:01,480 --> 00:27:05,920
이 선생님을 통해 진행되고 선생님이 예측을 해줄 것입니다.

291
00:27:05,920 --> 00:27:12,880
이제 이 MLP는 증류 토큰을 입력으로 사용하고 예측도 수행합니다.

292
00:27:12,880 --> 00:27:18,560
교사 또는 교사의 분포를 고려한 다음 손실을 계산합니다.

293
00:27:18,560 --> 00:27:20,040
선생님의.

294
00:27:20,040 --> 00:27:25,600
따라서 이 시나리오에서 학생 네트워크인 이 변환기는 다음을 기반으로 훈련됩니다.

295
00:27:25,600 --> 00:27:30,760
이 두 가지 손실.

296
00:27:30,760 --> 00:27:39,040
훈련을 위해 RAND 증대 및 COD 진드기와 같은 데이터 증대를 사용합니다.

297
00:27:39,040 --> 00:27:42,160
다른 보강도 있습니다.

298
00:27:42,160 --> 00:27:44,520
이것은 단지 두 가지 예일 뿐입니다.

299
00:27:44,520 --> 00:27:51,600
RAND 증대는 대비를 변경하거나 그림을 회전하거나 밝기를 변경하는 것과 같습니다.

300
00:27:51,600 --> 00:27:55,920
COD 진드기의 경우 개 사진이 있고 고양이 사진이 있고

301
00:27:55,920 --> 00:27:59,120
그런 다음 그것들을 함께 결합합니다.

302
00:27:59,120 --> 00:28:05,040
또한 헤드 수와 개수에 따라 변형 아키텍처를 도입합니다.

303
00:28:05,040 --> 00:28:10,440
레이어와 임베딩 크기가 다릅니다.

304
00:28:10,440 --> 00:28:20,240
여기 보시는 VIT-B는 이 아키텍처가 VIT와 유사하다는 것입니다.

305
00:28:20,240 --> 00:28:26,560
그들은 또한 우리가 보게 될 일부 결과에서 모델을 미세 조정한다고 언급했습니다.

306
00:28:26,560 --> 00:28:34,440
동일한 데이터세트를 사용했지만 사진의 해상도를 384로 변경했습니다.

307
00:28:34,440 --> 00:28:38,400
따라서 교사 네트워크의 경우 서로 다른 모델을 사용합니다.

308
00:28:38,400 --> 00:28:40,600
이 모델은 모두 CNN 기반 모델입니다.

309
00:28:40,600 --> 00:28:45,080
더 큰 CNN 모델이 실제로 더 나은 성능을 발휘합니다.

310
00:28:45,080 --> 00:28:49,120
이것은 모델의 정확성입니다.

311
00:28:49,120 --> 00:28:55,880
이것이 사전 훈련된 학생 네트워크의 정확도이고 이것이 정확도입니다.

312
00:28:55,880 --> 00:29:06,000
제가 언급한 사진에 미세 조정을 적용한 후.

313
00:29:06,000 --> 00:29:10,640
그들은 다양한 증류 전략에 대한 결과를 보고했습니다.

314
00:29:10,640 --> 00:29:18,080
여기 보이는 이 기호는 IT가 교차 엔트로피라는 두 가지 손실을 사용했음을 의미합니다.

315
00:29:18,080 --> 00:29:20,920
손실과 교사 손실.

316
00:29:20,920 --> 00:29:27,480
하지만 여기서 이것은 증류가 아닌 하나의 손실만 사용한다는 의미입니다.

317
00:29:27,480 --> 00:29:29,000
토큰.

318
00:29:29,000 --> 00:29:34,640
따라서 첫 번째는 어떠한 증류 없이 실제 레이블을 기반으로 학습됩니다.

319
00:29:34,640 --> 00:29:41,400
두 번째의 경우 자체 증류를 기반으로 훈련된 CLS와 같습니다.

320
00:29:41,400 --> 00:29:46,600
세 번째는 경증류(hard distillation)를 기반으로 학습된 CLS입니다.

321
00:29:46,600 --> 00:29:55,800
이제 이 세 가지 토큰은 CLS 토큰뿐만 아니라 증류 토큰도 고려합니다.

322
00:29:55,800 --> 00:30:05,080
그러나 첫 번째 행에서는 두 MLP 헤드 모두에 CLS 토큰을 전달하는 것을 볼 수 있습니다.

323
00:30:05,080 --> 00:30:12,520
두 번째 행에서는 증류 토큰을 MLP 헤드와 마지막 헤드 모두에 전달합니다.

324
00:30:12,520 --> 00:30:15,080
우리가 논의한 것과 동일합니다.

325
00:30:15,080 --> 00:30:20,840
이는 CLS 토큰이 하나의 MLP를 통과하고 증류 토큰이 다른 MLP를 통과하는 것과 같습니다.

326
00:30:20,840 --> 00:30:22,360
MLP 헤드.

327
00:30:22,360 --> 00:30:25,600
원래 아이디어와 같습니다.

328
00:30:25,600 --> 00:30:31,760
그리고 이 제품이 모든 사람보다 뛰어난 성능을 보인다는 것을 알 수 있습니다.

329
00:30:31,760 --> 00:30:38,240
또한 여기에서 증류를 사용하는 것이 실제로 증류만을 사용하는 것보다 더 나은 성능을 발휘한다는 것을 알 수 있습니다.

330
00:30:38,240 --> 00:30:44,880
진정한 라벨.

331
00:30:44,880 --> 00:30:50,800
CNN 모델인 ResNet, EfficientNet과 비교한 최종 결과입니다.

332
00:30:50,800 --> 00:31:00,800
이 표는 DIT와 CNN 사이에 동일한 수의 매개 변수가 있음을 보여줍니다.

333
00:31:00,800 --> 00:31:06,400
기본 모델인 DIT는 훨씬 더 빠르게 수행됩니다.

334
00:31:06,400 --> 00:31:18,400
처리량은 85.8이지만 CNN 기반의 경우 25.2와 같고 정확도는 85.8입니다.

335
00:31:18,400 --> 00:31:25,800
동일합니다.

336
00:31:25,800 --> 00:31:31,400
다음은 DIT와 EfficientNet 및 VIT의 최종 비교입니다.

337
00:31:31,400 --> 00:31:39,120
그래서 여기서 보면 VIT가 다른 모델에 비해 느리고 정확도가 낮다는 것을 알 수 있습니다.

338
00:31:39,120 --> 00:31:46,520
DIT는 VIT가 데이터 부족 체제에 대해 훈련을 받은 것과 동일하지만, 이는

339
00:31:46,520 --> 00:31:51,320
VIT보다는 낫지만 EfficientNet보다는 낫지는 않습니다.

340
00:31:51,320 --> 00:31:59,200
증류를 사용한 DIT는 실제로 CNN 기본 모델보다 성능이 뛰어납니다.

341
00:31:59,200 --> 00:32:04,960
또한 예를 들어 초당 이미지를 생각해 보세요.

342
00:32:04,960 --> 00:32:14,600
이 시점에서 동일한 속도로 DIT가 실제로 DIT보다 더 나은 성능을 발휘한다는 것을 알 수 있습니다.

343
00:32:14,600 --> 00:32:15,600
나머지.

344
00:32:15,600 --> 00:32:28,200
제가 이야기할 마지막 논문은 Microsoft에서 제안한 NIN Transformer입니다.

345
00:32:28,200 --> 00:32:34,720
이 문서의 실제 이름은 Shifted Windows를 사용하는 Hierarchical Vision Transformer입니다.

346
00:32:34,720 --> 00:32:37,480
이것이 바로 SWIN이 유래한 곳입니다.

347
00:32:38,480 --> 00:32:47,720
음, VIT의 계산 복잡성은 이미지 크기에 비례합니다.

348
00:32:47,720 --> 00:32:53,720
그리고 SWIN Transformer는 패치 병합(Patch Merging)이라는 두 가지 블록을 도입하여 이 문제를 해결하려고 합니다.

349
00:32:53,720 --> 00:32:56,960
및 SWIN 변압기.

350
00:32:56,960 --> 00:33:01,880
이것이 전체 아키텍처이며 각 아키텍처를 살펴볼 구성 요소는 다음과 같습니다.

351
00:33:01,880 --> 00:33:02,880
그들을.

352
00:33:02,880 --> 00:33:09,200
이제 이것은 H x W x 3, 3개 채널의 이미지입니다.

353
00:33:09,200 --> 00:33:14,640
이 이미지가 통과하는 첫 번째 구성 요소는 패치 파티션입니다.

354
00:33:14,640 --> 00:33:22,120
상상할 수 있듯이 이 구성 요소는 패치만 만듭니다.

355
00:33:22,120 --> 00:33:31,360
따라서 각 패치는 4 x 4 x 3이므로 이 패치의 크기는 48이고

356
00:33:31,360 --> 00:33:38,120
패치 수는 H를 4로 나눈 값, W를 4로 나눈 값입니다.

357
00:33:38,120 --> 00:33:41,720
이제 다음 구성 요소는 선형 임베딩입니다.

358
00:33:41,720 --> 00:33:49,440
이러한 패치가 포함된 이 이미지는 선형 임베딩 레이어를 통과하고 이 선형

359
00:33:49,440 --> 00:33:58,520
삽입하면 이 패치의 크기가 48에서 C로 변경됩니다. 이것이 유일한 변경 사항입니다.

360
00:33:58,520 --> 00:34:05,880
선형 임베딩으로 인해 패치 수가 동일해집니다.

361
00:34:05,880 --> 00:34:14,680
좋아요, 이제 여기서 SWIN Transformer 블록으로 이동합니다.

362
00:34:14,680 --> 00:34:17,680
SWIN Transformer에는 두 개의 하위 유닛이 포함되어 있습니다.

363
00:34:17,680 --> 00:34:22,440
각 장치는 Transformer 아키텍처와 동일합니다.

364
00:34:22,440 --> 00:34:31,920
첫 번째 장치의 MSA를 Window MSA로 교체하고 두 번째 하위 장치의 MSA를 교체합니다.

365
00:34:31,920 --> 00:34:35,560
Shifted Window MSA를 사용합니다.

366
00:34:35,560 --> 00:34:40,480
먼저 Window MSA에 대해 이야기해 보겠습니다.

367
00:34:40,480 --> 00:34:46,720
따라서 이는 Window MSA가 MSA와 정확히 동일하게 작동하는 것과 같습니다.

368
00:34:46,720 --> 00:34:55,200
Windows(예: 4 x 4 크기의 Windows)만 고려한 다음

369
00:34:55,200 --> 00:35:00,000
이 Windows 내에서 주의를 기울이십시오.

370
00:35:00,000 --> 00:35:09,520
이것이 바로 Window MSA가 변경한 것입니다. 이제 Window 크기가 고정되었으므로 복잡성이

371
00:35:09,520 --> 00:35:15,400
패치 수와 관련하여 선형입니다.

372
00:35:15,400 --> 00:35:19,600
이제 이 방법에는 명백한 문제가 있습니다.

373
00:35:19,600 --> 00:35:30,200
예를 들어, 여러분이 상상할 수 있듯이 여기 이 사진 중앙에 점이 있습니다.

374
00:35:30,200 --> 00:35:37,680
이제 Window MSA가 관심을 계산하는 동안 서로 다른 관심 사이에는 관심이 없습니다.

375
00:35:37,680 --> 00:35:38,680
윈도우.

376
00:35:38,680 --> 00:35:46,360
따라서 우리는 이 픽셀이 이 픽셀과 관련되어 있다는 것을 알고 있지만 Window MSA를 계산하는 동안

377
00:35:46,360 --> 00:35:49,040
그들 사이에는 관심이 없을 것입니다.

378
00:35:49,040 --> 00:35:53,480
이것이 문제입니다. 서로 다른 Windows 사이에 주의가 부족하다는 것입니다.

379
00:35:53,480 --> 00:35:57,920
그들은 두 번째 하위 단위를 도입했습니다.

380
00:35:57,920 --> 00:36:03,360
Shifted Window MSA의 작동 방식을 설명하기 전에 먼저 시도해 보겠습니다.

381
00:36:03,360 --> 00:36:09,600
이 제한을 해결하기 위해 창을 이 창 크기의 절반만큼 이동합니다.

382
00:36:09,600 --> 00:36:19,120
그래서 우리는 이 창을 여기에서 2x2씩 이동하고, 그다음 여기, 그 다음 여기, 그리고 여기로 이동합니다.

383
00:36:19,120 --> 00:36:27,040
이제 점이 그림 중앙에 있거나 그 안에 있으면 문제가 해결됩니다.

384
00:36:27,040 --> 00:36:30,680
사진 중간에 예약되어 있습니다.

385
00:36:30,800 --> 00:36:34,000
여기에 새로운 문제가 있습니다.

386
00:36:34,000 --> 00:36:43,040
4개의 Windows에서 주의력을 계산하는 대신 이제 주의력을 계산해야 합니다.

387
00:36:43,040 --> 00:36:44,280
9개의 Windows에서.

388
00:36:44,280 --> 00:36:52,080
그래서 우리는 계산 복잡성을 줄이고 싶었지만 실수로 일부 Windows를 추가했습니다.

389
00:36:52,080 --> 00:36:55,760
또한 여기에는 또 다른 문제가 있습니다.

390
00:36:55,760 --> 00:36:58,160
이 공백으로 무엇을 할까요?

391
00:36:58,160 --> 00:37:02,680
패딩을 추가합니까?

392
00:37:02,680 --> 00:37:08,160
이것이 Shifted Window MSA의 아이디어입니다.

393
00:37:08,160 --> 00:37:11,800
그들은 창을 한 방향으로 이동시킵니다.

394
00:37:11,800 --> 00:37:15,240
이제 여기 있는 녹색 부분이 여기로 이동합니다.

395
00:37:15,240 --> 00:37:21,400
이 분홍색 부분이 여기로 이동하고, 이 노란색 부분이 여기로 이동합니다.

396
00:37:21,400 --> 00:37:29,560
따라서 이것은 재정렬된 이미지와 같으며 Windows에 대한 MSA를 계산합니다.

397
00:37:29,560 --> 00:37:32,080
이 새로운 이미지.

398
00:37:32,080 --> 00:37:36,640
그런데 여기서 우리가 고려해야 할 것이 있습니다.

399
00:37:36,640 --> 00:37:42,320
이 이미지의 순서를 변경하여 원래 있던 픽셀 간의 관계를 만들었습니다.

400
00:37:42,320 --> 00:37:46,120
원본 이미지에는 존재하지 않았습니다.

401
00:37:46,120 --> 00:37:53,000
따라서 주의력을 계산하기 위해 마스크 MSA를 고려한 다음 마지막에는 역방향을 사용합니다.

402
00:37:53,000 --> 00:38:01,320
사이클 Shift는 이미지를 원래대로 재정렬하기 위한 것입니다.

403
00:38:01,320 --> 00:38:06,440
따라서 이것은 Swarm Transformer의 2스톱 장치가 될 것입니다.

404
00:38:06,440 --> 00:38:10,000
이후 이 출력은 Patch Merging을 거치게 됩니다.

405
00:38:10,000 --> 00:38:20,360
따라서 Patch Merging의 입력은 h를 4로 나눈 w, 4를 c로 나눈 값이 됩니다.

406
00:38:20,360 --> 00:38:21,640
그것은 이전의 것입니다.

407
00:38:21,640 --> 00:38:29,400
그리고 우리는 Transformer와 유사한 Swim Transformer가 치수를 변경하지 않는다는 것을 알고 있습니다.

408
00:38:29,400 --> 00:38:34,480
이제 패치 병합이 다시 Windows를 정의합니다.

409
00:38:34,480 --> 00:38:39,720
이제 Windows(예: 2x2)를 고려해 보겠습니다.

410
00:38:39,720 --> 00:38:43,120
Windows 내의 픽셀을 연결합니다.

411
00:38:43,120 --> 00:38:55,120
그래서 각 창 내에서 픽셀을 이렇게 모아 놓았습니다.

412
00:38:55,120 --> 00:39:05,680
따라서 이 과정이 끝나면 출력은 h를 8로 나누고 w를 8로 나눈 4c가 됩니다.

413
00:39:05,680 --> 00:39:09,840
각각의 치수는 c였습니다.

414
00:39:09,840 --> 00:39:13,000
이제 4c입니다.

415
00:39:13,000 --> 00:39:16,720
이는 선형 임베딩 레이어를 통과합니다.

416
00:39:16,720 --> 00:39:25,400
이 선형 임베딩은 차원을 4c에서 2c로 변경합니다.

417
00:39:25,400 --> 00:39:32,440
그리고 그것은 두 번째 Swim Transformer 블록의 입력이 될 것입니다.

418
00:39:32,440 --> 00:39:40,800
그래서 이 과정은 여기 보이는 4단계까지 계속됩니다.

419
00:39:40,800 --> 00:39:47,640
이제 이 모델의 출력을 다양한 작업에 사용할 수 있습니다.

420
00:39:47,640 --> 00:39:51,720
이미지 분류를 위해 마지막 출력을 사용할 수 있습니다.

421
00:39:51,720 --> 00:39:57,880
그리고 객체 감지 및 이미지 분할을 위해 이 모든 단계의 출력을 사용할 수 있습니다.

422
00:39:57,880 --> 00:40:04,560
또한 여기에 몇 가지 숫자가 표시될 수도 있습니다.

423
00:40:04,560 --> 00:40:11,320
이는 Swim Variance를 보여주므로 몇 가지 다른 Swin이 있을 것입니다.

424
00:40:11,320 --> 00:40:19,280
이들 사이의 차이점은 예를 들어 다음과 같이 이러한 레이어의 수에 따라 결정됩니다.

425
00:40:19,280 --> 00:40:27,160
수영 T는 2, 2, 6, 2가 되지만 수영 L은 2, 2, 18, 2가 됩니다.

426
00:40:27,160 --> 00:40:35,840
또한 이러한 변형의 또 다른 차이점은 c의 차원입니다.

427
00:40:35,840 --> 00:40:52,280
여기에서 볼 수 있듯이 마지막 Swim L은 VIT 및 다른 Transformer 아키텍처와 비교하여

428
00:40:52,280 --> 00:41:01,800
플롭도 적고 속도도 빠르고, 보세요, VIT에 비해 속도도 빠르고, 정확도도 높죠

429
00:41:01,800 --> 00:41:05,960
더 나은.

430
00:41:05,960 --> 00:41:10,120
이는 더 많은 모델을 사용해도 동일한 결과를 보여줍니다.

431
00:41:10,120 --> 00:41:18,560
이제 Swim을 이 네 개의 빨간색 블록과 비교해 보겠습니다.

432
00:41:18,560 --> 00:41:27,360
따라서 이러한 블록의 이미지 크기는 동일하거나 서로 유사합니다.

433
00:41:27,360 --> 00:41:36,840
다른 Transformer 모델에 비해 Swim은 플롭을 더 적게 사용합니다.

434
00:41:36,840 --> 00:41:41,480
EfficientNet에 비해 더 큽니다.

435
00:41:41,480 --> 00:41:50,800
다른 Transformer 모델보다 빠르지만, EfficientNet보다 느리지는 않습니다.

436
00:41:50,800 --> 00:42:00,240
또는 CNN 기반 모델이지만 Swim의 정확도는 CNN 기반 모델보다 낫습니다.

437
00:42:00,240 --> 00:42:08,280
EfficientNet보다 낫고 다른 Transformer보다 낫습니다.

438
00:42:08,280 --> 00:42:14,960
이는 Swim Transformer의 성능을 보여주며, 다른 Transformer에 비해 플롭을 덜 사용합니다.

439
00:42:14,960 --> 00:42:22,480
실제로 더 적은 수의 플롭을 사용하고 더 빠르며 정확도도 더 높습니다.