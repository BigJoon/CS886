1
00:00:00,000 --> 00:00:02,000
안녕하세요 이제 시작하겠습니다.

2
00:00:02,000 --> 00:00:06,000
오늘은 그 유명한 사전 훈련 모델을 소개하겠습니다.

3
00:00:06,000 --> 00:00:10,000
사전교육이 왜 필요한가 하는 질문부터 시작하겠습니다.

4
00:00:10,000 --> 00:00:14,000
일반적으로 새로운 작업이 오면 새로운 모델을 훈련해야 합니다.

5
00:00:14,000 --> 00:00:18,000
단점은 계산 비용이 많이 들고 소모적이라는 점입니다.

6
00:00:18,000 --> 00:00:23,000
또한 많은 저장 공간과 주석이 달린 데이터가 필요합니다.

7
00:00:23,000 --> 00:00:28,000
모델 압축, 가속과 같은 몇 가지 솔루션이 있었습니다.

8
00:00:28,000 --> 00:00:32,000
이러한 문제를 해결하는 것을 목표로 하는 시각적 학습.

9
00:00:32,000 --> 00:00:37,000
하지만 간단한 질문은 '전이 학습을 할 수 있는지 여부'입니다.

10
00:00:37,000 --> 00:00:45,000
따라서 새로운 작업이 오면 데이터 비용이 낮은 모델의 일부만 찾을 수 있습니다.

11
00:00:45,000 --> 00:00:50,000
컴퓨터 비전 언어 모델링의 목표는 비슷합니다.

12
00:00:50,000 --> 00:00:54,000
유일한 차이점은 데이터 형식이 다르다는 것입니다.

13
00:00:54,000 --> 00:00:58,000
이는 특징이 추출되는 방식의 차이로 이어집니다.

14
00:00:58,000 --> 00:01:01,000
이미 널리 적용되어 있습니다.

15
00:01:01,000 --> 00:01:06,000
우리는 TITOS가 컴퓨터 비전을 위해 사전 훈련된 많은 모델을 흡수했다는 것을 알고 있습니다.

16
00:01:06,000 --> 00:01:12,000
VGGFamily, ResNetFamily, InSectionFamily, MobileNetFamily 등과 같은 것입니다.

17
00:01:12,000 --> 00:01:17,000
이러한 사전 훈련된 모델은 이미 2012년부터 개발되어 대중에게 공개되었습니다.

18
00:01:17,000 --> 00:01:21,000
이미지의 계산이 만난 것처럼 추진력을 얻었습니다.

19
00:01:21,000 --> 00:01:26,000
언어 모델링부터 2018년 VIRT와 같은 모델의 등장

20
00:01:26,000 --> 00:01:32,000
자연어 처리 작업에 큰 영향을 미쳤습니다.

21
00:01:32,000 --> 00:01:37,000
여기에 포함된 모델은 오늘 소개할 여러 모델입니다.

22
00:01:37,000 --> 00:01:42,000
이 모델은 2019년경에 등장하기 시작합니다.

23
00:01:42,000 --> 00:01:46,000
그것은 NLP의 유망한 해였습니다.

24
00:01:46,000 --> 00:01:49,000
다음 질문은, 우리가 어디서부터 시작할 수 있는가 하는 것입니다.

25
00:01:49,000 --> 00:01:52,000
NLP에는 많은 작업이 있다는 것을 알고 있습니다.

26
00:01:52,000 --> 00:01:55,000
여기에서는 이러한 작업에 대한 간단한 검토를 제공합니다.

27
00:01:55,000 --> 00:02:00,000
첫 번째 작업은 복잡성 구문 분석입니다.

28
00:02:00,000 --> 00:02:03,000
이 작업의 목표는 작업을 이해하는 것입니다.

29
00:02:03,000 --> 00:02:07,000
태스크의 구조와 구문을 분석하고,

30
00:02:07,000 --> 00:02:11,000
문장을 나무와 같은 구조로 분해

31
00:02:11,000 --> 00:02:15,000
단어 간의 종속 관계를 결정합니다.

32
00:02:15,000 --> 00:02:21,000
이러한 다운스트림 애플리케이션에는 기계 번역 및 질문 응답 시스템이 포함됩니다.

33
00:02:21,000 --> 00:02:28,000
기계 번역의 목표는 작업을 한 언어에서 다른 언어로 자동 번역하는 것입니다.

34
00:02:28,000 --> 00:02:33,000
입력은 번역이 필요한 소스 언어 테스트이며,

35
00:02:33,000 --> 00:02:38,000
출력 데이터는 대상 언어로 번역된 작업입니다.

36
00:02:38,000 --> 00:02:41,000
그리고 세 번째 과제는 질문답변입니다.

37
00:02:41,000 --> 00:02:44,000
질의응답 방법은 여러 가지가 있지만,

38
00:02:44,000 --> 00:02:50,000
질문 답변의 목표는 사용자 쿼리에 대한 관련 답변을 생성하는 것입니다.

39
00:02:50,000 --> 00:02:55,000
주어진 상황이나 문서 세트를 기반으로 합니다.

40
00:02:55,000 --> 00:03:01,000
입력은 질문 또는 쿼리에 컨텍스트를 더한 것입니다.

41
00:03:01,000 --> 00:03:08,000
그런 다음 쿼리와 관련된 정보가 포함된 문서 집합을 검색합니다.

42
00:03:08,000 --> 00:03:12,000
그리고 출력은 질문에 대해 생성된 답변입니다.

43
00:03:12,000 --> 00:03:21,000
이제 우리는 일반 언어에 대한 사전 훈련 모델을 훈련해야 한다면,

44
00:03:21,000 --> 00:03:24,000
덜 동일한 양의 데이터로 훈련해야 합니다.

45
00:03:24,000 --> 00:03:28,000
그러나 이러한 작업 중 상당수는 감독되는 작업입니다.

46
00:03:28,000 --> 00:03:34,000
이미지 넷과 마찬가지로 광범위한 수동 주석이 필요합니다.

47
00:03:34,000 --> 00:03:40,000
그럼에도 불구하고 언어 데이터는 본질적으로 의미 정보와 일관성을 가지고 있습니다.

48
00:03:40,000 --> 00:03:47,000
그리고 이러한 기능을 활용하여 비지도 훈련 모델을 훈련합니다.

49
00:03:47,000 --> 00:03:52,000
이제 연구자들은 다음 단어 예측이라는 과제를 내놓았습니다.

50
00:03:52,000 --> 00:03:56,000
모델의 입력은 일련의 작업입니다.

51
00:03:56,000 --> 00:03:59,000
그런 다음 단어 임베딩 레이어로 이동합니다.

52
00:03:59,000 --> 00:04:05,000
입력 테스트의 각 단어를 조밀한 벡터 표현으로 테스트할 수 있습니다.

53
00:04:05,000 --> 00:04:09,000
각 단어에는 해당 단어 임베딩 벡터가 있습니다.

54
00:04:09,000 --> 00:04:14,000
이는 단어의 의미와 문맥 정보를 포착합니다.

55
00:04:14,000 --> 00:04:18,000
그런 다음 임베딩 벡터를 변환기에 입력합니다.

56
00:04:18,000 --> 00:04:23,000
그런 다음 패턴과 문맥을 학습하여 다음 단어를 예측합니다.

57
00:04:23,000 --> 00:04:27,000
모델 출력은 확률 분포입니다.

58
00:04:27,000 --> 00:04:34,000
이는 주어진 문맥에서 가능한 각 다음 단어가 발생할 가능성을 나타냅니다.

59
00:04:34,000 --> 00:04:40,000
가장 일반적인 다음 단어는 이 확률 분포에서 가장 높은 확률을 갖습니다.

60
00:04:40,000 --> 00:04:46,000
다음 단어 예측은 본질적으로 감독되지 않은 작업에 관한 것입니다.

61
00:04:46,000 --> 00:04:49,000
이제 단어 임베딩 레이어로 가보겠습니다.

62
00:04:49,000 --> 00:04:55,000
단어 임베딩의 목표는 이 훌륭한 기호의 단어를 연속적인 벡터 공간에 매핑하는 것입니다.

63
00:04:55,000 --> 00:05:01,000
단어의 문맥적 관계에 대한 의미론적 정보를 포착하는 것을 목표로 합니다.

64
00:05:01,000 --> 00:05:08,000
벡터 공간의 유사한 단어는 유사한 벡터 표현을 갖습니다.

65
00:05:08,000 --> 00:05:14,000
두 가지 전통적인 단어 임베딩 방법이 있습니다. 연속 단어 뒤와 건너뛰기 그랜드입니다.

66
00:05:14,000 --> 00:05:20,000
간단한 목표는 문맥의 주변 단어를 기반으로 타겟 단어를 예측하는 것입니다.

67
00:05:20,000 --> 00:05:25,000
입력은 고정된 크기의 문맥 내 주변 단어로 구성됩니다.

68
00:05:25,000 --> 00:05:30,000
일반적으로 몇 단어이며 출력은 대상 단어입니다.

69
00:05:30,000 --> 00:05:36,000
이 기호는 단어 신경망 아키텍처에 대한 얕은 적합성을 의미합니다.

70
00:05:36,000 --> 00:05:41,000
입력은 문맥 단어의 단어 임베딩 벡터의 평균이며,

71
00:05:41,000 --> 00:05:46,000
그런 다음 숨겨진 레이어를 통과하여 대상 단어를 예측합니다.

72
00:05:46,000 --> 00:05:55,000
스킵 그랜드 모델의 목적은 타겟 단어를 기반으로 문맥 내 주변 단어를 예측하는 것입니다.

73
00:05:55,000 --> 00:06:01,000
입력은 대상 단어이고 출력은 주변 단어로 구성됩니다.

74
00:06:01,000 --> 00:06:07,000
그리고 스킵 그랜드는 기호와 반대되는 신경망 아키텍처를 사용합니다.

75
00:06:07,000 --> 00:06:12,000
숨겨진 레이어를 통해 입력을 여러 출력에 매핑합니다.

76
00:06:12,000 --> 00:06:18,000
단어 표현을 위한 전역 벡터 주요 목표는 단어 벡터 표현을 배우는 것입니다.

77
00:06:18,000 --> 00:06:22,000
전 세계 어휘 통계를 기반으로 합니다.

78
00:06:22,000 --> 00:06:28,000
복잡한 창으로 어휘 동시출현 행렬을 구성합니다.

79
00:06:28,000 --> 00:06:34,000
그리고 단어들 사이의 영화적 관계를 배우세요.

80
00:06:34,000 --> 00:06:37,000
이제 우리는 또 다른 문제를 봅니다.

81
00:06:37,000 --> 00:06:46,000
단어 임베딩 벡터는 일반적으로 단어에서 앞뒤로 미학적 단어 임베딩을 얻습니다.

82
00:06:46,000 --> 00:06:48,000
그리고 그것들은 일대일 투영입니다.

83
00:06:48,000 --> 00:06:54,000
즉, 각 단어는 문맥을 고려하지 않고 고정된 벡터 표현을 갖습니다.

84
00:06:54,000 --> 00:07:03,000
이는 학습된 임베딩 모델의 사용법이 임베딩 레이어를 호출할 때 각 단어에 대한 임베딩을 검색할 수 있다는 것을 의미합니다.

85
00:07:03,000 --> 00:07:08,000
그러나 문제는 많은 단어에 다염색체 문제가 있다는 것입니다.

86
00:07:08,000 --> 00:07:12,000
하나의 단어가 문장마다 다른 의미를 가질 수 있습니다.

87
00:07:12,000 --> 00:07:21,000
예를 들어 Python과 Pandas라는 단어의 경우 둘 다 근처의 PI 잎을 나타낼 수 있습니다.

88
00:07:21,000 --> 00:07:24,000
또한 일종의 동물을 지칭하기도 합니다.

89
00:07:24,000 --> 00:07:28,000
이 문제를 해결하는 방법은 문맥화된 단어 임베딩(Contextualized Word Embedding)입니다.

90
00:07:28,000 --> 00:07:35,000
문맥화된 단어 임베딩의 주요 목표는 도식적 변형을 포착하는 것입니다.

91
00:07:35,000 --> 00:07:38,000
그리고 다른 맥락에서 단어의 다염색체.

92
00:07:38,000 --> 00:07:46,000
모델은 대상 단어의 이전 단어를 모두 사용하여 대상 단어를 예측합니다.

93
00:07:46,000 --> 00:07:48,000
그리고 직감이 나옵니다.

94
00:07:48,000 --> 00:07:54,000
목표 단어를 예측하기 위해 올바른 콘텐츠를 입력에 포함하지 않는 이유는 무엇입니까?

95
00:07:54,000 --> 00:08:00,000
이러한 방식으로 더 많은 정보를 통합함으로써 잠재적으로 예측 정확도를 향상시킬 수 있습니다.

96
00:08:00,000 --> 00:08:05,000
그러나 두 가지 중요한 사항을 염두에 두는 것이 중요합니다.

97
00:08:05,000 --> 00:08:08,000
가장 먼저 우려되는 점은 데이터 유출 문제다.

98
00:08:08,000 --> 00:08:13,000
예측 단어를 입력으로 포함할 수 없습니다.

99
00:08:13,000 --> 00:08:16,000
두 번째는 시퀀스 구조이다.

100
00:08:16,000 --> 00:08:21,000
시퀀스 순서가 중요하고 일부 정보가 포함되어 있으므로 이를 망칠 수 없습니다.

101
00:08:21,000 --> 00:08:26,000
그래서 연구자들은 이 문제를 해결하기 위해 양방향 언어 모델을 개발했습니다.

102
00:08:26,000 --> 00:08:30,000
여기서 건축은 출생과 조금 다릅니다.

103
00:08:30,000 --> 00:08:34,000
왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로 모듈로 구성됩니다.

104
00:08:34,000 --> 00:08:39,000
탄생과 동일하지 않은 것은 하나의 전체 모듈로 구성됩니다.

105
00:08:39,000 --> 00:08:43,000
ELMO는 언어 모델의 임베딩(Embeddings from Language Model)의 약어입니다.

106
00:08:43,000 --> 00:08:47,000
양방향 언어 모델은 ELMO의 기초입니다.

107
00:08:47,000 --> 00:08:50,000
입력은 일련의 토큰입니다.

108
00:08:50,000 --> 00:08:56,000
언어 모델은 기록을 바탕으로 다음 토큰의 수익성을 예측하는 방법을 학습합니다.

109
00:08:56,000 --> 00:09:02,000
정방향 경로에서는 기록에 대상 토큰 앞의 단어가 포함됩니다.

110
00:09:02,000 --> 00:09:07,000
양방향 예측은 숨겨진 상태가 있는 다층 LSTM으로 모델링됩니다.

111
00:09:07,000 --> 00:09:12,000
그런 다음 이 두 종류의 숨겨진 상태가 하위 최대 위치화에 들어갑니다.

112
00:09:12,000 --> 00:09:17,000
Sub-Max 레이어의 출력은 단어 예측의 확률입니다.

113
00:09:17,000 --> 00:09:21,000
모델은 음의 로그 가능성이 있는 후드를 최소화하도록 훈련되었습니다.

114
00:09:21,000 --> 00:09:27,000
이는 양방향에서 참 단어에 대한 로그 가능성 후드를 최대화하는 것을 의미합니다.

115
00:09:27,000 --> 00:09:34,000
여기서 매개변수는 LSTM, 임베딩 레이어, 하위 최대 레이어를 나타냅니다.

116
00:09:34,000 --> 00:09:36,000
아직 질문이 있습니다.

117
00:09:36,000 --> 00:09:42,000
지도 테스트를 훈련하는 경우 표현을 어떻게 포착하는지

118
00:09:42,000 --> 00:09:47,000
이러한 기능이 다운스트림 작업에 충분히 효과적일까요?

119
00:09:47,000 --> 00:09:52,000
그럼 ELMO 훈련을 통해 표현 캐치(representation catch)를 확인해 보겠습니다.

120
00:09:52,000 --> 00:09:58,000
ELMO에서는 bi-LN의 서로 다른 레이어가 서로 다른 숨겨진 상태를 생성합니다.

121
00:09:58,000 --> 00:10:02,000
각각은 테스트에 대한 다양한 유형의 정보를 포함합니다.

122
00:10:02,000 --> 00:10:06,000
다운스트림 관련 작업에 이 정보를 활용하려면

123
00:10:06,000 --> 00:10:14,000
ELMO는 숨겨진 상태의 모든 레이어를 집계하는 작업별 선형 조합을 학습합니다.

124
00:10:14,000 --> 00:10:19,000
여기서 감마와 s는 서로 다른 유형의 숨겨진 상태입니다.

125
00:10:19,000 --> 00:10:27,000
여기서 감마와 s는 튜닝에 대한 다운스트림 작업 중에 학습된 매개변수입니다.

126
00:10:27,000 --> 00:10:33,000
감마는 불일치를 수정하기 위해 도입된 스케일링 벡터입니다.

127
00:10:33,000 --> 00:10:41,000
bi-LN 숨겨진 상태의 분포와 작업별 표현의 분포 사이.

128
00:10:42,000 --> 00:10:44,000
s는 선형 계수입니다.

129
00:10:44,000 --> 00:10:53,000
각 최종 작업에 대해 학습하고 하위 최대값으로 정규화하여 제약 조건의 합을 1로 유지합니다.

130
00:10:53,000 --> 00:10:58,000
bi-LN 모듈의 최상위 계층에서 학습된 은닉 표현

131
00:10:58,000 --> 00:11:03,000
영화 작업에 대한 훨씬 더 나은 표현이 포함되어 있습니다.

132
00:11:03,000 --> 00:11:08,000
이 작업은 주어진 문맥에 따라 단어의 의미를 열광시킵니다.

133
00:11:08,000 --> 00:11:13,000
동일한 작업에는 첫 번째 레이어에서 학습한 것이 더 좋습니다.

134
00:11:13,000 --> 00:11:20,000
한 문장에서 단어의 문법적 역할을 추론합니다.

135
00:11:20,000 --> 00:11:24,000
누가 알겠는가, ELMO의 역량을 입증한 후,

136
00:11:24,000 --> 00:11:28,000
연구원들은 다양한 계층의 정보를 쌓을 수 있습니다.

137
00:11:28,000 --> 00:11:34,000
다양한 LLP 작업을 개선하는 데 도움이 됩니다.

138
00:11:34,000 --> 00:11:38,000
그렇다면 문제는 우리가 모델을 얻었느냐 하는 것입니다.

139
00:11:38,000 --> 00:11:41,000
그렇다면 다운스트림 작업에는 어떻게 적용하나요?

140
00:11:41,000 --> 00:11:49,000
이러한 출력 임베딩을 가져와 특정 작업에 사용하려는 아키텍처에 공급합니다.

141
00:11:49,000 --> 00:11:51,000
이것이 첫 번째 선택입니다.

142
00:11:51,000 --> 00:12:00,000
두 번째 선택은 다운스트림 특정 네트워크에 대한 가중치를 업데이트할 수 있다는 것입니다.

143
00:12:00,000 --> 00:12:04,000
하지만 ELMO 매개변수는 고정된 상태로 유지하세요.

144
00:12:04,000 --> 00:12:10,000
세 번째는 성과에 달려 있다는 것이다.

145
00:12:10,000 --> 00:12:18,000
성능이 좋지 않으면 전체 모델을 찾는 것을 선택할 수도 있습니다.

146
00:12:18,000 --> 00:12:23,000
여기서는 6개의 벤치마크 데이터 세트에서 엄청난 개선이 이루어진 것을 확인할 수 있습니다.

147
00:12:23,000 --> 00:12:33,000
질문 답변 및 텍스트 자격 부여 등.

148
00:12:33,000 --> 00:12:38,000
이제 우리는 그것이 다염색체의 핵심 문제를 해결하는지 여부에 대한 질문에 대답합니다.

149
00:12:38,000 --> 00:12:42,000
논문에 제시된 대조적인 사례를 살펴보겠습니다.

150
00:12:42,000 --> 00:12:46,000
원래의 지구 단어 임베딩을 사용한다면,

151
00:12:46,000 --> 00:12:50,000
그것은 오로지 공간에 담긴 단어에만 기반을 두고 있다.

152
00:12:50,000 --> 00:12:53,000
우리는 비슷한 문제를 많이 겪게 될 것입니다.

153
00:12:53,000 --> 00:12:59,000
놀이의 핵심 다염체성뿐만 아니라 다양한 놀이 설치도 포함됩니다.

154
00:12:59,000 --> 00:13:07,000
그러나 ELMO를 사용한다면 일부 상황에서는 유전적으로 유사한 놀이 요소를 검색할 것입니다.

155
00:13:07,000 --> 00:13:17,000
예를 들어, 희박한 게임이나 무대 공연을 언급하는 경우 ELMO는 동일한 의미의 연극만 찾습니다.

156
00:13:17,000 --> 00:13:20,000
그러나 ELMO에는 몇 가지 단점이 있습니다.

157
00:13:20,000 --> 00:13:24,000
변환기 대신 LSTN을 쉽게 나열합니다.

158
00:13:24,000 --> 00:13:31,000
변환기는 LSTN보다 더 나은 특징 추출 기능을 가지고 있다고 주장해야 합니다.

159
00:13:31,000 --> 00:13:35,000
당시 실제로 변압기가 제안됐다.

160
00:13:35,000 --> 00:13:47,000
누군가는 ELMO가 LSTN 대신 Transformer를 사용하면 더 나은 성능을 얻을 수 있다고 주장했습니다.

161
00:13:47,000 --> 00:13:52,000
다음 질문은 ELMO가 사전 훈련된 모델로 충분합니까?입니다.

162
00:13:52,000 --> 00:14:02,000
ELMO를 사전 훈련된 모델로 고수하고 다운스트림 작업과 모델 훈련을 위해 개선된 위치를 남겨 둘 것인지 여부

163
00:14:02,000 --> 00:14:07,000
그렇다면 왜 우리는 여전히 사전 훈련 모델을 추구합니까?

164
00:14:07,000 --> 00:14:10,000
이것은 슈퍼글루 종이의 이미지입니다.

165
00:14:10,000 --> 00:14:20,000
이 이미지의 주요 요점은 underglue 데이터 세트 요소에 비해 대규모 언어 모델의 성능이 얼마나 저조한지를 보여 주는 것입니다.

166
00:14:20,000 --> 00:14:22,000
Glue 데이터 세트는 벤치마크입니다.

167
00:14:22,000 --> 00:14:27,000
다양한 자연어 이해 작업을 수집합니다.

168
00:14:27,000 --> 00:14:32,000
이러한 작업은 NLP 모델의 성능을 평가하는 데 사용됩니다.

169
00:14:32,000 --> 00:14:36,000
이러한 작업에는 방금 ELMO에 대해 언급한 작업이 포함됩니다.

170
00:14:36,000 --> 00:14:44,000
감정 분류, 텍스트 매칭, 질문 페어링 매칭 등.

171
00:14:44,000 --> 00:14:47,000
각 작업에는 자체 교육과 작업이 설정되어 있습니다.

172
00:14:47,000 --> 00:14:54,000
연구자들이 모델의 첫 번째 웹 기능을 비교할 수 있습니다.

173
00:14:54,000 --> 00:14:58,000
여기서 각 열은 미래의 언어 모델을 나타냅니다.

174
00:14:58,000 --> 00:15:04,000
각 포인트는 이 모델의 성능, 즉 하나의 특정 작업을 나타냅니다.

175
00:15:04,000 --> 00:15:08,000
그리고 파란색 점선은 이 모델의 평균입니다.

176
00:15:08,000 --> 00:15:12,000
검은 선은 인간의 성과를 나타냅니다.

177
00:15:12,000 --> 00:15:15,000
따라서 우리 질문에 대한 대답은 다음과 같습니다.

178
00:15:15,000 --> 00:15:21,000
초기 ELMO 모델은 언어 작업에 대한 인간의 이해 수준에 도달하지 못했습니다.

179
00:15:21,000 --> 00:15:28,000
이것이 바로 연구자들이 언어 모델 사전 훈련에 대한 노력을 계속해야 하는 이유입니다.

180
00:15:28,000 --> 00:15:31,000
우리는 두 가지 수입이 있다는 것을 알고 있습니다.

181
00:15:31,000 --> 00:15:36,000
Baidu가 개발한 첫 번째 것은 지식 통합을 통해 향상된 표현입니다.

182
00:15:36,000 --> 00:15:40,000
내부에는 충분히 좋은 무작위 마스킹 반전이 있습니다.

183
00:15:40,000 --> 00:15:46,000
우리는 여러 단어가 함께 의미를 나타내는 문구가 있다는 것을 알고 있습니다.

184
00:15:46,000 --> 00:15:53,000
마스킹을 부여하는 전략은 구문이나 단어 조합의 무결성을 방해할 수 있습니다.

185
00:15:53,000 --> 00:15:57,000
또 다른 문제는 일반화 능력이다.

186
00:15:57,000 --> 00:16:01,000
예를 들어 이전 모델에는 Apple Watch만 표시됩니다.

187
00:16:01,000 --> 00:16:08,000
자, 애플이 애플 비전 프로(Apple Vision Pro)를 출시했는데, 해당 모델에서는 애플의 제품으로 인식할 수 있을까?

188
00:16:08,000 --> 00:16:12,000
또는 이러한 문구에 대한 임베딩 스탠드를 생성할 수 있습니다.

189
00:16:12,000 --> 00:16:17,000
임베딩 공간에서 다른 Apple 제품과 임베딩이 가까워지도록 합니다.

190
00:16:17,000 --> 00:16:27,000
그래서 그들은 단위가 해석을 유지하면서 전체 단어를 마스킹하는 시간에 맞춰 전체 단어 마스킹을 제안했습니다.

191
00:16:27,000 --> 00:16:32,000
그렇다면 첫 번째 질문은 어느 수준까지 마스킹해야 하는가입니다.

192
00:16:32,000 --> 00:16:36,000
얼굴 수준과 엔터티 수준의 두 가지 수준이 있습니다.

193
00:16:36,000 --> 00:16:46,000
예를 들어, 얼굴 수준에서는 시리즈와 같은 항목을 마스킹하거나 문장의 일부 문구를 마스킹합니다.

194
00:16:46,000 --> 00:16:56,000
엔터티 수준에서는 JK Rowling을 마스킹하므로 개인 이름, 조직 이름, 제품 이름과 같은 일부 이름 엔터티를 마스킹합니다.

195
00:16:56,000 --> 00:17:03,000
기본 레벨과 비교하여 기본 레벨에서는 각 단어에 한자가 하나씩 무작위로 가려집니다.

196
00:17:03,000 --> 00:17:05,000
그런 다음 더 많은 질문이 생성되었습니다.

197
00:17:05,000 --> 00:17:14,000
첫째, 비지도 학습이기 때문에 마스크할 개체와 문구를 어떻게 인식했습니까?

198
00:17:14,000 --> 00:17:22,000
우리가 우려하는 것은 다른 감독 정보가 포함되어 있는지, 추가 정보가 필요한지 여부입니다.

199
00:17:22,000 --> 00:17:30,000
음성 태깅 도구의 자동 부분과 공백과 같은 이름 엔터티 인식 도구를 사용합니다.

200
00:17:30,000 --> 00:17:39,000
글쎄, JK는 NLP를 대표하여 문구와 엔터티를 식별하므로 사람이 주석을 달지 않도록 할 수 있습니다.

201
00:17:39,000 --> 00:17:44,000
두 번째 질문은 두 수준 사이의 관계는 무엇입니까?

202
00:17:44,000 --> 00:17:55,000
대답은 이 세 가지 수준이 비순차적으로 마스크되고, 먼저 기본 수준에서 마스크되고, 엔터티 수준, 구문 수준에서 마스크됩니다.

203
00:17:55,000 --> 00:17:59,000
따라서 최종 마스크 입력에는 모든 종류의 마스크가 포함됩니다.

204
00:17:59,000 --> 00:18:02,000
마지막으로, 출력물에 어떻게 라벨을 붙였나요?

205
00:18:02,000 --> 00:18:09,000
상황에 맞는 학습이기 때문에 단어의 지속적인 마스크는 훈련하는 사람들에게 영향을 미치지 않습니다.

206
00:18:09,000 --> 00:18:13,000
그 입력 출력은 새와 동일합니다.

207
00:18:13,000 --> 00:18:22,000
아키텍처의 경우 학습은 Judy와 Bert가 했던 것처럼 기본 인코더로서의 레이어 변환기가 아닙니다.

208
00:18:22,000 --> 00:18:33,000
학습은 또한 대화 데이터를 훈련 데이터에 통합하여 최종적으로 참조 관계와 영화적 일관성이라는 두 가지 측면에 중점을 둡니다.

209
00:18:33,000 --> 00:18:41,000
예를 들어 여기서의 동기는 빨간색 선의 단어를 마스크하고 마지막 두 문장에 대해서만 훈련하는 경우입니다.

210
00:18:41,000 --> 00:18:44,000
우리는 그들이 노트북용으로 존재하는지 알 수 없습니다.

211
00:18:44,000 --> 00:18:49,000
또한 사용자가 무엇을 선택하고 구매하는지 알 수 없습니다.

212
00:18:49,000 --> 00:19:00,000
모델에게 마지막 두 문장만 주고 빈칸만 채워주면 바로 컴퓨터 구매용으로 출력될 수도 있습니다.

213
00:19:00,000 --> 00:19:02,000
성능을 확인해 보겠습니다.

214
00:19:02,000 --> 00:19:09,000
이는 새에 비해 이 NLP 작업이 약간 향상되었음을 보여줍니다.

215
00:19:09,000 --> 00:19:17,000
중국어에서는 항상 여러 단어를 사용하기 때문에 이러한 작업 중 대부분은 교차 혼합을 위한 것입니다.

216
00:19:17,000 --> 00:19:21,000
이 논문은 우리의 목표가 아니기 때문에 잠깐 살펴봅니다.

217
00:19:21,000 --> 00:19:27,000
이 논문은 또다른 수익과 동일한 약어를 가지고 있었습니다.

218
00:19:27,000 --> 00:19:30,000
이제 우리는 또 다른 문제를 고려합니다.

219
00:19:30,000 --> 00:19:37,000
대부분의 최신 사전 학습 언어 모델에는 많은 계산 리소스가 필요합니다.

220
00:19:37,000 --> 00:19:39,000
어떻게 비용을 줄일 수 있나요?

221
00:19:39,000 --> 00:19:50,000
앞서 언급했듯이 다른 사람들은 데이터 관점에 대해 생각하거나 미래 학습을 사용하거나 모델 압축과 같은 관점을 알 수도 있습니다.

222
00:19:50,000 --> 00:19:54,000
하지만 현재 우리는 대규모 언어 모델을 사전 훈련하는 작업을 진행하고 있습니다.

223
00:19:54,000 --> 00:19:59,000
성능과 시간 소모 사이의 균형을 고려할 수 없습니다.

224
00:19:59,000 --> 00:20:02,000
우리는 성능을 희생할 수 없습니다.

225
00:20:02,000 --> 00:20:11,000
그래서 일부 연구자들은 시퀀스 생성 작업 후에 작업을 변경할 수 있는지 여부는 꽤 시간이 많이 걸린다고 생각합니다.

226
00:20:11,000 --> 00:20:21,000
원래 시퀀스 예측을 이진 분류 작업으로 변경할 수 있는지 여부, 이 이진 신호는 이 토큰이 교체되었는지 또는 변경되었는지 여부입니다.

227
00:20:21,000 --> 00:20:24,000
예를 들어, 여기서는 두 단어가 필수입니다.

228
00:20:24,000 --> 00:20:27,000
하나는 이고 다른 하나는 8 입니다.

229
00:20:27,000 --> 00:20:32,000
이는 칩이 80보다 먼저 식사를 요리할 수 있음을 의미합니다.

230
00:20:32,000 --> 00:20:36,000
따라서 8은 대체될 ​​수 있는 단어입니다.

231
00:20:36,000 --> 00:20:48,000
그런 다음 이 설계가 합리적인지, 식별 작업에 대한 모델 훈련이 시퀀스 간 사전 훈련된 모델을 대체할 수 있는지에 대한 질문이 나옵니다.

232
00:20:48,000 --> 00:20:51,000
대답은 다소 합리적이라는 것입니다.

233
00:20:51,000 --> 00:20:59,000
또한 판별 작업을 수행하려면 모델이 주어진 문장에서 문법적, 의미적 정보를 캡처해야 합니다.

234
00:20:59,000 --> 00:21:04,000
예를 들어, 이 두 문장의 차이점은 동사입니다.

235
00:21:04,000 --> 00:21:10,000
따라서 이 단어는 주어진 문맥에 따라 다른 단어가 될 수 있는 유연성이 매우 높습니다.

236
00:21:10,000 --> 00:21:15,000
그러나 단어의 경우, 앞에 사용되거나 사용되지 않는 것이 일반적인 사용법입니다.

237
00:21:15,000 --> 00:21:18,000
따라서 교체될 가능성이 없습니다.

238
00:21:19,000 --> 00:21:24,000
따라서 언어 모델은 여기서 단어가 어떻게 작동하는지 알아야 합니다.

239
00:21:24,000 --> 00:21:30,000
그러면 여기에 있는 단어가 유연성이 높은지 여부를 구별할 수 있습니다.

240
00:21:30,000 --> 00:21:35,000
또 다른 관점은 부작용에 가깝다는 것입니다.

241
00:21:35,000 --> 00:21:43,000
여기서 언어 모델은 단어의 유연성뿐만 아니라 사전 사용법도 학습할 수 있습니다.

242
00:21:43,000 --> 00:21:51,000
따라서 사용자가 입력한 일부 문장의 화살표 및 오타를 처리할 수 있습니다.

243
00:21:51,000 --> 00:21:57,000
그러면 우리는 이것이 비지도 학습이라는 문제에 다시 직면하게 됩니다.

244
00:21:57,000 --> 00:22:00,000
주석 없이 데이터를 어떻게 생성할 수 있나요?

245
00:22:00,000 --> 00:22:08,000
가장 간단한 방법은 말뭉치에서 다른 단어를 무작위로 샘플링하여 대상 단어를 대체하는 것입니다.

246
00:22:08,000 --> 00:22:14,000
하지만 이 방법을 적용하면 우리가 얻은 네이티브 샘플을 구별할 수 없습니다.

247
00:22:14,000 --> 00:22:16,000
그것은 의미가 없습니다.

248
00:22:16,000 --> 00:22:24,000
다시 말하지만, 우리의 목표는 문장에서 의미 있는 정보를 표현할 수 있는 대규모 언어 모델을 훈련하는 것입니다.

249
00:22:24,000 --> 00:22:28,000
또한 강력한 차별 모델을 훈련해야 합니다.

250
00:22:28,000 --> 00:22:32,000
따라서 여기서 합리적인 시퀀스 데이터를 생성해야 합니다.

251
00:22:32,000 --> 00:22:39,000
그렇다면 이 시퀀스 생성 작업을 위한 가장 간단한 방법은 무엇입니까?

252
00:22:39,000 --> 00:22:46,000
이 연구자들은 마스크의 동기화된 문장을 기반으로 시퀀스를 생성하기 위해 작은 새를 사용하는 것에 대해 생각합니다.

253
00:22:46,000 --> 00:22:52,000
따라서 이것은 더 구별되는 모발을 생산하는 가장 저렴한 방법입니다.

254
00:22:52,000 --> 00:22:55,000
여러 가지 문제도 나왔습니다.

255
00:22:55,000 --> 00:23:03,000
이미 사전 훈련된 모델에 대해 훈련을 받았기 때문에 작은 새 한 마리와 비교하여 개선되는지 여부입니다.

256
00:23:03,000 --> 00:23:11,000
조금만 개선된다면 총 정류 비용도 대부분의 훈련 모델을 능가할 것입니다.

257
00:23:11,000 --> 00:23:13,000
그렇다면 그것은 합리적이지 않습니다.

258
00:23:13,000 --> 00:23:22,000
그리고 작은 질문은 그들이 작은 새를 찾을 것인지 아니면 그냥 작은 새를 얼릴 것인지입니다.

259
00:23:22,000 --> 00:23:31,000
대답은 '예'입니다. 모델에서 찾을 수 있지만 부동 소수점 연산을 저장하는 방식입니다.

260
00:23:31,000 --> 00:23:41,000
따라서 벤치마크 설정을 비교하기 위해 동일한 부동 소수점 연산을 사용하여 Google 데이터 세트의 성능을 비교했습니다.

261
00:23:41,000 --> 00:23:47,000
이에 대해서는 추후 성능 부분에서 확인해보도록 하겠습니다.

262
00:23:47,000 --> 00:23:55,000
또 다른 질문은 이 모델의 출력이 이진 신호인 경우 다운스트림 작업에 이 모델을 어떻게 적용할 것인가입니다.

263
00:23:55,000 --> 00:24:05,000
따라서 훈련 여부에 관계없이 먼저 이 부분에서 언어 모델의 더 많은 기능을 이미 보유하고 있다고 가정합니다.

264
00:24:05,000 --> 00:24:08,000
출력은 확률 벡터입니다.

265
00:24:08,000 --> 00:24:10,000
임베디드 출력이라고 볼 수 있습니다.

266
00:24:10,000 --> 00:24:22,000
따라서 출력을 다른 모듈에 전달하고 작은 신경망과 같은 다른 모듈을 추가하여 미세 조정하면 됩니다.

267
00:24:22,000 --> 00:24:30,000
분류 작업의 경우 일반적으로 하나 이상의 완전히 연결된 레이어가 선택 모델 위에 추가됩니다.

268
00:24:30,000 --> 00:24:40,000
그리고 시퀀스 라벨링 작업의 경우 레벨을 예측하기 위해 각 토큰의 기사 상단에 분류 레이어를 추가하는 작업이 포함될 수 있습니다.

269
00:24:40,000 --> 00:24:46,000
여기서의 디자인은 마스킹할 입력에서 위치를 무작위로 샘플링하는 것입니다.

270
00:24:46,000 --> 00:24:52,000
그런 다음 이 마스크 토큰은 생성기로 표시된 작은 새에 의해 예측됩니다.

271
00:24:52,000 --> 00:24:57,000
그런 다음 판별자는 이진 분류 작업을 학습합니다.

272
00:24:57,000 --> 00:25:01,000
레이블은 패킷 단어가 대체되는지 여부입니다.

273
00:25:01,000 --> 00:25:05,000
이 판별자의 아키텍처는 asperd입니다.

274
00:25:05,000 --> 00:25:12,000
여기서 GET과 GAN의 차이점은 여기서 단어가 마스크되어 있고 분리되어 있다는 것입니다.

275
00:25:12,000 --> 00:25:19,000
따라서 이 판별기의 기울기는 Ralu와 약간 유사하게 생성기로 전달될 수 없습니다.

276
00:25:19,000 --> 00:25:27,000
따라서 연결되어 있지 않기 때문에 발전기의 손실 목표는 정지된 스테인리스 새만 설정할 수 있습니다.

277
00:25:27,000 --> 00:25:35,000
생성기의 손실은 음의 로그 가능성이며 다른 대규모 언어 모델에서도 마찬가지입니다.

278
00:25:35,000 --> 00:25:41,000
판별기의 손실은 분류 작업이므로 교차 엔트로피입니다.

279
00:25:41,000 --> 00:25:48,000
그런 다음 선형 조합과 동시에 훈련됩니다.

280
00:25:48,000 --> 00:25:56,000
그럼 ELECTRA와 GAN을 비교해 보겠습니다. ELECTRA는 비슷한 아키텍처를 가지고 있더라도 GAN과 상당히 다릅니다.

281
00:25:56,000 --> 00:26:05,000
ELECTRA의 입력은 노이즈가 아닌 실제 테스트이고, 출력은 가짜가 아닌 실제 테스트입니다.

282
00:26:05,000 --> 00:26:10,000
그리고 판별자를 속이는 일이 전혀 발생하지 않습니다.

283
00:26:10,000 --> 00:26:19,000
그래디언트 관점에서 보면 판별자에서 생성자로 기울어질 수 없습니다.

284
00:26:19,000 --> 00:26:26,000
그러나 또한 판별자와 가중치를 공유하여 성능을 향상시킬 수 있는지도 고려했습니다.

285
00:26:26,000 --> 00:26:35,000
결국 판별자는 단일 이진 분류 작업에 대해서만 훈련되므로 훈련이 약간 어려울 수 있습니다.

286
00:26:35,000 --> 00:26:43,000
시험하는 동안 이 방법이 도움이 될 수 있으며 여기서는 시각화되지 않은 몇 가지 측정항목을 종이에 넣었습니다.

287
00:26:43,000 --> 00:26:52,000
두 번째 고려 사항은 그들의 구조와 임무에 기초하여 가장 기본적인 능력을 가질 수 있는 가장 작은 새는 무엇입니까?

288
00:26:52,000 --> 00:26:56,000
그래서 그들은 레이어를 줄이고 성능을 확인하려고 노력합니다.

289
00:26:56,000 --> 00:27:03,000
여기서 이미지의 X 라벨은 생성기의 크기이고 Y 라벨은 접착 성능입니다.

290
00:27:03,000 --> 00:27:13,000
결론적으로, 생성기의 크기가 판별기의 절반 또는 25%일 때 성능이 가장 좋습니다.

291
00:27:13,000 --> 00:27:17,000
생성기와 판별기 사이의 다른 비율 설정과 비교합니다.

292
00:27:17,000 --> 00:27:24,000
여기 내부에는 판별자 훈련의 난이도를 높이는 강력한 생성기가 있습니다.

293
00:27:24,000 --> 00:27:27,000
세 번째는 그라데이션 문제입니다.

294
00:27:27,000 --> 00:27:34,000
저자는 판별기에서 생성기로 기울기를 전달하기 위해 몇 가지 트릭과 함께 다른 법칙을 사용했습니다.

295
00:27:34,000 --> 00:27:43,000
여기서 X 라벨은 원래의 전자가 다른 트릭을 추가하는 전자보다 우수하다는 것을 보여주므로 자세한 내용은 다루지 않겠습니다.

296
00:27:43,000 --> 00:27:50,000
또 다른 비결은 발전기를 먼저 훈련시킨 다음 첫 번째 단계에서 방문하는 것입니다.

297
00:27:50,000 --> 00:27:56,000
두 번째 단계에서는 판별기 가중치의 가중치를 생성기 가중치로 초기화합니다.

298
00:27:56,000 --> 00:28:00,000
그런 다음 동일한 단계로 판별자를 훈련합니다.

299
00:28:00,000 --> 00:28:02,000
그럼 성능을 확인해 볼까요?

300
00:28:02,000 --> 00:28:06,000
왼쪽은 왼쪽, 오른쪽의 축소 버전입니다.

301
00:28:06,000 --> 00:28:15,000
세로 축에는 글루 점수가 있고 가로 축에는 부동 소수점 연산을 측정합니다.

302
00:28:15,000 --> 00:28:22,000
결론은 전자가 항상 동일한 흐름 지점에서 새보다 성능이 뛰어나다는 것입니다.

303
00:28:22,000 --> 00:28:27,000
여기서 목표는 전자가 훈련 효율성을 향상시키는지 확인하는 것입니다.

304
00:28:27,000 --> 00:28:35,000
여기서 측정항목은 추론, 부동 소수점 연산 횟수 및 매개변수 수를 교육합니다.

305
00:28:35,000 --> 00:28:39,000
속도 향상은 흐름을 기반으로 계산됩니다.

306
00:28:39,000 --> 00:28:47,000
단일 GPU에서 훈련하기 위해 저자는 Bird Small을 기반 모델로 하여 Electron Small을 생성합니다.

307
00:28:47,000 --> 00:28:50,000
그런 다음 L 모드 GVT 및 새와 비교합니다.

308
00:28:50,000 --> 00:29:00,000
결론은 부동 백만 개의 매개 변수 또는 1억 천만 개의 매개 변수에 관계없이 다음과 같습니다.

309
00:29:00,000 --> 00:29:07,000
동일한 수준의 매개변수 하에서 전자는 항상 새보다 성능이 뛰어납니다.

310
00:29:07,000 --> 00:29:15,000
그런 다음 저자는 로봇과 비교하기 위해 더 큰 전자의 성능이 어떤지 확인합니다.

311
00:29:15,000 --> 00:29:20,000
여기서 측정항목은 부동 소수점입니다.

312
00:29:20,000 --> 00:29:34,000
동일한 수준의 부동 소수점 연산에서 전자는 항상 여러 클래스에서 로봇보다 성능이 뛰어나다는 것을 알 수 있습니다.

313
00:29:34,000 --> 00:29:36,000
여기에 또 다른 수입이 있습니다.

314
00:29:36,000 --> 00:29:41,000
정보 엔터티로 언어 표현이 있습니다.

315
00:29:41,000 --> 00:29:47,000
또 다른 통찰력은 구조화된 기업 정보를 수익에 통합할 수 있는지 여부입니다.

316
00:29:47,000 --> 00:29:53,000
예를 들어, 여기서 사람은 작곡가와 작가라는 두 가지 정체성을 가지고 있습니다.

317
00:29:53,000 --> 00:29:58,000
따라서 이 개인 이름 엔터티와 다른 항목 간의 관계는 다릅니다.

318
00:29:58,000 --> 00:30:03,000
우리는 또한 이 관계를 모델에 통합하고 싶습니다.

319
00:30:03,000 --> 00:30:12,000
그러나 우리는 이 추가 정보가 관계와 같은 몇 가지 작업을 시험하는 데 도움이 되기를 바랍니다.

320
00:30:12,000 --> 00:30:17,000
여기서 입력은 관계가 있는 텍스트와 엔터티입니다.

321
00:30:17,000 --> 00:30:20,000
아키텍처는 두 부분으로 구성됩니다.

322
00:30:20,000 --> 00:30:32,000
첫 번째 부분의 입력은 문장뿐이고 두 번째 부분의 입력은 첫 번째 모듈의 임베딩 출력과 엔터티 임베딩입니다.

323
00:30:32,000 --> 00:30:34,000
여기서 우리의 관심사는 작업입니다.

324
00:30:34,000 --> 00:30:38,000
엔터티 관계 예측 작업을 준비합니다.

325
00:30:38,000 --> 00:30:41,000
일부 토큰 엔터티 정렬을 무작위로 마스킹합니다.

326
00:30:41,000 --> 00:30:47,000
그런 다음 모델이 정렬 토큰을 기반으로 해당 엔터티를 모두 예측하도록 합니다.

327
00:30:47,000 --> 00:30:50,000
그래서 여기 부분적으로는 알려지지 않았고 부분적으로는 가려져 있지 않습니다.

328
00:30:50,000 --> 00:30:58,000
여기서 또 다른 관심사는 두 정보를 모두 나타내는 더 나은 임베딩 출력을 얻기 위해 병합하는 방법입니다.

329
00:30:58,000 --> 00:31:08,000
두 번째 부분에서는 엔터티와 토큰이 모두 멀티 헤드와 어텐션으로 독립적으로 전달되어 동일한 임베딩 공간에 투영됩니다.

330
00:31:08,000 --> 00:31:13,000
그런 다음 비엔티티 정렬 관계에 따라 병합됩니다.

331
00:31:13,000 --> 00:31:16,000
엔터티에 따라 첫 번째 토큰까지.

332
00:31:16,000 --> 00:31:20,000
그런 다음 융합을 위해 융합층으로 들어갑니다.

333
00:31:20,000 --> 00:31:27,000
여기서는 엔터티 관계 분류 작업이므로 글로는 크로스엔더맨입니다.

334
00:31:28,000 --> 00:31:31,000
다운스트림 작업에 이 모델을 적용하려면

335
00:31:31,000 --> 00:31:35,000
유일한 노력은 입력을 수정하는 것입니다.

336
00:31:35,000 --> 00:31:37,000
따라서 LLP 작업을 계산해 보세요.

337
00:31:37,000 --> 00:31:40,000
자리표시자를 blend로 남겨두기만 하면 됩니다.

338
00:31:40,000 --> 00:31:44,000
그리고 타이핑이나 관계 분류의 엔터티에 대해서는

339
00:31:44,000 --> 00:31:51,000
이 작업에 사용되는 특수 토큰으로 자리 표시자를 대체합니다.

340
00:31:52,000 --> 00:31:58,000
성과는 Ernie가 이러한 이름 엔터티 작업과 관계 작업을 개선하는 것을 목표로 한다는 것입니다.

341
00:31:58,000 --> 00:32:04,000
따라서 이 작업에서는 Bert보다 성능이 뛰어납니다.

342
00:32:04,000 --> 00:32:07,000
이제 우리는 또 다른 문제를 고려합니다.

343
00:32:07,000 --> 00:32:10,000
Bert는 작업을 생성하는 데 능숙하지 않습니다.

344
00:32:10,000 --> 00:32:15,000
마스크 단어가 포함된 보류 문장에 대해 훈련되었기 때문입니다.

345
00:32:15,000 --> 00:32:20,000
GVD 입력과 같은 부분 시퀀스 데이터에서는 단어를 생성할 수 없습니다.

346
00:32:20,000 --> 00:32:24,000
따라서 Bert는 인코더 역할만 수행합니다.

347
00:32:24,000 --> 00:32:30,000
시퀀스 간 작업을 위해서는 디코더를 훈련시켜야 합니다.

348
00:32:30,000 --> 00:32:32,000
그래서 질문이 나온다.

349
00:32:32,000 --> 00:32:36,000
시퀀스 간 작업을 Bert에 흡수하는 방법은 무엇입니까?

350
00:32:36,000 --> 00:32:38,000
이 질문에 대답하려면,

351
00:32:38,000 --> 00:32:42,000
먼저 다양한 초과 언어 모델의 요소를 확인합니다.

352
00:32:42,000 --> 00:32:46,000
따라서 여러 종류의 LLP 작업이 있습니다.

353
00:32:46,000 --> 00:32:50,000
그러나 세 가지 유형의 작업으로 분류할 수 있습니다.

354
00:32:50,000 --> 00:32:55,000
Bert가 포함하는 양방향 언어 모델입니다.

355
00:32:55,000 --> 00:32:59,000
두 번째는 왼쪽에서 오른쪽 언어 모델입니다.

356
00:32:59,000 --> 00:33:01,000
완전 한 방향이군요.

357
00:33:01,000 --> 00:33:04,000
세 번째는 시퀀스 간 작업입니다.

358
00:33:04,000 --> 00:33:06,000
첫 문장에서는,

359
00:33:06,000 --> 00:33:11,000
타겟 단어의 오른쪽 정보를 입력으로 볼 수 있습니다.

360
00:33:11,000 --> 00:33:14,000
그러나 두 번째 순서에서는

361
00:33:14,000 --> 00:33:18,000
우리는 단방향 예측만 할 수 있습니다.

362
00:33:18,000 --> 00:33:21,000
따라서 Bert와 GVD의 두 부분이 모두 포함됩니다.

363
00:33:21,000 --> 00:33:26,000
작업은 실제로 입력과 출력에 의해 결정됩니다.

364
00:33:26,000 --> 00:33:30,000
LLP, Bert, GVD의 경우 입력이 손실되어야 합니다.

365
00:33:30,000 --> 00:33:35,000
그리고 출력은 단어 예측의 확률입니다.

366
00:33:35,000 --> 00:33:39,000
그래서 그들은 언어 모델의 핵심이 상실되었음을 발견합니다.

367
00:33:39,000 --> 00:33:43,000
이는 언어 모델을 통합하는 자연스러운 방법이기도 합니다.

368
00:33:43,000 --> 00:33:48,000
그래서 저자는 그것을 UnitLM이라고 불렀습니다.

369
00:33:48,000 --> 00:33:53,000
그렇다면 문제는 질량 측정의 관점에서 발생합니다.

370
00:33:53,000 --> 00:33:58,000
입력과 출력을 표시된 대로 둘 수 있는지 여부,

371
00:33:58,000 --> 00:34:03,000
그러나 다양한 작업을 제어하기 위해 질량 측정법을 변경합니다.

372
00:34:03,000 --> 00:34:07,000
따라서 우리는 네트워크 훈련을 제어할 수 있습니다.

373
00:34:07,000 --> 00:34:11,000
이 질량 측정법을 변경하면 됩니다.

374
00:34:11,000 --> 00:34:14,000
이전 설명에서,

375
00:34:14,000 --> 00:34:17,000
이는 입력에 다음이 포함될 수 있는지 여부를 나타내야 합니다.

376
00:34:17,000 --> 00:34:19,000
어떤 종류의 정보인지.

377
00:34:19,000 --> 00:34:23,000
왼쪽, 오른쪽, 또는 조합.

378
00:34:24,000 --> 00:34:29,000
이 문서에는 정확도를 높이기 위해 다른 폭발이 포함되어 있습니다.

379
00:34:43,000 --> 00:34:46,000
여기서 우리의 관심사는 각 작업의 개선입니다.

380
00:34:46,000 --> 00:34:49,000
모든 종류의 작업, unitLM 성능.

381
00:34:49,000 --> 00:34:53,000
요약 작업의 성능은 다음과 같습니다.

382
00:34:53,000 --> 00:34:58,000
추출적 QA 작업, 질문 및 응답 생성 작업입니다.

383
00:35:00,000 --> 00:35:04,000
다음으로 사전 훈련이 다운스트림 작업에 어떻게 도움이 되는지 확인하고 싶습니다.

384
00:35:04,000 --> 00:35:06,000
이것은 종이로 만든 그림입니다.

385
00:35:06,000 --> 00:35:10,000
Bert의 효과를 시각화하고 이해합니다.

386
00:35:10,000 --> 00:35:14,000
그림을 보면 비교를 해보자면

387
00:35:14,000 --> 00:35:17,000
처음부터 훈련된 모델

388
00:35:17,000 --> 00:35:22,000
사전 훈련된 모델을 통해 훈련된 미세 조정된 모델,

389
00:35:22,000 --> 00:35:26,000
그들의 훈련 경사 곡선은 완전히 다릅니다.

390
00:35:26,000 --> 00:35:28,000
미세 조정된 모델을 볼 수 있습니다.

391
00:35:28,000 --> 00:35:31,000
처음에는 경사도가 더 빠르게 내려갑니다.

392
00:35:33,000 --> 00:35:37,000
사전 훈련이 어떻게 도움이 되는지에 대한 또 다른 관점이 있습니다.

393
00:35:37,000 --> 00:35:40,000
다음은 오류 표면의 수치입니다.

394
00:35:40,000 --> 00:35:45,000
수백만 가지 방법을 2D 또는 3D 그림으로 시각화합니다.

395
00:35:45,000 --> 00:35:47,000
이 그림의 의미는,

396
00:35:47,000 --> 00:35:54,000
수치가 매끄러우면 일반화 능력이 더 뛰어난 것입니다.

397
00:35:54,000 --> 00:35:58,000
미세 조정된 Bert의 플랫폼이 더 작은 것을 볼 수 있습니다.

398
00:35:58,000 --> 00:36:00,000
처음부터 훈련받은 사람보다

399
00:36:00,000 --> 00:36:03,000
따라서 사전 훈련된 모델을 통해 모델을 미세 조정했습니다.

400
00:36:03,000 --> 00:36:06,000
더 많은 일반화 능력을 갖게 될 것입니다

401
00:36:06,000 --> 00:36:08,000
처음부터 훈련받은 사람보다

402
00:36:10,000 --> 00:36:12,000
감사합니다.