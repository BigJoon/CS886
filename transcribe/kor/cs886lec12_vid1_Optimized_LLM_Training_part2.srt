1
00:00:00,000 --> 00:00:10,780
좋아. 그래서 제가 강의할 부분에서는 세 가지 주제를 다루겠습니다. 나 먼저 갈게

2
00:00:10,780 --> 00:00:16,480
위치 인코딩에 대해 기본적으로 위치 인코딩을 통합하는 방법을 논의합니다.

3
00:00:16,480 --> 00:00:23,120
정보를 단어 임베딩에 추가합니다. 그런 다음 주의 추론에 대해 살펴보겠습니다.

4
00:00:23,120 --> 00:00:30,520
가속은 주의 매트릭스를 결합하여 가속하는 방법입니다.

5
00:00:30,520 --> 00:00:38,520
추론에 주의를 기울이십시오. 셋째, 기반 주의력 가속 방법을 착용하겠습니다.

6
00:00:38,520 --> 00:00:46,720
이제 위치 인코딩을 시작해 보겠습니다. 따라서 메커니즘에 대해 이야기하기 전에

7
00:00:46,720 --> 00:00:52,720
첫 번째 메커니즘 로프, 위치 인코딩이 필요한 이유에 대해 이야기하겠습니다.

8
00:00:52,720 --> 00:00:58,960
변환기가 있고 변환기에 문장을 입력한다고 가정해 보겠습니다.

9
00:00:58,960 --> 00:01:10,520
당신은 말이 많습니다. 특히, wn이라고 말하려면 w1이 있습니다. 그리고 가장 먼저

10
00:01:10,520 --> 00:01:15,720
당신은 임베딩 레이어를 통해 그것들을 공급하는 것입니다. e라고 부르죠.

11
00:01:16,720 --> 00:01:26,520
그런 다음 벡터 x1부터 xn까지를 얻습니다. 이는 임베딩 레이어의 출력입니다.

12
00:01:26,520 --> 00:01:37,600
w1 ~ wn은 단지 원시 단어입니다. 그리고 x1, xn을 입력하고 이를 큰 행렬로 결합합니다.

13
00:01:37,600 --> 00:01:55,200
엑스. 그리고 당신은 말합니다. 모든 xi에 대해 나는 행렬 행렬 q, k를 얻고 싶습니다.

14
00:01:55,200 --> 00:02:04,400
m, v. 그래서 우리가 하는 방법은 세 개의 행렬 pq, pk, pv를 갖는 것입니다. 그리고 우리는

15
00:02:04,400 --> 00:02:17,280
xpq를 q와 동일하게 하세요. 그리고 k와 v도 마찬가지입니다. 여기서 무슨 일이 일어나는지 볼 수 있습니다.

16
00:02:17,360 --> 00:02:27,640
x1은 다음과 같습니다. 이것은 효과적으로 수행되고, 곱해지며, 이것은 효과적으로 여기에서 발생합니다.

17
00:02:27,640 --> 00:02:52,200
사실상 x1 pq, xn pq를 말하고 있습니다. 먼저 x1에 첫 번째 열 pq를 곱하기 때문에

18
00:02:52,200 --> 00:03:02,720
그러면 두 번째 항목도 pq의 두 번째 항목에 x1을 곱하게 됩니다.

19
00:03:02,720 --> 00:03:11,000
이것이 바로 우리가 여기서 얻으려는 것입니다. 하지만 요점은 이 전체 과정에 걸쳐,

20
00:03:11,000 --> 00:03:17,480
위치 정보는 인코딩되지 않습니다. 여기 임베딩에서는 임베딩 기능이,

21
00:03:17,480 --> 00:03:26,000
그것은 인덱스 wi를 취하지 않고, 이 i를 어떤 고려에도 취하지 않습니다. 그리고 여기서는 언제

22
00:03:26,000 --> 00:03:37,320
우리가 그것을 q, 즉 pq에 매핑할 때, 행렬 곱셈만으로 행렬 pq를 곱할 때,

23
00:03:37,320 --> 00:03:43,120
따라서 특정 단어의 위치 정보인 정보도 가져오지 않습니다.

24
00:03:43,120 --> 00:03:53,760
계정에. 그리고 나중에 q, q, k, t를 수행하고 그것의 소프트맥스를 취하고 곱하면

25
00:03:53,760 --> 00:04:01,320
v, 알다시피, 위치 정보를 고려할 곳은 아직 없습니다. 그러니까 어떤 면에서는 그렇죠

26
00:04:01,320 --> 00:04:08,480
원시 변환기만 수행하고 살펴보지 않으면 위치 정보가 손실됩니다.

27
00:04:08,480 --> 00:04:15,000
위치 인코딩이 전혀 없습니다. 그래서 위치정보가 필요한 것입니다. 그리고 원래는

28
00:04:15,000 --> 00:04:20,440
주의를 기울이는 데는 종이가 필요합니다. 이 문제를 해결하는 데는 두 가지 방법이 있었습니다.

29
00:04:20,440 --> 00:04:32,000
그래서 한 가지 방법은 사인 코사인 함수를 사용하는 것입니다. 그런 다음 기본적으로 우리가 하는 일은 다음을 추가하는 것입니다.

30
00:04:32,000 --> 00:04:40,520
우리는 이들 x i 각각에 사인 코사인 값을 추가합니다. 이에 대해서는 나중에 다루겠습니다.

31
00:04:40,520 --> 00:04:44,760
긴장 속에 다루어졌던 다른 접근 방식에 대해 간략하게 설명하는 것만으로도 충분합니다.

32
00:04:44,760 --> 00:04:54,000
당신이 필요합니다. 이러한 각 단어에 적용할 수 있는 학습된 위치 임베딩이 있었습니다.

33
00:04:54,000 --> 00:05:08,620
사인 코사인에 대해 이야기하려면 먼저 사인 코사인 함수를 사용하세요. 기본적으로 우리가 하는 일은 이전에 하는 일입니다.

34
00:05:08,620 --> 00:05:18,960
우리는 지원합니다. 기본적으로 w 곱하기 x에 썼다는 점을 제외하고는 방금 이야기했습니다.

35
00:05:18,960 --> 00:05:32,880
i, 저는 wx 곱하기 p, qk v라고 썼습니다. 그래서 우리는 x i 각각에 대해 곱합니다.

36
00:05:32,880 --> 00:05:50,280
여기에 위치 벡터를 추가합니다. 그리고 기본적으로 이 위치 벡터 i는 매우 커질 것입니다.

37
00:05:50,280 --> 00:06:06,920
내가 무엇인지에 따라 달라집니다. 그리고 보세요, 이것이 여기 파이입니다. 그리고 x의 각 위치에서

38
00:06:06,920 --> 00:06:29,760
나. 따라서 xi가 작은 벡터라고 가정해 보겠습니다. 짝수 인덱스에서는 사인을 추가합니다.

39
00:06:30,760 --> 00:06:52,480
i는 1보다 10,000이 넘고 T는 D보다 큽니다. 그리고 홀수 위치에서는 T에 10,000이 넘는 i의 사인을 추가합니다. 죄송합니다.

40
00:06:52,800 --> 00:07:02,440
D에 대한 코사인. 그리고 i는 i에서의 위치 벡터입니다. 보시다시피, 이것은 파이가 무거울 것입니다

41
00:07:02,440 --> 00:07:14,200
내가 무엇인지에 따라. 그리고 그것은 특정 i에 고유할 것입니다. 그리고 T는 여기 인덱스입니다

42
00:07:14,520 --> 00:07:24,440
pi 내에서 pi는 단일 값이 아니기 때문에 전체 벡터입니다. 아시다시피,

43
00:07:24,440 --> 00:07:34,440
여기서 T는 1이 될 것이고, 여기서 T는 2가 될 것이며, 계속해서 증가할 것입니다. 그래서 그것은

44
00:07:34,440 --> 00:07:41,080
기본적으로 파이는 사인과 코사인 값의 고유한 조합이 될 것입니다. 그리고 전체적으로,

45
00:07:41,080 --> 00:07:46,440
그들은 고유한 신호를 전송해야 합니다. 이 벡터는 고유한 정보를 전송해야 합니다.

46
00:07:46,440 --> 00:07:58,120
이 i가 무엇인지에 따라 신호가 전달됩니다. 따라서 이 위치 벡터를 xi에 추가함으로써 우리의 목표는 다음과 같습니다.

47
00:07:58,120 --> 00:08:04,120
일부 정보를 유지하기 위해. 이제 어텐션 부분에 입력하기 전에 몇 가지를 추가하고 싶습니다.

48
00:08:04,120 --> 00:08:14,200
주의 부분이 알 수 있도록 신호를 보내세요. 여기가 아마도 i번째 위치일지도 모르니까요.

49
00:08:14,200 --> 00:08:23,800
우리는 p i를 추가하여 일부 신호를 감지했습니다. 그리고 이런 일이 있었습니다.

50
00:08:23,800 --> 00:08:33,560
위치 인코딩을 기반으로 실제로 학습할 수 있음을 보여준 절제 연구는

51
00:08:34,520 --> 00:08:46,360
기본적으로 기본적으로 우리는 할 수 있다는 것을 알고 있습니다.

52
00:08:47,160 --> 00:08:55,080
xi에 pi를 더하면 선형 선형 회귀를 통해 i를 학습할 수 있습니다.

53
00:08:57,560 --> 00:09:03,640
기본적으로 이것이 우리에게 필요한 이유입니다. 우선 위치 인코딩이 필요합니다.

54
00:09:07,560 --> 00:09:14,360
좋아요, 그렇다면 로프는 위치 인코딩을 어떻게 인코딩하나요? 그래서 난 그냥 당신을 다시 데려오려고 해요

55
00:09:14,360 --> 00:09:22,280
여기로. 따라서 위치 정보를 반드시 인코딩할 필요는 없습니다.

56
00:09:22,840 --> 00:09:33,400
토양 단계에 서명합니다. 다음 설정을 고려하십시오. Wi가 있고, 패스는 인코딩을 거치며,

57
00:09:34,440 --> 00:09:39,960
x가 되고 이 투영 행렬을 거치면 qk와 v가 됩니다.

58
00:09:39,960 --> 00:09:50,920
그리고 우리가 할 수 있는 것은 우리가 q와 k에 대해 다음을 믿는다고 말하는 것입니다.

59
00:09:53,320 --> 00:10:01,640
여기, 여기, 각 항목과 마찬가지로 이 행렬의 i-j번째 항목은 qitk j입니다. 우리는 말할 것이다

60
00:10:01,640 --> 00:10:07,560
멀리 떨어져 있는 i와 j의 경우 이 신호는 그다지 중요하지 않습니다.

61
00:10:08,520 --> 00:10:15,240
음, 가까운 i와 j 이전에 이 신호가 많이 중요할 것 같아요. 그리고 우리는

62
00:10:15,240 --> 00:10:22,440
우리는 이 상대적인 위치 정보를 변환기로 전달하고 싶습니다.

63
00:10:24,280 --> 00:10:29,960
그래, 그게 아이디어야. 그래서, 그래서, 밧줄이 하는 방식은 다음과 같습니다.

64
00:10:32,040 --> 00:10:36,760
기본적으로 아이디어는 복잡한 평면에서 나옵니다. 복소 평면에서 회전하려는 경우

65
00:10:36,840 --> 00:10:45,320
벡터 x x 세타에 e를 i 세타로 곱합니다.

66
00:10:47,640 --> 00:10:56,440
그래서 여기서 우리는 xm을 회전시키고 싶다면 xm을 m theta만큼 회전시킨다고 말합니다.

67
00:10:56,920 --> 00:11:14,440
그리고 xn을 mθ만큼 회전시킵니다. 보시다시피, 만약에 가정된 xm과 xn이,

68
00:11:15,000 --> 00:11:24,920
임베딩이라는 단어로 매핑되는 영역과 거의 동일합니다.

69
00:11:24,920 --> 00:11:31,160
여기서는 특정 지역을 좋아하는 것으로 매핑된다고 가정합니다. 즉, 전체의 작은 부분에 해당합니다.

70
00:11:31,160 --> 00:11:39,000
더 높은 차원에서. 음, m과 n이 가까우면 대략 같은 영역으로 회전됩니다.

71
00:11:39,000 --> 00:11:46,120
하지만 m과 n이 멀리 떨어져 있다면, m과 n도 멀리 떨어져 있을 것입니다. 그리고 그게 정말 뭐야?

72
00:11:46,120 --> 00:11:53,880
즉, 어, 여기, 여기서는 더 이상 xm을 회전하지 않는다는 뜻입니다. 우리는 qm처럼 회전하고 있고

73
00:11:53,880 --> 00:12:02,920
qn, km, kn이 있죠. 이것이 의미하는 바는 우리가 내적을 취하면

74
00:12:02,920 --> 00:12:15,320
여기서 내적은 기본적으로 작을 것입니다. 음, 우리는 그것을 내적이라고 부르기 때문에

75
00:12:15,320 --> 00:12:21,720
기본적으로는 이렇게 될 것입니다. 알겠습니다. 어쩌면 이것은 올바른 기호가 아닐 수도 있지만

76
00:12:21,720 --> 00:12:35,080
기본적으로 그것은 qikj에 the를 곱한 것과 같습니다. 사이 각도의 cos입니다.

77
00:12:35,080 --> 00:12:43,240
그들을. 그래서 이것이 m 세타만큼 회전한다면, 음, 그리고 이것이 n 세타만큼 회전한다면, 이것 중 일부는,

78
00:12:43,240 --> 00:12:49,560
cos, theta는 m theta - n theta, plus, plus some과 같은 것이 될 것입니다.

79
00:12:49,720 --> 00:12:59,480
일부는 더 작은 값을 좋아합니다. 음, m 빼기 n이 크면 세타가 커진다는 의미입니다.

80
00:13:00,280 --> 00:13:05,320
그러면 코사인 값은 작아질 것입니다. 음, 그렇다면 신호는 그리 크지 않을 것입니다.

81
00:13:05,320 --> 00:13:11,640
그러나 m과 n이 가까우면 이 내적의 값은 커질 것입니다. 그래서 그건,

82
00:13:11,640 --> 00:13:16,440
이는 i와 j가 다음과 같은 경우 제품의 크기를 크게 유지하도록 변환기에 지시하는 방법입니다.

83
00:13:16,440 --> 00:13:25,960
닫다. 음, 하지만 멀리 떨어져 있을 때는 작게 유지하세요. 어, 그리고 우리가 그렇게 하는 방식은

84
00:13:25,960 --> 00:13:33,080
복소 평면은 m θ의 i에 e를 곱합니다. 하지만 생각해 보면, 생각해 보면, 어,

85
00:13:34,760 --> 00:13:41,320
우리가 이것을 고려한다면, 복소 평면은 r2 평면에 대한 전단사(bijection)가 될 것입니다. 음,

86
00:13:42,200 --> 00:13:49,800
좋아요, 우선, e의 i, e의 im 세타는 cos와 같습니다.

87
00:13:50,920 --> 00:14:00,040
m 세타 더하기 i 사인은 m 세타입니다. 그리고 이것을 하나의 좌표로 삼고 이것을 취하면

88
00:14:01,000 --> 00:14:06,040
다른 좌표가 되려면 이것이 가상 좌표이고 이것이 실제 좌표입니다.

89
00:14:06,040 --> 00:14:12,280
음, 그리고 우리는 그것을 어떤 임의의 숫자에 곱합니다.

90
00:14:16,280 --> 00:14:27,880
기본적으로 이것을 곱하면 이 숫자에 이 숫자를 곱하면, 어,

91
00:14:28,200 --> 00:14:40,600
이것에 대한 매트릭스로. 그리고 그 행렬은 바로 여기에 있습니다. 기본적으로 우리는 이것을 성공적으로 계산합니다.

92
00:14:40,600 --> 00:14:47,320
에서, 어, 에서, c에서 r, r2. 그리고 r2에서 벡터를 회전하는 방식은 기본적으로

93
00:14:47,320 --> 00:14:52,920
여기 이 행렬을 곱하면 됩니다. 그리고 음, 이것에 대해서는 더 자세히 설명하지 않겠습니다. 그리고 우리가 원한다면

94
00:14:52,920 --> 00:15:00,920
더 높은 차원, 예를 들어 d차원 공간에서 이 작업을 수행하려면 음, 이 사람들이 제안하는 것은 매우

95
00:15:00,920 --> 00:15:10,520
단순한. 그래서 우리가 여기에 이 ​​벡터를 가지고 있다고 가정해 보세요. 이것은 기본적으로 qm 또는 km입니다.

96
00:15:11,720 --> 00:15:16,680
그리고 그것은 다음과 같습니다. 우리는 모든 좌표를 취하고 우리는

97
00:15:16,680 --> 00:15:23,960
첫 번째 쌍을 m 세타 1만큼 회전하고, 두 번째 쌍을 m 세타 2만큼 회전하는 식으로 계속됩니다.

98
00:15:26,200 --> 00:15:31,800
그런데, 우리는 상대적으로 더 작은 세타 i를 이렇게 유지합니다.

99
00:15:31,800 --> 00:15:41,160
숫자. 음, 더 작네요. 따라서 전체 사이클을 회전하지 않도록 보장합니다. 따라서 기본적으로 m과

100
00:15:41,160 --> 00:15:48,520
n은, 만약 m과 n이 실제로 충분히 멀다면, m 빼기 n 곱하기 세타 i는, 아시다시피, 그것은 다음과 같습니다.

101
00:15:49,160 --> 00:15:53,880
2 파이 같은 거요. 그렇다면 그것은 실제로 m과 n이 일종의 가깝다는 것을 의미합니다. 그래서 우리는 하지 않습니다.

102
00:15:53,880 --> 00:16:01,720
우리는 그런 일이 일어나기를 원하지 않습니다. 그래서 우리는 그것을 작게 유지합니다. 음, 그리고 그래, 그래서 내가 하는 말이야

103
00:16:01,720 --> 00:16:07,400
음, 기본적으로 위치 인코딩을 적용하려고 하는 동안에 대해 이야기했는데,

104
00:16:08,360 --> 00:16:13,720
어, q와 k 사이의 행렬 곱셈 중에요. 그리고 이것은 q입니다. 이것은 k입니다.

105
00:16:15,080 --> 00:16:23,880
이것은 기본적으로 qm입니다. 이것은 kn입니다. 그리고 우리는 이것을 n 세타만큼 회전시켰고 이것은 m 세타만큼 회전시켰습니다. 그리고,

106
00:16:25,320 --> 00:16:34,120
아시다시피, 이는 기본적으로 이 행렬을 곱하는 것과 동일합니다.

107
00:16:34,840 --> 00:16:44,840
음, 여기요. 모든 것을 전치하면 기본적으로 우리가 할 수 있는 일은 이 두 가지를 곱하는 것입니다.

108
00:16:44,840 --> 00:16:53,640
먼저, 어, 이건 이렇게 생겼어요. 그리고 이것을 다른 것의 전치와 곱하면,

109
00:16:53,640 --> 00:17:00,680
이것은 음의 세타가 될 것입니다. 여러분은 m - m 세타를 얻게 될 것입니다.

110
00:17:01,080 --> 00:17:08,760
그리고 우리는 실제로 m 빼기 m 세타를 아주 빠르게 할 수 있습니다. 따라서 실제로 곱할 필요는 없습니다.

111
00:17:09,560 --> 00:17:17,400
이 거대한 행렬은 바로 여기 또 다른 거대한 행렬입니다. 음, 대신 여기서 빠른 작업을 수행할 수 있습니다.

112
00:17:18,280 --> 00:17:23,880
그리고 우리는 선형적 관심을 위해 이것을 계속할 수 있습니다. 왜냐하면 공통된 선택이 있기 때문입니다.

113
00:17:23,880 --> 00:17:29,560
여기 지도는 음, 기본적으로 정규화 방식과 같습니다.

114
00:17:30,280 --> 00:17:39,160
그래서, 그들은 아직 주변에 있어요. 그래서 이것은 q와 k와 같습니다. 그리고 이것은 v of

115
00:17:39,880 --> 00:17:49,880
k의 q와 v. 예를 들어, 그들은 정규화 계획 이후에도 여전히 같은 영역에 있습니다.

116
00:17:49,880 --> 00:17:55,080
그래서 그것들을 회전시키는 것은, 음, 회전하는 것이 여전히 작동할 것이라는 생각인 것 같아요.

117
00:17:59,640 --> 00:18:06,280
좋아요. 또 한 가지 주목해야 할 점은

118
00:18:07,320 --> 00:18:14,360
정현파 인코딩은 맨 처음에 적용됩니다. 그래서 적용

119
00:18:15,800 --> 00:18:23,720
바로 여기, 단어가 임베딩에 매핑되면, 어, 여기에 위치 인코딩처럼 추가합니다.

120
00:18:23,720 --> 00:18:30,520
피. 응, 여기처럼. 그리고 이는 전체 변압기에 걸쳐 한 번만 적용되며

121
00:18:30,520 --> 00:18:39,880
건축학. 그러나 로프의 경우 q와 k마다 적용된다. 곱할 때마다,

122
00:18:39,880 --> 00:18:52,120
이렇게 하면 로프를 적용하게 됩니다. 음, 그리고 그래, 이건 기본적으로 이렇게 말하는 게 한계인 것 같아, 어,

123
00:18:52,120 --> 00:19:00,040
만약, 멀리 떨어져 있는 두 가지가 있다면, 음, 이것이 상대적인 거리입니다. 그래서 이것은

124
00:19:00,040 --> 00:19:11,560
기본적으로 m 빼기 m입니다. 이것은 상한, 음, 그리고 어, 상한, 상한입니다.

125
00:19:11,880 --> 00:19:20,040
음, 회전된 q와 m 사이의 내적은 얼마나 큰가요?

126
00:19:21,400 --> 00:19:31,640
어, 예, 이것을 q와 m으로 곱한 다음, 어, 회전된 k를 곱하고 얼마나 멀리 있는지,

127
00:19:31,960 --> 00:19:44,200
알겠습니다. 그리고 여기에는 아무것도 없습니다. 기본적으로 이것이 얼마나, 얼마나 멀리 있을 수 있는지.

128
00:19:46,120 --> 00:19:52,440
그리고 그것은 기본적으로 언제, 언제, n 마이너스 m이 크고 서로 멀리 떨어져 있는지를 말합니다.

129
00:19:53,400 --> 00:19:59,880
이 값, 이 경계는 내려갑니다. 이것이 바로 우리가 원하는 것입니다. 음,

130
00:20:01,720 --> 00:20:09,160
로프의 성능은 다음과 같습니다. 여기에서 볼 수 있듯이 여기 이 작업에 대해

131
00:20:09,160 --> 00:20:16,200
조금 더 나아졌습니다. 음, 계속해서, 이러한 작업에 대해, 이러한 다운스트림 작업에 대해,

132
00:20:17,320 --> 00:20:18,680
일부에서는 더 나은 성능을 발휘합니다.

133
00:20:19,240 --> 00:20:33,560
음, 그리고 마스크 영화 점수의 경우 음, 기본적으로 롤 포머인데 파란색 선이 더 낮습니다.

134
00:20:35,720 --> 00:20:41,480
여기 영화도 마찬가지야. 이것은 마스크되어 있습니다. 이것은 단지 일반적인 lm이며 gb3에서 사용됩니다.

135
00:20:41,960 --> 00:20:49,400
좋아요. 자, 그것은 매우 멋진 메커니즘이었습니다. 그리고, 음, 다음을 봅시다

136
00:20:49,400 --> 00:20:55,480
이를 알리바이라고 합니다. 그리고 이전에 비해 훨씬 간단해졌습니다.

137
00:20:56,600 --> 00:21:07,400
그리고, 음, 알리바이의 동기는, 음, 모델이 임의로 오랜 시간이 걸릴 수 있다는 것입니다.

138
00:21:07,400 --> 00:21:12,440
입력 길이. Transformers 모델은 무엇에 비해 임의의 긴 입력 길이를 사용할 수 있습니다.

139
00:21:12,440 --> 00:21:19,320
훈련 중에 본 적이 있습니다. 음, 다시 말씀드리지만, 트랜스포머가 하는 일은 기본적으로

140
00:21:20,280 --> 00:21:26,680
그것은 이 x를 곱하는데, 이것은 여러분의, 어, 단어에서 나온 임베딩입니다.

141
00:21:27,480 --> 00:21:38,360
음, 그리고 n개의 단어가 있고 각 단어의 차원이 d라고 가정해 보겠습니다.

142
00:21:39,160 --> 00:21:47,480
그리고 도중에 행렬 q에 투영하면 x와 행렬 q를 곱합니다. 즉,

143
00:21:48,280 --> 00:21:52,040
음, 그건 차원이잖아

144
00:21:54,600 --> 00:21:58,200
여기서는 d 곱하기 dq입니다.

145
00:22:06,600 --> 00:22:13,720
따라서 n이 아무리 크더라도 우리는 항상 이 두 행렬을 곱할 수 있습니다.

146
00:22:13,720 --> 00:22:19,480
변압기도 항상 작동합니다. 문제는 우리가 더 짧은 길이로 훈련할 때,

147
00:22:19,480 --> 00:22:26,840
길이 x, 추론 시에는 일반적으로 더 긴 길이의 x, x, 음으로 일반화할 수 없습니다.

148
00:22:28,520 --> 00:22:35,400
따라서 알리바이는 이를 극복하고 더 긴 시퀀스 길이로 추정할 수 있습니다.

149
00:22:37,160 --> 00:22:41,400
그리고 다시 말하지만, 우리는 의도적으로 필요한 전부인 사인곡선을 고려했습니다. 로프는 방금 다루었습니다.

150
00:22:41,400 --> 00:22:51,320
그리고 t5 편향이라는 또 다른 학습된 임베딩도 있습니다. 음, 그리고 아이디어는,

151
00:22:52,920 --> 00:22:59,240
알다시피, 정현파는 최악이고 그 다음은 로프이고 t5 편향이지만 알리바이는 여전히 더 좋습니다. 음,

152
00:23:01,880 --> 00:23:08,680
또 다른 아이디어는 정확한 접촉을 계산하는 것이 아니라 어떻게 계산해야 하는지를 계산한다는 것입니다.

153
00:23:08,680 --> 00:23:13,720
입력된 길이입니다. 또 다른 주장과 마찬가지로 이것이 공정한 비교가 아닐 수도 있다는 것입니다.

154
00:23:13,720 --> 00:23:19,640
실제로 공평한 것은 필요한 계산량을 고려하는 것입니다.

155
00:23:19,640 --> 00:23:28,280
훨씬 가볍습니다. 예를 들어, 음, 정현파의 경우 1024 길이와 같은 훈련은 동일할 수 있습니다.

156
00:23:28,280 --> 00:23:38,040
t5 바이어스, 어, 비슷한 컨텍스트 길이 512를 사용하여 동일한 변환기에서 훈련하는 것입니다.

157
00:23:38,760 --> 00:23:45,720
음, t5 편향에는 더 많은 계산이 필요하기 때문에 이를 훈련하고 실제로 훈련하는 데에는

158
00:23:45,720 --> 00:23:55,320
같은 계산 주위에. 따라서 알리바이가 하는 일은 기본적으로 추가하는 것입니다. 따라서 쿼리 i에 대해

159
00:23:56,280 --> 00:24:03,400
음, 내가 k로 점을 찍은 쿼리는 여기에 하나의 q가 있는 것과 같습니다.

160
00:24:06,040 --> 00:24:12,200
그리고 당신은 거대한 k 행렬을 가지고 있습니다. 이렇게 하고 이것을 곱하면 여기 이 값이 나옵니다.

161
00:24:12,200 --> 00:24:17,960
그리고 이것을 곱하면 여기서 이 값을 얻게 됩니다. 음, 그리고 기본적으로는 이렇습니다

162
00:24:17,960 --> 00:24:32,520
여기 위치는 많은 것 중에서 i번째입니다. 음, 여기에 마이너스 i 마이너스 1을 더합니다. 여기에 마이너스를 추가해보겠습니다.

163
00:24:32,520 --> 00:24:44,760
나는 2를 빼고 0까지 갑니다. 그리고 그 이후에는 0이 됩니다. 음, 그래, 우리가, 우리가,

164
00:24:44,760 --> 00:24:50,440
우리는 이것이 음, 이전과 같은 이유라고 덧붙입니다. q에 대해, q에 대해, i와 j에 대해,

165
00:24:51,720 --> 00:25:01,080
j는 k 인덱스와 동일합니다. 멀리 떨어져 있는 i와 j에 대해 우리는 qi, qi dot k, 우리는 qi를 원합니다

166
00:25:01,640 --> 00:25:07,880
kjt. 우리는 이 값이 변압기 모델에 대해 상대적으로 작은 신호가 되기를 원합니다.

167
00:25:08,840 --> 00:25:15,640
음, 어떻게 보면 별로 중요하지 않은 것 같아요. 음, 그리고 결국 우리가 0을 갖게 된 이유는

168
00:25:15,640 --> 00:25:22,760
그 이유는 음, 훈련 중에 변환기가 부정 행위를 하지 않도록 마스크를 추가하고 싶기 때문입니다.

169
00:25:22,760 --> 00:25:32,520
기본적으로, 음, 우리는 다음과 같은 것을 가지고 있습니다. 어, 우리는 무엇을 가지고 있습니까? 우리는 내 이름과 같은 것을 가지고 있습니다

170
00:25:32,680 --> 00:25:41,400
매트예요. 그리고 훈련 중에 q는 첫 번째 q가 내 것과 같을 것입니다.

171
00:25:42,680 --> 00:25:47,480
그리고 우리가 언어 모델링을 하려고 한다면 k도 my에 있을 것이고, 음,

172
00:25:48,120 --> 00:25:55,960
그래서 그것은 첫 번째 것과 같을 것입니다. 그리고 나서, 그리고 나서, 그리고 나서 q는,

173
00:25:56,680 --> 00:26:09,000
음, 기본적으로 내, 이것은 기본적으로, 어, wq 곱하기 x1 = q1입니다.

174
00:26:11,480 --> 00:26:15,480
그런 다음 우리는 다음을 갖게 될 것입니다. 죄송합니다. 이건 아닙니다.

175
00:26:21,240 --> 00:26:22,120
그런 다음 우리는

176
00:26:26,440 --> 00:26:35,480
이름은 x2와 같습니다. 그래서 우리는 여기서 q2를 얻게 될 것입니다. 그리고

177
00:26:38,040 --> 00:26:48,840
아이디어는 우리가 q1 곱하기 kt를 할 때, 음, 우리는 k2를 얻을 수 있기를 원하지 않는다는 것입니다.

178
00:26:49,800 --> 00:26:54,840
이는 이름에 대해 알려줍니다. 왜냐하면 이것은 다음과 같기 때문입니다.

179
00:26:57,240 --> 00:27:07,960
wq, wk x x2, x2는 이름을 기준으로 합니다. 따라서 기본적으로 우리가 완료하려고 하면

180
00:27:12,280 --> 00:27:18,120
우리는 qi 문장을 완성하려고 합니다. 마치 my라는 문장을 완성하려고 하는 것처럼,

181
00:27:18,200 --> 00:27:23,400
우리는 k, k2를 원하지 않습니다. 다음 단어 이름이 무엇인지 이미 알려줍니다.

182
00:27:25,320 --> 00:27:29,880
그래서 그렇습니다. 어쨌든 마스크를 추가하기 때문에 그 이후에는 모두 0이 되는 이유가 됩니다.

183
00:27:34,360 --> 00:27:40,920
그래, 어, 이것도 같은 생각이고 m은 훈련 전에 지정된 머리 특정 상수입니다.

184
00:27:41,720 --> 00:27:47,400
그리고 이것은 n개의 머리로 설정될 것입니다. m을 b로 설정합니다.

185
00:27:49,720 --> 00:27:58,200
그리고 그것은 단지 그들에게 잘 맞는 숫자인 것 같아요. 그리고 그들은 이것을 관찰했습니다.

186
00:27:58,920 --> 00:28:02,280
훈련 가능한 m은 제대로 작동하지 않았고 0과 1 사이의 기울기가 있었습니다.

187
00:28:02,920 --> 00:28:05,480
0 부근의 밀도가 더 높은 것이 잘 작동했습니다.

188
00:28:05,880 --> 00:28:13,480
어, 그럼 그렇지, 그럼, 이건, 각 머리의 m이 다를 것입니다.

189
00:28:15,320 --> 00:28:17,640
따라서 시간이 걸릴 것입니다.

190
00:28:21,880 --> 00:28:26,600
l 곱하기 l 곱하기 n 공간, 여기서 l은 기본적으로,

191
00:28:28,600 --> 00:28:32,840
우리는 여기서 용어를 혼동한 것 같습니다. l은 길이입니다.

192
00:28:32,840 --> 00:28:38,680
문맥에 따르면 그것은 l 곱하기 l입니다. 왜냐하면 그것은 q, k, t의 크기이기 때문입니다.

193
00:28:39,960 --> 00:28:46,680
그리고 여러분은 모든 머리에 대해 서로 다른, 음, 서로 다른 n을 가져야 합니다.

194
00:28:49,240 --> 00:28:54,840
그래서 저는 머리마다 다른 m을 사용했습니다. 따라서 모든 머리에 대해 aq, k, t를 유지합니다.

195
00:28:54,840 --> 00:28:59,080
그래서 공간이 많이 필요한 거죠. 따라서 정현파처럼 모든 레이어에도 적용됩니다.

196
00:29:03,800 --> 00:29:09,560
음, 여기서 당혹감에 대해서도 이야기할 수 있습니다. 그래서 이것은 기본적으로 무엇을 설명하는 것입니다.

197
00:29:09,560 --> 00:29:16,200
당혹감은. 그래서 우리는 LM이 실제, 실제 문장, 높은 확률을 할당할 수 있기를 원합니다.

198
00:29:16,200 --> 00:29:22,040
그것이 바로 아이디어입니다. 음, 그리고 당혹감은 이 확률 함수의 일부 함수입니다.

199
00:29:22,040 --> 00:29:32,040
따라서 기본적으로 당혹도가 낮을수록 좋습니다. 그리고 알리바이와 정현파를 비교하자면,

200
00:29:33,480 --> 00:29:44,040
음, 정현파 토큰에 비해 절반의 토큰에 대해 알리바이가 훈련되었습니다. 음,

201
00:29:45,960 --> 00:29:51,640
알리바이는 더 긴 시퀀스에서 테스트할 때 정현파와 거의 동일하게 수행됩니다.

202
00:29:51,640 --> 00:29:56,200
정현파는 훈련을 받았지만 사용하더라도 더 빠르고 더 적은 메모리를 사용합니다.

203
00:29:57,080 --> 00:30:06,520
그리고 여기에 몇 가지 결과가 있습니다. 음, 여기는 모두 Trino 512 토큰이에요. 보시다시피 알리바이는 유지됩니다.

204
00:30:06,520 --> 00:30:15,960
꽤 잘 지내요. 다른 것들은 단지 갈라지는 반면. 10,000, 10,024개의 토큰에 대해서도 마찬가지입니다.

205
00:30:15,960 --> 00:30:28,680
그리고 여기에서 알리바이가 꽤 잘 추정된다는 것을 볼 수 있습니다. Trino 512 여기서 정현파, 음,

206
00:30:28,680 --> 00:30:34,600
512에서 훈련하면 기본적으로 여기서는 수행되지 않습니다. 그리고 넌 다음 장면을 전혀 볼 수 없어

207
00:30:34,600 --> 00:30:45,800
폭발하기 때문이죠. 그리고, 어, 여기서도 같은 현상이군요. 그냥 폭발합니다. 그래서 성능은

208
00:30:45,800 --> 00:30:53,160
음, 입력 길이를 조금 늘리자마자

209
00:30:53,160 --> 00:31:02,760
하지만 알리바이는 괜찮아요. 그리고 음, 여기에 검증의 복잡성이 있습니다. 그리고 그것은 알리바이를 보여줍니다.

210
00:31:03,720 --> 00:31:09,320
음, 토큰의 절반에 대해 훈련을 받았지만 토큰에 대해서는 기본적으로 다음과 같은 일을 잘 수행합니다.

211
00:31:10,040 --> 00:31:16,840
기본적으로 10,024개의 토큰으로 정현파 게임을 플레이하고 있습니다. 그리고 여기에는 기본적으로 2048개의 토큰이 있습니다.

212
00:31:21,560 --> 00:31:26,920
좋아요. 이제 다중 쿼리 주의를 살펴보겠습니다. 다중 쿼리 주의는 기본적으로

213
00:31:26,920 --> 00:31:40,120
동일한, 음, 동일한 WQK, 알았어, V, 알았어, KQKV 행렬을 다른 헤드에 걸쳐 재사용합니다.

214
00:31:40,120 --> 00:31:44,920
그래서 일반적으로 우리는 각 머리마다 이것들 중 하나를 갖게 됩니다. 그래서 그것은 다음과 같습니다.

215
00:31:48,200 --> 00:31:54,280
하지만 지금은, 어, 우리 모두 이것을 사용합니다. 동일한 것을 사용하면 다음과 같이 보입니다. 그리고

216
00:31:54,280 --> 00:32:01,800
우리는 행렬이 하나뿐이기 때문에 속도가 빨라질 것으로 예상됩니다. 음, 주의 기본 사항은

217
00:32:01,800 --> 00:32:08,600
나는 그것에 대해 다시 다루지 않을 것입니다. 그리고 네, 그냥, 어, 심층적인 분석은 하지 않겠습니다

218
00:32:08,600 --> 00:32:15,080
여기서는 각 변수가 무엇을 의미하는지, 적어도 with가 무엇을 의미하는지 말씀드리겠습니다. 그래서

219
00:32:15,960 --> 00:32:25,640
기본적으로 X는 음, 입력 행렬이고 컨텍스트 길이는 N입니다.

220
00:32:29,800 --> 00:32:32,040
그리고 이것은 인코더에 대한 입력입니다.

221
00:32:36,520 --> 00:32:42,760
그리고 이것은 M이 기본적으로 디코더의 입력입니다. 그리고 이것은 길이 M을 가집니다.

222
00:32:45,560 --> 00:33:01,400
따라서 D는 모든 단어 임베딩의 크기입니다.

223
00:33:02,920 --> 00:33:07,320
음, H는 앞면의 수입니다. B는 당신이 이것을 인계받는 배치입니다.

224
00:33:08,280 --> 00:33:16,440
음, 네, K, K, V는 W, K, WV의 크기입니다.

225
00:33:20,680 --> 00:33:26,360
그리고 네, 그게 전부라고 생각합니다. 기본적으로 우리가 원하는 것은 총계를 원하는 것입니다.

226
00:33:26,360 --> 00:33:33,160
멀티 헤드 어텐션의 메모리 액세스는 멀티 헤드 어텐션과 혼동하지 않아야 합니다.

227
00:33:33,160 --> 00:33:38,200
다중 쿼리 의도가 바로 여기서 다루는 내용입니다. 멀티헤드 어텐션은

228
00:33:38,200 --> 00:33:45,640
제안된 원래 주의 메커니즘과 주의만 있으면 됩니다. 음, 여기 있습니다.

229
00:33:47,240 --> 00:33:53,400
훈련을 위한 메모리 대 작업 비율이 있습니다. 알다시피, 그것은 1보다 훨씬 적습니다. 그것은

230
00:33:54,120 --> 00:34:05,240
1 나누기 K, 음, K는 여기쯤이에요. 그리고 BN에 1을 더하면 1보다 훨씬 작아야 합니다.

231
00:34:07,240 --> 00:34:14,840
하지만 추론을 위해서는 D 분의 N 더하기 B 분의 1이 됩니다. 그리고 B 분의 1은 다루기가 쉽습니다.

232
00:34:14,840 --> 00:34:19,960
배치 크기를 더 크게 늘립니다. 하지만 N/D는 우리가 걱정해야 할 부분입니다

233
00:34:19,960 --> 00:34:27,640
에 대한. 음, 컨텍스트 길이가 대략적으로 같을 때, 예를 들어 복잡한 길이가

234
00:34:27,640 --> 00:34:34,920
천 단어 정도이고, 그리고 우리 날짜는 D의 값, 음, 차원입니다.

235
00:34:34,920 --> 00:34:42,280
각각의 각 임베딩, 어, 단어 임베딩도 약 1,000개에 달합니다. 그러면 우리는

236
00:34:42,280 --> 00:34:46,440
그렇지 않기 때문에 걱정할 필요가 있습니다. 이것은 더 이상 1보다 적지 않습니다.

237
00:34:50,840 --> 00:35:00,280
네, 그렇습니다. 매우 간단한 아이디어입니다. 우리는 기본적으로 하나만 사용합니다. 여기서 볼 수 있는 것은,

238
00:35:00,280 --> 00:35:09,000
우리는 PK를 가지고 있었고, 각 머리에 H개와 D 곱하기 K가 있었습니다. 음, 하지만 우리가 사용하자마자

239
00:35:09,000 --> 00:35:15,640
다중 쿼리 주의, 이제 DK와 DV만 남았습니다. 어, 그리고 여기서는 실제로 그럴 필요는 없을 것 같습니다.

240
00:35:16,600 --> 00:35:22,120
우리는 실제로 걱정할 필요가 없습니다. 여기서는 실제로 PQ에 대해 걱정할 필요가 없습니다. 왜냐하면,

241
00:35:22,920 --> 00:35:31,880
음, 우리는 인코더, 디코더 작업을 다루고 있기 때문입니다. 병목 현상은 다음과 같습니다.

242
00:35:32,680 --> 00:35:38,680
K와 V를 반복적으로 다시 로드합니다. 그러니 생각해 보면 내가 좋아하고 싶다면 다시 추론을 하세요.

243
00:35:38,680 --> 00:35:53,400
내 이름은 매튜인데, 프랑스어 같은 것으로 번역하고 싶다면

244
00:35:53,400 --> 00:36:04,600
그것은 마치, je m'appelle과 같을 것입니다. 알았어, 내 프랑스어 실력은 이 일을 하기엔 부족해, 어, 마티유든 뭐든.

245
00:36:04,600 --> 00:36:12,120
음, 우리가 지금 하고 있다고 가정해 보세요, 우리가 하고 있어요, 우리가 이 일을 하고 있고, 우리가 시작하는 것은

246
00:36:13,800 --> 00:36:24,600
나의, 나의, 음, 추론에서 우리가 해야 할 일은, 어, 추론에서 우리는 이 전체를 가지고 있습니다.

247
00:36:24,600 --> 00:36:29,640
질문. 제 이름은 매튜입니다. 음, 우리는 그것을 번역하고 싶습니다. 그리고 처음으로 우리는

248
00:36:29,640 --> 00:36:36,600
우리는 얻을 것이고, 음, Q, PQ, 또는 Q 행렬과 같은 것은 동일하게 유지될 것입니다.

249
00:36:37,480 --> 00:36:48,520
Q는 PQ 및 X와 같지만, 음, K와 V는 P, K, V에 M을 곱한 것과 같습니다.

250
00:36:48,520 --> 00:36:52,360
이 M은 계속 변할 것입니다. 디코더에 대한 입력은 계속해서 변경될 것입니다.

251
00:36:52,920 --> 00:36:58,120
첫 번째 단어 뒤에는 a가 있고 그 다음에는 uh, my가 있고 그 다음에는 my,

252
00:36:59,080 --> 00:37:03,320
그래서 우리는 이 K와 V를 계속해서 다시 로드해야 하며, 여기서 병목 현상이 발생합니다.

253
00:37:04,040 --> 00:37:09,720
음, 그래, 훈련 중에도 여전히 꽤 잘 지내고 있어. 여기서는 놀랄 일이 아닙니다.

254
00:37:09,720 --> 00:37:13,720
하지만 추론하는 동안에는 이 다중 쿼리 주의 릴리스가 빛을 발합니다.

255
00:37:13,720 --> 00:37:20,680
그래서 이전에는, 어, D 나누기 N이 있었는데, 음, 이것이 우리가 줄이고 싶었던 것입니다.

256
00:37:20,680 --> 00:37:28,840
이제 DH에 대한 N이 있습니다. 어, 기본적으로 우리는 숫자를 줄였기 때문에,

257
00:37:28,840 --> 00:37:34,600
H배로 다시 로드하려는 K 및 V의 수는 여기를 통해 표시됩니다. 음,

258
00:37:35,560 --> 00:37:42,520
그래서, 그래서, 메모리 대 작업 비율이 Ketflow에 도달하고 있으며, 이것이 바로 다중 쿼리가 수행되는 이유입니다.

259
00:37:42,520 --> 00:37:48,680
주의는 꽤 잘 작동합니다. 아, 그렇군요. 그럼 몇 가지 결과를 살펴보겠습니다. 음,

260
00:37:50,680 --> 00:37:55,000
여기 기본적으로 어떻게, 어, 어떻게 작동하는지 나와 있습니다.

261
00:38:00,440 --> 00:38:07,560
음, 다중 헤드가 여전히 최고의 성능을 발휘한다는 것을 알 수 있습니다. 왜냐하면 더 많은 헤드가 있기 때문입니다.

262
00:38:07,560 --> 00:38:15,640
하지만 다중 쿼리는 이와 비슷합니다. 그리고, 어, 그리고 똑같습니다. 여기서도 마찬가지입니다. 멀티헤드

263
00:38:15,640 --> 00:38:22,520
그게 최고야, 어, 하지만 다중 쿼리는, 그게, 그게, 그게, 편해. 음, 그리고,

264
00:38:23,320 --> 00:38:30,840
그리고 우리가 저장한 시간을 보면, 어, 여기, 다시 인코더 시간을 보면,

265
00:38:30,840 --> 00:38:36,360
Q가 동일하게 유지되기 때문에 모든 것이 동일합니다. 이것이 바로 번역 작업입니다.

266
00:38:36,360 --> 00:38:43,800
따라서 Q는 동일하게 유지됩니다. 제 이름은 매튜인데, 음, 그러니까, 그러니까, 그러니까, 아시다시피,

267
00:38:43,800 --> 00:38:48,280
다중 쿼리 주의는 실제로 속도를 높이지는 못했지만 디코더 부분에서는

268
00:38:48,280 --> 00:39:00,360
my, my, my를 계속해서 다시 로드해야 하므로 그렇게 하면 음, 다시 로드하면 많은 시간을 낭비하게 됩니다.

269
00:39:00,360 --> 00:39:07,080
H만큼, H배 더 많은 헤드와 멀티 헤드 대 멀티 쿼리. 그러니까, 그래, 그러니까, 그러니까,

270
00:39:07,080 --> 00:39:15,800
여기가 훨씬 빠릅니다. 음, 응. 그래서 다중 쿼리에는 여전히 몇 가지 문제가 있습니다.

271
00:39:15,800 --> 00:39:23,000
주목. 어, 기본적으로 두 가지, 두 가지, 두 가지 큰 문제일 뿐입니다. 첫 번째 문제는

272
00:39:23,880 --> 00:39:29,480
이미 가지고 있다면 우리는 20억 달러 정도의 지출을 위해 개방형 AI 교육을 받은 GPT와 같았습니다.

273
00:39:29,480 --> 00:39:35,160
그것에 대해 또는 뭔가. 그리고 이제 당신은 '아, 당신이 정말 속도를 높이기 위해 무엇을 해야 하는지 알잖아'라고 말합니다.

274
00:39:35,160 --> 00:39:42,200
추론은 다중 쿼리 주의를 사용하는 것이지만, 모델 학습을 시작하면

275
00:39:42,200 --> 00:39:46,040
다중 헤드 주의를 사용했습니다. 그럼 이제 무엇을 할 건가요? 훈련할 것인가,

276
00:39:46,040 --> 00:39:51,320
모델을 처음부터 훈련시키고 훈련에 수십억 달러를 지출하시겠습니까?

277
00:39:51,320 --> 00:39:57,240
아니면 기존 모델을 다중 쿼리 주의에 적용하는 더 빠른 방법이 있습니까? 그래서 그것은

278
00:39:57,240 --> 00:40:02,760
추론하는 동안 더 빠릅니다. 음, 대답은 긍정이에요. 훈련이라고 불리는 것이 있습니다.

279
00:40:03,320 --> 00:40:08,600
또 다른 점은 우리가 갖는 것 사이의 중간 단계를 찾고자 한다는 것입니다.

280
00:40:09,400 --> 00:40:18,680
이 예에서처럼, 어, 8, 8개, 여기처럼 8개의 머리가 있습니다. 아픈

281
00:40:18,680 --> 00:40:31,320
이것을 공유하면 하나의 어, k, kv 행렬을 공유하고 8개의 별도의 어, kv 행렬을 갖게 됩니다.

282
00:40:31,320 --> 00:40:35,000
우리는 그 사이 어딘가에서 일부를 찾고 싶습니다. 그렇다면 두 사람이 어떤 느낌이었을지,

283
00:40:35,000 --> 00:40:39,960
예를 들어. 음, 그건 그룹 쿼리 어텐션(group query attention)이라고 하는 거죠. 그게 바로 우리가 다룰 내용이야

284
00:40:39,960 --> 00:40:49,080
다음. 음, 먼저 은폐하려고 합니다. 훈련과 아이디어는 매우 간단합니다. 어, 그래서 우리는 기본적으로

285
00:40:49,800 --> 00:40:58,760
기본적으로 원본이 있습니다. 음, 예를 들어 이미 원본이 있습니다. k, k, 어,

286
00:40:58,760 --> 00:41:06,120
k개의 서로 다른 투영 행렬을 사용하고 평균을 취하여 다음과 같이 말합니다. 이제 새로운 것이 생겼습니다.

287
00:41:06,200 --> 00:41:12,120
새로운 투영 매트릭스. 어, 음, 처음에는 잘 안 될 수도 있겠지만, 우리는, 이렇게 말하겠습니다.

288
00:41:12,120 --> 00:41:17,880
좋습니다. 우리는 다중 쿼리 주의를 기울여 훈련할 예정입니다. 그리고 결과는 다음과 같습니다.

289
00:41:17,880 --> 00:41:22,760
원래 훈련 단계의 5%만 수행하면 이 행렬은 충분히 좋은 것으로 나타납니다.

290
00:41:22,760 --> 00:41:29,400
무작위로 초기화할 수도 있습니다. 어, 안 돼요, 안 돼, 안 돼, 잘 안 돼요.

291
00:41:29,400 --> 00:41:34,600
그리고 우리는 또한 그것을 초기화할 수도 있습니다. 어, 특정 키 투영 행렬도 그렇지 않습니다.

292
00:41:34,600 --> 00:41:42,120
일도. 어, 의미 있는 것이 가장 잘 작동합니다. 음, 그리고 제가 얘기하고 있던 또 다른 점은

293
00:41:42,120 --> 00:41:46,200
하나만 갖는 대신에 4개 정도를 갖고 이 두 개를 분리할 수도 있습니다.

294
00:41:46,200 --> 00:41:52,040
이들은 네 그룹으로 나뉜다. 그래서 두 사람이 하나를 공유합니다. 알다시피, 이것은 일종의

295
00:41:52,040 --> 00:42:02,360
사이, 음, 어, 업 트레이닝은 그룹 쿼리에도 적용될 수 있습니다. 어, 즉 우리는 단지,

296
00:42:02,360 --> 00:42:06,200
우리는 이 두 개의 머리를 잡아당겼고, 저는 이 두 개의 머리를 이것으로 끌어당겨 훈련시켰습니다.

297
00:42:07,960 --> 00:42:13,240
좋아요. 따라서 그룹 쿼리 관심의 이점은 다음과 같습니다. 그래서 하나는 그것이 자연 거래와 같을 수도 있다는 것입니다.

298
00:42:13,240 --> 00:42:19,960
성능과 시간 사이에는 MQA와 MHA 사이의 차이가 있습니다. 음, 알다시피, 그럴 것입니다.

299
00:42:19,960 --> 00:42:22,920
속도 측면에서는 중간 정도가 되겠지만, 측면에서는

300
00:42:24,200 --> 00:42:27,720
성능 측면에서는 항상 중간 정도일 것입니다.

301
00:42:27,720 --> 00:42:33,480
그리고 그것은 단지 최고의 중간 지점일 수도 있습니다. 음, 게다가 MQA를 고려한다면,

302
00:42:34,200 --> 00:42:37,560
아시다시피 이 중 8개, 이 머리 중 8개는

303
00:42:39,960 --> 00:42:45,400
저는 하나의 프로젝션 매트릭스를 공유하겠습니다. 음,

304
00:42:47,480 --> 00:42:53,560
표준 차트에서 이 행렬은 이 네 개가 하나로 그룹화될 수 있습니다.

305
00:42:53,560 --> 00:42:59,640
차트가 있으며 이 4개는 다른 차트로 그룹화해야 할 수도 있습니다. 그리고 이거, 이거, 어, 이거,

306
00:42:59,640 --> 00:43:05,320
여기 이 투영 행렬은 원래 어쨌든 두 번 복사되어야 합니다.

307
00:43:05,320 --> 00:43:09,880
그러니까, 아시다시피, 이 정도의 메모리 비용이 든다면,

308
00:43:09,880 --> 00:43:18,760
어쨌든, 그냥 GQA를 하면 안 되나요? 이제 GQA 결과를 보면, 보시다시피 시간 측면에서 보면

309
00:43:19,480 --> 00:43:27,480
MQA와 비슷하지만 MQA 성능을 능가합니다. 그리고 성능면에서는

310
00:43:27,480 --> 00:43:33,000
MHA와 비슷하지만 시간이 지나면 이깁니다. 음, 여기 거래를 보여주는 그래프가 있습니다.

311
00:43:33,000 --> 00:43:43,240
그룹 수가 늘어나면 GQA 사이에서 떨어져요. 기본적으로, 음, GQA는 기본적으로

312
00:43:43,880 --> 00:43:51,480
MQA와 동일합니다. 음, 하지만 규모를 확대하면 시간이 걸릴 것입니다. 어, 내 말은, 시간이 걸릴 것이라는 뜻입니다.

313
00:43:51,480 --> 00:43:58,360
샘플링하는 데 더 많은 시간이 걸립니다. 어, 그리고 그것은 기본적으로 절충안을 보여줍니다. 그리고 여기, 어, 거래가 표시됩니다

314
00:43:58,360 --> 00:44:07,800
다시, 시간과 공연 사이에. 보시다시피 여기 MHA가 가장 높은 시간을 가지고 있습니다.

315
00:44:08,440 --> 00:44:16,680
성능도 있습니다. 그리고 MQA는 음, 가장 낮은 시간을 가지고 있지만 알다시피 괜찮은 수준이기도 합니다.

316
00:44:16,680 --> 00:44:21,560
성능. 내 말은, 이건 무시하자는 거죠. 그리고 3개 중 성능이 가장 낮습니다. 그리고

317
00:44:22,440 --> 00:44:29,960
8개 그룹으로 구성된 GQA의 경우 상대적으로 좋은 시간과 상대적으로 좋은 성과를 거두었습니다.

318
00:44:31,720 --> 00:44:36,200
그리고 업 트레이닝 결과는 다음과 같습니다. 보시다시피, 평균을 취하면 음,

319
00:44:37,160 --> 00:44:44,200
즉, 알파가 0.05일 때 최고의 성능을 달성한 것입니다. 그리고, 어,

320
00:44:46,600 --> 00:44:53,400
여기에서 알파가 0.05인 것을 볼 수 있습니다. 비슷한 성능을 얻을 수 있습니다.

321
00:44:53,400 --> 00:45:01,000
GQA와 함께하고 QA 내에서는 약간의 가치가 있습니다. 그럼 1%면, 어, 기본적으로 동일한 성능

322
00:45:01,000 --> 00:45:13,640
GQA와 함께라면 MQA와 함께라면 가치가 있습니다. 좋아요. 그리고 이제, 어, 그룹 쿼리에 대한 섹션을 마쳤습니다.

323
00:45:13,640 --> 00:45:18,920
다중 쿼리 보존. 그리고 저는 IO 인식 기반 방법, 즉 플래시 어텐션(Flash Attention)으로 이동할 것입니다.

324
00:45:19,720 --> 00:45:28,680
음, 기본적으로 이전의 주의 속도 향상은 음, 그 중 많은 부분이 예를 들어

325
00:45:28,680 --> 00:45:35,880
관심이 적거나 순위 근사치가 낮습니다. 음, 그, 그, 그, 그, 그들이 무엇을

326
00:45:35,880 --> 00:45:44,280
입력 및 출력을 인식하는 입력 출력에 중점을 두지 마십시오. 제가 의미하는 바는 존재한다는 것입니다.

327
00:45:44,280 --> 00:45:49,720
SRAM, 즉 인칩 SRAM을 스마트하게 사용하고 있습니다. 그들은 그렇지 않습니다. 그들은 그것을 받아들이지 않습니다.

328
00:45:49,720 --> 00:45:58,280
고려 사항. 음, 그리고 플래시 어텐션은 이것에 대해 생각하고 SRAM을 매우 똑똑한 방식으로 사용합니다.

329
00:45:58,760 --> 00:46:06,120
그래서, 음, 기본적으로 더 크지만 느린 메모리인 HBM에 대한 액세스가 더 적습니다.

330
00:46:07,960 --> 00:46:17,080
응. 따라서 이 HBM과 SRAM에 대한 몇 가지 기본 사항을 알려드리자면 HBM의 용량은 40~80GB 정도 됩니다.

331
00:46:18,200 --> 00:46:23,960
아시다시피, 그다지 빠르지는 않고 SRAM 장치도 훨씬 작지만 훨씬 빠릅니다.

332
00:46:24,040 --> 00:46:31,720
그래서 아이디어는, 음, HBM을 많이 여행하는 대신, 이유 권리는 다음과 같습니다.

333
00:46:31,720 --> 00:46:42,600
이것이 HBM이고, 음, 이것이 SRAM이고, 이것이 여러분의, 음, 이것이 여러분의 CPU라고 가정해 보세요.

334
00:46:42,600 --> 00:46:54,600
아니면 미안해요, GPU. 여기가 네가 좋아하는 곳이야, 여기가 네가 있는 곳이야, 여기가 네 유닛이야, 이게 바로

335
00:46:54,600 --> 00:47:04,040
처리 장치. 기본적으로는 정보를 검색하는 것이라고 생각하시면 됩니다. 계속하다 보면,

336
00:47:04,040 --> 00:47:09,720
아시다시피, HBM에서 물건을 보내고 다시 보내고, 계속해서 다시 보내고,

337
00:47:09,720 --> 00:47:16,280
시간이 오래 걸릴 거예요. 하지만 만약 당신이 하는 일이 당신이라면, 당신은 '알겠습니다. 먼저 내가 할게요'라고 말합니다.

338
00:47:16,280 --> 00:47:21,960
HBM에서 읽은 다음 HBM으로 다시 가져오는 것이 아니라 중간체를 다시 넣을 것입니다.

339
00:47:21,960 --> 00:47:25,720
시간이 너무 오래 걸리기 때문에 SRAM에 중간체를 다시 넣을 예정입니다.

340
00:47:25,720 --> 00:47:30,280
그런 다음 다시 처리하고 이 중 일부를 수행한 다음 HBM으로 돌아갑니다.

341
00:47:30,280 --> 00:47:34,520
알다시피, 이것은 손실이 될 것이고, 훨씬 더 비용 효율적일 것입니다. 그리고 이것이 바로 뒤에 있는 아이디어입니다.

342
00:47:34,520 --> 00:47:44,600
플래시주의. 그리고, 음, 아시다시피, 행렬 차원을 다시 한번 상기시켜 드리고,

343
00:47:45,800 --> 00:47:55,800
아시다시피, 저장하려면 1제곱 메모리가 필요합니다. 왜냐하면 이것 때문입니다. 그리고, 그리고, 어, 그래,

344
00:47:55,800 --> 00:48:01,560
그래서, 그래서, 플래시 어텐션에 대해 알아봅시다. 그리고 그 전에 표준을 넘어가자

345
00:48:01,640 --> 00:48:09,080
다시 주목. 그리고 이것이 바로 제가 이야기하고 있던 것입니다. 음, 두 가지 사이를 오가는 것에 대한 것입니다.

346
00:48:09,080 --> 00:48:16,760
HBM 및 처리 장치. 아시다시피, 여기 HBM에서 로드하고, 다시 쓰고, 로드하고,

347
00:48:16,760 --> 00:48:21,560
알다시피, 로드하고, 로드하고, 다시 쓰고, 다시 로드하고, 다시 쓰고,

348
00:48:21,560 --> 00:48:25,880
다시 로드하고, 다시 쓰고, 마지막으로 반환합니다.

349
00:48:26,760 --> 00:48:31,400
음, 아시다시피, 그건, 그게, 그건 일을 하는 가장 현명한 방법도 아니고 실제로도 그렇지 않습니다

350
00:48:31,400 --> 00:48:39,160
이 프로세스의 속도를 높이려면 SRAM을 사용하십시오. 음, 그래서 아이디어는 우리가 SRAM을 사용할 수 있다는 것입니다. 잘,

351
00:48:39,160 --> 00:48:44,120
우리는 그것을 어떻게 사용할 수 있나요? 먼저 타일링이라는 방법을 사용할 수 있습니다. 기본적으로는 다음과 같습니다.

352
00:48:44,120 --> 00:48:54,120
좋아요, 여기가 SRAM이군요. 당신은 그것을 몇 개의 다른 슬롯으로 나눕니다.

353
00:48:55,080 --> 00:49:03,160
Q에 한 슬롯, K에 한 슬롯, V에 한 슬롯, O에 한 슬롯을 할당합니다.

354
00:49:04,040 --> 00:49:11,480
그런 다음 여기서는 두 개의 더 작은 선형 슬롯을 L에 할당하고 다른 하나는 M에 할당합니다.

355
00:49:11,480 --> 00:49:21,880
나중에 그 의미를 설명하겠습니다. 음, 그럼 우리가 이루고 싶은 최종 목표는,

356
00:49:21,880 --> 00:49:31,800
그래서 우리가 원래 X로 시작했다고 가정해 보겠습니다. 우리가 하고 싶은 것은 계산을 하는 것입니다.

357
00:49:31,800 --> 00:49:45,480
당신의, 음, 당신의 Q, K, T 그리고 우리는 그것의 소프트맥스를 취하고 싶습니다.

358
00:49:49,400 --> 00:50:00,520
그 이유는, 그렇죠, 그렇죠, 그렇죠, 소프트맥스 함수는 어떤 모습일까요? 좋아요,

359
00:50:00,520 --> 00:50:05,000
그럼 이 행렬이 다음과 같다고 가정해 보세요.

360
00:50:12,520 --> 00:50:18,200
이 행렬이 단일 벡터라고 가정하면, 음, 우리가 하고 싶은 것은 exp를 취하는 것입니다.

361
00:50:19,160 --> 00:50:24,600
우리는 그것을 지수 함수로 취한 다음 최대값을 뺍니다.

362
00:50:25,400 --> 00:50:32,920
그런 다음 이를 L로 나눕니다. L은 모든 항목의 합입니다.

363
00:50:36,120 --> 00:50:41,800
음, 이쪽은 L입니다. 그리고 음, 그게 기본적으로 우리의 최종 목표입니다.

364
00:50:41,960 --> 00:50:55,160
이제 SRM이 너무 작기 때문에 매번 전체 Q를 로드하는 것이 불가능합니다.

365
00:50:55,720 --> 00:51:02,200
또는 전체 K 또는 전체 V. 따라서 우리는 그것의 아주 똑똑한 부분만 계산할 수 있습니다.

366
00:51:02,760 --> 00:51:09,560
그것의 작은 부분을, 어, 한 번에, 그리고 우리는 그것들을 하나로 연결해야 할 것입니다. 문제는

367
00:51:09,640 --> 00:51:22,440
Q, K, T 외부에 소프트맥스가 있습니다. 음, 최종 목표가 다음이라면 이것은, 좋습니다.

368
00:51:22,440 --> 00:51:31,080
소프트맥스 Q, K, T, V, 어, 설명을 위한 FR이고, 기본적으로 이것은, 어, 이것이 우리의 O입니다.

369
00:51:31,080 --> 00:51:35,560
소프트맥스 없이. 우리의 최종 목표가 다음과 같은 것을 계산하는 것이라면

370
00:51:35,560 --> 00:51:46,680
그리고 매번 우리는 Q의 작은 구성요소인 Q, Q만을 갖게 됩니다. 그래서 이것이

371
00:51:46,680 --> 00:51:54,120
전체 Q, 어, 이게 Q입니다. 그리고 이것은, 우리가 Q를 아주 조금만 로드할 때마다, 어, 이것을 로드합니다

372
00:51:54,120 --> 00:52:02,040
는 K이고, 우리는 약간만 로드하고, 이것은 KT이고, 우리는 K를 조금만 로드합니다. 음, 그러니까 이것은 밖으로 나가라는 뜻입니다

373
00:52:02,040 --> 00:52:10,760
전체 출력에서 ​​우리는 하나에서 좋아요까지의 Q만 가지고 있습니다. 아마도 여기 어딘가와 같을 것입니다. 그리고 K,

374
00:52:12,280 --> 00:52:18,280
어, 미안해요. Q는 이쪽으로 가요, Q는 이쪽으로 가요. Q, 여기 어딘가에 마음에 드는 것 하나. 그리고 K

375
00:52:18,280 --> 00:52:25,320
여기에서 여기까지. 그래서 우리는 이 부분만 가지고 있습니다. 그리고 우리가 계산해야 할 것은,

376
00:52:26,040 --> 00:52:31,400
우리가 하는 일은 이것을 계산하고 이것을 계산하고 또 이것을 계산하는 것입니다.

377
00:52:31,400 --> 00:52:37,000
이것을 계산하고, 이것을 계산하고 여기에 더하고, 이것을 계산하고 더합니다.

378
00:52:37,000 --> 00:52:49,640
이에. 음, 그리고 그다음에는 좀 더 명확하게 그려보겠습니다. 먼저 이 작은 것을 계산해 보세요.

379
00:52:49,640 --> 00:52:55,400
부분, 그리고 이것, 그리고 이것, 그리고 이것을 계산하고, 이것에 더하고, 계산합니다.

380
00:52:55,400 --> 00:53:00,200
이것, 이것에 더하고, 이것을 계산하고, 이것에 더하세요. 이제 문제는 만약 우리가

381
00:53:00,200 --> 00:53:06,840
가르친 대로, 우리는 소프트맥스를 취해야 합니다. 따라서 우리는 이것을 계산하고 소프트맥스를 취할 수는 없습니다.

382
00:53:06,840 --> 00:53:10,960
그런 다음 이것을 계산하고 소프트맥스를 가져와 여기에 추가합니다. 알잖아, 그건 그렇지 않아

383
00:53:10,960 --> 00:53:16,720
그렇게 일하세요. 잘못된 소프트맥스 값을 얻게 됩니다. 그래서 우리가 지켜야 할 이유는

384
00:53:16,800 --> 00:53:26,680
여기 이 행의 최대값인 m의 배열, 그리고 또 다른 l의 배열,

385
00:53:26,680 --> 00:53:36,800
기본적으로 여기에 있는 합계입니다. 그리고 우리는 이것을 사용하여 최종적으로 올바른 값을 계산할 것입니다.

386
00:53:36,800 --> 00:53:46,120
맨 끝에는 소프트맥스가 있습니다. 좋아요, 그럼 어떻게 할까요? 너무 높은 수준의 아이디어입니다. 우리가 하는 일은

387
00:53:46,160 --> 00:53:57,200
우리는 이것으로 시작합니다. 예, 어쩌면 의사 코드를 살펴보도록 하겠습니다. 아니면 예.

388
00:53:57,200 --> 00:54:03,320
그래서 높은 수준의 아이디어는 우리가 이것을 하고, 그런 다음 이것을 하고, 이것을 하고, 이것을 한다는 것입니다.

389
00:54:03,320 --> 00:54:12,160
그리고 여기까지 오면 기본적으로 어떤 요인에 의해 이를 정규화합니다. 그리고 우리가

390
00:54:12,200 --> 00:54:18,160
여기까지 와서 해당 요소를 다시 곱한 다음 이를 소프트맥스로 나눕니다.

391
00:54:18,160 --> 00:54:24,280
모든 것에 대해. 그런 다음 반복합니다. 그래서 마지막에 도달하면 기본적으로

392
00:54:24,280 --> 00:54:34,880
곱하면 전체를 모든 것에 대한 알파 인수로 나눈 값이 있습니다.

393
00:54:34,880 --> 00:54:46,240
네, 그럼 의사 코드를 살펴보겠습니다. 따라서 먼저 K와 V를 반복합니다. 즉

394
00:54:46,240 --> 00:55:00,400
내부 루프는 Q와 O에 있습니다. 예, 아마도 Q, O, K, V, L, M이 있어야 할 것입니다.

395
00:55:00,400 --> 00:55:14,560
우리의 SRAM 칩입니다. 처음에는 기본적으로 하위 집합과 같은 K, K, I, KJ, VJ를 로드합니다.

396
00:55:14,560 --> 00:55:25,360
K와 V의 전체 행렬을 KT라고 생각할 수 있습니다. 그리고 KJ는 좀 그렇죠

397
00:55:25,360 --> 00:55:38,080
K의 열 수. 그리고 V는 VJ의 수입니다. KJT입니다. 이것은 VJT와 같습니다.

398
00:55:38,400 --> 00:55:49,520
이것은 V입니다. Q와 O도 마찬가지입니다. 따라서 HBM에서 SRAM으로 KJ, VJ를 로드합니다. 만약 너라면

399
00:55:49,520 --> 00:55:56,560
Q, KT, V의 최종 출력을 살펴보세요. 이는 우리가 범위를 좁히려고 노력하고 있음을 의미합니다.

400
00:55:57,760 --> 00:56:04,640
예를 들어 J에 두 개의 행만 포함되어 있다면 여기서는 이 그룹만 살펴보겠습니다.

401
00:56:05,440 --> 00:56:17,600
그런 다음 Q를 반복합니다. 이는 우리가 이 방향으로 내려간다는 것을 의미합니다. 그런 다음 Q와 O를 로드합니다.

402
00:56:17,600 --> 00:56:25,920
그래서 우리는 첫 번째 것을 로드합니다. 그리고 칩에서는 Q, Q, I, J, Q, I, K, J, T를 계산합니다. 그래서 먼저 계산합니다.

403
00:56:25,920 --> 00:56:32,080
이것. 그리고 우리는 이것을 계산합니다. 그리고 우리는 이것을 계산합니다. 그리고 문제는, 그 후에 우리가 무엇을 하느냐는 것입니다.

404
00:56:32,080 --> 00:56:41,920
하나하나? 좋아요, 예를 들어 먼저 여기서 이것을 계산해 보겠습니다. 그리고 우리는

405
00:56:42,640 --> 00:56:57,040
이 행의 최대값입니다. 그런 다음 이를 정규화하고 이 행을 취하고 그 값을 취합니다.

406
00:56:57,040 --> 00:57:11,760
그리고 우리는 그것을 지수로 가져갑니다. 그래서 우리는 이 모든 값을 그것에 지수화합니다. 오,

407
00:57:11,760 --> 00:57:17,120
그건 그렇고, 죄송합니다. 이것은 중간 값이 아닙니다. 이것이,

408
00:57:17,920 --> 00:57:27,760
제가 정말로 해야 할 일은 이것에 대한 것입니다. 이것은 Q, K, T입니다. 그래서 우리는 그것의 작은 부분을 취합니다.

409
00:57:29,200 --> 00:57:38,160
먼저 여기 어딘가처럼요. 그런 다음 각 행을 살펴보겠습니다. 첫 번째 행에 대해 최대값을 살펴보겠습니다.

410
00:57:38,160 --> 00:57:45,120
그런 다음 이 값에서 행 최대값을 뺀 지수를 계산합니다. 그런 다음 행을 뺀 다음 값

411
00:57:45,120 --> 00:57:51,520
최대 다음 값에서 행 최대 값을 뺍니다. 그런 다음 다음 행을 보고 이 행, 이 값이라고 말합니다.

412
00:57:51,520 --> 00:58:02,320
e는 이것에서 이것의 최대값을 뺀 것입니다. 그런 다음 우리는 이 L로 나눕니다. 그리고 우리는 이렇게 말합니다.

413
00:58:04,400 --> 00:58:10,320
좋습니다. 이것이 최대값인지 살펴보겠습니다. 그러니 우리가 다음과 같다고 가정해보자

414
00:58:10,320 --> 00:58:17,040
이 최대값이 이전 값 전체의 최대값이라면 여기 중간 어딘가에 있을 것입니다. 그래서

415
00:58:21,440 --> 00:58:26,400
이 부분을 이미 수행했다면 이 부분도 이미 수행한 것이며 이 파티션을 살펴보는 것과 같습니다.

416
00:58:27,520 --> 00:58:33,520
아마도 세 번째, 세 번째 K, K 3 등이 될 것입니다. 그리고 나서 우리는 Q 1을 보고 있습니다.

417
00:58:34,480 --> 00:58:41,680
여기서 우리는 최대값을 보고 '아, 이 최대값이 이전 두 개보다 큰가요?'라고 말합니다.

418
00:58:45,920 --> 00:58:51,440
그래서 계속해서 여기서 최대값을 취합니다. 여기에서 진정한 최대값을 취합니다. 그리고 나서

419
00:58:51,440 --> 00:58:56,240
이전 두 개에는 이전에 계산된 최대값이 있습니다. 그리고 우리는 기본적으로

420
00:58:57,760 --> 00:59:02,480
이미 저장된 이전 두 항에서 이를 제거하려면 해당 최대값을 곱합니다.

421
00:59:02,480 --> 00:59:08,800
이전 O에서. 그런 다음 모든 것을 나누어야 하기 때문에 실제 최대값을 뺍니다.

422
00:59:08,800 --> 00:59:14,800
기본적으로 e를 음수 mi 소수로 변환합니다. 그런 다음 현재에 대해서도 동일한 작업을 수행합니다.

423
00:59:14,800 --> 00:59:22,880
현재 것. 우리는 현재 나누어진 값, 즉 전역 변수로 나누어진 값을 곱하고 싶습니다.

424
00:59:22,880 --> 00:59:36,400
최고. 그리고 우리가 OI에 쓰고 싶은 것은 기본적으로 원래의 O I를 갖고 싶다는 것입니다.

425
00:59:36,400 --> 00:59:39,440
그래서 우리는 이미 이것을 계산했습니다. 우리는 이것을 이미 계산했습니다. 이제 우리는 여기 어딘가에 있습니다.

426
00:59:40,640 --> 00:59:48,080
우리는 그것을 정규화하고 LLI로 곱하고 싶습니다. 왜냐하면 우리는 이전에 LLI로 많이 나누었기 때문입니다.

427
00:59:48,720 --> 00:59:57,840
그리고 우리는 글로벌 소프트맥스 소프트맥스 트루 소프트맥스로 나눴습니다. 그리고, 그래, 우리도 같은 일을 해

428
00:59:57,840 --> 01:00:04,000
여기 세 번째. 바로 이 용어입니다. 그리고 이것은 기본적으로 이미 사용된 용어입니다.

429
01:00:04,000 --> 01:00:11,520
쓴. 그리고 우리는 헹구고 반복한 다음 전체를 계산합니다. 그렇죠, 바랍니다.

430
01:00:11,520 --> 01:00:17,520
그것은 충분히 분명했습니다. 그래서 플래시 주의가 보장됩니다. 따라서 추가 메모리가 필요합니다.

431
01:00:17,600 --> 01:00:28,400
이는 L과 M에서 비롯됩니다. 물론 기본적으로 HBM 액세스가 훨씬 적게 필요합니다.

432
01:00:28,400 --> 01:00:33,200
우리는 이미 모든 중간 단계를 수행하고 기본적으로 칩에서 수행한 다음 기본적으로

433
01:00:33,200 --> 01:00:42,960
마지막에 마지막 쓰기를 하나 하세요. 응, 이것들은 칩에 있지, 그렇지? 그래서, 그렇습니다, 그리고 그들은 증명합니다

434
01:00:42,960 --> 01:00:48,080
메모리 액세스 복잡성 측면에서 일부 정리에서는 더 나은 것이 없다는 정리입니다.

435
01:00:48,080 --> 01:00:57,200
가벼운 가정. 다음은 블록 바에 주의가 집중되는 것입니다. 기본적으로 추가 기능입니다.

436
01:00:57,200 --> 01:01:09,520
이것. 그래서 블록 바에서는 주의를 기울이세요. 기본적으로, 여기 이 매트릭스에서, 기본적으로 우리는 이렇게 말합니다.

437
01:01:09,520 --> 01:01:14,960
이것들은 예를 들어 중앙의 것들에만 관심이 있습니다. 그리고 이것들에 대해서는 우리는 그렇지 않습니다

438
01:01:14,960 --> 01:01:20,880
너무 멀리 떨어져 있기 때문에 정말로 그들을 걱정합니다. 그리고 그건 아주 쉬운 일이죠.

439
01:01:20,880 --> 01:01:33,840
이 QKT를 블록으로 자릅니다. 그리고 이것을 0으로 설정하고, 이것을 0으로 설정하고, 이것을 0으로 설정하세요. 그리고

440
01:01:33,840 --> 01:01:39,200
for 루프 중에는 그냥 건너뛰면 됩니다. 해당 for 루프에서 continue 문을 수행하세요.

441
01:01:39,280 --> 01:01:51,440
그리고 당신은 그것에 아무것도 할당하지 않을 것입니다. 네, 그럼 결과를 보면, 네, 아시다시피

442
01:01:51,440 --> 01:01:59,280
그것은 이전보다 훨씬 빨라진 것입니다. 그리고 네, 다시 말씀드리지만, 성능은 다음과 같아야 합니다.

443
01:01:59,280 --> 01:02:04,640
우리는 실제로 아무것도 바꾸지 않기 때문에 똑같습니다. 모델에 내재된 모든 것 또는

444
01:02:04,640 --> 01:02:09,280
모델 훈련에서 우리가 하고 있는 일은 숫자를 읽는 방식을 바꾸는 것뿐입니다.

445
01:02:13,360 --> 01:02:19,920
그리고 다시, 속도 향상을 보여줍니다. 다음은 훈련 시간입니다.

446
01:02:24,800 --> 01:02:29,840
보시다시피 훨씬 더 빠릅니다. 아시다시피 이전보다 속도가 훨씬 빨라졌습니다.

447
01:02:30,480 --> 01:02:34,320
포드는 포드 패스만 커버했고, 실제로는 추가적인 백워드 패스도 있었습니다.

448
01:02:35,200 --> 01:02:38,240
플래시 어텐션 페이퍼 부록에서 확인하실 수 있습니다.

449
01:02:39,840 --> 01:02:43,200
네, 그래서 순간적인 관심은 검은 것입니다.

450
01:02:45,360 --> 01:02:52,160
희소한 플래시 주의를 차단하는 것은 물론 가장 빠른 속도입니다. 그리고 그 다음은 녹색입니다.

451
01:02:52,160 --> 01:02:57,280
그리고 그것은 순간적인 관심입니다. 사실 린 전이 가끔 이기기도 해요. 하지만 그건 일종의 속임수죠.

452
01:02:57,280 --> 01:03:01,760
왜냐하면, 플래시 어텐션은 여전히 ​​n 제곱이기 때문입니다. 여러분은 여전히 ​​저장하고 있고, 여전히

453
01:03:01,760 --> 01:03:06,720
전체 QKT 행렬을 저장하는 반면 Lin 전자는 선형적인 것만 저장하는 것 같습니다.

454
01:03:08,240 --> 01:03:10,800
또는 계산은 선형입니다.

455
01:03:12,800 --> 01:03:20,160
좋아요, 그래서 순간적인 관심은 실제로 꽤 놀라운 결과를 얻습니다.

456
01:03:20,160 --> 01:03:27,760
경로 x에서. 그래서 그것이 하는 일은 경로 x에서 무작위가 아닌 성능을 달성하는 것입니다.

457
01:03:30,320 --> 01:03:38,960
달성할 첫 번째 변압기인 곳입니다. 그녀는 이것을 무작위가 아닌 달성했습니다. 죄송합니다.

458
01:03:44,080 --> 01:03:49,360
따라서 플래시 어텐션은 비랜덤 성능을 가장 먼저 달성하는 것이기도 합니다. 최초의 트랜스포머다.

459
01:03:49,360 --> 01:03:54,880
16k 컨텍스트를 사용하여 경로 x에서 무작위가 아닌 성능을 달성합니다. 그리고 주의를 집중시키세요

460
01:03:54,880 --> 01:04:01,840
Block Sparse Attention은 무작위가 아닌 성능을 달성하기 위한 최초의 시퀀스 대 시퀀스 모델입니다.

461
01:04:01,840 --> 01:04:08,000
16 64k 컨텍스트에서. 이제 Flash Attention 버전 2에 대해 이야기하겠습니다.

462
01:04:10,480 --> 01:04:15,840
그래서 기존에 비해 몇 가지 개선된 점이 있습니다. 따라서 이전 내용을 기억해 보면

463
01:04:16,400 --> 01:04:20,800
나누는 대신,

464
01:04:23,920 --> 01:04:29,120
기본적으로 이것을 로드하고 로드한 다음 이를 로드하여 계산한 방법을 기억하세요.

465
01:04:29,120 --> 01:04:34,960
그것을 계산했다. 그리고 우리는 이것을 계산했습니다. 그리고 여기 다시 우리는 l로 나눴습니다.

466
01:04:37,280 --> 01:04:42,320
그리고 나서 여기서 우리는 l을 다시 곱하고 l 소수로 나눕니다. 이는 다음의 합입니다.

467
01:04:42,320 --> 01:04:48,240
모든 것. 기본적으로는 그렇게 할 필요가 없다는 뜻입니다. 우리는 l로 나누지 않습니다.

468
01:04:48,240 --> 01:04:58,480
첫 번째 장소를 선택하고 끝까지 기다려 이 큰 L로 나누세요. 그래서 예, 그게 아이디어입니다.

469
01:04:58,480 --> 01:05:06,960
여기. 따라서 일반적으로 계산 속도가 느린 일부 비행렬 곱셈 플롭이 줄어듭니다.

470
01:05:07,360 --> 01:05:13,120
그리고 또 다른 점은 인과 마스킹 언어 모델링을 할 때

471
01:05:14,320 --> 01:05:22,480
마스크를 적용한다면 이것이 q이고 이것이 qkt 행렬입니다. 그리고 만약 우리가 하려고 한다면

472
01:05:22,480 --> 01:05:28,000
어쨌든 이 부분을 가리세요. 우리는 애초에 계산할 필요가 없었고 그냥 탈출했습니다.

473
01:05:28,000 --> 01:05:35,040
for 루프 동안에 말이죠. 그리고 Flash Attention v2가 우리에게 제공하는 또 다른 기능은 병렬성입니다.

474
01:05:35,760 --> 01:05:41,760
따라서 v2에서는 이것이 k에 있었고 v가 플래시의 내부 루프에 있었다는 것을 기억하는 대신

475
01:05:41,760 --> 01:05:49,200
주목. v2에서는 외부 루프에 유지합니다. 우리는 그것을 외부 루프에 유지합니다. 왜냐하면

476
01:05:50,480 --> 01:05:56,640
여기 이 다이어그램을 보도록 하겠습니다. 기본적으로 정사각형을 고려하세요.

477
01:05:57,520 --> 01:06:11,200
qkt. 그리고 우리가 q에 걸쳐 병렬화한다면, 그것은 무엇을 의미합니까? 즉, 직원이 한 명 있다는 뜻입니다.

478
01:06:11,200 --> 01:06:17,040
여기서 시작해서 이쪽으로 가세요. 전에는 그랬었다는 걸 기억해, 만약 우리가 이렇게 가고 있었다면

479
01:06:17,120 --> 01:06:30,960
이런 식으로, 우리가 좋아하는 곳에서는 계산된 것을 좋아합니다. 좋아요, 재미있습니다. 이렇게 계산하면 됩니다.

480
01:06:30,960 --> 01:06:38,560
먼저, 다음은 이것, 이것, 이것, 이것, 이것 등등. 자, 이제 우리가 할 일은 우리입니다.

481
01:06:38,560 --> 01:06:43,680
이런 식으로 병렬화하세요. 그래서 작업자가 이 작업을 수행하면 즉시 뒤로 이동할 수 있습니다.

482
01:06:43,760 --> 01:06:48,720
이것을 전달하십시오. 그리고 두 번째 역방향 패스에 도달할 때 작업자 2도 이 작업을 수행했습니다.

483
01:06:49,360 --> 01:06:54,960
그러면 여기 있는 이 5명의 작업자는 제가 6가지 작업을 수행하게 되고 모두가 할 수 있는 작업을 수행하는 것을 볼 수 있습니다.

484
01:06:56,400 --> 01:07:02,160
해. 우리가 배열하는 방식이기 때문에 순차적으로 하는 것이 가능합니다. 그래서

485
01:07:02,160 --> 01:07:09,760
이것이 바로 우리가 병렬화하는 방식입니다. Flash Attention v2가 제공하는 또 다른 기능은 워프입니다.

486
01:07:09,760 --> 01:07:15,520
파티셔닝. 따라서 각 스레드 블록에는 서로 다른 스레드가 있습니다. 그리고 우리가 하고 싶은 일은

487
01:07:15,520 --> 01:07:27,760
우리는 k가 아닌 q를 통해 이러한 스레드를 분할하고 싶습니다. 그리고 우리가 하고 싶은 이유는

488
01:07:27,760 --> 01:07:37,200
그 이유는 우리가 q를 통해 분할할 때 각 스레드가 q를 통해 분할되면 알다시피 완료되기 때문입니다.

489
01:07:37,200 --> 01:07:44,240
이는 스레드가 교차하여 서로 통신할 필요가 없음을 의미합니다. 하지만 만약 우리가

490
01:07:44,880 --> 01:07:49,120
k로 나눠서 계산하면 여전히 더해야 합니다.

491
01:07:49,120 --> 01:07:53,360
함께. 반면에 이것은 당신이 알고 있기 때문에 모든 것이 이미 당신을 위해 추가되었습니다. 그리고 그것은 마치

492
01:07:53,360 --> 01:08:00,560
행 현명한, 우리는 추가 통신 비용을 지불할 필요가 없습니다. 그리고 결과를 보면,

493
01:08:00,560 --> 01:08:05,200
왼쪽에는 전진 속도가 있고 오른쪽에는 후진 속도가 있습니다.

494
01:08:05,200 --> 01:08:12,080
속도. 그리고 보시다시피 보라색은 플래시로 주목을 받고 있으며 다른 것보다 훨씬 빠릅니다.

495
01:08:12,080 --> 01:08:21,120
기존 방식. 그리고 그렇습니다. 여기에 참고 자료가 있습니다. 그리고 감사합니다.