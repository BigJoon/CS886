1
00:00:00,960 --> 00:00:06,160
안녕하세요. 검색 확대에 대한 강연에 오신 것을 환영합니다. 내 이름은 Joel Rorseth이고

2
00:00:06,160 --> 00:00:11,200
워털루 대학교의 컴퓨터 과학 학생입니다. 오늘은 두 편의 논문을 발표하겠습니다.

3
00:00:11,200 --> 00:00:18,000
2024년 겨울 과정 CS886, 최근 발전의 일부인 검색 증강 세대에 대해

4
00:00:18,000 --> 00:00:23,760
Wenhu Chen 박사가 지도한 기초 모델. 더 이상 고민하지 말고 시작해 보겠습니다.

5
00:00:25,520 --> 00:00:29,360
내가 제시할 이 두 논문의 공통 주제는 검색에 대한 접근 방식입니다.

6
00:00:29,360 --> 00:00:34,480
증강, 즉 그들은 검색기와 언어 모델을 공동으로 미세 조정하려고 시도합니다.

7
00:00:35,920 --> 00:00:40,800
시간순으로 살펴보면 첫 번째로 소개할 논문은 검색증강세대(Retrieval Augmented Generation)이다.

8
00:00:40,800 --> 00:00:48,240
지식 집약적인 NLP 작업용으로 2020년에 출판되었습니다. 몇 가지 주요 용어를 검토해 보겠습니다. 저자

9
00:00:48,240 --> 00:00:53,440
지식 집약적인 작업을 말하며, 이를 수행하기 위해 외부 지식이 필요한 작업을 말합니다.

10
00:00:53,440 --> 00:00:59,920
저자는 또한 파라메트릭 메모리와 비파라메트릭 메모리를 언급합니다. 파라메트릭 메모리는

11
00:00:59,920 --> 00:01:05,920
모델과 사전 훈련된 가중치에 저장된 지식입니다. 비모수적 기억은 지식을 의미합니다

12
00:01:05,920 --> 00:01:12,880
데이터베이스, 검색 시스템, 그렇지 않으면 외부에 저장됩니다. 하이브리드 메모리는 접근 방식입니다

13
00:01:12,880 --> 00:01:19,520
영역과 같이 매개변수와 비모수를 결합합니다. 이 논문에서 저자들은 동기를 부여받았다.

14
00:01:19,520 --> 00:01:25,120
특히 시퀀스 간 모델에 대한 하이브리드 접근 방식을 제안합니다. 그들은 여러 가지를 지적합니다.

15
00:01:25,120 --> 00:01:29,280
업데이트나 확장이 어렵다는 파라메트릭 메모리의 약점,

16
00:01:29,920 --> 00:01:34,880
직접 검사할 수는 없으며 물론 우리가 알고 있듯이 언어 모델에서 사용하면

17
00:01:34,880 --> 00:01:40,240
환각으로 이어질 수 있습니다. 반면, 비모수적 메모리에는 이러한 문제가 없습니다.

18
00:01:40,960 --> 00:01:45,600
비모수적 지식은 업데이트하거나 확장하기 쉽고 직접 검사할 수 있습니다.

19
00:01:46,560 --> 00:01:50,000
적어도 이 글을 쓰는 시점에는 하이브리드 접근 방식이 다소 제한되어 있었습니다.

20
00:01:50,000 --> 00:01:55,200
그들은 개방형 도메인 추출 QA만 탐색했으며 아직 시퀀스 간 QA에 적용되지 않았습니다.

21
00:01:55,200 --> 00:02:00,720
따라서 시퀀스 간 모델에 대한 특정 하이브리드 접근 방식을 개발하려는 동기가 부여됩니다.

22
00:02:02,320 --> 00:02:07,040
저자가 제안하는 솔루션은 Retrieval Augmented라는 미세 조정 전략입니다.

23
00:02:07,040 --> 00:02:13,040
세대, 줄여서 RAG. 그들의 아키텍처는 사전 훈련된 세 가지 구성 요소로 구성됩니다.

24
00:02:13,120 --> 00:02:19,120
사전 훈련된 신경 검색기인 BART를 사용하는 시퀀스-시퀀스 변환기.

25
00:02:19,120 --> 00:02:25,680
밀집 통로 검색기, DPR 및 밀집 벡터 인덱스를 사용합니다. 이 경우에는 Wikipedia를 색인화했습니다.

26
00:02:26,480 --> 00:02:29,680
이러한 모델은 이미 충분히 학습되어 다음과 같은 결과를 얻을 수 있다고 가정합니다.

27
00:02:29,680 --> 00:02:36,080
지식에 접근하는 능력. RAG 아키텍처는 두 가지 주요 구성 요소로 요약됩니다.

28
00:02:36,080 --> 00:02:40,720
비모수적 메모리인 검색기(Retriever)와 매개변수적 메모리인 생성자(Generator)입니다.

29
00:02:41,600 --> 00:02:45,440
여기에 각각의 확률 분포에 대한 수학적 정의가 있습니다.

30
00:02:46,400 --> 00:02:52,960
검색기 p sub eta는 잠재적으로 예측된 ​​상위 k개의 잘린 분포를 제공합니다.

31
00:02:52,960 --> 00:02:59,840
쿼리 x가 주어지면 텍스트 구절 z. 반면에 생성기는 p sub theta,

32
00:03:00,400 --> 00:03:06,880
이전 토큰, 쿼리 x가 주어지면 예측된 다음 토큰에 대한 확률을 제공합니다.

33
00:03:06,880 --> 00:03:13,760
그리고 리트리버에 의해 검색된 통로, z. 저자는 실제로 두 가지 변형을 제안합니다.

34
00:03:13,760 --> 00:03:20,880
RAG 모델, RAG 시퀀스 및 RAG 토큰. RAG 시퀀스를 사용하면 하나의 문서를 먼저 검색합니다.

35
00:03:20,880 --> 00:03:26,400
이를 사용하여 전체 시퀀스를 생성합니다. RAG 토큰을 사용하면 잠재적으로 다른 토큰을 사용합니다.

36
00:03:26,400 --> 00:03:32,560
최종 시퀀스에서 각 토큰을 생성하는 문서입니다. 여기에 수학적 분포가 있습니다.

37
00:03:32,560 --> 00:03:38,240
이 두 가지 생성기 변형 각각에 대해. RAG 시퀀스의 경우,

38
00:03:38,240 --> 00:03:44,400
상위 k개 문서 각각에 대해 이러한 확률을 무시합니다. 따라서 각 상위 k 문서에 대해

39
00:03:44,400 --> 00:03:50,080
격리를 통해 전체 출력 시퀀스에 대한 확률을 생성합니다. 하지만 RAG의 경우에는

40
00:03:50,080 --> 00:03:56,080
토큰을 사용하면 이 곱이 방정식의 시작 부분으로 이동하는 것을 볼 수 있습니다. 즉, 이제

41
00:03:56,080 --> 00:04:00,880
각 토큰에 대해 우리는 이러한 상위 k개 문서 각각을 소외시키고 다음을 생성할 수 있습니다.

42
00:04:00,880 --> 00:04:07,040
다음 출력 토큰을 한 번에 하나씩 배포합니다. 그럼 구현에 대해 논의해 보겠습니다.

43
00:04:07,040 --> 00:04:12,800
인코딩부터 시작합니다. 리트리버는 앞서 언급한 것처럼 DPR입니다.

44
00:04:12,800 --> 00:04:18,560
BERT를 사용하여 쿼리와 문서를 인코딩합니다. 그런 다음 이 둘 사이의 유사성이 계산됩니다.

45
00:04:18,560 --> 00:04:24,160
최대 내적 탐색, MIPS를 사용합니다. 발전기는 그냥 BERT 대형 모델이고,

46
00:04:24,160 --> 00:04:29,520
4억 개의 매개변수가 있습니다. 입력된 쿼리와 검색된 문서는 단순히 연결되어 있으며,

47
00:04:29,520 --> 00:04:33,280
오늘날의 최신 RAG 및 신속한 엔지니어링 기술만큼 멋진 것은 없습니다.

48
00:04:34,560 --> 00:04:40,000
다음 토큰을 결정하는 방법인 디코딩은 조금 더 미묘합니다. 경우에

49
00:04:40,000 --> 00:04:45,280
RAG 토큰을 사용하면 이 확률 분포를 표준 빔 디코더에 연결할 수 있습니다. 하지만

50
00:04:45,280 --> 00:04:51,040
RAG 시퀀스의 경우 실제로 토큰별 가능성으로 나뉘지 않습니다. 그래서 우리는 할 수 없습니다

51
00:04:51,040 --> 00:04:55,360
저자는 각각에 대해 빔 검색을 실행하도록 제안하는 반면 단일 빔 검색을 실행하십시오.

52
00:04:55,360 --> 00:05:00,080
검색된 문서. 미세 조정 구현은 상대적으로

53
00:05:00,080 --> 00:05:05,120
똑바로. 그들은 리트리버와 생성기를 공동으로 미세 조정합니다. 구체적으로,

54
00:05:05,120 --> 00:05:10,560
검색기 쿼리 인코더와 생성기는 업데이트하지만 검색기의 쿼리 인코더는 업데이트하지 않습니다.

55
00:05:10,560 --> 00:05:15,280
문서 인코더. 이는 단순히 강한 목표를 달성하는 것이 필요하지 않다고 생각했기 때문입니다.

56
00:05:15,280 --> 00:05:20,800
성능도 좋고 가격도 상대적으로 비싸기 때문이죠. 그들이 사용하는 미세 조정 데이터

57
00:05:20,880 --> 00:05:25,040
는 실험 섹션에서 정의될 x와 y 쌍의 모음입니다.

58
00:05:26,240 --> 00:05:32,560
목적 함수는 확률론을 사용하여 수행된 음의 로그 확률을 간단히 최소화하는 것입니다.

59
00:05:32,560 --> 00:05:39,440
경사하강법과 원자 최적화 프로그램. 여기에 내용을 시각적으로 요약하는 멋진 다이어그램이 있습니다.

60
00:05:39,440 --> 00:05:45,360
검색기가 생성기로 흘러 들어간 다음 다시 전파되는 것을 볼 수 있습니다.

61
00:05:45,360 --> 00:05:49,600
이 다이어그램의 생성기와 검색기의 쿼리 인코더 큐.

62
00:05:51,120 --> 00:05:55,840
이제 실험에 대한 논의로 넘어가서 먼저 검색 설정에 대해 이야기해 보겠습니다.

63
00:05:56,640 --> 00:06:02,880
저자는 2018년 12월의 Wikipedia 덤프를 사용하고, 100개의 단어 청크를 사용한 후,

64
00:06:02,880 --> 00:06:08,480
그들은 색인을 생성하기로 선택한 문서가 2,100만 개 있습니다. 인덱스 프로세스 자체가 예쁘네요

65
00:06:08,480 --> 00:06:14,160
똑바로. 그들은 BERT 모델을 사용하여 각 문서를 인코딩하고 이를 통해 임베딩을 제공합니다.

66
00:06:14,160 --> 00:06:21,680
임베딩은 FICE를 사용하여 MIPS 인덱스에 삽입됩니다. 또한 계층적 탐색 가능을 사용합니다.

67
00:06:21,680 --> 00:06:26,960
더 빠른 검색을 위한 작은 세계와 검색 자체를 훈련하는 동안

68
00:06:26,960 --> 00:06:32,880
K에는 5개 또는 10개의 문서를 사용하고 테스트 중에는 깊이 데이터를 사용하여 K를 설정합니다.

69
00:06:35,120 --> 00:06:38,480
실험에서 저자는 네 가지 다른 작업을 통해 방법을 테스트합니다.

70
00:06:39,120 --> 00:06:45,840
오픈 도메인 QA, 추상적인 QA, 위험 질문 생성 및 사실 확인.

71
00:06:47,360 --> 00:06:52,800
물론 오픈 도메인 QA 작업부터 시작하여 목표는 자신의 방법을 작업하는 것입니다.

72
00:06:52,800 --> 00:06:58,400
객관적이고 사실적인 질문에 답하면서 그들은 실제로 4개의 서로 다른 QA 데이터 세트를 가지고 있습니다.

73
00:06:58,400 --> 00:07:03,200
따로 평가하고 있어요. 설정은 이러한 각 질문 답변을 사용하는 것입니다.

74
00:07:03,200 --> 00:07:07,680
두 가지 모델을 찾기 위해 이러한 데이터 세트에서 쌍을 찾고, 그들이 비교하는 기준선을 찾습니다.

75
00:07:07,680 --> 00:07:13,360
반대되는 것은 외부 지식에 접근할 수 있는 개방형 북 모델과 일부 폐쇄형 북 모델입니다.

76
00:07:13,360 --> 00:07:18,640
외부 지식에 접근할 수 없습니다. 사용된 측정항목은 정확히 일치합니다. 즉,

77
00:07:18,640 --> 00:07:22,000
생성된 답변은 목표 실제값과 정확히 일치해야 합니다.

78
00:07:23,920 --> 00:07:28,800
따라서 RAG는 이 QA 작업에서 매우 강력한 성능을 발휘합니다. 아래 두 행에서 RAG가 표시된 것을 볼 수 있습니다.

79
00:07:28,800 --> 00:07:33,520
토큰 및 RAG 시퀀스는 비공개 및 공개 도서 방식을 상당히 향상시킵니다.

80
00:07:34,080 --> 00:07:38,640
이유에 대한 대답은 매우 간단합니다. RAG는 ​​무엇이든 관계없이 답변을 생성할 수 있기 때문입니다.

81
00:07:38,640 --> 00:07:43,840
검색한 문서에 실제로 답변이 포함되어 있는지 여부. 대답이 그렇지 않다면

82
00:07:43,840 --> 00:07:48,560
문서에서 다른 문서는 유용한 정보를 제공할 수 있습니다.

83
00:07:48,560 --> 00:07:54,400
답변으로 이어집니다. 일반적으로 RAG는 두 세계의 장점을 결합합니다. 장점을 결합할 수 있습니다.

84
00:07:54,400 --> 00:07:58,720
열린 책과 닫힌 책의 차이로 인해 여기에서 성능이 향상되는 것을 볼 수 있습니다.

85
00:07:59,680 --> 00:08:06,160
작업 2로 이동하면 QA 개요, 특히 자연어 생성에 중점을 두고 있습니다.

86
00:08:06,160 --> 00:08:11,760
NLG. 이를 테스트하는 데 사용한 데이터 세트는 MS Marko NLG 작업 버전 2.1입니다.

87
00:08:12,320 --> 00:08:17,680
이는 질문의 대규모 데이터 세트로, 각 질문에는 10개의 골드 검색 엔진 구절이 있습니다.

88
00:08:17,680 --> 00:08:23,280
전체 문장 답변에 주석이 추가됩니다. 그런 다음 설정을 위해 다음을 사용하여 미세 조정합니다.

89
00:08:23,280 --> 00:08:29,360
질문 답변 쌍인 XY 쌍입니다. 특히 RAG가 파라메트릭에 의존하도록 강제합니다.

90
00:08:29,360 --> 00:08:34,240
MS Marko 데이터 세트에 제공되는 이러한 목표 구절을 무시하여 더 많은 지식을 얻을 수 있습니다.

91
00:08:35,040 --> 00:08:39,040
그들이 비교하는 기준은 BART와 최첨단 성능입니다.

92
00:08:39,040 --> 00:08:45,520
파란색과 연지라는 두 가지 측정항목을 사용합니다. 이 NLG 작업을 위해 RAG는 실제로 접근합니다.

93
00:08:45,520 --> 00:08:49,760
최첨단. 그리고 이 모든 것은 최첨단 기술이 접근한다는 사실을 고려하고 있습니다.

94
00:08:49,760 --> 00:08:54,320
몇 가지 답을 결정하는 데 필요한 목표 구절에 접근할 수 있습니다.

95
00:08:54,880 --> 00:08:59,760
그리고 RAG가 액세스할 수 있는 Wikipedia 데이터만으로 모든 질문에 답할 수 있는 것은 아닙니다.

96
00:08:59,760 --> 00:09:04,640
이 실험에서는. 그럼에도 불구하고 우리는 다음과 같이 최첨단 기술에 접근하고 있습니다.

97
00:09:04,640 --> 00:09:11,600
여기에 표가 있으며 RAG는 지속적으로 BART를 능가합니다. 평가 중인 세 번째 작업

98
00:09:11,600 --> 00:09:16,880
위험 질문 생성입니다. 그렇다면 위험 질문이란 무엇입니까? 사실에 근거한 진술입니다

99
00:09:16,880 --> 00:09:23,760
정확한 답변을 요구하는 것입니다. 예를 들어, 1986년 멕시코는 최초로 개최국으로 기록되었습니다.

100
00:09:23,760 --> 00:09:28,560
이번 국제 스포츠 대회는 두 번이나 진행됐다. 답은 당연히 월드컵이다.

101
00:09:29,760 --> 00:09:35,440
RAG 모델의 임무는 답변을 조건으로 이러한 위험 질문을 생성하는 것입니다.

102
00:09:35,440 --> 00:09:41,280
엔터티. 따라서 미세 조정 설정은 실제로 다른 QA 설정에서 반전됩니다. 그래서

103
00:09:41,280 --> 00:09:47,440
대답, 우리는 질문을 예측하려고 노력하고 있습니다. 그래서 우리는 답변 질문 쌍을 가지고 있습니다. 검색 QA를 사용합니다.

104
00:09:47,440 --> 00:09:52,960
데이터 세트는 꽤 일반적인 QA 데이터 세트입니다. 기본 방법은 다시 한번 BART입니다.

105
00:09:52,960 --> 00:09:59,840
파란색과 파란색의 다른 변형이라는 두 가지 측정항목을 사용하고 있습니다. 그래서 우리는 그것을 다시 한 번 봅니다.

106
00:09:59,840 --> 00:10:05,600
이 작업에서 RAG는 매우 강력한 성능을 발휘합니다. RAG 토큰은 실제로 두 RAG 시퀀스보다 성능이 뛰어납니다.

107
00:10:05,600 --> 00:10:10,400
그리고 바트. 이는 아마도 RAG 토큰이 여러 다른 문서에서 가져올 수 있기 때문일 것입니다.

108
00:10:10,400 --> 00:10:14,800
물론 두 RAG 접근 방식 모두 두 가지를 모두 사용할 수 있다는 이점을 유지합니다.

109
00:10:14,800 --> 00:10:20,480
리트리버와 생성기가 이러한 질문을 완성합니다. 실제로는 별도로 운영하고 있는

110
00:10:20,480 --> 00:10:26,240
인간 평가자에게 RAG와 BART를 비교하도록 요청하는 실험입니다. 그리고 그들은 다음과 같은 사실을 발견했습니다.

111
00:10:26,240 --> 00:10:31,680
인간 평가자는 RAG를 선호합니다. 특히, 그들은 세대가 더 사실적이고

112
00:10:31,680 --> 00:10:38,240
그들은 더 구체적입니다. 이 실험의 마지막 과제는 사실 검증이다. 그리고 이건 뭐지

113
00:10:38,240 --> 00:10:44,480
이는 모델이 주장이 참인지, 거짓인지, 검증할 수 없는지 분류해야 함을 의미합니다. 그들은 실제로

114
00:10:44,480 --> 00:10:48,800
테스트하는 두 가지 접근 방식, 즉 3방향 분류와 양방향 분류가 있습니다. 에서

115
00:10:48,800 --> 00:10:54,640
3방향의 경우, 그들이 참조하는 레이블이나 클래스는 지지, 반박,

116
00:10:54,640 --> 00:11:00,880
또는 정보가 충분하지 않습니다. 양방향 분류는 단지 지지 또는 반박일 뿐입니다. 그들은 데이터 세트를 사용합니다

117
00:11:00,880 --> 00:11:06,320
FEVER라고 불리는 이는 주장과 이러한 주장에 대한 증거 주석의 데이터 세트입니다.

118
00:11:06,400 --> 00:11:12,720
위키피디아 데이터. 그들은 다음의 클레임과 클래스 쌍을 사용하여 미세 조정 샘플을 구성할 수 있습니다.

119
00:11:12,720 --> 00:11:18,160
이 데이터. 각 클래스는 고유한 출력 토큰에 매핑됩니다. 그러면 자연과 비슷하게

120
00:11:18,160 --> 00:11:22,640
언어 생성 작업에서 우리는 도전 과제를 만들기 위해 증거 주석을 무시합니다.

121
00:11:22,640 --> 00:11:28,000
RAG에게는 조금 더. 그들은 BART와 최첨단 성능이라는 두 가지 기준을 사용합니다.

122
00:11:28,560 --> 00:11:31,440
측정항목은 이 분류에 대한 라벨 정확도일 뿐입니다.

123
00:11:31,440 --> 00:11:38,320
이 접근 방식의 RAG 점수와 왼쪽 표는 실제로 꽤 좋습니다. 그들은

124
00:11:38,320 --> 00:11:42,080
최첨단에 접근하고 있지만 완전히 최첨단은 아니지만 몇 가지 지적 사항만 있습니다.

125
00:11:42,800 --> 00:11:46,960
최첨단 방법으로 접근할 수 있다는 점을 고려하면 이는 매우 인상적입니다.

126
00:11:46,960 --> 00:11:52,560
FEVER 데이터 세트의 금 증거 주석과 이러한 최첨단 방법은 복잡한

127
00:11:52,560 --> 00:11:57,840
감독을 통해 훈련된 파이프라인. 그럼에도 불구하고 RAG 점수는 최고 수준에 근접하고 있습니다.

128
00:11:57,840 --> 00:12:02,800
미술. 저자는 또한 RAG 문서 사이에 중복되는 부분이 많다는 점을 지적했습니다.

129
00:12:02,800 --> 00:12:08,400
이 금 증거물을 회수했습니다. 이로써 첫 번째 논문에 대한 논의가 끝났습니다.

130
00:12:08,400 --> 00:12:14,800
이제 Replug라는 두 번째 문서로 넘어갑니다. 이 문서의 전체 제목은 Replug입니다.

131
00:12:14,800 --> 00:12:20,160
2023년에 매우 최근에 발표된 증강 블랙박스 언어 모델 검색.

132
00:12:21,520 --> 00:12:26,240
이전 논문 및 이전 RAG 논문과 비교하여 본 논문의 주요 차이점은 다음과 같습니다.

133
00:12:26,240 --> 00:12:31,680
블랙박스 언어 모델에 중점을 두고 있습니다. 이 논문의 동기를 부여하기 위해 그들이 지적한 문제

134
00:12:31,680 --> 00:12:36,960
검색 기능을 갖춘 미세 조정 언어 모델에는 소위 화이트 박스 액세스가 필요하다는 것입니다.

135
00:12:36,960 --> 00:12:41,840
이는 언어 모델 매개변수를 미세 조정하려면 해당 매개변수에 액세스해야 함을 의미합니다.

136
00:12:42,960 --> 00:12:49,200
그리고 실제로 ChatGBT나 Gemini와 같은 인기 있는 많은 대규모 언어 모델은

137
00:12:49,200 --> 00:12:55,360
API를 통해 액세스할 수 있습니다. 그래서 우리는 그런 종류의 데이터를 얻을 수 없습니다. 우리는 확실히 미세 조정을 할 수 없습니다.

138
00:12:55,360 --> 00:13:02,320
많은 경우 모델 매개변수에 액세스합니다. 이것이 바로 Replug가 등장하는 곳입니다. Replug의 초점

139
00:13:02,320 --> 00:13:07,600
특히 블랙박스 설정에서 검색 기능이 강화됩니다. 블랙박스는 우리가

140
00:13:07,600 --> 00:13:13,280
모델 매개변수에 대한 액세스를 가정하지 않습니다. 따라서 언어 모델은 구체적으로

141
00:13:13,280 --> 00:13:18,240
블랙박스다. 리트리버는 블랙박스일 수도 있지만 선택적으로 조정 가능하며

142
00:13:18,240 --> 00:13:24,560
이러한 잠재적인 훈련 가능성을 해결하기 위한 다양한 방법을 논의합니다. 하지만 어떻게 작동하나요?

143
00:13:24,560 --> 00:13:30,240
먼저 Replug는 검색기로부터 문서를 가져온 다음 이 문서를

144
00:13:30,240 --> 00:13:33,680
원래 언어 모델을 입력한 다음 이를 언어 모델에 제공합니다.

145
00:13:35,840 --> 00:13:39,680
검색부터 시작하여 이 아키텍처의 구현을 살펴보겠습니다.

146
00:13:40,480 --> 00:13:47,200
m개의 문서로 구성된 코퍼스 d와 입력 컨텍스트 또는 쿼리 x가 제공됩니다. 의 출력

147
00:13:47,200 --> 00:13:52,960
검색 모델은 d 소수로 표시되는 x와 가장 관련성이 높은 d의 k 문서입니다.

148
00:13:52,960 --> 00:13:59,680
우리는 때때로 이것을 d의 상위 k 문서라고 부를 것입니다. 그러면 우리는 이것을 어떻게 결정합니까?

149
00:13:59,680 --> 00:14:05,120
서류? 음, 우리는 다음을 구성할 때 각 문서 d에 대한 임베딩을 미리 계산합니다.

150
00:14:05,120 --> 00:14:11,440
지난 논문에서와 마찬가지로 FICE 지수입니다. 임베딩을 얻기 위해 입력 x를 인코딩합니다.

151
00:14:12,000 --> 00:14:18,080
그런 다음 코사인 유사성 함수를 사용하여 x와 가장 유사한 상위 k개 문서를 찾습니다.

152
00:14:18,080 --> 00:14:24,160
s로 표시됩니다. 언어 모델 또는 생성기를 구현할 때 참조된 대로

153
00:14:24,160 --> 00:14:28,560
이전 논문에 약간의 질문이 있었습니다. 기억나시면 우리가 가겠다고 했잖아요.

154
00:14:28,560 --> 00:14:35,360
이 k개 문서를 입력 텍스트 x 앞에 추가합니다. 하지만 진짜 질문은, 이 모든 k가 과연 그럴 것인가 하는 것입니다.

155
00:14:35,360 --> 00:14:41,200
서류가 맞나요? 여기서 약간의 용어를 정의해 보겠습니다. 컨텍스트 창은 다음에 대한 입력 텍스트입니다.

156
00:14:41,200 --> 00:14:45,520
언어 모델이며 이 컨텍스트 창의 크기는 그 안에 들어갈 수 있는 토큰의 수입니다.

157
00:14:46,080 --> 00:14:50,880
그런데 문제는 고정된 크기의 컨텍스트 창이 k개의 문서에 맞지 않을 수 있다는 것입니다.

158
00:14:51,440 --> 00:14:56,080
컨텍스트 크기는 언어 모델에 따라 다릅니다. 각 문서의 크기는 다를 수 있으며,

159
00:14:56,640 --> 00:15:01,760
검색된 문서의 수 k 자체는 확실히 다를 수 있습니다. 그래서 우리는 정말로 모릅니다.

160
00:15:02,960 --> 00:15:06,720
이러한 불확실성을 해결하기 위해 저자는 다음과 같은 솔루션을 제안합니다.

161
00:15:06,720 --> 00:15:12,560
앙상블 추론 방식. 아이디어는 아주 간단합니다. k개의 별도 언어 모델을 수행해 보겠습니다.

162
00:15:12,560 --> 00:15:17,920
예측을 한 다음 마지막에 다시 합칩니다. 구체적으로 각 항목을 연결해 보겠습니다.

163
00:15:17,920 --> 00:15:24,560
상위 k개 문서 중 d는 x입니다. 그런 다음 k개의 연결이 있으면 별도의

164
00:15:24,560 --> 00:15:30,000
각각에 대해 추론을 호출하고 모든 출력 확률을 간단히 앙상블합니다. 이점은

165
00:15:30,000 --> 00:15:34,080
물론 추론 호출을 병렬화할 수 있다는 것입니다. 그리고 저자들은 다음과 같이 주장합니다.

166
00:15:34,080 --> 00:15:40,560
k개의 문서를 모두 포함하는 단일 추론에 비해 오버헤드가 최소화됩니다. 언어 모델,

167
00:15:40,640 --> 00:15:45,840
컨텍스트 창은 x와 k개의 문서 중 가장 큰 문서에 맞도록 충분히 커야 합니다.

168
00:15:46,880 --> 00:15:51,280
여기 하단에는 확률 분포에 대한 수학적 정의가 있습니다.

169
00:15:51,280 --> 00:15:55,920
이는 단순히 d와 x의 연결을 사용하는 생성기의 곱입니다.

170
00:15:55,920 --> 00:16:00,480
여기에 검색자를 나타내는 것으로 추정되는 d와 x 사이의 유사성 함수를 곱합니다.

171
00:16:03,360 --> 00:16:08,560
앞서 언급했듯이 replug는 선택적으로 검색기를 미세 조정할 수 있으며, 이 경우 변형이 가능합니다.

172
00:16:08,560 --> 00:16:14,400
replug lsr로 알려져 있습니다. 언어 모델은 검색을 감독합니다. 이것 뒤에 숨어있는 아이디어

173
00:16:14,400 --> 00:16:19,360
최적화는 언어 모델의 오류를 최소화하는 문서를 찾도록 검색기를 훈련시키는 것입니다.

174
00:16:19,360 --> 00:16:24,880
당황. 이에 대한 몇 가지 단계가 있습니다. 상위 k개 문서를 검색하는 것으로 시작합니다.

175
00:16:24,880 --> 00:16:31,840
d 소수. 그런 다음 각 문서의 검색 가능성을 계산합니다. 간단한 소프트맥스입니다

176
00:16:31,840 --> 00:16:37,600
해당 문서와 x 간의 유사성을 사용하는 함수입니다. 그런 다음 언어를 계산합니다.

177
00:16:37,600 --> 00:16:42,000
훈련 데이터 세트의 실제 정보를 사용하여 각 문서의 모델 가능성.

178
00:16:42,640 --> 00:16:47,520
이것은 언어의 확률 측면에서 매우 유사한 간단한 소프트맥스 함수입니다.

179
00:16:47,520 --> 00:16:54,320
모델. 마지막으로, 이 두 확률 분포 사이의 KL 차이를 최소화합니다.

180
00:16:54,320 --> 00:16:59,840
공동으로 정렬하기 위해. 물론 이 문서의 교육 구현은 다음과 같습니다.

181
00:16:59,840 --> 00:17:05,120
미세 조정 작업을 덜 수행하기 때문에 지난 논문보다 훨씬 간단합니다. 일반적인 재플러그 방식

182
00:17:05,120 --> 00:17:10,560
미세 조정이 필요하지 않으므로 기성품 검색 모델과 기성품을 사용할 수 있습니다.

183
00:17:10,560 --> 00:17:16,480
언어 모델. 이 논문에서는 Contriver 또는 Contriever와 GPT-3 카레를 사용하고 있습니다.

184
00:17:16,480 --> 00:17:22,720
언어 모델. lsr을 다시 연결하기 위해 더미 데이터세트에서 훈련 쿼리를 샘플링합니다.

185
00:17:22,720 --> 00:17:30,480
스크랩된 텍스트로 구성된 매우 큰 데이터 세트입니다. 이들 각각은 256개의 토큰이며 800,000개의 시퀀스를 갖습니다.

186
00:17:30,720 --> 00:17:37,200
쿼리는 입력 컨텍스트 x와 실제 연속 y로 분할됩니다. 이들 각각은

187
00:17:37,200 --> 00:17:44,000
128개 토큰, 이 x, y 쌍은 미세 조정 데이터 세트의 기초가 됩니다. 리트리버의 경우,

188
00:17:44,000 --> 00:17:50,240
동일한 데이터 세트인 더미에서 외부 문서를 샘플링합니다. 각각은 128개의 토큰이 됩니다.

189
00:17:50,240 --> 00:17:57,360
결과적으로 3,600만 개의 문서가 생성되었습니다. 저자는 중복되는 쿼리를 확인한다는 점에 유의합니다.

190
00:17:58,080 --> 00:18:02,480
및 문서를 확인하고, 중복되는 쿼리와 문서가 없는지 확인합니다.

191
00:18:04,160 --> 00:18:08,480
다시 한 번, 모든 것을 하나로 모으고 시각적으로 요약할 수 있는 멋진 다이어그램이 있습니다.

192
00:18:09,440 --> 00:18:14,640
왼쪽 하단에는 요청되는 쿼리의 예인 테스트 컨텍스트 x가 표시됩니다.

193
00:18:14,640 --> 00:18:21,040
이 경우 잡스는 블랭크의 CEO이다. 물론 대답은 Apple입니다.

194
00:18:21,040 --> 00:18:26,240
검색자는 Steve Jobs 및 Apple과 관련된 문서 몇 개를 찾은 다음 피드를 제공합니다.

195
00:18:26,240 --> 00:18:30,720
테스트 컨텍스트 x와 이러한 각 문서 d를 블랙박스 언어 모델로 변환합니다.

196
00:18:31,520 --> 00:18:35,280
그런 다음 오른쪽에는 앙상블 접근 방식이 표시됩니다.

197
00:18:35,280 --> 00:18:39,040
테스트 컨텍스트 x와 쌍을 이루고 언어 모델에 제공됩니다.

198
00:18:39,760 --> 00:18:44,240
그런 다음 이러한 출력 확률을 함께 앙상블하여 Apple의 답변을 얻습니다.

199
00:18:46,240 --> 00:18:50,160
이제 실험 섹션으로 이동하면 저자는 이 논문에서 세 가지 작업을 수행합니다.

200
00:18:50,160 --> 00:18:54,080
그들은 그들의 접근 방식을 평가하고 있습니다. 첫 번째는 일반 언어 모델링이며,

201
00:18:54,640 --> 00:18:59,040
대규모 멀티태스킹 언어 이해에 대한 성과가 이어졌습니다.

202
00:18:59,600 --> 00:19:05,200
줄여서 MMLU. 그 다음에는 오픈 도메인 질문 답변 작업이 이어집니다.

203
00:19:05,200 --> 00:19:11,040
이전 논문과 비슷하다. 이 첫 번째 작업에서 저자는 이해하려고 노력하고 있습니다.

204
00:19:11,040 --> 00:19:15,440
Replug가 언어 모델의 언어 모델링을 개선하는 방법. 그래서 그들이 측정하는 것은

205
00:19:15,440 --> 00:19:20,720
이를 평가하는 데 사용되는 것은 바이트당 비트수로 알려져 있으며 측정하는 데 매우 널리 사용되는 측정항목입니다.

206
00:19:20,720 --> 00:19:26,800
언어 모델링 성능. 그들은 파일 데이터세트를 다시 한 번 사용하고 있으며 설정을 위해

207
00:19:26,800 --> 00:19:34,320
Replug는 128개의 토큰 컨텍스트 창을 사용합니다. 검색 코퍼스는 3억 6,700만 개의 문서이며,

208
00:19:34,320 --> 00:19:40,320
각각은 더미에서 샘플링된 128개의 토큰입니다. 그러면 리트리버가 검색합니다.

209
00:19:40,320 --> 00:19:47,120
상위 10개 문서. Replug를 통합할 구체적인 기준은 다음과 같습니다.

210
00:19:47,120 --> 00:19:52,560
GPT 2가지 모델과 다양한 크기에 걸친 다수의 GPT 3가지 모델이 있습니다.

211
00:19:54,560 --> 00:19:58,720
이러한 각 모델에 대해 Replug를 추가하면 지속적으로 개선되는 것을 볼 수 있습니다.

212
00:19:58,720 --> 00:20:04,960
언어 모델링 작업. 파란색으로 강조 표시된 부분에는 다음을 추가하면 얻을 수 있는 이득 비율 열이 있습니다.

213
00:20:04,960 --> 00:20:10,720
미세 조정 없이 정기적으로 다시 연결하면 약 3~6% 정도 성능이 향상될 수 있습니다.

214
00:20:10,720 --> 00:20:15,520
그런 다음 Replug LSR을 추가하면 성능이 6~9% 향상됩니다.

215
00:20:15,520 --> 00:20:21,600
꽤 인상적인 언어 모델링 작업입니다. 이 두 번째 작업에 저자가 관심을 갖고 있습니다.

216
00:20:21,600 --> 00:20:28,400
Replug가 MMLU 벤치마크에 대한 성능 이점을 가지고 있는지 이해합니다. MMLU는

217
00:20:28,400 --> 00:20:34,000
다시 한번 Massive Multitask Language Understanding의 약자이며 객관식 데이터 세트입니다.

218
00:20:34,000 --> 00:20:40,000
질문. 이 질문은 57가지 작업과 4가지 카테고리에 걸쳐 있습니다.

219
00:20:40,000 --> 00:20:47,360
인문학, STEM, 사회과학 등. 이 설정에서는 언어 모델인 Codex를 사용하고 있습니다.

220
00:20:47,360 --> 00:20:51,200
여기에 Replug를 추가하고 5회 설정에서 모든 것을 평가합니다.

221
00:20:52,000 --> 00:20:59,520
각 MMLU 테스트 쿼리에 대해 10k를 검색하여 10개의 Wikipedia 문서와 동일하게 앞에 추가합니다.

222
00:20:59,520 --> 00:21:05,760
2018년 12월 덤프에서 추출한 다음 출력을 앙상블합니다. 이 비교의 기준은 다음과 같습니다.

223
00:21:05,760 --> 00:21:11,200
Codex, Palm, Flan Palm 등 MMLU를 위한 최첨단 언어 모델,

224
00:21:11,920 --> 00:21:18,480
RAG LLM 접근 방식, 특히 Atlas도 마찬가지입니다. 이 특정 작업에 대한 측정항목

225
00:21:18,480 --> 00:21:25,440
단순히 이러한 질문의 정확성입니다. 이 결과에서 우리는 지속적인 개선을 확인했습니다

226
00:21:25,440 --> 00:21:31,120
Replug를 통합하여 간단히 Codex를 사용할 수 있습니다. 파란색으로 강조 표시된 맨 위 행에는 성능이 표시됩니다.

227
00:21:31,120 --> 00:21:37,040
Codex 자체적으로. 그런 다음 파란색으로 강조 표시된 맨 아래 두 행에 Replug 기능이 있는 Codex가 표시됩니다.

228
00:21:37,040 --> 00:21:43,680
통합. 통합되는 Replug와 Replug LSR의 차이는 다소 미미합니다.

229
00:21:43,680 --> 00:21:48,640
그러나 일반적으로 Replug를 통합하면 Codex의 성능을 여러 지점으로 향상시킬 수 있습니다.

230
00:21:50,880 --> 00:21:54,800
이 마지막 작업에서 저자는 오픈 도메인 질문 답변을 테스트하고 있습니다.

231
00:21:54,800 --> 00:22:00,080
지난번 논문처럼요. 따라서 과제는 다시 객관적이고 사실적인 질문에 답하는 것입니다.

232
00:22:00,960 --> 00:22:05,280
이번에는 자연 질문과 퀴즈 QB라는 두 가지 데이터 세트만 보고 있습니다.

233
00:22:06,240 --> 00:22:10,240
설정은 이전 작업과 매우 유사합니다. Codex 언어 모델을 사용하고 있습니다.

234
00:22:10,240 --> 00:22:15,680
Replug를 통합하고 이를 16샷 설정에서 평가합니다. 그들은 Wikipedia를 다음 용도로 사용하고 있습니다.

235
00:22:15,680 --> 00:22:22,400
상위 10개 문서를 검색하고 검색합니다. 기준으로는 Chinchilla, Palm,

236
00:22:22,400 --> 00:22:30,000
그리고 그냥 평범한 코덱스. 그리고 Retro, R2D2, Atlas와 같은 몇 가지 RAG LLM 접근 방식을 사용하고 있습니다.

237
00:22:30,240 --> 00:22:37,040
다시 한 번 Replug를 Codex에 통합하면 상당한 성능을 얻을 수 있음을 알 수 있습니다.

238
00:22:37,040 --> 00:22:42,480
개선 사항은 대략 4~5점 정도입니다. 일반 언어 모델과 비교했을 때 심지어

239
00:22:42,480 --> 00:22:47,280
다른 RAG 지원 모델의 경우 Codex와 Replug가 적어도 부문에서는 모든 모델을 능가한다는 것을 알 수 있습니다.

240
00:22:47,280 --> 00:22:54,080
여기에서 평가되고 있는 몇 장의 설정입니다. 이것으로 이 두 가지에 대한 발표를 마치겠습니다.

241
00:22:54,080 --> 00:22:58,800
증강세대 논문 검색. 주목 해 주셔서 감사합니다. 만약 질문이 있다면,

242
00:22:58,800 --> 00:23:01,600
화면에 표시된 이메일로 언제든지 문의해 주세요.