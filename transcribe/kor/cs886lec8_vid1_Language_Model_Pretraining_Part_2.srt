1
00:00:00,000 --> 00:00:03,160
양방향 인코더를 나타냅니다.

2
00:00:03,160 --> 00:00:05,700
변압기에 대한 표현.

3
00:00:05,700 --> 00:00:09,640
Vert는 Google이 2018년에 개발한 언어 모델입니다.

4
00:00:09,640 --> 00:00:12,760
가장 중요한 혁신 중 하나로 간주됩니다.

5
00:00:12,760 --> 00:00:15,280
자연어 처리 분야에서요.

6
00:00:15,280 --> 00:00:18,080
새로운 접근 방식을 도입하여 패러다임 전환을 일으켰습니다.

7
00:00:18,080 --> 00:00:21,200
언어 이해 과제를 다루기 위해,

8
00:00:21,200 --> 00:00:23,840
텍스트 데이터의 대용량 사본을 표현하고,

9
00:00:23,840 --> 00:00:26,880
그런 다음 특정 작업에 맞게 조정하고,

10
00:00:26,880 --> 00:00:30,520
질의 응답, 퇴적물 분석 등.

11
00:00:30,520 --> 00:00:33,720
이 접근 방식은 매우 효과적인 것으로 입증되었습니다.

12
00:00:33,720 --> 00:00:35,000
그리고 기초가 되었어요

13
00:00:35,000 --> 00:00:39,120
LP의 이후 많은 발전을 위해.

14
00:00:39,120 --> 00:00:41,640
다음은 Bert의 사전 훈련을 보여주는 그림입니다.

15
00:00:41,640 --> 00:00:43,280
그리고 펀튜닝 과정,

16
00:00:43,280 --> 00:00:46,480
이에 대해서는 다음 슬라이드에서 자세히 설명하겠습니다.

17
00:00:48,440 --> 00:00:52,280
그래서 Bert는 트랜스포머 아키텍처를 기반으로 구축되었습니다.

18
00:00:52,280 --> 00:00:54,000
신경망의 일종이다

19
00:00:54,000 --> 00:00:58,200
이는 후속 시퀀스 작업에서 큰 가능성을 보여주었습니다.

20
00:00:58,200 --> 00:01:01,440
우리는 이미 변환기 아키텍처를 다루었으므로

21
00:01:01,440 --> 00:01:03,120
지난 강의에서,

22
00:01:03,120 --> 00:01:05,880
주요 구성 요소를 간략하게 요약하겠습니다.

23
00:01:05,880 --> 00:01:09,920
따라서 변환기는 인코더와 디코더로 구성되며,

24
00:01:09,920 --> 00:01:13,240
self-attention의 여러 층으로 구성됨

25
00:01:13,240 --> 00:01:15,400
그리고 피드포워드 신경망.

26
00:01:15,400 --> 00:01:17,800
인코더는 입력 시퀀스를 처리합니다.

27
00:01:17,800 --> 00:01:20,880
일련의 숨겨진 표현을 생성합니다.

28
00:01:20,880 --> 00:01:23,240
디코더가 출력 시퀀스를 생성하는 동안

29
00:01:23,240 --> 00:01:26,080
인코더의 표현을 기반으로,

30
00:01:26,080 --> 00:01:28,840
변압기의 초기 성공 이후

31
00:01:28,840 --> 00:01:32,080
기계 번역 작업에서는

32
00:01:32,080 --> 00:01:35,640
연구원들은 변환기의 디코더 부분을 사용하여 탐구했습니다.

33
00:01:35,640 --> 00:01:38,520
독립형 언어 모델로.

34
00:01:38,520 --> 00:01:41,120
그래서 이것이 모델의 개발로 이어졌습니다.

35
00:01:41,120 --> 00:01:46,120
OpenAI의 GBT1과 같은 생성적 사전 훈련된 변환기는

36
00:01:48,240 --> 00:01:50,240
효과를 보여주는 것

37
00:01:50,240 --> 00:01:54,080
언어 모델에 변환기 디코더를 사용하는 방법.

38
00:01:54,080 --> 00:01:56,840
그러나 GBT1은 단방향이었습니다.

39
00:01:58,440 --> 00:02:01,160
이는 이전 토큰에만 주의를 기울일 수 있음을 의미합니다.

40
00:02:01,160 --> 00:02:02,360
순서대로.

41
00:02:02,360 --> 00:02:06,120
이러한 선택은 다음 세계 예측에 있어 자연스러운 일입니다.

42
00:02:06,120 --> 00:02:08,200
그러나 그것은 또한 한계가 있습니다.

43
00:02:08,200 --> 00:02:11,160
따라서 다음 슬라이드에서 이에 대해 더 자세히 논의하겠습니다.

44
00:02:16,240 --> 00:02:17,960
그럼 단방향을 비교해 보겠습니다.

45
00:02:17,960 --> 00:02:20,800
양방향 언어 모델.

46
00:02:20,800 --> 00:02:23,640
따라서 GBT1과 같은 단방향 모델

47
00:02:23,640 --> 00:02:26,520
단어 표현을 점진적으로 구축

48
00:02:26,520 --> 00:02:30,240
순서대로 앞에 나온 단어를 기반으로 합니다.

49
00:02:30,240 --> 00:02:32,720
이는 각 단어의 표현을 의미합니다.

50
00:02:32,720 --> 00:02:36,320
앞에 나온 단어에만 영향을 받습니다.

51
00:02:36,320 --> 00:02:38,840
이 접근 방식은 일부 작업에는 적합하지만

52
00:02:38,840 --> 00:02:40,480
전체 맥락을 포착하지 못함

53
00:02:40,480 --> 00:02:42,840
문장 속의 단어.

54
00:02:42,840 --> 00:02:44,320
예를 들어, 다음 문장을 생각해 보세요.

55
00:02:44,320 --> 00:02:47,360
나는 돈을 입금하기 위해 은행에 갔다.

56
00:02:47,360 --> 00:02:50,240
와드 은행은 금융 기관을 의미할 수 있습니다.

57
00:02:50,240 --> 00:02:51,600
아니면 강변.

58
00:02:51,600 --> 00:02:53,640
그리고 돈은 상황에 따라 결정됩니다

59
00:02:53,640 --> 00:02:56,400
필요한 작업에 대한 전체 문장 중

60
00:02:56,400 --> 00:02:57,880
단어의 의미를 이해하기

61
00:02:57,880 --> 00:02:59,880
전체 문장의 맥락에서,

62
00:02:59,880 --> 00:03:02,680
질문 응답이나 자연어 추론과 같은 것입니다.

63
00:03:03,600 --> 00:03:06,120
단방향 모델은 어려움을 겪을 수 있습니다.

64
00:03:06,120 --> 00:03:10,040
이것이 바로 BERT와 같은 양방향 모델이 등장하는 곳입니다.

65
00:03:10,040 --> 00:03:13,040
따라서 이러한 모델의 경우 각 단어의 표현은

66
00:03:13,040 --> 00:03:17,080
전체 입력 순서를 고려하여 구축됩니다.

67
00:03:17,080 --> 00:03:21,040
대상 단어 앞과 뒤의 단어 모두입니다.

68
00:03:22,480 --> 00:03:25,560
이를 통해 모델은 더 풍부한 컨텍스트를 포착할 수 있습니다.

69
00:03:25,560 --> 00:03:28,440
각 단어마다 유익할 수 있습니다.

70
00:03:28,440 --> 00:03:30,560
광범위한 NLP 작업을 위한 것입니다.

71
00:03:30,560 --> 00:03:33,960
그러나 양방향 언어 모델 구축

72
00:03:33,960 --> 00:03:36,200
그 자체로 일련의 과제가 있습니다.

73
00:03:36,200 --> 00:03:38,120
이에 대해서는 다음 슬라이드에서 논의하겠습니다.

74
00:03:41,440 --> 00:03:44,000
그래서 앞서 논의한 ELMO는

75
00:03:44,000 --> 00:03:46,560
첫 번째 성공적인 시도 중 하나였습니다.

76
00:03:46,560 --> 00:03:49,200
양방향 언어 모델을 구축할 때.

77
00:03:49,200 --> 00:03:51,800
그러나 그 표현을 종합해 보면

78
00:03:51,800 --> 00:03:54,920
서로 다른 방향에서 고립된 방식으로

79
00:03:54,920 --> 00:03:58,080
복잡한 종속성을 포착하는 능력이 제한되었습니다.

80
00:03:58,080 --> 00:03:59,200
단어 사이.

81
00:03:59,200 --> 00:04:01,720
BERT는 이러한 제한 사항을 해결했습니다.

82
00:04:01,720 --> 00:04:05,360
LSTM을 변환기 아키텍처로 대체합니다.

83
00:04:05,360 --> 00:04:09,200
특히 BERT는 다층 변압기 인코더를 사용합니다.

84
00:04:09,200 --> 00:04:11,720
심층적인 양방향 언어 모델을 구축하기 위해,

85
00:04:11,720 --> 00:04:14,080
각 단어의 맥락을 포착하는

86
00:04:14,080 --> 00:04:17,680
입력 순서의 다른 모든 단어를 고려하여.

87
00:04:17,680 --> 00:04:20,240
이 아키텍처를 통해 BERT는 다음을 생성할 수 있습니다.

88
00:04:20,240 --> 00:04:22,960
풍부한 문맥화된 단어 표현,

89
00:04:22,960 --> 00:04:25,760
이는 광범위한 NLP 작업에 사용될 수 있습니다.

90
00:04:28,680 --> 00:04:32,880
BERT 입력 표현은 세 가지 주요 구성 요소로 구성됩니다.

91
00:04:32,880 --> 00:04:35,120
토큰 임베딩, 세그먼트 임베딩,

92
00:04:35,120 --> 00:04:36,680
및 위치 임베딩.

93
00:04:36,680 --> 00:04:38,480
토큰 임베딩은 단어 임베딩입니다.

94
00:04:38,480 --> 00:04:40,680
입력 시퀀스의 각 토큰에 대해.

95
00:04:40,680 --> 00:04:42,200
BERT는 단어 조각 임베딩을 사용합니다.

96
00:04:42,200 --> 00:04:45,480
어휘 크기는 30,000개 토큰입니다.

97
00:04:45,480 --> 00:04:47,640
하위 단어 토큰화 방법입니다.

98
00:04:47,640 --> 00:04:50,800
모델이 어휘 단어를 처리할 수 있게 해줍니다.

99
00:04:50,800 --> 00:04:54,880
단어의 형태학적 변화를 포착합니다.

100
00:04:54,880 --> 00:04:57,680
입력 시퀀스는 특수 토큰으로도 강화됩니다.

101
00:04:57,680 --> 00:05:00,840
CRS, SAP 등

102
00:05:00,840 --> 00:05:04,160
시작을 나타내는 데 사용되는 것

103
00:05:04,160 --> 00:05:07,480
그리고 문장의 분리.

104
00:05:07,480 --> 00:05:11,160
그래서 CRS 토큰이 특히 중요합니다.

105
00:05:11,160 --> 00:05:13,960
고정된 크기 표현을 생성하는 데 사용되기 때문입니다.

106
00:05:13,960 --> 00:05:15,760
전체 입력 시퀀스 중

107
00:05:15,760 --> 00:05:19,080
이는 다운스트림 작업에 대한 입력으로 사용될 수 있습니다.

108
00:05:20,440 --> 00:05:23,000
세그먼트 임베딩은 구별하는 데 사용됩니다.

109
00:05:23,000 --> 00:05:26,480
입력 시퀀스의 서로 다른 문장 사이.

110
00:05:26,480 --> 00:05:28,960
이를 통해 BERT는 작업을 처리할 수 있습니다.

111
00:05:28,960 --> 00:05:31,280
여러 문장이 포함된

112
00:05:31,280 --> 00:05:33,160
자연어 추론과 같은 것입니다.

113
00:05:35,120 --> 00:05:37,960
마지막으로 위치 임베딩이 추가되었습니다.

114
00:05:37,960 --> 00:05:40,200
위치를 포착하기 위해 토큰 임베딩에

115
00:05:40,200 --> 00:05:42,480
입력 시퀀스의 각 토큰.

116
00:05:42,480 --> 00:05:44,560
기존의 포지션 임베딩과는 다르게,

117
00:05:44,560 --> 00:05:46,720
BERT는 학습된 위치 임베딩을 사용합니다.

118
00:05:46,720 --> 00:05:50,280
이는 사전 훈련 과정에서 최적화됩니다.

119
00:05:52,520 --> 00:05:56,080
따라서 BERT는 BERT-BASE와 BERT-LARGE의 두 가지 크기로 제공됩니다.

120
00:05:56,080 --> 00:05:58,760
BERT-BASE에는 12개의 변압기 레이어가 있습니다.

121
00:05:58,760 --> 00:06:01,520
768개의 히든 유닛,

122
00:06:01,520 --> 00:06:04,120
12개의 셀프 어텐션 헤드,

123
00:06:04,120 --> 00:06:09,120
총 1억 1천만 개의 매개변수가 생성됩니다.

124
00:06:10,800 --> 00:06:15,800
반면 BERT-LARGE에는 24개의 변압기 레이어가 있습니다.

125
00:06:17,160 --> 00:06:21,680
1024개의 히든 유닛과 16개의 셀프 어텐션 헤드

126
00:06:21,680 --> 00:06:26,280
결과적으로 3억 4천만 개의 매개변수가 생성됩니다.

127
00:06:26,280 --> 00:06:29,520
더 큰 모델이 달성되는 것으로 나타났습니다.

128
00:06:30,560 --> 00:06:33,160
다양한 NLP 작업에서 더 나은 성능,

129
00:06:33,160 --> 00:06:36,400
하지만 더 많은 계산 리소스가 필요합니다.

130
00:06:36,400 --> 00:06:38,400
그리고 더 긴 훈련 시간.

131
00:06:38,400 --> 00:06:41,280
2018년에는 BERT-LARGE가 고려되었습니다.

132
00:06:41,280 --> 00:06:43,760
해당 분야에서 가장 큰 모델 중 하나입니다.

133
00:06:43,760 --> 00:06:46,800
그러나 언어 모델의 지속적인 확장으로 인해

134
00:06:46,800 --> 00:06:50,080
우리는 훨씬 더 큰 모델이 개발되는 것을 보았습니다.

135
00:06:50,080 --> 00:06:55,080
1,750억 개의 매개변수가 있는 GPT-3와 같은

136
00:06:55,120 --> 00:06:59,000
BERT-LARGE보다 약 500배 더 많습니다.

137
00:06:59,000 --> 00:07:04,000
두 모델 모두 대용량 텍스트 데이터 사본에 대해 사전 학습되어 있습니다.

138
00:07:04,240 --> 00:07:07,000
Wikipedia 및 책 표지와 같은

139
00:07:07,000 --> 00:07:10,320
두 가지 비지도 학습 목표를 사용하여

140
00:07:10,320 --> 00:07:13,880
대량 언어 모델링 및 다음 문장 예측.

141
00:07:18,200 --> 00:07:21,200
따라서 언어 모델링에 대한 전통적인 접근 방식은

142
00:07:21,200 --> 00:07:23,160
다음 단어 순서를 예측하는 것입니다

143
00:07:23,160 --> 00:07:25,360
이전 단어가 주어졌습니다.

144
00:07:25,360 --> 00:07:27,040
그렇다면 이 접근 방식이 효과적인 이유는 무엇입니까?

145
00:07:27,040 --> 00:07:29,240
단방향 언어 모델의 경우?

146
00:07:29,240 --> 00:07:32,880
BERT와 같은 양방향 모델에는 적합하지 않습니다.

147
00:07:32,880 --> 00:07:36,200
하지만 미래의 전쟁을 조건으로 하기 때문에

148
00:07:36,600 --> 00:07:39,600
사전 훈련 중에 데이터 유출이 발생할 수 있습니다.

149
00:07:39,600 --> 00:07:41,280
이 문제를 해결하기 위해,

150
00:07:41,280 --> 00:07:43,640
BERT는 새로운 사전 훈련 목표를 도입합니다

151
00:07:43,640 --> 00:07:46,960
대량 언어 모델링(MLM)이라고 합니다.

152
00:07:46,960 --> 00:07:50,600
따라서 MLM에서는 입력 토큰의 무작위 하위 집합이 집단화됩니다.

153
00:07:50,600 --> 00:07:53,080
모델은 원래 단어를 예측하려고 합니다.

154
00:07:53,080 --> 00:07:54,840
상황에 따라.

155
00:07:54,840 --> 00:07:58,160
이를 통해 BERT는 양방향 컨텍스트를 캡처할 수 있습니다.

156
00:07:58,160 --> 00:07:59,480
사전 훈련을 하는 동안,

157
00:07:59,480 --> 00:08:02,040
하지만 모델은 대량의 단어를 예측하는 방법을 학습해야 합니다.

158
00:08:02,040 --> 00:08:05,400
이전 단어와 다음 단어를 모두 기반으로 합니다.

159
00:08:05,440 --> 00:08:07,800
원래 BERT 논문에서는

160
00:08:07,800 --> 00:08:12,160
토큰의 약 15%가 사전 훈련 중에 대량으로 생성됩니다.

161
00:08:12,160 --> 00:08:14,640
대량 토큰이 입력에 표시되지 않기 때문에

162
00:08:14,640 --> 00:08:15,480
미세 조정하는 동안,

163
00:08:15,480 --> 00:08:17,640
사전 훈련 간의 불일치가 발생할 수 있습니다.

164
00:08:17,640 --> 00:08:18,640
미세 조정하고,

165
00:08:18,640 --> 00:08:20,960
BERT는 다음 전략을 사용합니다

166
00:08:20,960 --> 00:08:22,840
대량 토큰을 처리합니다.

167
00:08:24,480 --> 00:08:29,400
80%의 경우 토큰은 대량 토큰으로 대체됩니다.

168
00:08:29,400 --> 00:08:33,000
10%의 경우 토큰은 무작위 토큰으로 대체됩니다.

169
00:08:33,000 --> 00:08:36,120
10%의 경우 토큰은 변경되지 않은 채로 남아 있습니다.

170
00:08:36,120 --> 00:08:38,640
따라서 이 전략은 모델을 장려했습니다.

171
00:08:38,640 --> 00:08:42,240
대량 토큰의 강력한 표현을 학습하기 위해,

172
00:08:42,240 --> 00:08:44,800
이는 다운스트림 작업에 도움이 될 수 있습니다.

173
00:08:48,040 --> 00:08:50,280
대량 언어 모델링 외에도

174
00:08:50,280 --> 00:08:53,720
BERT는 다음 문장 예측도 사용합니다.

175
00:08:53,720 --> 00:08:57,160
사전 훈련 중 NISP 목표.

176
00:08:57,160 --> 00:08:59,680
NISP에서 모델은 예측을 시도합니다.

177
00:08:59,680 --> 00:09:02,400
한 쌍의 문장이 연속되는지 여부

178
00:09:02,400 --> 00:09:05,440
원본 문서에 있든 없든.

179
00:09:05,440 --> 00:09:07,080
사전 훈련 중에,

180
00:09:07,080 --> 00:09:10,080
학습 예시의 50%는 긍정적 예시입니다.

181
00:09:10,080 --> 00:09:12,280
두 번째 문장이 첫 번째 문장 뒤에 오는 동안

182
00:09:12,280 --> 00:09:13,720
원본 문서에서는

183
00:09:13,720 --> 00:09:16,120
50%는 부정적인 예입니다.

184
00:09:16,120 --> 00:09:18,640
두 번째 문장이 쉽게 샘플링되는 곳

185
00:09:18,640 --> 00:09:20,320
구리를 위해.

186
00:09:20,320 --> 00:09:22,400
이 목표는 모델을 장려합니다

187
00:09:22,400 --> 00:09:24,960
문장 사이의 관계를 포착하기 위해,

188
00:09:24,960 --> 00:09:26,960
이는 다운스트림 작업에 도움이 될 수 있습니다.

189
00:09:26,960 --> 00:09:30,280
질문 변경, 자연어 추론 등

190
00:09:30,280 --> 00:09:31,880
관계를 이해하는 곳

191
00:09:31,880 --> 00:09:33,560
문장 사이가 중요합니다.

192
00:09:37,200 --> 00:09:40,520
따라서 일단 사전 훈련되면 BERT를 미세 조정할 수 있습니다.

193
00:09:40,520 --> 00:09:43,720
지도 학습을 사용하여 특정 작업을 수행합니다.

194
00:09:43,720 --> 00:09:46,600
미세 조정 프로세스에는 추가 작업이 포함됩니다.

195
00:09:46,600 --> 00:09:48,440
작업별 출력 레이어

196
00:09:48,440 --> 00:09:50,600
사전 훈련된 BERT 모델 위에

197
00:09:50,600 --> 00:09:52,760
전체 모델을 미세 조정하고

198
00:09:52,760 --> 00:09:56,080
대상 작업에 대한 레이블 데이터 세트에 대해

199
00:09:56,080 --> 00:09:59,480
작업별 출력 레이어는 단일 레이어일 수 있습니다.

200
00:09:59,480 --> 00:10:00,840
또는 여러 레이어,

201
00:10:00,840 --> 00:10:03,840
작업의 복잡성에 따라 다릅니다.

202
00:10:03,840 --> 00:10:05,400
원래 BERT 논문에서는

203
00:10:05,400 --> 00:10:07,840
저자는 효율성을 입증합니다.

204
00:10:07,840 --> 00:10:10,800
광범위한 NLP 작업에 대한 BERT 미세 조정,

205
00:10:10,800 --> 00:10:13,480
문장 병렬 분류와 같은

206
00:10:13,480 --> 00:10:15,200
단일 문장 분류,

207
00:10:15,200 --> 00:10:17,560
질문 변경 및 시퀀스 태깅.

208
00:10:17,560 --> 00:10:18,720
토큰 수준 작업,

209
00:10:18,720 --> 00:10:21,880
질문 변경 및 시퀀스 태깅과 같은

210
00:10:21,880 --> 00:10:25,360
BERT의 토큰 수준 표현을 사용합니다.

211
00:10:25,360 --> 00:10:26,960
넓은 문장 수준의 작업,

212
00:10:26,960 --> 00:10:28,720
문장 병렬 분류와 같은

213
00:10:28,720 --> 00:10:31,680
CRS 토큰 표현 사용

214
00:10:31,680 --> 00:10:34,240
작업별 출력 레이어에 대한 입력으로 사용됩니다.

215
00:10:36,760 --> 00:10:38,920
그래서 원본 BERT 논문은 보고했습니다.

216
00:10:38,920 --> 00:10:41,000
광범위한 분야에서 최첨단 결과를

217
00:10:41,000 --> 00:10:43,760
GRU 벤치마크를 포함한 NLP 벤치마크 중

218
00:10:43,760 --> 00:10:45,480
9개의 서로 다른 작업으로 구성되어 있으며,

219
00:10:45,480 --> 00:10:48,600
질문 변경, 자연어 추론,

220
00:10:48,600 --> 00:10:50,600
그리고 감정 분석.

221
00:10:50,600 --> 00:10:52,920
BERT는 상당한 개선을 달성했습니다.

222
00:10:52,920 --> 00:10:55,680
대부분의 작업에 대한 이전 모델에 비해.

223
00:10:55,680 --> 00:10:57,560
특히, BERT 대규모 아웃폼

224
00:10:57,560 --> 00:11:00,120
GRU 벤치마크의 다른 모든 모델,

225
00:11:00,120 --> 00:11:02,120
효과를 입증하는

226
00:11:02,120 --> 00:11:04,960
NLP 작업을 위한 대규모 사전 훈련.

227
00:11:04,960 --> 00:11:07,840
BERT의 성공은 근본적으로 바뀌었습니다

228
00:11:07,840 --> 00:11:10,640
NLP 연구의 풍경

229
00:11:10,640 --> 00:11:13,520
다른 많은 언어 모델의 개발로 이어졌습니다.

230
00:11:13,520 --> 00:11:16,000
BERT 아키텍처를 기반으로 구축되었습니다.

231
00:11:17,000 --> 00:11:19,760
따라서 BERT의 성공에 따라

232
00:11:19,760 --> 00:11:22,760
여러 가지 변형이 개발되었습니다

233
00:11:22,760 --> 00:11:26,160
한계를 해결하고 성능을 향상시킵니다.

234
00:11:26,160 --> 00:11:29,600
여기 BERT 가계도에 대한 그림이 있습니다.

235
00:11:29,600 --> 00:11:33,400
이는 BERT와 그 변형의 진화를 보여줍니다.

236
00:11:33,400 --> 00:11:34,720
다음 몇 개의 슬라이드에서는

237
00:11:34,720 --> 00:11:38,760
가장 주목할만한 BERT 변형에 대해 논의하겠습니다.

238
00:11:38,760 --> 00:11:41,360
로베르타, 앨버트를 비롯해

239
00:11:41,360 --> 00:11:43,360
엘렉트라와 어니.

240
00:11:44,360 --> 00:11:47,360
로버타(Roberta)는 견고하게 최적화된 모델을 의미합니다.

241
00:11:47,360 --> 00:11:49,360
BERT 사전 훈련 접근법,

242
00:11:49,360 --> 00:11:52,360
Facebook AI가 개발한 BERT의 변형입니다.

243
00:11:52,360 --> 00:11:55,360
가장 널리 사용되는 BERT 변종 중 하나입니다.

244
00:11:55,360 --> 00:11:57,920
그리고 최고 수준의 성과를 달성했습니다.

245
00:11:57,920 --> 00:11:59,920
많은 NLP 벤치마크에서

246
00:11:59,920 --> 00:12:01,920
Roberta는 BERT 아키텍처를 기반으로 구축되었습니다.

247
00:12:01,920 --> 00:12:04,920
디자인에 몇 가지 수정 사항을 도입합니다.

248
00:12:04,920 --> 00:12:06,920
BERT의 사전 훈련 과정.

249
00:12:06,920 --> 00:12:08,920
또한 BERT 아키텍처는

250
00:12:09,480 --> 00:12:11,480
디자인에 여러 가지 수정 사항을 적용합니다.

251
00:12:11,480 --> 00:12:13,480
그리고 BERT의 사전 훈련 과정.

252
00:12:13,480 --> 00:12:16,480
또한 새로운 사전 훈련 데이터 세트를 도입했습니다.

253
00:12:16,480 --> 00:12:18,480
CCNEWS라고 불리는데,

254
00:12:18,480 --> 00:12:20,480
6300만개로 구성

255
00:12:20,480 --> 00:12:23,480
웹에서 자랑스러운 영어 뉴스 기사.

256
00:12:23,480 --> 00:12:27,480
Roberta의 저자는 광범위한 연구를 수행했습니다.

257
00:12:27,480 --> 00:12:30,480
다양한 디자인 선택의 영향을 확인하기 위해

258
00:12:30,480 --> 00:12:33,480
BERT 성능에 대한 사전 훈련 데이터입니다.

259
00:12:33,480 --> 00:12:37,480
그들은 사전 훈련 과정을 최적화함으로써

260
00:12:38,040 --> 00:12:41,040
더 큰 데이터 세트에 대한 교육,

261
00:12:41,040 --> 00:12:43,040
상당한 개선을 이룰 수 있었습니다

262
00:12:43,040 --> 00:12:45,040
원래 BERT 모델에 비해

263
00:12:45,040 --> 00:12:49,040
다음은 Roberta에 도입된 주요 수정 사항 중 일부입니다.

264
00:12:49,040 --> 00:12:52,040
그 중 몇 가지에 대해 더 자세히 논의하겠습니다.

265
00:12:52,040 --> 00:12:54,040
다음 슬라이드에서 확인해보세요.

266
00:12:58,040 --> 00:13:01,040
Roberta와 BERT의 주요 차이점 중 하나

267
00:13:01,040 --> 00:13:04,040
마스크된 언어 모델링을 처리하는 방식입니다.

268
00:13:04,040 --> 00:13:06,040
사전 훈련 중.

269
00:13:06,600 --> 00:13:08,600
즉, 마스킹이 수행됩니다.

270
00:13:08,600 --> 00:13:10,600
데이터 전처리 단계에서

271
00:13:10,600 --> 00:13:13,600
그 결과 고정된 마스킹된 토큰 세트가 생성됩니다.

272
00:13:13,600 --> 00:13:15,600
각 훈련 에포크마다.

273
00:13:15,600 --> 00:13:17,600
동일한 마스킹된 토큰을 사용하지 않으려면

274
00:13:17,600 --> 00:13:19,600
각 훈련 에포크마다

275
00:13:19,600 --> 00:13:24,600
BERT는 입력 데이터를 임시로 10번 복제합니다.

276
00:13:24,600 --> 00:13:27,600
각 복제본에 서로 다른 토큰을 마스크합니다.

277
00:13:27,600 --> 00:13:30,600
대조적으로 Roberta는 동적 마스킹을 사용합니다.

278
00:13:30,600 --> 00:13:34,600
매번 마스킹이 생성되는 곳

279
00:13:34,600 --> 00:13:37,600
사전 훈련 중에 입력이 모델에 공급됩니다.

280
00:13:37,600 --> 00:13:40,600
이를 통해 Roberta는 다양한 마스크 토큰을 사용할 수 있습니다.

281
00:13:40,600 --> 00:13:42,600
각 훈련 에포크마다

282
00:13:42,600 --> 00:13:46,600
이는 강력한 표현을 학습하는 데 도움이 될 수 있습니다.

283
00:13:46,600 --> 00:13:48,600
마스크된 토큰의

284
00:13:48,600 --> 00:13:51,600
Roberta의 저자는 동적 마스킹을 발견했습니다.

285
00:13:51,600 --> 00:13:55,600
정적 마스킹에 비해 성능이 향상되었습니다.

286
00:13:56,600 --> 00:14:01,600
여러 연구에서 효과에 의문을 제기했습니다.

287
00:14:01,600 --> 00:14:04,600
BERT의 다음 문장 예측 목표.

288
00:14:04,600 --> 00:14:08,600
NSP의 영향을 조사하기 위해,

289
00:14:08,600 --> 00:14:10,600
Roberta의 저자는 실험을 수행했습니다.

290
00:14:10,600 --> 00:14:13,600
다양한 입력 형식과 사전 훈련 목표를 가지고 있습니다.

291
00:14:13,600 --> 00:14:16,600
그들이 고려한 네 가지 설정은 다음과 같습니다.

292
00:14:16,600 --> 00:14:18,600
세그먼트 쌍과 NSP,

293
00:14:18,600 --> 00:14:21,600
원래 BERT 입력 형식입니다.

294
00:14:22,600 --> 00:14:24,600
NSP 목표를 가지고 있습니다.

295
00:14:24,600 --> 00:14:27,600
각 훈련 예시는 한 쌍의 세그먼트로 구성됩니다.

296
00:14:27,600 --> 00:14:30,600
여러 개의 자연스러운 문장을 포함할 수 있습니다.

297
00:14:30,600 --> 00:14:33,600
그리고 문장 쌍에 NSP를 더한 것입니다.

298
00:14:33,600 --> 00:14:37,600
따라서 각 훈련 예제는 한 쌍의 자연 문장으로 구성됩니다.

299
00:14:37,600 --> 00:14:41,600
하나의 문서에서 연속적으로 샘플링됨

300
00:14:41,600 --> 00:14:43,600
또는 다른 문서에서.

301
00:14:43,600 --> 00:14:45,600
그리고 완전한 문장.

302
00:14:45,600 --> 00:14:48,600
그래서 각 훈련 예시는 자연스러운 문장으로 구성되어 있으며,

303
00:14:48,600 --> 00:14:51,600
하나 이상의 문서에서 연속적으로 샘플링됨

304
00:14:51,600 --> 00:14:55,600
최대 길이는 512개 토큰입니다.

305
00:14:55,600 --> 00:14:58,600
NSP 목표 없이.

306
00:14:58,600 --> 00:15:01,600
그리고 문서 문장은,

307
00:15:01,600 --> 00:15:04,600
완전한 문장과 비슷합니다.

308
00:15:04,600 --> 00:15:08,600
하지만 샘플링은 문서 경계를 넘지 않습니다.

309
00:15:08,600 --> 00:15:11,600
또한 NSP 목표가 없습니다.

310
00:15:11,600 --> 00:15:14,600
그래서 저자는 NSP 목표가

311
00:15:14,600 --> 00:15:17,600
상당한 개선을 제공하지 못했습니다

312
00:15:17,600 --> 00:15:19,600
다른 설정보다.

313
00:15:19,600 --> 00:15:22,600
그리고 어떤 경우에는 공연을 듣기도 했습니다.

314
00:15:22,600 --> 00:15:25,600
그래서 그들은 또한 더 긴 시퀀스를 사용하는 것을 발견했습니다.

315
00:15:25,600 --> 00:15:27,600
완전한 문장과 같은 사전 훈련 중

316
00:15:27,600 --> 00:15:29,600
또는 문서 수준 문장

317
00:15:29,600 --> 00:15:31,600
더 나은 성과로 이어졌습니다

318
00:15:31,600 --> 00:15:34,600
개별 문장 쌍을 사용하는 것과 비교됩니다.

319
00:15:34,600 --> 00:15:36,600
그래서 이번 조사 결과를 바탕으로

320
00:15:36,600 --> 00:15:38,600
Roberta는 전체 문장 설정을 사용했습니다.

321
00:15:38,600 --> 00:15:41,600
사전 훈련을 위한 NSP 목표가 없습니다.

322
00:15:42,600 --> 00:15:45,600
그래서 변화에 더해

323
00:15:45,600 --> 00:15:48,600
사전 훈련 목표 및 입력 형식에서

324
00:15:48,600 --> 00:15:50,600
Roberta는 또한 몇 가지 수정 사항을 소개합니다.

325
00:15:50,600 --> 00:15:52,600
BERT의 사전 훈련 과정을 살펴보겠습니다.

326
00:15:52,600 --> 00:15:56,600
먼저 저자는 더 큰 배치 크기를 사용하는 것을 발견했습니다.

327
00:15:56,600 --> 00:15:58,600
그것은 더 나은 preprenexity로 이어진다

328
00:15:58,600 --> 00:16:00,600
및 다운스트림 테스트 성능.

329
00:16:00,600 --> 00:16:03,600
둘째, 저자는 원래 BERT가

330
00:16:03,600 --> 00:16:05,600
훈련이 부족하다

331
00:16:05,600 --> 00:16:09,600
더 많은 데이터를 사용하면 성능이 향상됩니다.

332
00:16:09,600 --> 00:16:11,600
원래의 BERT 사전 학습 데이터 외에도

333
00:16:11,600 --> 00:16:13,600
Roberta는 CC News 데이터 세트를 사용했습니다.

334
00:16:13,600 --> 00:16:16,600
6,300만 개의 영어 뉴스 기사로 구성되어 있습니다.

335
00:16:16,600 --> 00:16:18,600
그래서 마침내 그들은 또한 그것을 발견했습니다

336
00:16:18,600 --> 00:16:21,600
훈련 시간이 길수록 성과가 향상됩니다.

337
00:16:21,600 --> 00:16:24,600
그들은 Roberta를 100,000명에게 훈련시켰습니다.

338
00:16:24,600 --> 00:16:27,600
그러니까 300,000걸음과 500,000걸음이군요.

339
00:16:27,600 --> 00:16:29,600
그들은 더 긴 훈련이 더 나은 성과로 이어진다는 것을 발견했습니다

340
00:16:29,600 --> 00:16:31,600
다운스트림 작업에 대해

341
00:16:31,600 --> 00:16:34,600
Roberta는 과적합 징후를 보이지 않았습니다.

342
00:16:34,600 --> 00:16:38,600
50만 걸음을 걸어도

343
00:16:38,600 --> 00:16:40,600
이는 더 긴 훈련이 유익할 수 있음을 시사합니다.

344
00:16:40,600 --> 00:16:42,600
대규모 언어 모델의 경우.

345
00:16:46,600 --> 00:16:48,600
그래서 Roberta에 도입된 수정 사항은

346
00:16:48,600 --> 00:16:50,600
상당한 개선을 가져왔습니다

347
00:16:50,600 --> 00:16:52,600
원래 BERT 모델에 비해

348
00:16:52,600 --> 00:16:54,600
그래서 Roberta는 최첨단 기술을 달성했습니다.

349
00:16:54,600 --> 00:16:56,600
접착제 벤치마크에서

350
00:16:56,600 --> 00:16:58,600
BERT 및 기타 언어 모델을 능가함

351
00:16:58,600 --> 00:17:00,600
대부분의 테스트에서.

352
00:17:02,600 --> 00:17:04,600
그 다음은 알버트.

353
00:17:04,600 --> 00:17:07,600
그래서 Light BERT를 뜻하는 ALBERT는,

354
00:17:07,600 --> 00:17:09,600
BERT의 변형입니다

355
00:17:09,600 --> 00:17:11,600
Google Research에서 개발했습니다.

356
00:17:11,600 --> 00:17:13,600
따라서 BERT와 같은 언어 모델을 확장합니다.

357
00:17:13,600 --> 00:17:15,600
더 나은 성능을 달성하기 위해

358
00:17:15,600 --> 00:17:18,600
종종 계산 비용이 증가합니다.

359
00:17:18,600 --> 00:17:20,600
및 메모리 요구 사항.

360
00:17:20,600 --> 00:17:23,600
ALBERT는 다음을 도입하여 이 문제를 해결했습니다.

361
00:17:23,600 --> 00:17:25,600
여러 매개변수 감소 기술

362
00:17:25,600 --> 00:17:27,600
모델을 더 효율적으로 만들기 위해

363
00:17:27,600 --> 00:17:29,600
그 효율성을 손상시키지 않고.

364
00:17:29,600 --> 00:17:32,600
저자는 ALBERT의 구성을 보여줍니다.

365
00:17:32,600 --> 00:17:34,600
BERT 대형과 유사

366
00:17:34,600 --> 00:17:38,600
최대 18배 더 적은 매개변수를 가질 수 있습니다.

367
00:17:38,600 --> 00:17:41,600
최대 1.7배 더 빠르게 훈련할 수 있습니다.

368
00:17:41,600 --> 00:17:43,600
따라서 세 가지 주요 매개변수 감소 기술이 있습니다.

369
00:17:43,600 --> 00:17:45,600
ALBERT에서 소개되었으며,

370
00:17:45,600 --> 00:17:49,600
인수분해된 임베딩 매개변수화,

371
00:17:49,600 --> 00:17:51,600
변환기, 매개변수 공유,

372
00:17:51,600 --> 00:17:53,600
및 관심-문장 일관성 손실

373
00:17:53,600 --> 00:17:55,600
또는 문장 순서 예측.

374
00:17:55,600 --> 00:17:58,600
그래서 저는 이러한 각 기술에 대해 논의하겠습니다.

375
00:17:58,600 --> 00:18:00,600
다음 슬라이드에서 자세히 알아보세요.

376
00:18:01,600 --> 00:18:04,600
따라서 첫 번째 매개변수 감소 기술은

377
00:18:04,600 --> 00:18:06,600
ALBERT에 소개됨

378
00:18:06,600 --> 00:18:08,600
인수분해된 임베딩 매개변수화입니다.

379
00:18:08,600 --> 00:18:10,600
BERT에서는

380
00:18:10,600 --> 00:18:12,600
토큰 임베딩의 차원

381
00:18:12,600 --> 00:18:14,600
히든 레이어는 동일합니다.

382
00:18:14,600 --> 00:18:16,600
이는 하위 자동 모드 선택입니다.

383
00:18:16,600 --> 00:18:18,600
토큰 임베딩은 다음을 나타내는 데 사용됩니다.

384
00:18:18,600 --> 00:18:20,600
입력 토큰 및 캡처만

385
00:18:20,600 --> 00:18:22,600
토큰의 컨텍스트 없는 정보.

386
00:18:22,600 --> 00:18:24,600
반면에 은닉층은

387
00:18:24,600 --> 00:18:26,600
상황에 따른 캡처에 사용됩니다.

388
00:18:26,600 --> 00:18:28,600
토큰 정보입니다.

389
00:18:28,600 --> 00:18:31,600
BERT의 힘은 숨겨진 레이어에서 나옵니다.

390
00:18:31,600 --> 00:18:33,600
토큰 임베딩이 아닙니다.

391
00:18:33,600 --> 00:18:35,600
그러니 은닉층이 생기는 것은 당연하다.

392
00:18:35,600 --> 00:18:37,600
토큰 임베딩보다 더 높은 차원을 가져야 합니다.

393
00:18:37,600 --> 00:18:40,600
하지만 은닉층의 차원을 늘리기 위해

394
00:18:40,600 --> 00:18:42,600
토큰 임베딩의 차원

395
00:18:42,600 --> 00:18:44,600
또한 증가해야합니다.

396
00:18:44,600 --> 00:18:46,600
이로 인해 상당한 증가가 발생할 것입니다.

397
00:18:46,600 --> 00:18:48,600
매개변수의 수

398
00:18:48,600 --> 00:18:50,600
일반적으로 어휘 크기가 매우 크기 때문입니다.

399
00:18:50,600 --> 00:18:52,600
예를 들어,

400
00:18:52,600 --> 00:18:56,600
BERT의 어휘 크기는 30,000개 토큰입니다.

401
00:18:56,600 --> 00:19:00,600
768차원 토큰 임베딩을 사용합니다.

402
00:19:00,600 --> 00:19:03,600
결과적으로 2,300만 개의 매개변수가 생성됩니다.

403
00:19:03,600 --> 00:19:05,600
그래서 BERT는 이 문제를 해결합니다.

404
00:19:05,600 --> 00:19:09,600
토큰 임베딩의 차원을 분리하여

405
00:19:09,600 --> 00:19:11,600
숨겨진 레이어의 차원에서.

406
00:19:11,600 --> 00:19:13,600
그래서 BERT에서는

407
00:19:13,600 --> 00:19:15,600
토큰 임베딩이 먼저 예상됩니다.

408
00:19:15,600 --> 00:19:17,600
저차원 공간으로

409
00:19:17,600 --> 00:19:19,600
그런 다음 더 높은 차원의 공간으로 투영됩니다.

410
00:19:19,600 --> 00:19:21,600
숨겨진 레이어 중.

411
00:19:21,600 --> 00:19:24,600
이를 통해 BERT는 더 높은 차원의 숨겨진 레이어를 사용할 수 있습니다.

412
00:19:24,600 --> 00:19:27,600
매개변수 수를 크게 늘리지 않고.

413
00:19:27,600 --> 00:19:29,600
그래서 이 기술을 사용하면,

414
00:19:29,600 --> 00:19:31,600
매개변수 수가 다음에서 감소합니다.

415
00:19:31,600 --> 00:19:34,600
V 곱하기 H의 큰 O

416
00:19:34,600 --> 00:19:37,600
V 곱하기 E 더하기 H 곱하기 E의 큰 O입니다.

417
00:19:37,600 --> 00:19:39,600
여기서 V는 어휘 크기입니다.

418
00:19:39,600 --> 00:19:41,600
H는 숨겨진 레이어 차원입니다.

419
00:19:41,600 --> 00:19:43,600
E는 임베딩 차원입니다.

420
00:19:46,600 --> 00:19:48,600
두 번째 매개변수 감소 기법

421
00:19:48,600 --> 00:19:51,600
BERT에 도입된 것은 크로스 레이어 매개변수 공유입니다.

422
00:19:51,600 --> 00:19:52,600
그래서 BERT에서는

423
00:19:52,600 --> 00:19:55,600
각 레이어에는 고유한 매개변수 세트가 있습니다.

424
00:19:55,600 --> 00:19:58,600
이로 인해 많은 수의 매개변수가 발생합니다.

425
00:19:58,600 --> 00:20:00,600
Albert가 이 문제를 해결합니다.

426
00:20:00,600 --> 00:20:02,600
레이어 간에 매개변수를 공유함으로써

427
00:20:02,600 --> 00:20:06,600
레이어 간에 매개변수를 공유하는 방법에는 여러 가지가 있습니다.

428
00:20:06,600 --> 00:20:08,600
관심의 물결을 공유하는 것,

429
00:20:08,600 --> 00:20:11,600
피드오버 신경망 파동 또는 둘 다.

430
00:20:11,600 --> 00:20:12,600
기본적으로,

431
00:20:12,600 --> 00:20:15,600
Albert는 여러 레이어에서 모든 매개변수를 공유합니다.

432
00:20:15,600 --> 00:20:18,600
저자는 이 매개변수 공유 기술을 발견했습니다.

433
00:20:19,600 --> 00:20:22,600
상당한 매개변수 감소로 이어졌습니다.

434
00:20:22,600 --> 00:20:25,600
모델의 성능을 저하시키지 않고

435
00:20:27,600 --> 00:20:29,600
상당히.

436
00:20:30,600 --> 00:20:33,600
그리고 세 번째 매개변수 감소 기술이 도입되었습니다.

437
00:20:33,600 --> 00:20:35,600
관심 문장 일관성,

438
00:20:35,600 --> 00:20:37,600
또는 문장 순서 예측.

439
00:20:37,600 --> 00:20:38,600
그래서 BERT에서는

440
00:20:38,600 --> 00:20:40,600
다음 문장 예측 목표

441
00:20:40,600 --> 00:20:42,600
모델을 격려하는 데 사용됩니다.

442
00:20:42,600 --> 00:20:44,600
문장 사이의 관계를 포착합니다.

443
00:20:44,600 --> 00:20:47,600
그러나 앞서 논의한 것처럼,

444
00:20:47,600 --> 00:20:50,600
NASP 목표는 덜 효과적인 것으로 나타났습니다.

445
00:20:50,600 --> 00:20:52,600
다른 사전 훈련 목표보다

446
00:20:52,600 --> 00:20:55,600
그래서 Albert는 새로운 사전 훈련 목표를 도입했습니다.

447
00:20:55,600 --> 00:20:57,600
문장 순서 예측이라고 합니다.

448
00:20:57,600 --> 00:20:59,600
NASP보다 더 효과적이라고 합니다.

449
00:20:59,600 --> 00:21:01,600
그래서 문장 순서 예측에서는

450
00:21:01,600 --> 00:21:03,600
모델이 예측을 시도하고 있습니다.

451
00:21:03,600 --> 00:21:05,600
한 쌍의 문장인지

452
00:21:05,600 --> 00:21:07,600
순서가 맞는지 아닌지.

453
00:21:07,600 --> 00:21:09,600
저자는 이러한 목표를 발견했습니다.

454
00:21:09,600 --> 00:21:11,600
NASP에 비해 성능이 더 좋습니다.

455
00:21:11,600 --> 00:21:15,600
그리고 사실 이건 매개변수 감소 기법이 아니고,

456
00:21:15,600 --> 00:21:18,600
NASP 목표가 맞는지 조사하고 싶습니다.

457
00:21:18,600 --> 00:21:20,600
효과가 있든 없든,

458
00:21:21,600 --> 00:21:23,600
로봇이 했던 것과 비슷합니다.

459
00:21:26,600 --> 00:21:29,600
그래서 도입된 매개변수 감소 기술은

460
00:21:29,600 --> 00:21:32,600
Albert의 경우 상당한 개선이 이루어졌습니다.

461
00:21:32,600 --> 00:21:34,600
타협하지 않고 효율성을 높이다

462
00:21:34,600 --> 00:21:36,600
모델의 성능.

463
00:21:36,600 --> 00:21:38,600
그래서 최고 수준의 성과까지 달성했습니다.

464
00:21:38,600 --> 00:21:40,600
그룹 벤치마크에서는

465
00:21:40,600 --> 00:21:43,600
놀라운 BERT, 로봇 및 기타 언어 모델

466
00:21:43,600 --> 00:21:45,600
대부분의 텍스트에 대해.

467
00:21:45,600 --> 00:21:47,600
그리고 이 퍼포먼스는 꽤 인상적이다.