1
00:00:00,000 --> 00:00:07,080
안녕하세요 여러분. 제 이름은 Mohamed입니다. 이번 프레젠테이션에서는 Yunyi와 함께

2
00:00:07,080 --> 00:00:12,160
대규모 다중 모드 모델에 대한 주제를 제시합니다.

3
00:00:12,160 --> 00:00:15,640
그럼 우리가 다룰 내용을 간단히 검토해 보겠습니다.

4
00:00:15,640 --> 00:00:21,080
기본적으로 우리는 8개의 모델이나 8개의 논문을 다룰 것입니다.

5
00:00:21,080 --> 00:00:23,960
실제로 마지막 보고서인 독일은 기술 보고서에 가깝습니다.

6
00:00:23,960 --> 00:00:31,120
따라서 모델은 Flamingo, 시각적 지침 튜닝, Lava, InstructBleep, Poly, Poly3,

7
00:00:31,120 --> 00:00:36,920
Emo2, InternVL 및 Gemini.

8
00:00:36,920 --> 00:00:45,400
따라서 이 모델 중 처음 4개는 제가 제시하고 다음 4개는 Yunyi가 제시할 것입니다.

9
00:00:45,400 --> 00:00:47,920
플라밍고를 찾으러 가자.

10
00:00:47,920 --> 00:00:55,440
그래서 Flamingo는 GPT-3 Momentum 다중 모드 비전 언어 작업이라고 합니다.

11
00:00:55,440 --> 00:00:56,720
플라밍고가 무엇인지 봅시다.

12
00:00:56,720 --> 00:01:02,560
Flamingo는 캡션, 시각적 등 다양한 작업을 수행할 수 있는 시각적 언어 모델입니다.

13
00:01:02,560 --> 00:01:07,080
대화, 분류 및 시각적 질문 답변.

14
00:01:07,080 --> 00:01:10,480
여기에서 시각적 질문 답변의 예를 볼 수 있습니다.

15
00:01:10,480 --> 00:01:17,400
예를 들어 여기 사진을 찍으세요. 이 이미지에는 CD와 플로피 디스크가 있습니다.

16
00:01:17,400 --> 00:01:22,160
그리고 이들의 용량이 무엇이라고 생각하는지 묻는 질문에 모델이나 Flamingo는

17
00:01:22,160 --> 00:01:26,960
해당 객체를 식별하고 해당 객체의 용량에 대한 정보를 제공합니다.

18
00:01:26,960 --> 00:01:36,920
보시다시피 플로피 디스크의 경우 1.44MB이고 CD의 경우 700MB입니다.

19
00:01:36,920 --> 00:01:40,920
또 다른 예는 시각적 대화를 위한 Flamingo의 능력입니다.

20
00:01:40,920 --> 00:01:46,280
예를 들어, 이 이미지에는 수프 한 그릇 위에 괴물이 그려져 있습니다.

21
00:01:46,280 --> 00:01:52,520
그리고 사용자가 이 사진에 무엇이 있는지 물으면 모델은 그것이 수프 한 그릇이라고 말합니다.

22
00:01:52,520 --> 00:01:56,800
괴물 얼굴을 보고 그 괴물이 무엇으로 만들어졌는지 물었습니다.

23
00:01:56,800 --> 00:02:03,480
물론 그것이 틀렸다고 말할 수 있으며 사용자가 모델을 수정하면 모델은

24
00:02:03,480 --> 00:02:06,760
또 다른 질문을 하고 대답했다.

25
00:02:06,760 --> 00:02:12,080
여기에서 비디오를 이해하는 예를 볼 수 있습니다.

26
00:02:12,080 --> 00:02:16,920
예를 들어, 여기 이것을 가져가면 이것이 비디오의 프레임입니다.

27
00:02:16,920 --> 00:02:19,760
여기 펜이 있고, 가위와 고무줄이 있습니다.

28
00:02:19,760 --> 00:02:25,320
모델이 개체를 식별하도록 요청하면 이름을 가위, 펜으로 지정할 수 있습니다.

29
00:02:25,320 --> 00:02:28,320
그리고 고무줄.

30
00:02:28,320 --> 00:02:31,640
Flamingo의 동기는 무엇입니까?

31
00:02:31,640 --> 00:02:37,360
지능의 중요한 측면 중 하나는 주어진 과제를 빠르게 학습하는 능력입니다.

32
00:02:37,360 --> 00:02:39,840
짧은 지침.

33
00:02:39,840 --> 00:02:45,920
그리고 우리는 데이터를 더 잘 활용하기 위해 환경을 학습하는 모델을 좋아하고

34
00:02:45,920 --> 00:02:52,160
이 속성을 달성하기 위해 시각 및 언어 작업을 처리하는 다중 모드 시스템입니다.

35
00:02:52,160 --> 00:02:58,800
Flamingo 이전에는 지배적인 컴퓨터 비전 패러다임이 대규모 사전 훈련과

36
00:02:58,800 --> 00:03:00,560
작업별 미세 조정.

37
00:03:00,560 --> 00:03:06,960
그러나 이 접근 방식에는 수천 개의 훈련 샘플이 필요하다는 몇 가지 문제가 있습니다.

38
00:03:06,960 --> 00:03:13,840
작업별 하이퍼파라미터 조정이 필요하고 상당한 계산이 필요함

39
00:03:13,840 --> 00:03:14,920
자원.

40
00:03:14,920 --> 00:03:19,720
그래서 기본적으로 저자는 좋은 성능을 가진 다중 모드 모델을 훈련할 수 있는지 묻습니다.

41
00:03:19,720 --> 00:03:22,560
퓨샷 체제에서?

42
00:03:22,560 --> 00:03:29,520
우리 모델이 갖기를 원하는 또 다른 능력은 개방형 작업을 처리하는 능력입니다.

43
00:03:29,520 --> 00:03:36,120
Flamingo 이전의 클립 및 정렬과 같은 다중 모드 모델은 우수한 제로샷 성능을 보여줍니다.

44
00:03:36,240 --> 00:03:37,760
하지만 그들은 융통성이 없습니다.

45
00:03:37,760 --> 00:03:42,560
그들은 언어를 생성하는 능력이 부족합니다.

46
00:03:42,560 --> 00:03:48,680
NLP 도메인에서 영감을 얻어 GPT-3과 같은 대규모 언어 모델을 볼 수 있습니다.

47
00:03:48,680 --> 00:03:54,440
유연한 소수 학습자이며 다음과 같은 형식으로 작업의 몇 가지 예를 제공할 때

48
00:03:54,440 --> 00:04:00,640
프롬프트와 쿼리에 대해 언어 모델은 연속을 생성하여 예측된 결과를 생성합니다.

49
00:04:00,640 --> 00:04:02,280
산출.

50
00:04:02,280 --> 00:04:07,560
그리고 성공의 핵심 요소는 대규모 사전 교육입니다.

51
00:04:07,560 --> 00:04:13,960
이를 관찰하면서 저자는 원칙적으로 다음과 같은 이미지 및 비디오 이해 작업을 수행한다고 말합니다.

52
00:04:13,960 --> 00:04:20,280
분류, 캡션, 질문 답변은 기본적으로 텍스트 예측 문제입니다.

53
00:04:20,280 --> 00:04:21,960
시각적 입력 조건화.

54
00:04:21,960 --> 00:04:27,480
그래서 그들은 이런 질문을 합니다. 개방형 다중 모드 작업이 가능한 모델을 배울 수 있습니까?

55
00:04:27,480 --> 00:04:34,800
대규모 언어 모델에 대해 수행된 것과 동일한 방식으로 사전 학습을 통해?

56
00:04:34,800 --> 00:04:40,680
다중 모드 생성 모델링을 생성하는 데는 몇 가지 과제가 있습니다.

57
00:04:40,680 --> 00:04:47,880
과제 중 하나는 대규모 언어 모델을 훈련하는 데 계산 비용이 많이 든다는 것입니다.

58
00:04:47,880 --> 00:04:54,400
그리고 우리는 자원을 절약하고 싶어합니다. 실제로 저자는 자원을 절약하는 것을 좋아합니다.

59
00:04:54,400 --> 00:04:59,720
사전 훈련된 언어 모델에서 시작하지만 우리는 언어 모델이 그렇지 않다는 것을 알고 있습니다.

60
00:04:59,720 --> 00:05:04,600
다양한 양식의 입력을 처리할 수 있습니다.

61
00:05:04,600 --> 00:05:11,760
그래서 우리는 모델이 다중 모드 입력을 처리할 수 없도록 모델에 장애를 부여하고 싶습니다.

62
00:05:11,760 --> 00:05:17,960
우리가 사용하려는 원래 언어 모델에 대한 지식.

63
00:05:17,960 --> 00:05:25,520
그래서 저자는 교차 주의 레이어를 인터리브하는 이 솔루션을 제안합니다.

64
00:05:25,520 --> 00:05:31,360
훈련 중에 동결된 언어 전용 self-attention 레이어를 사용합니다.

65
00:05:31,360 --> 00:05:39,240
그리고 나중에 슬라이드에서 교차 주의에 대해 논의할 것이므로 걱정하지 마십시오.

66
00:05:39,240 --> 00:05:44,880
또 다른 과제는 모델이 비디오와 이미지 입력을 모두 얻을 수 있도록 하려는 것입니다.

67
00:05:44,880 --> 00:05:50,200
하지만 우리는 이것이 고차원 데이터라는 것을 알고 있으므로 이를 1차원으로 평면화합니다.

68
00:05:50,200 --> 00:05:55,880
텍스트 생성에 사용되는 시퀀스는 비용이 많이 들고 2차 비용으로 인해 더욱 악화됩니다.

69
00:05:55,880 --> 00:05:57,440
자기 관심의.

70
00:05:57,440 --> 00:06:03,720
우리가 달성하고 싶은 두 번째 목표는 이미지와 비디오의 통일된 처리입니다.

71
00:06:03,720 --> 00:06:05,640
우리 모델에서는.

72
00:06:05,640 --> 00:06:11,640
저자가 제안한 접근 방식은 고정된 인식자 기반 아키텍처를 사용하는 것입니다.

73
00:06:11,640 --> 00:06:17,960
이미지와 비디오 모두에 대한 시각적 토큰의 수에 대해 다시 논의할 것이므로 걱정하지 마세요.

74
00:06:17,960 --> 00:06:20,800
자세한 내용은 이후 슬라이드에서 알아보겠습니다.

75
00:06:20,800 --> 00:06:25,480
또 다른 과제는 이질적인 학습 데이터를 확보하는 것입니다.

76
00:06:25,480 --> 00:06:31,200
대규모 모델에는 방대한 교육 데이터 세트가 필요하지만 기존 이미지 텍스트가 필요하다는 것을 알고 있습니다.

77
00:06:31,200 --> 00:06:38,040
클립 및 정렬에 사용되는 데이터 세트는 GPT-3에 도달할 만큼 일반적이지 않습니다.

78
00:06:38,040 --> 00:06:40,760
스타일의 퓨샷 학습.

79
00:06:40,760 --> 00:06:49,360
대규모 인터넷 기반 텍스트 전용 데이터 세트가 있지만 다중 모드 데이터 및 확장 가능한 데이터 세트는 없습니다.

80
00:06:49,360 --> 00:06:57,320
사용할 수 있는 접근 방식은 이미지와 텍스트가 인터리브된 웹페이지를 스크랩하는 것이지만 문제는

81
00:06:57,320 --> 00:07:02,360
이러한 접근 방식을 사용하면 이미지와 텍스트가 매주만 관련되는 경우가 많습니다.

82
00:07:02,360 --> 00:07:08,400
그래서 저자는 웹 스크래핑과 결합하려는 다음 접근 방식을 제안했습니다.

83
00:07:08,400 --> 00:07:14,880
기존의 쌍을 이루는 이미지 텍스트 및 비디오 텍스트 데이터 세트.

84
00:07:14,880 --> 00:07:23,600
Flamingo는 Flamingo가 시각적 언어가 될 수 있도록 Flamingo에 대한 목표를 요약해 보겠습니다.

85
00:07:23,600 --> 00:07:32,360
텍스트, 이미지 또는 비디오를 볼 때 인터리브된 입력을 받아들이는 모델이 제공됩니다.

86
00:07:32,360 --> 00:07:36,360
Flamingo와 Flamingo는 텍스트 출력을 통해 우리에게 알려줄 것입니다.

87
00:07:36,360 --> 00:07:41,480
이 프레임워크는 Flamingo가 완료할 수 있는 개방형 테스트와 같은 광범위한 테스트를 가능하게 합니다.

88
00:07:41,480 --> 00:07:47,480
테스트 및 종료, 테스트, 시각적 질문 답변 및 캡션이 예입니다.

89
00:07:47,480 --> 00:07:52,720
오픈과 테스트 그리고 클로즈와 테스트의 분류입니다.

90
00:07:52,720 --> 00:07:58,640
첫 번째 목표는 사전 훈련 모델을 활용하여 비전과 같은 계산을 저장하는 것입니다.

91
00:07:58,640 --> 00:08:05,360
그들은 클립의 인코더를 사용하고 언어 구성 요소에는 냉동 친칠라와

92
00:08:05,360 --> 00:08:13,640
두 번째 목표는 인지 리샘플러를 사용하여 이러한 사전 학습 단봉 모델을 조화롭게 연결하는 것입니다.

93
00:08:13,640 --> 00:08:16,680
그리고 교차주의.

94
00:08:16,680 --> 00:08:21,640
따라서 이러한 목표를 달성하기 위해 작성자가 보유한 아키텍처가 무엇인지 살펴보겠습니다.

95
00:08:21,640 --> 00:08:22,640
디자인되었습니다.

96
00:08:22,640 --> 00:08:29,320
여기에서 입력을 볼 수 있습니다. 입력에서 가져온 이미지는 텍스트 인터리브 이미지입니다.

97
00:08:29,320 --> 00:08:34,080
비전 인코더에 제공되며 훈련 중에 고정된 상태로 유지됩니다.

98
00:08:34,080 --> 00:08:40,440
비전 인코더의 출력은 인식 리샘플러로 이동하고 인식 리샘플러의 출력은

99
00:08:40,440 --> 00:08:47,160
Gated Extension Density라고 불리는 이러한 교차 주의 레이어에 옵니다.

100
00:08:47,160 --> 00:08:54,320
입력 텍스트도 가져와서 게이트 확장으로 직접 전달됩니다.

101
00:08:54,320 --> 00:08:59,440
이 시점에서 그들은 혼합됩니다. 이는 교차 주의 레이어이고 출력은 고정 상태로 이동합니다.

102
00:08:59,440 --> 00:09:05,540
언어 블록을 사용하고 기본적으로 이것은 실제로는 아니지만 몇 번 반복됩니다.

103
00:09:05,540 --> 00:09:13,040
몇 번이지만 출력이 나올 때까지 몇 번과 같습니다.

104
00:09:13,040 --> 00:09:18,960
여기의 텍스트와 분홍색은 처음부터 훈련될 구성 요소를 보여줍니다.

105
00:09:18,960 --> 00:09:25,920
파란색은 훈련 중에 고정된 구성 요소를 나타냅니다.

106
00:09:25,920 --> 00:09:33,320
그래서 Flamingo는 여기 보이는 확률을 모델링하려고 합니다.

107
00:09:33,320 --> 00:09:40,240
시각적 조건이 주어지면 다음에 생성될 텍스트 토큰이 동일합니다.

108
00:09:40,240 --> 00:09:49,280
모든 이전 토큰 텍스트를 고려하여 ELF 토큰의 확률을 곱합니다.

109
00:09:49,280 --> 00:09:56,920
그 앞에 오는 토큰과 그 앞에 오는 이미지 토큰입니다.

110
00:09:56,920 --> 00:10:02,720
이제 비전 인코더를 살펴보고 조금 더 자세히 살펴보겠습니다. 이것이 비전입니다.

111
00:10:02,720 --> 00:10:10,120
Flamingo가 F6 노멀라이저 프리 resnet 또는 NFnet을 사용하는 인코더 및 비전 인코더

112
00:10:10,120 --> 00:10:18,080
간단히 말해서 클립과 Bert에서 사용되는 대비 손실을 사용하여 이중 인코더로 사전 훈련됩니다.

113
00:10:18,080 --> 00:10:22,720
텍스트 인코더에 사용되며 사전 훈련 후에 폐기됩니다.

114
00:10:22,720 --> 00:10:32,000
클립에는 약간의 차이가 있습니다. 예를 들어 여기 Flamingo에서는 전역 평균 풀링이 사용됩니다.

115
00:10:32,000 --> 00:10:38,160
글로벌 관심 풀링 및 해상도 대신 비전 임베딩을 생성합니다.

116
00:10:38,160 --> 00:10:47,640
인코더의 사전 훈련 단계에 사용되는 이미지는 288 x 288 픽셀이고

117
00:10:47,640 --> 00:10:52,320
출력 임베딩의 차원은 1376입니다.

118
00:10:52,320 --> 00:10:59,400
따라서 비전 인코더가 생성하는 출력은 2D 특수 기능 그리드입니다.

119
00:10:59,400 --> 00:11:07,720
1D로 평면화되고 비디오의 경우 프레임이 꺼내지고 기본적으로 샘플링됩니다.

120
00:11:07,720 --> 00:11:14,920
초당 하나의 프레임으로 생성된 다음 각 프레임에 대해 생성된 특징이 연결되어

121
00:11:14,920 --> 00:11:16,920
서로.

122
00:11:16,920 --> 00:11:23,640
또한 사전 훈련 단계 후에 비전 인코더가 정지된다는 점에 유의하세요.

123
00:11:23,640 --> 00:11:28,480
이제 Percever Sampler를 살펴보고 이것이 필요한 이유를 살펴보겠습니다.

124
00:11:28,480 --> 00:11:33,200
따라서 우리가 말했듯이 비디오가 있기 때문에 다양한 수의 입력 프레임이 처리됩니다.

125
00:11:33,200 --> 00:11:39,600
비전 인코더는 다양한 수의 기능을 생성하므로 입력에 포함됩니다.

126
00:11:39,600 --> 00:11:44,720
다음과 같은 경우 고정된 수의 시각적 토큰을 출력하는 Percever가 필요합니다.

127
00:11:44,720 --> 00:11:50,200
종이는 64이므로 주의의 복잡성을 제한할 수 있습니다.

128
00:11:50,200 --> 00:11:54,520
이제 Percever Sampler 아키텍처를 살펴보겠습니다.

129
00:11:54,520 --> 00:11:59,480
보시다시피 비디오의 프레임은 비전 인코더에 제공됩니다.

130
00:11:59,480 --> 00:12:07,000
기본적으로 각 프레임마다 그리고 시간적 특성을 추가한 후 서로 다른 기능 세트가 생성됩니다.

131
00:12:07,000 --> 00:12:15,880
비전 인코더가 생성하는 기능에 대한 인코딩은 모든 기능이 평면화되고

132
00:12:15,880 --> 00:12:17,600
주의를 기울이고 있습니다.

133
00:12:17,600 --> 00:12:26,200
여기에는 학습된 잠재 쿼리가 있지만 이러한 잠재 쿼리는 볼 수 없습니다.

134
00:12:26,200 --> 00:12:32,280
우리는 그것들을 쿼리로 취급하지만 평면화된 생성된 쿼리로 연결합니다.

135
00:12:32,280 --> 00:12:38,560
기능을 제공하고 이를 Attention 레이어에 전달한 다음 출력을

136
00:12:38,560 --> 00:12:43,460
피드 포워드 레이어를 만들고 최종 출력을 얻습니다.

137
00:12:43,460 --> 00:12:53,240
쿼리 수는 항상 64개로 고정되어 있으므로 항상 64개가 됩니다.

138
00:12:53,240 --> 00:12:58,240
출력시 토큰.

139
00:12:58,240 --> 00:13:03,920
이제 언어 모델의 조건과 이것이 어떻게 발생하는지 살펴보겠습니다.

140
00:13:03,920 --> 00:13:12,640
앞서 언급한 것처럼 Percever Sampler의 출력은 다음과 같습니다.

141
00:13:12,640 --> 00:13:18,560
주의를 기울이면 텍스트도 여기에 오고 출력은 고정된 언어로 이동됩니다.

142
00:13:18,560 --> 00:13:23,800
하지만 이 게이트 확장 밀집 계층의 아키텍처가 무엇인지 살펴보겠습니다.

143
00:13:23,800 --> 00:13:30,400
비전 입력은 여기에 오고 언어 입력은 쿼리로 오는 것을 볼 수 있습니다.

144
00:13:30,400 --> 00:13:35,680
이 교차 주의 레이어에 태닝 게이팅을 적용한 다음

145
00:13:35,680 --> 00:13:40,640
다시 피드 포워드하고 태닝 게이팅을 한 다음 우리는 얼어붙은 자기 주의를 갖게 됩니다.

146
00:13:40,640 --> 00:13:45,600
언어 구성요소와 피드포워드가 다시 정지되었습니다.

147
00:13:45,600 --> 00:13:51,560
보시다시피 여기에 태닝 게이트가 몇 개 있습니다.

148
00:13:51,560 --> 00:13:57,560
그것은 분홍색 레이어가 훈련되지 않은 초기화에서 우리가 원하는 것입니다.

149
00:13:57,560 --> 00:14:03,240
적어도 고정된 언어 부분만큼 좋은 성능을 좋아하므로 초기화될 것입니다.

150
00:14:03,240 --> 00:14:08,760
0으로 설정하려면 여기에 표시된 것처럼 알파 매개변수가 있으므로 0으로 설정됩니다.

151
00:14:08,760 --> 00:14:13,000
언어 구성 요소의 원래 성능이 유지됩니다.

152
00:14:13,000 --> 00:14:26,900
따라서 각 레이어 뒤에는 GPT 도구 스타일과 같은 레이어 표준이 있습니다.

153
00:14:26,900 --> 00:14:31,600
Flamingo 훈련 데이터 정보 Flamingo는 이미지 텍스트 쌍 비디오 텍스트에 대해 훈련되었습니다.

154
00:14:31,600 --> 00:14:37,920
보시다시피 쌍과 웹 페이지 데이터는 일종의 이미지 텍스트 쌍 중 하나입니다.

155
00:14:37,920 --> 00:14:44,080
Flamingo의 이미지와 해당 캡션이 있으며 비디오 텍스트 쌍은 다음에서 볼 수 있습니다.

156
00:14:44,080 --> 00:14:49,480
중간에는 비디오 프레임과 비디오에서 무슨 일이 일어나고 있는지 설명하는 캡션이 있습니다.

157
00:14:49,480 --> 00:14:55,480
이것은 내 웹사이트에 오신 것을 환영합니다라는 이미지가 있는 일종의 웹페이지 데이터입니다.

158
00:14:55,480 --> 00:15:02,420
개 사진인데 이게 내 개 사진이고 고양이 사진 등이 나와요.

159
00:15:02,420 --> 00:15:11,940
Flamingo 훈련 목표는 음의 러그 가능성의 평균과 마찬가지로 음수입니다.

160
00:15:11,940 --> 00:15:20,820
이전 토큰과 이미지를 기반으로 다음 토큰이 생성되며 이와 같은 기대가 발생합니다.

161
00:15:20,820 --> 00:15:27,620
데이터 세트에 대해 여러 데이터 세트가 있기 때문에 합계가 있습니다.

162
00:15:27,620 --> 00:15:39,060
물론 우리는 각 데이터 세트에 가중치를 부여하거나 균형을 맞춰야 합니다.

163
00:15:39,060 --> 00:15:46,980
dm은 m번째 데이터 세트를 나타내고, 람다 m은 m번째 데이터 세트에 대한 양의 스칼라 비율입니다.

164
00:15:46,980 --> 00:15:53,220
비전 인코더 사전 훈련과 마찬가지로 이러한 가중치 또는 이 람다 m을 조정하는 것이 중요합니다.

165
00:15:53,220 --> 00:16:00,500
좋은 성능을 위해 그리고 우리가 볼 수 있듯이 축적 전략은 손실에 대해 사용됩니다.

166
00:16:00,500 --> 00:16:03,980
모든 데이터 세트에 대해.

167
00:16:03,980 --> 00:16:10,380
두 종류, 두 가지 유형, 두 가지 범주가 있음을 알 수 있는 평가 데이터 세트를 살펴보겠습니다.

168
00:16:10,380 --> 00:16:18,860
개방형 및 폐쇄형 작업에는 이미지 벤치마크와 비디오 벤치마크가 있습니다.

169
00:16:18,860 --> 00:16:25,740
개방형 작업의 경우 캡션, 시각적 질문 답변 등이 있음을 알 수 있습니다.

170
00:16:25,740 --> 00:16:31,340
폐쇄형 작업의 경우 일부 예와 같이 분류 또는 이미지가 있음을 알 수 있습니다.

171
00:16:31,340 --> 00:16:39,260
여기에 텍스트 분류와 시각적 대화가 추가되어 기어 표시가 있는 것을 볼 수 있습니다.

172
00:16:39,260 --> 00:16:45,340
기본적으로 이는 해당 데이터 세트가 하이퍼 매개변수 조정 및 모델에 사용되었음을 의미합니다.

173
00:16:45,340 --> 00:16:50,740
훈련과 그러한 표시가 없는 것들은 모델에게 보이지 않는다는 것을 의미하며

174
00:16:50,740 --> 00:16:56,380
우리는 테스트에만 사용합니다.

175
00:16:56,380 --> 00:17:04,460
여기에서 다양한 데이터 세트를 볼 수 있는 것처럼 주요 결과를 살펴보겠습니다.

176
00:17:04,460 --> 00:17:10,500
이전 슬라이드에서 논의한 내용을 보면

177
00:17:10,500 --> 00:17:20,140
800억 개의 매개변수로 구성된 32샷 플라밍고는 핑크색으로 보입니다.

178
00:17:20,140 --> 00:17:28,380
막대와 이전의 최첨단 0을 보여주는 회색 막대를 볼 수 있습니다.

179
00:17:28,380 --> 00:17:38,340
또는 소수의 모델이며 최첨단 미세 조정 버전과 관련이 있습니다.

180
00:17:38,340 --> 00:17:44,580
like flamingo가 최첨단 미세 조정된 것보다 지속적으로 더 나은 성능을 발휘한다는 것을 알 수 있습니다.

181
00:17:44,580 --> 00:17:53,060
또한 이전에 나온 소수의 모델도 일부 데이터 세트에서 볼 수 있습니다.

182
00:17:53,060 --> 00:18:00,580
오른쪽에서 볼 수 있는 성능에 완전히 새로운 최첨단 기술을 적용한 것과 같습니다.

183
00:18:00,580 --> 00:18:09,260
스케일 스케일링의 효과와 샷 수를 보여주려는 또 다른 다이어그램

184
00:18:09,260 --> 00:18:16,820
가로축은 우리가 볼 수 있는 샷 수이며 범위는 1개 샷에서 32개 샷까지입니다.

185
00:18:16,820 --> 00:18:24,900
샷 수를 높이면 성능이 향상된다는 것을 알 수 있습니다.

186
00:18:24,900 --> 00:18:34,380
모델을 30억에서 90억, 800억으로 확장하면

187
00:18:34,380 --> 00:18:41,940
우리 모델의 성능 다음 논문이나 우리가 논의할 모델은 시각적 교육입니다.

188
00:18:41,940 --> 00:18:51,220
튜닝과 여기서 소개하는 모델은 Lava이므로 본 논문에서는 Lava를 소개합니다.

189
00:18:51,300 --> 00:18:57,940
언어 및 시각 보조원은 엔드투엔드 교육을 받은 대규모 다중 모드 모델입니다.

190
00:18:57,940 --> 00:19:05,460
비전 인코더 및 LLM은 다중 모드 언어 이미지 명령어에 맞춰 조정된 명령어입니다.

191
00:19:05,460 --> 00:19:13,460
데이터를 따르고 그러한 데이터를 생성하기 위해 논문에서는 흥미로운 GPT-4 언어만 사용합니다.

192
00:19:13,460 --> 00:19:20,900
그리고 그것이 무엇을 할 수 있는지 알아보기 위해 정성적인 예를 통해 이것을 볼 수 있습니다.

193
00:19:20,900 --> 00:19:26,260
이미지를 보고 모델에게 이 이미지의 특이한 점을 물으면 특이하다고 말할 수 있습니다.

194
00:19:26,260 --> 00:19:33,700
이 이미지의 측면은 미니밴 뒤에 앉아 옷을 다림질하는 남자의 모습입니다. 이는 전형적인 모습이 아닙니다.

195
00:19:33,700 --> 00:19:38,420
일반적으로 고정된 장소에서 옷을 다림질하는 것처럼 이 활동을 수행할 수 있는 장소입니다.

196
00:19:38,420 --> 00:19:44,580
안전한 위치 등 이 작품의 배경은 무엇인가요? 이 작품은 다음에서 영감을 받았습니다.

197
00:19:44,580 --> 00:19:52,340
Flamingo와 같은 이전의 대형 다중 모드 모델은 또한 다음의 영향을 받습니다.

198
00:19:53,060 --> 00:19:58,740
대규모 언어 모델에 대한 명령 조정 및 다른 모델과 같은 예에서 영감을 얻었습니다.

199
00:19:58,740 --> 00:20:05,940
LLM이 어떻게 조정되었는지 알아보기 위한 정성적 예를 살펴보겠습니다.

200
00:20:05,940 --> 00:20:12,500
교육은 인간의 행동을 설명하고 우리가 주는 행동은 울음이며

201
00:20:12,500 --> 00:20:18,580
사람이 우는 데에는 여러 가지 이유가 있을 수 있다고 말해요. 슬프거나 화났을 수도 있고 등등

202
00:20:21,220 --> 00:20:30,260
또한 제가 언급했듯이 이 작업은 이전 LMM에서 영감을 얻었습니다. 그 중 하나는 blip2이고 다른 하나는

203
00:20:30,260 --> 00:20:38,340
Flamingo Flamingo 앞서 논의했지만 blip2에서는 논의하지 않겠습니다. 하지만 문제는 무엇입니까?

204
00:20:38,340 --> 00:20:44,180
이러한 이전 작업은 이전 방법에는 일반적으로 다음 지침이 부족하다는 것이 밝혀졌습니다.

205
00:20:44,180 --> 00:20:50,340
따라서 저자가 묻는 질문은 이러한 다중 모드 모델을 어떻게 만들 수 있느냐는 것입니다.

206
00:20:50,340 --> 00:20:57,620
인간의 의도를 따르며 제안된 접근 방식은 시각적 지침을 조정하기 전에 시각적 지침을 조정하는 것입니다.

207
00:20:57,620 --> 00:21:05,940
명령어 튜닝 LLM에 사용되는 명령어 튜닝 방법이 무엇인지 살펴보겠습니다. 두 가지가 있습니다.

208
00:21:05,940 --> 00:21:11,540
인간이 지시하는 명령 튜닝 데이터를 수집하는 방법

209
00:21:11,540 --> 00:21:18,740
답변의 질은 높으나 사람이 직접 작성한 것이므로 비용이 많이 들고

210
00:21:18,740 --> 00:21:25,860
다른 방법은 인간이 관여하지만 데이터는 주로 기계에 의해 생성되므로

211
00:21:25,860 --> 00:21:33,700
채팅 GPT와 같은 강력한 LLM 기반 교사가 필요하므로 비용이 더 저렴합니다.

212
00:21:34,340 --> 00:21:42,820
이제 GPT 채팅을 사용하여 명령어 튜닝 데이터를 생성하는 방법을 살펴보겠습니다. 시드를 볼 수 있습니다.

213
00:21:42,820 --> 00:21:47,540
여기서 명령 출력 쌍은 인간의 행동과 행동의 외침과 참조를 설명합니다.

214
00:21:47,540 --> 00:21:54,340
대답은 인간이 제공하고 다른 사람도 나에게 영화를 추천하고

215
00:21:54,340 --> 00:21:59,860
또 다른 대답이 주어지면 모델에 새로운 명령어 출력 쌍을 생성해 달라고 요청합니다.

216
00:21:59,860 --> 00:22:08,020
다음 요구 사항을 충족하고 이전 LLM 중 일부를 생성하기 시작합니다.

217
00:22:08,020 --> 00:22:16,180
이 기술은 알파카용 alpaca vicona GPT이며 알파카의 데이터 소스는 다음과 같습니다.

218
00:22:16,180 --> 00:22:24,180
비코나용 GPT 3.5는 GPT와 인간이 관련된 공유 GPT이고 알파카용 GPT입니다.

219
00:22:24,180 --> 00:22:31,300
텍스트에만 GPT를 사용하고 툴루에는 혼합된 데이터 소스가 있습니다.

220
00:22:33,780 --> 00:22:41,380
이제 시각적 지침 조정을 위해 모델에 원하는 지침과 이미지를 제공하겠습니다.

221
00:22:41,380 --> 00:22:49,060
지침을 따르고 원하는 출력을 제공하려면 세 가지 구성 요소인 시각적 인코더가 필요합니다.

222
00:22:49,780 --> 00:22:55,540
크로스 모달 커넥터와 언어 디코더는 우리에게 강력한 기능이 필요한 것입니다.

223
00:22:55,540 --> 00:23:03,460
사전 훈련된 비전 및 언어 모델 교차 모달 커넥터를 확보한 후

224
00:23:04,500 --> 00:23:12,100
미세 조정하고 다중 모드 지침을 따르도록 모델을 조정하지만 문제는 무엇입니까?

225
00:23:12,100 --> 00:23:18,260
문제는 우리가 이러한 교육을 생성하는 데 사용된 모든 이전 교사를 보았듯이

226
00:23:18,260 --> 00:23:26,100
다음 데이터는 실제로 GPT 3.5 및 기타 모델과 같은 텍스트 전용 모델이므로 어떻게 해야 할까요?

227
00:23:26,100 --> 00:23:32,900
저자가 제안한 GPT 지원 시각적 지침 데이터 생성과 같은 데이터 생성

228
00:23:34,100 --> 00:23:40,660
이 슬라이드에서 논의할 내용은 여기에 실제로 이미지가 있는 것처럼 볼 수 있습니다.

229
00:23:40,660 --> 00:23:49,060
그들은 맥락에서 Microsoft Coco 공통 개체를 사용합니다. 이것은 해당 데이터 세트의 이미지 중 하나이며

230
00:23:49,060 --> 00:23:54,660
두 가지 맥락은 검정색 밖에 서 있는 한 무리의 사람들의 이미지를 설명하는 캡션입니다.

231
00:23:54,660 --> 00:23:59,700
쌀 수하물이 있는 차량과 레이아웃인 또 다른 컨텍스트가 제공됩니다.

232
00:24:00,420 --> 00:24:09,140
기본적으로 개체가 상자로 둘러싸여 있고 상자의 정보나 좌표가 있음을 알 수 있습니다.

233
00:24:10,100 --> 00:24:13,780
또한 이 사람들이 직면할 수 있는 어려움이 무엇인지에 대한 지침도 있습니다.

234
00:24:13,780 --> 00:24:18,820
그런 다음 모든 수하물을 뒤쪽에 넣는 데 어려움을 겪을 수 있다는 결과가 있습니다.

235
00:24:18,820 --> 00:24:29,620
이 uv 등 GPT 4가 사용되는 방식은 캡션 컨텍스트가 제공되고

236
00:24:29,620 --> 00:24:39,220
레이아웃 컨텍스트가 주어지고 지침과 출력도 제공되며 몇 가지를 제공합니다.

237
00:24:39,220 --> 00:24:45,460
시각적 컨텍스트 명령 출력 삼중항의 컨텍스트 예에서 몇 가지 더 추가한 다음

238
00:24:46,820 --> 00:24:53,940
새 이미지에 대해 캡션에 대한 새 컨텍스트를 레이아웃에 대한 새 컨텍스트로 제공한 다음

239
00:24:53,940 --> 00:24:58,900
모델에게 다음을 충족하는 새로운 컨텍스트 명령어 출력 삼중항을 생성하도록 요청하세요.

240
00:24:58,900 --> 00:25:07,940
요구 사항이 충족되고 GPT 4는 작성자가 만든 이 기술을 사용하여 이 데이터를 생성하기 시작합니다.

241
00:25:07,940 --> 00:25:17,860
lava 지침 158k에는 58k 대화 예제 23k 세부 설명 및 77k가 있습니다.

242
00:25:19,060 --> 00:25:26,900
단일 샘플이 여기에 동일한 이미지가 있는 것처럼 보이는지 확인하기 위한 복잡한 추론

243
00:25:26,900 --> 00:25:34,180
이전 슬라이드에는 캡션과 상자가 있습니다. 첫 번째 응답 유형은 대화입니다.

244
00:25:34,180 --> 00:25:39,780
예는 이미지에 어떤 유형의 차량이 등장하는지이고, 대답은 SUV입니다.

245
00:25:41,140 --> 00:25:46,900
이미지에 대한 자세한 설명이 있는 상세 설명 응답 유형이 있습니다.

246
00:25:46,900 --> 00:25:53,700
마지막은 질문이 무엇인지 알 수 있는 복잡한 추론 응답 유형입니다.

247
00:25:53,700 --> 00:25:59,540
이 사람들은 얼굴을 마주하고 대답은 글쎄 그들은 모든 짐을 뒤쪽에 넣는 데 어려움을 겪는다는 것입니다.

248
00:25:59,540 --> 00:26:10,260
SUV의 구조이므로 용암의 구조가 무엇인지 살펴보겠습니다. 용암에는 다시 세 가지 구성 요소가 있습니다. 비전

249
00:26:10,260 --> 00:26:19,140
클립형 인코더는 크고 14개의 이미지 패치를 사용합니다. 투영은 선형입니다.

250
00:26:19,140 --> 00:26:28,020
레이어와 언어 모델은 Vekona Loma가 될 수 있으며 MPT 채팅 등을 통해 해당 이미지를 볼 수 있습니다.

251
00:26:28,020 --> 00:26:34,260
여기로 와서 비전 인코더를 거치고 이에 의해 투영되는 임베딩이 생성됩니다.

252
00:26:35,220 --> 00:26:44,820
프로젝션 레이어를 새로운 임베딩에 추가하고 언어 언어 토큰을 따라 이러한 토큰을

253
00:26:44,820 --> 00:26:48,820
언어 모델 및 언어 모델은 필요한 출력을 생성합니다.

254
00:26:51,460 --> 00:26:57,860
용암 훈련에는 두 단계가 있습니다. 첫 번째 단계는 기능 정렬을 위한 사전 훈련입니다.

255
00:26:59,220 --> 00:27:06,100
언어 모델이 고정되어 있습니다. 비전 인코더가 고정되어 있으며 프로젝션 레이어가 다를 수 있습니다.

256
00:27:07,060 --> 00:27:14,900
해당 목적을 위해 사용되는 데이터는 595k를 갖는 필터링된 cc3m 하위 집합입니다.

257
00:27:15,940 --> 00:27:24,100
두 번째 단계는 비전 인코더를 다시 조정하는 엔드투엔드 시각적 명령입니다.

258
00:27:24,100 --> 00:27:31,220
고정되어 있지만 프로젝션 레이어와 언어 모델은 다를 수 있으며 두 작업은

259
00:27:31,220 --> 00:27:38,900
용암인 이 엔드투엔드 시각적 지침 조정 단계 시각적 채팅은 158k에게 우리가

260
00:27:38,900 --> 00:27:45,380
방금 어떻게 생성되었는지 살펴보았는데 이는 개방형 사용자 중심의 시각적 작업과 기타 작업을 위한 것입니다.

261
00:27:45,380 --> 00:27:51,060
엔드투엔드 훈련에 사용하는 것은 과학 qa 과학을 위한 다중 모달 추론 데이터 세트입니다.

262
00:27:51,060 --> 00:28:02,260
훈련 용암 이후의 도메인은 우리가 데이터 세트에 대해 논의한 것처럼 몇 가지 새로운 속성을 보여줍니다.

263
00:28:02,260 --> 00:28:09,940
두 단계의 훈련에 사용된 것은 cc3m과 Instruction Tuning Coco였지만 이것들은 단지

264
00:28:09,940 --> 00:28:16,660
일반적인 개념이므로 도메인이 제한되어 있고 사람 이름 주석이 언급되지 않았습니다.

265
00:28:16,660 --> 00:28:24,740
명시적인 ocr은 없으며 ocr은 이미지의 텍스트와 같지만 용암이 다음을 수행할 수 있음을 알 수 있습니다.

266
00:28:24,740 --> 00:28:33,780
예를 들어 이름이 무엇인지 물었을 때 이러한 점과 관련된 일부 작업을 처리합니다.

267
00:28:33,780 --> 00:28:40,580
여기 있는 사람은 이것이 Elon Musk라고 말합니다. 심지어는 더 어려운 상황에서도

268
00:28:40,580 --> 00:28:46,180
그가 Elon Musk의 모델로 교체된 모델은 여전히 ​​그를 식별할 수 있습니다.

269
00:28:48,900 --> 00:28:57,460
평가를 위해 저자는 다시 GPT 지원 방법을 제안하고 우리가 볼 수 있듯이

270
00:28:57,460 --> 00:29:03,540
캡션 컨텍스트를 제공하고 레이아웃 컨텍스트를 제공하고 지침을 제공하며

271
00:29:03,540 --> 00:29:10,740
예를 들어 모델 1 모델 2와 같은 다른 모델에서 생성된 출력을 제공하고 제공한 후

272
00:29:10,740 --> 00:29:15,380
다양한 모델의 출력에 대해 모델에 문의하고 싶습니다. 이에 대한 피드백을 요청하고 싶습니다.

273
00:29:15,380 --> 00:29:21,300
다음 요구 사항에 따라 두 가지 AI 지원 지원 성능이 향상되고 모델이 시작됩니다.

274
00:29:21,300 --> 00:29:28,980
모델 1에 점수를 부여하고 모델 2에 점수를 부여했으며 작성자도 모델을 요구했습니다.

275
00:29:28,980 --> 00:29:36,180
그것이 제공하는 점수에 대한 직관이나 추론을 제공하여

276
00:29:36,180 --> 00:29:41,060
예를 들어 보조자 1이 다음 질문에 간결하고 정확한 답변을 제공한다고 말합니다.

277
00:29:41,060 --> 00:29:47,220
그러나 두 번째 보조자는 사기 개념을 식별했을 뿐만 아니라 그 이상을 수행했습니다.

278
00:29:47,220 --> 00:29:53,220
또한 그렇게 하면 점수가 일관되게 나오는지 확인하는 데 도움이 됩니다.

279
00:29:54,020 --> 00:30:01,460
평가를 위해 그들은 또 다른 벤치마크인 낮은 벤치를 만들거나 야생에서 이들의 기능을 만듭니다.

280
00:30:01,460 --> 00:30:08,980
벤치마크는 까다로운 데이터 세트로 설계되었습니다.

281
00:30:08,980 --> 00:30:15,220
훈련 데이터 이상의 지식도 다국어 이해와 인식이 필요합니다.

282
00:30:15,220 --> 00:30:24,900
평가하는 동안 까다로운 기능이 있기 때문에 미묘한 세부 사항을 파악할 수 있습니다.

283
00:30:25,540 --> 00:30:32,020
예를 들어 우리가 평가하는 모델의 일관성과 즉각적인 견고성을 평가하는 것과 같습니다.

284
00:30:32,020 --> 00:30:38,980
낮은 벤치의 예는 여기에서 볼 수 있습니다. 전통 중국 요리와 예를 들어

285
00:30:38,980 --> 00:30:43,460
묻는 질문 중 하나는 레스토랑의 이름이 무엇이며 모델이 해야 할 일이 무엇인지입니다.

286
00:30:43,460 --> 00:30:50,820
이해해요 여기에서 극에서 온 것처럼 한자를 추측하고 이름을 이해해요

287
00:30:50,820 --> 00:30:56,420
레스토랑 이름을 말하면 또 다른 예로 냉장고를 살펴보겠습니다.

288
00:30:56,420 --> 00:31:03,380
음식이 가득한데 질문 중 하나는 블루베리 맛 요거트의 브랜드가 무엇인지입니다.

289
00:31:03,380 --> 00:31:07,940
모델은 브랜드를 읽고 말할 수 있어야 합니다.

290
00:31:08,740 --> 00:31:18,820
여기에서 용암 벤치에 대한 평가 결과를 볼 수 있습니다. 용암이 블립보다 성능이 더 뛰어나다는 것을 알 수 있습니다.

291
00:31:18,820 --> 00:31:25,780
대화의 상세한 설명과 복잡한 추론을 위해 플라밍고를 지속적으로 열고 오픈합니다.

292
00:31:25,780 --> 00:31:34,100
전반적으로 우리가 논의할 다음 것은 blip 명령이므로 먼저 무엇이 가능한지 살펴보겠습니다.

293
00:31:34,100 --> 00:31:39,860
예를 들어 현재 장면을 기준으로 무슨 일이 일어날 수 있었는지 물었을 때 어 여기서 하는 것

294
00:31:39,860 --> 00:31:44,740
모델은 이미지의 현재 장면을 기반으로 허리케인이 발생할 가능성이 있다고 말할 수 있습니다.

295
00:31:44,740 --> 00:31:51,860
또는 악천후로 인해 심각한 피해가 발생하는 경우 등 예를 들어 그림이 있습니다.

296
00:31:51,860 --> 00:31:58,900
그리고 이 그림을 소개해달라고 하면 그림의 이름을 말할 수도 있습니다.

297
00:31:58,900 --> 00:32:08,820
이 예술적 그림을 그린 화가의 이름을 지정하고 모델도 설명할 수 있습니다.

298
00:32:08,820 --> 00:32:17,060
예를 들어 그림의 흰색 진주 귀걸이와 기타 특징을 자세히 언급했습니다.

299
00:32:18,660 --> 00:32:24,100
그래서 Instruct Blip의 동기는 무엇입니까? 목표는 작성자가 단일

300
00:32:24,100 --> 00:32:31,300
사용자가 지정한 임의의 작업을 해결하기 위한 모델을 사용하면 lm의 명령 조정이

301
00:32:31,300 --> 00:32:38,100
유망하며 최근에는 블립 같은 작품에도 활용되었지만 그들은 이것이

302
00:32:38,100 --> 00:32:45,060
다양한 비전 언어 작업을 예비하고 일반화하는 것은 여전히 ​​​​어려운 과제입니다.

303
00:32:45,060 --> 00:32:53,140
이러한 목표를 달성하기 위해 두 가지 접근 방식이 사용됩니다. 첫 번째 접근 방식은 멀티태스킹입니다.

304
00:32:53,140 --> 00:33:01,060
학습 어 그들이 공식화하는 곳에서 모든 작업을 동일한 형식과 문제로 공식화합니다.

305
00:33:01,060 --> 00:33:06,900
이 접근 방식은 다른 접근 방식이 확장하는 보이지 않는 작업으로 일반화되지 않는다는 것입니다.

306
00:33:06,900 --> 00:33:12,900
시각적 구성 요소가 포함된 사전 학습된 작품을 만들고 앞서 본 것처럼 이미지 캡션 데이터를 사용하여 학습시킵니다.

307
00:33:12,900 --> 00:33:19,220
플라밍고의 예를 들자면, 이 모델들이 안고 있는 문제는 어려움을 겪고 있다는 것입니다.

308
00:33:20,100 --> 00:33:22,580
시각적 설명 작업을 넘어 일반화

309
00:33:27,060 --> 00:33:34,500
저자가 사용하는 작업 및 데이터 세트는 이 슬라이드에 설명되어 있습니다.

310
00:33:34,500 --> 00:33:40,420
11가지 비전 언어 카테고리 카테고리는 다음과 같습니다. 이미지 캡션 시각적 추론 시각적

311
00:33:40,420 --> 00:33:47,700
대화형 QA 및 기타 범주이므로 이들 각각의 데이터 세트를 계산합니다.

312
00:33:47,700 --> 00:33:55,300
카테고리 전체에는 26개의 공개적으로 사용 가능한 데이터 세트가 있습니다. 이러한 데이터 세트는 다음과 같이 변환됩니다.

313
00:33:55,300 --> 00:34:02,580
보시다시피 명령 튜닝 형식과 10~15개의 명령 템플릿이 모든 작업에 사용됩니다.

314
00:34:02,580 --> 00:34:09,700
데이터 세트에 13개의 노란색 데이터 세트가 있고 흰색 데이터 세트가 있습니다.

315
00:34:10,660 --> 00:34:17,700
그리고 저자가 4가지 작업 범주를 보류한 것 외에 13개의 데이터 세트가 있습니다.

316
00:34:18,820 --> 00:34:26,980
예를 들어 모든 데이터 세트를 시각적으로 추론하는 것과 같은 테스트 목적으로만 사용합니다.

317
00:34:26,980 --> 00:34:33,380
모델에 응답하는 비디오 질문이 완전히 보류되어 있으므로 이러한 데이터 세트가 전혀 표시되지 않습니다.

318
00:34:33,460 --> 00:34:40,260
전체 작업 카테고리가 표시되지 않습니다. 표시되지 않는 다른 작업 카테고리가 표시됩니다.

319
00:34:40,260 --> 00:34:52,500
모델별 이미지 분류 및 시각적 대화 QA이므로 해당 모델은 처음입니다.

320
00:34:52,500 --> 00:35:01,220
테스트 중에 이러한 내용이 표시됩니다. 여기서는 다음에 사용되는 아키텍처를 볼 수 있습니다.

321
00:35:01,220 --> 00:35:09,060
블립에게 다시 이미지 인코더가 있다고 지시하세요. 거기에는 고정된 대규모 언어 모델이 있습니다.

322
00:35:09,060 --> 00:35:15,140
또한 그것은 인코더의 경우이고 qformer가 있으며 qformer에 대해서도 다시 볼 수 있습니다.

323
00:35:15,140 --> 00:35:23,380
학습된 쿼리가 있고 지침에 따라 학습된 쿼리가 qformer로 전달됩니다.

324
00:35:23,380 --> 00:35:30,420
qformer 내부의 qformer 내부에서 무슨 일이 일어나는지 봅시다. 쿼리가 있습니다. 쿼리를 배웠습니다.

325
00:35:30,500 --> 00:35:37,060
self-attention 레이어와 출력을 거치는 토큰 형태의 지침이 있습니다.

326
00:35:37,060 --> 00:35:44,740
교차 주의로 이동합니다. 이 교차 주의에 대한 다른 입력은 이미지 임베딩입니다.

327
00:35:44,740 --> 00:35:50,180
이미지 인코더에 의해 생성되고 교차 주의 후에 피드포워드를 거친 다음

328
00:35:50,180 --> 00:35:57,300
출력에 새로 생성된 토큰과 유사하므로 새로 생성된 토큰은 다음을 거칩니다.

329
00:35:57,300 --> 00:36:05,860
완전히 연결된 레이어에는 지침과 함께 또 다른 출력 세트가 있습니다.

330
00:36:05,860 --> 00:36:12,340
그것들은 대규모 언어 모델에 주어지고 이에 대한 참신함으로 응답이 생성됩니다.

331
00:36:12,340 --> 00:36:18,580
qformer의 self-attention을 통해 명령 토큰이 쿼리 임베딩과 상호 작용하는 방법을 문서화합니다.

332
00:36:19,220 --> 00:36:23,940
참신한 점은 명령을 통해 시각적 특징을 추출할 수 있다는 점입니다.

333
00:36:25,940 --> 00:36:34,420
몇 가지 구현 세부 사항이 있습니다. 세부 사항 중 하나는 균형 교육과 관련되어 있습니다.

334
00:36:34,420 --> 00:36:41,460
저자는 작은 데이터 세트에 대한 과적합과 큰 데이터 세트에 대한 과소적합을 방지하기 위해 데이터 세트를 사용한다고 말합니다.

335
00:36:41,460 --> 00:36:47,700
그들은 샘플링 전략을 사용하며 그들의 전략은 기본적으로 s1부터 sd까지 있는 경우입니다.

336
00:36:48,980 --> 00:36:56,660
데이터세트 각 데이터세트에서 샘플링할 확률은 다음의 제곱근에 비례합니다.

337
00:36:56,660 --> 00:37:02,580
다른 데이터세트 크기의 제곱근 합계에 대한 해당 데이터세트의 크기

338
00:37:04,100 --> 00:37:10,020
다른 하나는 2세대 접근 방식을 사용하는 열등한 방법과 관련이 있습니다.

339
00:37:10,580 --> 00:37:17,780
이미지 캡션 모델과 같은 대부분의 데이터 세트의 경우 생성하라는 메시지가 직접 표시됩니다.

340
00:37:17,780 --> 00:37:24,340
분류 및 객관식 vqa와 같은 다른 작업에는 어휘 순위가 사용됩니다.

341
00:37:25,700 --> 00:37:31,540
다른 하나는 아키텍처와 이미지 인코더에 대한 세부 정보가 있습니다.

342
00:37:31,540 --> 00:37:39,540
그들이 사용하는 것은 14개의 이미지 패치가 있는 vitg입니다. 대규모 언어 모델은 flanty5입니다.

343
00:37:39,540 --> 00:37:47,860
t5 또는 vicona 디코더 전용 변압기에서 조정된 인코더 디코더 변압기 명령

344
00:37:47,860 --> 00:37:55,380
loma에서 조정된 지침 여기서는 지침 블립에 대한 평가를 볼 수 있습니다.

345
00:37:56,100 --> 00:38:02,180
지시 블립 모델은 여기에 있으며 이들은 blip2 모델과 플라밍고이며 우리는 그것을 볼 수 있습니다.

346
00:38:02,260 --> 00:38:11,220
캡션 시각적 질문 답변과 같은 다양한 작업 및 과학적인 작업과 같은 기타 작업

347
00:38:11,220 --> 00:38:20,020
질문에 대답하는 지침 블립 모델은 다음과 같은 다른 모델보다 지속적으로 성능이 뛰어납니다.

348
00:38:20,020 --> 00:38:30,340
blip2와 flamingo에 대해 마지막으로 논의할 것은 폴리입니다. 한 번 볼까요?

349
00:38:30,340 --> 00:38:36,340
예를 들어 이 이미지에 대한 정성적 예는 영어로 출력 텍스트를 생성하라는 요청을 받을 때입니다.

350
00:38:36,340 --> 00:38:42,020
와인통으로 가득 찬 지하실을 생성할 수 있으며 우리는 다른 영어 몇 가지를 볼 수 있습니다

351
00:38:42,020 --> 00:38:51,060
예를 들어 여기에서는 프랑스어로 태국어로 텍스트를 생성할 수도 있습니다.

352
00:38:51,860 --> 00:39:01,060
그리고 여기 마지막 이미지의 중국어에서는 저자의 폴리 뒤에 숨은 동기가 무엇인가요?

353
00:39:01,060 --> 00:39:07,060
예를 들어 언어 분야에서는 네트워크 용량을 늘리는 것이 성공적인 추세라고 합니다.

354
00:39:07,060 --> 00:39:16,260
모델 t5 및 gpt3는 vit와 같은 뛰어난 성능의 비전 모델을 보유하고 있으며 크기가 커질수록 성능이 향상됩니다.

355
00:39:16,260 --> 00:39:23,300
성능은 플라밍고와 같은 언어 비전 모델의 경우에도 마찬가지입니다.

356
00:39:23,300 --> 00:39:29,620
일반적으로 대용량 언어 비전 모델에서는 규모 분포가 공평하지 않습니다.

357
00:39:30,660 --> 00:39:36,980
언어 구성 요소가 훨씬 더 크고 다른 문제는 이러한 모델이 영어로만 제공된다는 것입니다.

358
00:39:37,940 --> 00:39:48,420
따라서 이러한 문제가 주어지면 폴리는 다음 목표를 충족하도록 설계되었습니다.

359
00:39:49,860 --> 00:39:56,900
그들은 대규모 단봉 백본을 재사용하고 싶어하며 모델이 공동으로 이점을 얻길 원합니다.

360
00:39:56,900 --> 00:40:03,140
언어와 비전 간의 보다 균형 잡힌 매개변수 공유로 비전과 언어 확장

361
00:40:03,220 --> 00:40:09,380
구성 요소를 캐스팅하여 작업 간의 지식 공유를 활성화하려고 합니다.

362
00:40:09,380 --> 00:40:19,940
작업과 같은 일반화된 vqa를 수행하고 수백억 개의 새로운 대용량 데이터 세트를 훈련합니다.

363
00:40:19,940 --> 00:40:25,140
영어뿐만 아니라 100개 언어의 이미지 납세자

364
00:40:25,300 --> 00:40:35,140
그러면 단일 모드와 다중 모드를 모두 수행하는 것을 목표로 하는 폴리를 위해 설계된 폴리가 무엇인지 요약해 보겠습니다.

365
00:40:35,140 --> 00:40:41,460
작업은 모든 작업을 작업과 같은 일반화된 vqa로 캐스팅하여 지식 공유를 가능하게 합니다.

366
00:40:42,500 --> 00:40:48,500
기본적으로 프레임워크는 다음과 같습니다. 이미지와 텍스트 문자열이 입력되고 출력에는

367
00:40:48,500 --> 00:40:58,820
텍스트를 전달하며 사전 훈련된 단일 모드 모델을 사용하여 해당 모델의 기존 기능을 전송합니다.

368
00:40:58,820 --> 00:41:07,300
또한 훈련 비용을 줄이고 시각적 토큰은 크로스를 통해 인코더 디코더로 전달됩니다.

369
00:41:07,300 --> 00:41:12,580
이전에 본 일부 모델과 유사한 방식으로 주목한 것은 아키텍처입니다.

370
00:41:12,660 --> 00:41:19,300
기본적으로 비전 인코더 부분으로 AV가 있고 트랜스포머 인코더와 디코더가 있습니다.

371
00:41:20,180 --> 00:41:26,260
기본적으로 폴리에 대한 비전 언어 작업을 가능하게 합니다.

372
00:41:28,180 --> 00:41:35,860
각 구성 요소가 무엇인지 더 자세히 살펴보겠습니다. 비전을 위해 다음을 볼 수 있습니다.

373
00:41:35,860 --> 00:41:43,460
여기서 av it 구성 요소와 같이 그들이 사용하는 구성 요소는 가장 큰 바닐라 v it입니다.

374
00:41:43,460 --> 00:41:49,780
그들은 v를 거대하다고 부릅니다. 훈련된 것 중 가장 큰 것입니다. 40억 개의 매개변수가 있습니다.

375
00:41:50,900 --> 00:41:59,540
그리고 그들은 다중 모드 데이터에서 이를 확장하면 포화되지 않을 뿐만 아니라 더 높은 결과를 얻을 수 있다고 말합니다.

376
00:41:59,620 --> 00:42:06,980
매개변수 또는 플롭 수당 정확도 향상 측면에서 우리는 잘 알고 있습니다.

377
00:42:06,980 --> 00:42:15,620
이는 일반적으로 실제로 유사한 작업에서 v 규모의 일부 임계값을 초과하는 경우가 아닙니다.

378
00:42:15,620 --> 00:42:22,180
분류와 같은 것은 더 나은 성능을 제공하지 않지만 그것은 사실이 아니라고 말합니다.

379
00:42:22,180 --> 00:42:32,420
이 경우 다중 모드 데이터가 있는 경우 언어 구성 요소는 mt5 백본입니다.

380
00:42:32,420 --> 00:42:41,460
치명적인 망각을 방지하기 위해 여러 가지 작업을 혼합하여 훈련했으며 이러한 포인트가 전체적으로 주어졌습니다.

381
00:42:41,460 --> 00:42:50,900
모델은 거대하거나 mt5이거나 언어 구성 요소 중 하나일 것입니다.

382
00:42:50,900 --> 00:42:58,260
xx 크기가 커서 폴리가 훈련된 데이터는 다음과 같습니다. 이것은 몇 가지 예입니다.

383
00:42:59,300 --> 00:43:05,780
이것은 예를 들어 매트릭스와 조수 OCR 카드의 무료 스톡 사진입니다. 카드가 작성되었습니다.

384
00:43:05,780 --> 00:43:15,060
여기에 통신이 적혀 있고 여기에 번호가 5624가 있고 여기에는 이런 것이 있습니다.

385
00:43:15,620 --> 00:43:22,260
이미지와 출력 텍스트가 프랑스어로 되어 있고 프랑스어로 된 OCR도 있고 이 이미지가 있습니다.

386
00:43:22,260 --> 00:43:30,820
이에 대한 텍스트를 묶고 이 장면에 대한 텍스트를 중국어로 묶어서 다음과 같은 몇 가지 예를 보여줍니다.

387
00:43:30,820 --> 00:43:36,820
그들이 사용하는 데이터 세트는 공개된 이미지 텍스트로 구축된 웹 데이터 세트라고 합니다.

388
00:43:36,820 --> 00:43:45,620
109개 언어를 다루는 웹에는 100억 개의 이미지, 120억 개의 텍스트 및 290억 개의 이미지 OCR 쌍이 있습니다.

389
00:43:47,060 --> 00:43:54,260
하지만 상위 10% 점수만 사용되며 이는 폴리 훈련에 약 10억 개에 해당합니다.

390
00:43:55,620 --> 00:44:04,580
여기에 우리가 처음부터 추측할 수 있었던 것처럼 볼 수 있는 데이터 세트의 유사한 통계가 있습니다.

391
00:44:04,580 --> 00:44:11,300
가장 큰 덩어리는 영어, 일본어, 중국어 및 기타 언어 및 일부 매체용입니다.

392
00:44:11,300 --> 00:44:18,100
터키어 및 말레이어와 같은 크기의 언어와 언어에 대한 데이터 시체와 같은 일부 작은 언어

393
00:44:18,100 --> 00:44:25,540
oigur와 같은 데이터 세트를 사용하면 폴리에서 사용하는 데이터 세트와

394
00:44:25,540 --> 00:44:32,020
예를 들어 이전에 사용된 데이터 세트는 18억 라인에 대해 4억 클립에 대한 데이터 세트가 있습니다.

395
00:44:32,020 --> 00:44:38,420
라이트 40억이지만 마지막 두 열은 폴리 웹블리용입니다. 기본적으로 텍스트에는 120억이 있습니다.

396
00:44:38,420 --> 00:44:48,340
webbly ocr에는 290억 쌍이 있고 이것들은 실험 결과입니다.

397
00:44:48,340 --> 00:44:59,620
죄송합니다. 많은 작업 카테고리에 대해 비슷한 실험 결과가 몇 가지 있지만 예를 들어 보겠습니다.

398
00:44:59,620 --> 00:45:07,940
캡션 결과를 여기로 가져왔습니다. Poly가 일관되게 캡션을 출력할 수 있다는 것을 알 수 있습니다.

399
00:45:07,940 --> 00:45:16,900
여기에서 플라밍고를 포함한 이전 방법과 내가 수행한 다른 실험 결과를 수행합니다.

400
00:45:16,900 --> 00:45:25,220
여기에 가져온 것은 크기 조정의 효과에 관한 것입니다. 여기에는 세 가지 색상이 있고

401
00:45:25,940 --> 00:45:34,260
세로축은 이와 같은 모델 간의 절대 점수 차이를 보여줍니다.

402
00:45:35,860 --> 00:45:43,540
여기서는 파란색으로 표시되고 해당 모델의 다른 확장 버전에서는 파란색으로 표시됩니다.

403
00:45:43,540 --> 00:45:51,380
기본적으로 언어 구성 요소가 큰 30억 개의 매개변수가 있는 폴리를 보여줍니다.

404
00:45:52,260 --> 00:45:58,820
비전 구성 요소는 녹색이고 빨간색은 150억 개의 폴리를 나타냅니다.

405
00:45:59,540 --> 00:46:06,580
매개변수 언어 구성요소는 xx 크고 비전 구성요소는 vitg이며 이것이 있습니다.

406
00:46:06,580 --> 00:46:13,780
170억 개의 매개변수가 있는 폴리를 표시하는 노란색, 언어 구성 요소는 xx 크고

407
00:46:14,260 --> 00:46:22,980
비전 인코더는 엄청 크며 노란색 테두리가 있는 흰색도 있습니다.

408
00:46:24,100 --> 00:46:31,300
노란색 모델과 마찬가지로 170억 개의 매개변수가 있는 동일한 폴리 모델을 보여주지만 더 높은 수치가 있습니다.

409
00:46:31,300 --> 00:46:38,740
비전 구성요소와 관련된 해상도 훈련 단계를 통해 다음과 같은 작업에 대해 이를 확인할 수 있습니다.

410
00:46:38,740 --> 00:46:49,300
캡션은 실제로 비전 구성 요소를 확장하는 세 가지 첫 번째 작업과 같습니다.

411
00:46:49,300 --> 00:46:57,540
언어 구성 요소를 확장하는 것과 비교하여 훨씬 더 나은 절대 점수 차이를 제공합니다.

412
00:46:57,540 --> 00:47:05,540
시각적 질문 응답과 같은 다른 작업의 경우 언어 확장과 같은 것을 볼 수 있습니다.

413
00:47:05,540 --> 00:47:12,980
언어 구성 요소는 절대 점수를 더 높일 수 있지만 전반적으로 평균적으로 우리는

414
00:47:12,980 --> 00:47:20,020
비전 구성 요소를 확장하면 더 나은 수익을 얻을 수 있음을 알 수 있습니다.

415
00:47:23,540 --> 00:47:31,780
그리고 여기에서 다음 4개의 다중 모드 모델에 대해 yunyi는 계속해서 감사할 것입니다.

416
00:47:31,780 --> 00:47:34,020
주목