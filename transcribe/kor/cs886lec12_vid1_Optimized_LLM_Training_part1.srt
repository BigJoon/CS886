1
00:00:00,000 --> 00:00:05,320
좋습니다. 여러분 안녕하세요. 12번째 강의에 오신 것을 환영합니다.

2
00:00:05,320 --> 00:00:14,240
이번 강의에서는 대규모 언어 모델의 최적화된 학습에 대해 논의합니다.

3
00:00:14,240 --> 00:00:21,160
이번편에서는 GPU 활용도 극대화와 관련된 부분을 소개해드리겠습니다.

4
00:00:21,160 --> 00:00:24,640
효율적이고 확장 가능한 교육을 위한 것입니다.

5
00:00:24,640 --> 00:00:27,880
이제 우리는 왜 이런 일을 하고 있는 걸까요?

6
00:00:28,320 --> 00:00:32,640
우선, 더 큰 모델을 훈련시키면서 여전히 많은 개선이 이루어지고 있습니다.

7
00:00:32,640 --> 00:00:34,400
더 큰 언어 모델.

8
00:00:34,400 --> 00:00:44,120
그리고 현재로서는 현재 GPU는 하나의 GPU에서 14억 개가 넘는 매개변수를 훈련할 수 없습니다.

9
00:00:44,120 --> 00:00:49,800
그렇다면 여러 GPU에서 더 큰 모델을 어떻게 달성할 수 있을까요?

10
00:00:49,800 --> 00:00:51,360
어떻게 나눌 수 있나요?

11
00:00:51,360 --> 00:00:54,160
이것이 오늘 프레젠테이션의 주요 주제가 될 것입니다.

12
00:00:54,160 --> 00:00:59,560
우리는 문제에 많은 GPU를 투입하고 문제가 해결될 것이라고 생각할 수 없습니다.

13
00:00:59,560 --> 00:01:06,800
이 프레젠테이션을 마친 후에는 여기에 적용된 엔지니어링에 대해 감사하게 될 것입니다.

14
00:01:06,800 --> 00:01:14,080
여기에는 약간의 야심찬 개요가 있지만 병렬성에 대해 이야기하겠습니다.

15
00:01:14,080 --> 00:01:14,600
기초 요소.

16
00:01:14,600 --> 00:01:19,040
그래서 우리는 데이터 병렬성, 파이프라인 병렬성, 모델 병렬성,

17
00:01:19,040 --> 00:01:22,160
그리고 마지막으로 제로 옵티마이저입니다.

18
00:01:22,160 --> 00:01:26,840
그리고 우리는 이러한 것들에 필요한 몇 가지 최적화에 대해서도 이야기할 것입니다.

19
00:01:26,840 --> 00:01:31,720
활성화 체크포인트 및 혼합 정밀 훈련과 같은 작업을 수행합니다.

20
00:01:31,720 --> 00:01:36,680
먼저, 데이터 병렬성에 대해 이야기해 보겠습니다.

21
00:01:36,680 --> 00:01:39,040
데이터 병렬성이란 무엇입니까?

22
00:01:39,040 --> 00:01:47,280
데이터 병렬화는 데이터 병렬화의 일부 형태를 최초로 직접적으로 구현한 것입니다.

23
00:01:47,280 --> 00:01:49,320
다중 GPU.

24
00:01:49,320 --> 00:01:58,520
따라서 아이디어는 미니 배치가 주어지면 여러 개로 배포하여 미니 배치를 분할하는 것입니다.

25
00:01:58,520 --> 00:02:02,520
모든 GPU에 대해 여러 개의 마이크로 배치로 구성됩니다.

26
00:02:02,520 --> 00:02:06,840
그리고 기본적으로 모든 GPU에는 모델의 복사본이 있습니다.

27
00:02:06,840 --> 00:02:12,200
이제 모든 GPU는 순방향 패스, 역방향 패스를 수행하고 기울기를 계산한 다음

28
00:02:12,200 --> 00:02:18,240
매개변수 서버를 사용하여 이러한 그라데이션을 함께 집계할 수 있습니다.

29
00:02:18,240 --> 00:02:24,000
GPU이거나 어떤 형태의 오래된 축소 작업을 수행합니다.

30
00:02:24,000 --> 00:02:29,640
이런 방식으로 그들은 모든 경사도를 함께 줄인 다음 업데이트된 내용을 방송합니다.

31
00:02:29,640 --> 00:02:36,760
그들 모두에게 모델을 적용한 후 다음 미니 배치로 넘어갑니다.

32
00:02:36,760 --> 00:02:41,600
이것의 좋은 점은 즉시 얻을 수 있다는 것입니다. 따라서 이 세 가지 GPU의 경우

33
00:02:41,600 --> 00:02:46,040
3개의 추가 처리량을 얻습니다.

34
00:02:46,040 --> 00:02:50,240
그리고 꼭 해야 할 디자인도 없습니다.

35
00:03:16,040 --> 00:03:34,080
파이프라인 병렬 처리를 시작하세요.

36
00:03:34,080 --> 00:03:45,840
GPype 및 PyveDream에 도입되었으며 아이디어는 모델을 갖는 대신

37
00:03:45,840 --> 00:03:50,840
모든 GPU에 복사되면 레이어 간 분할이 발생합니다.

38
00:03:50,840 --> 00:03:54,840
따라서 모델을 분할하면 첫 번째 레이어는 첫 번째 GPU가 아니고 두 번째 레이어는 GPU가 아닙니다.

39
00:03:54,840 --> 00:03:59,440
두 번째 GPU, 세 번째 레이어는 세 번째 GPU가 아닙니다.

40
00:03:59,440 --> 00:04:05,120
그리고 바라건대, 모든 레이어에는 가중치 장치 수만큼의 장치가 있을 것입니다.

41
00:04:05,120 --> 00:04:15,360
따라서 메모리가 더 적으므로 미니 배치를 전달하고 이를 다음과 같이 분할합니다.

42
00:04:15,360 --> 00:04:21,480
3개의 마이크로 배치를 파이프라인에서 순서대로 전달합니다.

43
00:04:21,480 --> 00:04:30,080
하지만 이렇게 하는 순진한 방법은 효과가 없습니다. 왜냐하면 단지 앞으로 나아가면 되기 때문입니다.

44
00:04:30,080 --> 00:04:34,960
첫 번째 레이어, 그 다음 앞으로, 두 번째 레이어, 세 번째 레이어, 네 번째 레이어, 그리고

45
00:04:34,960 --> 00:04:42,760
각각을 거꾸로 보면 기본적으로 하나의 GPU 성능을 갖게 됩니다.

46
00:04:42,760 --> 00:04:49,240
여기 있는 흰색 블록은 모두 유휴 상태이거나 거품입니다.

47
00:04:49,240 --> 00:04:52,160
그러면 어떻게 최적화할 수 있을까요?

48
00:04:52,160 --> 00:04:59,000
글쎄, 이것이 GPype의 아이디어입니다.

49
00:04:59,000 --> 00:05:09,000
아이디어는 첫 번째 배치가 진행되는 동안 그 후 다음 배치가 시작된다는 것입니다.

50
00:05:09,000 --> 00:05:14,560
파이프라인 방식으로 세 번째 배치가 들어가고 네 번째 배치가 들어가고

51
00:05:14,560 --> 00:05:20,120
아이디어는 우리가 이러한 전체 길이 열의 수를 최대화하고 싶다는 것입니다.

52
00:05:20,120 --> 00:05:26,400
전체 길이 열은 파이프라인을 완전히 활용한 경우이며 이는 최대입니다.

53
00:05:26,400 --> 00:05:32,280
우리가 원하는 활용.

54
00:05:32,280 --> 00:05:38,120
이것은 이 아이디어가 실행되는 광범위한 예입니다.

55
00:05:38,120 --> 00:05:40,600
그래서 우리는 8개의 마이크로 배치를 갖게 되었습니다.

56
00:05:40,600 --> 00:05:47,440
다음과 같은 방법으로 앞뒤로 인터리브할 것입니다.

57
00:05:47,440 --> 00:05:56,560
따라서 첫 번째 배치가 끝나는 순간 두 번째 배치가 계속됩니다.

58
00:05:56,560 --> 00:06:01,560
따라서 첫 번째 레이어, 두 번째 레이어, 세 번째 레이어, 네 번째 레이어에 대해 그런 다음 거꾸로 수행합니다.

59
00:06:01,560 --> 00:06:06,800
각각에 대해 본질적으로 이것이 당신이 가지고 있는 것입니다.

60
00:06:06,800 --> 00:06:15,880
전체 길이 열이 더 많아 처리량과 활용도가 더 높아집니다.

61
00:06:15,880 --> 00:06:23,440
이를 좀 더 확실하게 분석하고 싶다면 미니 배치당 버블 시간을 살펴보겠습니다.

62
00:06:23,440 --> 00:06:28,440
따라서 버블 시간은 장치 수에서 앞으로 + 뒤로 + 1을 뺀 값입니다.

63
00:06:28,440 --> 00:06:34,680
시간은 본질적으로 미니 배치가 완전히 진입하는 데 걸리는 시간입니다.

64
00:06:34,680 --> 00:06:38,360
파이프라인을 완전히 철수합니다.

65
00:06:38,360 --> 00:06:47,560
이상적으로 우리는 다음을 사용하고 싶습니다. 여기서 m은 마이크로 배치의 수이므로 우리는

66
00:06:47,560 --> 00:06:53,000
이상적인 시간은 m 곱하기 tf + tb 입니다.

67
00:06:53,000 --> 00:06:57,760
그래서 우리가 얻는 효율성은, 음, 계산을 하면,

68
00:06:57,760 --> 00:07:01,520
1 빼기 k 빼기 1 나누기 m.

69
00:07:01,520 --> 00:07:06,800
그리고 GPype에서는 경험적으로 거의 선형적인 스케일링을 위해서는 다음 숫자가 필요하다는 것을 알았습니다.

70
00:07:06,800 --> 00:07:13,480
마이크로 배치의 수는 장치 수의 4배 이상이어야 합니다.

71
00:07:13,480 --> 00:07:21,520
여기 분석에 이를 연결하면 사실상 75% 확장이 가능하다는 것을 알 수 있습니다.

72
00:07:21,520 --> 00:07:26,480
효율성은 충분합니다.

73
00:07:26,480 --> 00:07:35,360
그리고 m을 그 이상으로 늘리면 효율성이 높아집니다.

74
00:07:35,360 --> 00:07:41,320
그리고 파이프라인 병렬화는 파이프라인 수에 비례하여 메모리를 줄입니다.

75
00:07:41,320 --> 00:07:43,800
단계.

76
00:07:43,800 --> 00:07:48,920
따라서 모델 크기가 작업자 수에 따라 선형적으로 확장될 수 있으며,

77
00:07:48,920 --> 00:07:50,800
최저 통신량.

78
00:07:50,800 --> 00:07:56,600
그러나 파이프라인 크기를 늘리면 파이프라인이 줄어들기 때문에 무한정 확장할 수는 없습니다.

79
00:07:56,600 --> 00:08:02,360
파이프라인 단계당 계산.

80
00:08:02,360 --> 00:08:04,400
그리고 로드 밸런싱이 필요합니다.

81
00:08:04,400 --> 00:08:10,280
아키텍처 독립적이지만 약간의 코드 재작성이 필요합니다.

82
00:08:10,280 --> 00:08:14,080
그리고 메모리 쌍 장치는 최대 파티션이어야 합니다.

83
00:08:14,080 --> 00:08:22,440
레이어 너비와 모든 마이크로 배치에 대한 활성화가 저장되기 때문입니다.

84
00:08:22,440 --> 00:08:26,000
우리가 가는 대로.

85
00:08:26,000 --> 00:08:34,360
다른 방식으로 분할하는 다음 병렬 처리 아이디어를 입력합니다.

86
00:08:34,360 --> 00:08:41,880
레이어별로 분할하는 대신 레이어 자체를 가로로 분할합니다.

87
00:08:41,880 --> 00:08:49,080
그래서 우리는 여기서 첫 번째 레이어를 분할하고, 이 두 가지를 분할하고, 분할하는 식으로 진행합니다.

88
00:08:49,080 --> 00:08:53,920
그리고 아이디어는 우리가 모든 레이어를 병렬로 계산한다는 것입니다.

89
00:08:53,920 --> 00:09:02,040
그리고 이를 위해서는 실제로 이러한 블록을 구현하는 방법을 자세히 살펴보아야 하므로,

90
00:09:02,040 --> 00:09:09,880
우리는 종이 메가트론과 정확히 같은 트랜스포머 블록에만 초점을 맞출 것입니다.

91
00:09:09,880 --> 00:09:14,920
했다.

92
00:09:14,920 --> 00:09:25,800
그렇다면 대규모 언어 모델을 훈련하는 데 있어 주요 구성 요소는 무엇입니까?

93
00:09:25,800 --> 00:09:29,660
기본적으로 행렬 곱셈입니다.

94
00:09:29,660 --> 00:09:34,600
그래서 우리는 여기서 일반화된 행렬 곱셈에 초점을 맞추고 있습니다.

95
00:09:34,600 --> 00:09:39,320
그리고 논문에서는 두 가지 형태의 병렬성을 소개합니다.

96
00:09:39,960 --> 00:09:48,760
열 병렬 처리에서는 x 곱하기 a를 곱하고 이를 2번에 수행하려고 합니다.

97
00:09:48,760 --> 00:09:49,920
장치.

98
00:09:49,920 --> 00:09:56,520
그래서 당신이 하는 일은 열을 기준으로 나누는 것입니다. 따라서 여기에 첫 번째 열을 놓고

99
00:09:56,520 --> 00:10:04,480
여기에 두 번째 열을 입력하고 두 장치 모두에서 x를 복제한 다음 곱셈을 수행합니다.

100
00:10:04,480 --> 00:10:11,360
이제 곱셈을 하면 이 출력이 출력의 첫 번째 열이 됩니다.

101
00:10:11,360 --> 00:10:15,280
이 출력은 출력의 두 번째 열이 될 것이며 연결만 하면 됩니다.

102
00:10:15,280 --> 00:10:19,280
그들을.

103
00:10:19,280 --> 00:10:28,320
그러나 행 병렬 처리의 경우 열을 기준으로 분할하는 대신 행을 기준으로 분할합니다.

104
00:10:28,320 --> 00:10:33,820
따라서 상단을 첫 번째 장치로 보내고 하단을 두 번째 장치로 보냅니다.

105
00:10:33,820 --> 00:10:37,580
이번에는 x를 열로 나눕니다.

106
00:10:37,580 --> 00:10:43,260
따라서 왼쪽을 첫 번째 장치에 놓고 오른쪽을 두 번째 장치에 놓습니다.

107
00:10:43,260 --> 00:10:51,460
그리고 곱셈을 하면 이제 출력은 다음의 부분 합이 됩니다.

108
00:10:51,460 --> 00:10:52,460
최종 출력.

109
00:10:52,460 --> 00:11:01,900
따라서 이 항목은 이 모든 시간의 모든 시간이 되어야 하는 것과 같습니다.

110
00:11:01,900 --> 00:11:06,500
첫 번째 장치는 이번 시간에 컴퓨팅을 수행하게 됩니다.

111
00:11:06,500 --> 00:11:11,020
그래서 그것은 여기 x11, x12, a11, a21이 될 것입니다.

112
00:11:11,020 --> 00:11:16,860
이것과 이것을 곱하면 y1/1, 1이 됩니다.

113
00:11:16,860 --> 00:11:24,180
나머지를 얻으려면 두 번째 장치가 이 시간을 계산해야 합니다.

114
00:11:24,180 --> 00:11:28,820
여기에 있을 것이고 출력은 여기에 있을 것입니다.

115
00:11:29,220 --> 00:11:35,180
여기서 아이디어는 비용이 많이 드는 작업을 요약해야 한다는 것입니다.

116
00:11:35,180 --> 00:11:39,140
그리고 여러 장치에 걸쳐 합산한다는 것은 다음과 같은 형태의 통신이 필요하다는 것을 의미합니다.

117
00:11:39,140 --> 00:11:40,140
일어나다.

118
00:11:40,140 --> 00:11:43,460
연결, 논리적으로 연결을 수행할 수 있습니다.

119
00:11:43,460 --> 00:11:46,260
그들 사이에 의사소통이 있을 필요는 없습니다.

120
00:11:46,260 --> 00:11:51,900
텐서는 기계에 존재하지만 분할되는 것으로 생각할 수 있습니다.

121
00:11:51,900 --> 00:11:53,940
여기에 있는 동안에는 그런 생각을 할 수 없습니다.

122
00:11:53,940 --> 00:11:56,900
당신은 그들을 추가해야합니다.

123
00:11:56,900 --> 00:12:02,460
그렇다면 이것이 변압기에 어떻게 적용됩니까?

124
00:13:56,900 --> 00:14:01,340
이 아이디어는 기본적으로 병렬화한다는 점을 제외하면 self-attention 블록에 적용될 수 있습니다.

125
00:14:01,340 --> 00:14:02,500
지금 주목하세요.

126
00:14:02,500 --> 00:14:06,700
예를 들어 주의 헤드가 두 개만 있다고 가정해 보겠습니다.

127
00:14:06,700 --> 00:14:11,940
우리는 모든 주의를 별도의 장치에 집중시킬 것입니다.

128
00:14:11,940 --> 00:14:19,020
따라서 첫 번째 장치는 다시 열 병렬성을 사용하게 됩니다.

129
00:14:19,020 --> 00:14:25,420
x를 선택하고 v, q, k를 열별로 분할하겠습니다.

130
00:14:25,420 --> 00:14:31,300
그런 다음 주의를 기울이고 행 병렬성을 순서대로 사용합니다.

131
00:14:31,300 --> 00:14:41,100
self-attention이 끝나면 선형 레이어를 완성합니다.

132
00:14:41,100 --> 00:14:42,100
기본적으로 그게 다입니다.

133
00:14:42,100 --> 00:14:53,900
이것이 변압기 블록을 수평으로 병렬화하는 방법입니다.

134
00:14:53,900 --> 00:14:59,420
요약하면 기본적으로 두 개의 전체 축소 작업이 있습니다. 하나는 순방향, 다른 하나는 역방향입니다.

135
00:14:59,420 --> 00:15:05,140
어텐션 블록의 경우 모델 병렬 블록의 경우에도 동일합니다.

136
00:15:05,140 --> 00:15:11,100
전체적으로 축소된 작업을 수행하기 위한 것인데, 이는 많은 양입니다.

137
00:15:11,100 --> 00:15:21,700
따라서 병렬성에 대한 아이디어는 본질적으로 통신 비용 측면에서 매우 비쌉니다.

138
00:15:21,700 --> 00:15:26,780
따라서 논문의 결과는 음, 무엇보다도 먼저 다음을 작성할 수 있다는 것입니다.

139
00:15:26,780 --> 00:15:29,220
데이터 병렬성을 사용합니다.

140
00:15:29,220 --> 00:15:34,780
따라서 모델 병렬 자체의 확장 효율성은 77% 정도입니다.

141
00:15:34,780 --> 00:15:45,500
따라서 8개의 GPU에서는 8배의 성능을 얻는 대신 77% x 8배의 성능을 얻습니다.

142
00:15:45,500 --> 00:15:50,780
그리고 이를 데이터 병렬과 결합하면 512개의 GPU에서 74%를 얻게 됩니다.

143
00:15:50,780 --> 00:15:54,380
많이.

144
00:15:54,380 --> 00:16:00,540
그리고 이는 기본적으로 점점 더 큰 규모의 훈련에 사용할 수 있는 매우 높은 처리량을 제공합니다.

145
00:16:00,540 --> 00:16:05,740
언어 모델.

146
00:16:05,740 --> 00:16:11,780
하지만 우리가 말했듯이 통신량이 가장 높지만 메모리 효율성도 가장 높습니다.

147
00:16:11,780 --> 00:16:15,380
우리는 실제로 계산을 여러 장치에 걸쳐 분할하기 때문입니다.

148
00:16:15,380 --> 00:16:21,460
그러나 이는 계산의 세분성을 감소시키는 동시에 통신을 증가시킵니다.

149
00:16:21,460 --> 00:16:32,660
오버헤드가 발생하므로 여러 장치에 걸쳐 있는 경우에는 그다지 좋지 않습니다.

150
00:16:32,660 --> 00:16:41,100
따라서 여러 장치에서 GPU를 사용하는 경우 통신 병목 현상이 발생합니다.

151
00:16:42,060 --> 00:16:55,420
그래서 일반적으로 우리는 노드나 서버와 같은 내부의 GPU에서 모델 병렬성을 사용합니다.

152
00:16:55,420 --> 00:16:57,660
예를 들어.

153
00:16:57,660 --> 00:17:01,820
다른 세부 사항은 임베딩 레이어와 손실 레이어를 실제로 병렬화한다는 것입니다.

154
00:17:01,820 --> 00:17:08,020
그리고 아이디어는 동일합니다. 기본적으로 우리는 병렬 GMM과 마스킹을 사용합니다.

155
00:17:08,020 --> 00:17:13,980
그리고 내부 GPU 전체에서는 좋지 않기 때문에 더 높은 수준으로 확장하기 위해 데이터 병렬 또는 모델을 사용합니다.

156
00:17:13,980 --> 00:17:24,980
다른 병렬성 매개변수로 구성하는 것처럼 파이프라이닝합니다.

157
00:17:24,980 --> 00:17:28,860
요약하자면, 모델이 아닌 데이터를 분할할 수 있습니다.

158
00:17:28,860 --> 00:17:35,740
데이터를 여러 장치로 전송하고 모든 장치에 모델이 존재하도록 유지할 수 있습니다.

159
00:17:35,740 --> 00:17:38,900
그리고 아주 빠른 훈련을 하세요.

160
00:17:38,900 --> 00:17:45,420
모델을 수평으로 분할할 수 있으므로 모든 레이어가 분할되어 전체를 제공할 수 있습니다.

161
00:17:45,420 --> 00:17:47,300
한 번에 일괄 처리합니다.

162
00:17:47,300 --> 00:17:53,340
이는 매우 통신 집약적이지만 메모리 사용량은 가장 낮습니다.

163
00:17:53,340 --> 00:17:59,820
그리고 우리는 이것을 이렇게 수직으로 분할하고 배치를 순차적으로 공급할 수 있습니다.

164
00:17:59,900 --> 00:18:07,140
그리고 그것은 최고의 메모리 효율성이며, 또한 매우 높은 메모리 효율성입니다.

165
00:18:07,140 --> 00:18:12,980
통신 병목 현상이 가장 낮습니다.

166
00:18:12,980 --> 00:18:14,940
그럼 그것들을 함께 결합할 수 있을까요?

167
00:18:14,940 --> 00:18:19,660
이 경우 세 가지 세계 중 가장 좋은 점을 얻을 수 있습니까?

168
00:18:19,660 --> 00:18:23,300
대답은 '예'입니다.

169
00:18:23,300 --> 00:18:28,860
이들 각각을 하나의 장치로 생각하고 추상화하여 세분화할 수 있습니다.

170
00:18:28,860 --> 00:18:34,460
이것이 바로 3D 병렬성입니다.

171
00:18:34,460 --> 00:18:42,540
따라서 배치를 가져와 마이크로배치로 나누고, 각 마이크로배치는

172
00:18:42,540 --> 00:18:44,940
데이터 병렬 슬라이스.

173
00:18:44,940 --> 00:18:50,020
데이터 병렬 슬라이스는 기본적으로 전체 모델로 구성되지만, 모델 자체는

174
00:18:50,020 --> 00:18:56,620
이 경우 순차적으로 세 단계로 나눕니다.

175
00:18:56,620 --> 00:19:02,700
앞서 설명한 것처럼 파이프라인 병렬 처리 블록입니다.

176
00:19:02,700 --> 00:19:09,700
그리고 이러한 파이프라인 병렬 블록 각각은 예를 들어 두 개의 모델 병렬로 분할됩니다.

177
00:19:09,700 --> 00:19:17,180
블록 또는 텐서 병렬 블록을 사용하여 수준에서 병렬성을 가질 수 있습니다.

178
00:19:17,180 --> 00:19:19,140
텐서 그 자체.

179
00:19:19,140 --> 00:19:25,460
그리고 이게 NR급이라 통신용으로도 좋고, 이것도 좋아요

180
00:19:25,460 --> 00:19:28,380
효율성을 위해 이는 확장 목적에 적합합니다.

181
00:19:28,380 --> 00:19:37,540
따라서 이들을 모두 혼합하면 달성 가능한 가장 높은 병렬성을 얻을 수 있습니다.

182
00:19:37,540 --> 00:19:39,300
현재 기술로.

183
00:19:39,300 --> 00:19:43,860
그리고 이것은 실제로 현재의 최첨단 기술입니다.

184
00:19:43,860 --> 00:19:46,500
예를 들어 이것은 Bloom의 모델입니다.

185
00:19:46,500 --> 00:19:52,620
허용하므로 이는 384 GPU와 같습니다.

186
00:19:52,620 --> 00:19:54,020
마치 사인과 같습니다.

187
00:19:54,020 --> 00:20:01,300
총 384개의 GPU에서 훈련된 모델의 8개 복사본을 얻습니다.

188
00:20:01,300 --> 00:20:03,780
따라서 8개의 사본을 얻게 됩니다.

189
00:20:03,780 --> 00:20:05,420
이것이 데이터 병렬성이다.

190
00:20:05,420 --> 00:20:15,180
각 복사본은 12개의 파이프라인 병렬 처리 단계로 나뉩니다.

191
00:20:15,180 --> 00:20:17,700
이것이 일련의 단계입니다.

192
00:20:17,700 --> 00:20:28,220
그런 다음 각 파이프라인 단계는 4개의 모델 병렬 단계로 나뉩니다.

193
00:20:28,220 --> 00:20:33,300
따라서 하나의 전체 복사본은 384개를 얻으려면 48개의 GPU x 8이 필요합니다.

194
00:20:33,300 --> 00:20:34,300
놀라운.

195
00:20:34,300 --> 00:20:41,740
자, 이제 혼합 정밀도 훈련에 대해 이야기해 보겠습니다.

196
00:20:41,740 --> 00:20:47,380
혼합 정밀도 훈련은 기본적으로 더 작은 정밀도로 훈련할 수 있게 해주는 아이디어입니다.

197
00:20:47,380 --> 00:20:56,100
32비트보다 32비트가 실제로 그렇듯이 이 문서에서는 그렇지 않다는 것을 보여줍니다.

198
00:20:56,100 --> 00:21:00,180
32비트로 학습하는 데 필요합니다.

199
00:21:00,180 --> 00:21:05,980
기울기 시간이 학습이기 때문에 원래 가중치를 32비트로 유지해야 합니다.

200
00:21:05,980 --> 00:21:13,540
속도는 때로는 너무 작지만 메모리는 활성화에 의해 지배됩니다.

201
00:21:13,540 --> 00:21:18,980
그러니까 원래의 가중치를 32비트로 유지하고, 실제로는 모든 것을 변환해도 괜찮습니다.

202
00:21:18,980 --> 00:21:25,380
다른 것을 16비트로 나누면 매우 가까운 정확도를 얻을 수 있습니다.

203
00:21:25,380 --> 00:21:34,300
이에 대한 몇 가지 순 선택이 있는데, 이는 FP16에 많이 있습니다.

204
00:21:34,300 --> 00:21:41,340
기본적으로 여기에 있는 그래디언트의 일부입니다. 이 부분은 정규화되지 않은 범위에 있습니다.

205
00:21:41,340 --> 00:21:47,500
따라서 FP16에서는 표현할 수 없으며 0이 됩니다.

206
00:21:47,500 --> 00:21:55,780
그래서 당신이 하는 방식은 실제로 손실의 크기를 조정하는 것입니다. 왜냐하면 이것이 중요하기 때문입니다.

207
00:21:55,780 --> 00:22:00,780
부분이므로 사용하는 모델의 종류에 따라 손실을 약간 조정할 수 있습니다.

208
00:22:00,780 --> 00:22:08,500
그래서 FP16에서 0이 되는 부분을 이 빨간색 선에서 이 주황색 선으로 이동합니다.

209
00:22:08,500 --> 00:22:14,740
모든 그라디언트의 크기를 조정하고 모든 그라디언트의 크기를 조정하는 가장 쉬운 방법은

210
00:22:14,740 --> 00:22:15,740
손실 규모를 조정하세요.

211
00:22:15,740 --> 00:22:23,460
여기에서는 손실에 2제곱 4를 곱하고 다른 상황에서는 2제곱 8 또는

212
00:22:23,460 --> 00:22:34,900
2제곱 16, 2제곱 8 또는 2제곱 10이 아니라 기본적으로 기본적으로 얻습니다.

213
00:22:35,340 --> 00:22:45,220
정확도가 동일하므로 32비트에서 수행한 것과 동일한 정확한 모델을 얻을 수 있습니다.

214
00:22:45,220 --> 00:22:50,420
어떤 상황에서는 실제로 약간 더 정확하기도 합니다.

215
00:22:50,420 --> 00:22:55,780
16비트로 훈련할 때 발생하는 정규화 효과처럼

216
00:22:55,780 --> 00:23:02,220
모델이 좀 더 일반화되는 것과 같습니다.

217
00:23:02,220 --> 00:23:09,780
요약하자면 FP16은 어쨌든 FP32보다 빠르므로 우리는 이를 선호하고 메모리도 덜 차지합니다.

218
00:23:09,780 --> 00:23:18,020
메모리를 약 2로 나누면 좋습니다.

219
00:23:18,020 --> 00:23:25,700
우리는 확실히 그것을 통합하고 싶고 이 논문 이후에 나온 대부분의 논문도

220
00:23:25,700 --> 00:23:30,820
대규모 언어 모델을 훈련할 때 이러한 혼합 정밀도 훈련을 활용합니다.

221
00:23:30,820 --> 00:23:39,140
우리는 또한 실제로 매우 간단한 아이디어인 활성화 체크포인트를 활용합니다.

222
00:23:39,140 --> 00:23:44,900
모델을 훈련할 때 순방향 상태를 유지하므로 모델을 훈련하면

223
00:23:44,900 --> 00:23:48,940
전달을 수행하면 모든 전달 상태를 그대로 유지하고 전달을 수행한 후에는

224
00:23:48,940 --> 00:23:53,980
손실을 계산하고 손실이 발생하면 뒤로 가기 시작합니다.

225
00:23:53,980 --> 00:24:00,460
뒤로 이동하면 저장된 상태에서 역방향 그래디언트를 계산하고 역방향

226
00:24:00,460 --> 00:24:04,860
그래디언트가 다가오고 있으므로 여기서 다시 전파하고 여기로 가면 이 상태가 됩니다.

227
00:24:04,860 --> 00:24:08,620
그리고 이것을 다시 전파하는 식입니다.

228
00:24:08,620 --> 00:24:14,980
하지만 이 모든 상태를 메모리에 보관할 필요는 없습니다. 실제로는 정사각형만 유지할 수 있습니다.

229
00:24:14,980 --> 00:24:20,140
예를 들어, 하나를 하고 세 개를 건너뛰고, 하나를 하고 세 개를 건너뛰고, 매번

230
00:24:20,140 --> 00:24:25,140
거꾸로 수행하고 마지막 체크포인트에서 다시 계산하므로 호출됩니다.

231
00:24:25,140 --> 00:24:26,220
검문소.

232
00:24:26,220 --> 00:24:31,020
그래서 이 체크포인트부터 뒤로 가기 시작하면 자동으로 시작됩니다.

233
00:24:31,020 --> 00:24:36,780
일부는 여러분을 위해 전달하고 이를 역전파할 준비가 되도록 제공합니다.

234
00:24:36,780 --> 00:24:44,380
그렇게 하면 이 항목도 컴퓨팅을 시작하게 되므로 앞으로 진행되기 시작하면

235
00:24:44,380 --> 00:24:49,900
중간 어딘가에서 만날 수 있으므로 별로 기다리지 않아도 됩니다.

236
00:24:49,900 --> 00:24:55,820
메모리를 엄청나게 줄이기 위해 약간의 추가 컴퓨팅을 소비하면 됩니다.

237
00:24:55,820 --> 00:25:01,220
이를 수행하는 경우 실제로 모든 제곱근 단계마다 n 메모리의 제곱근을 얻습니다.

238
00:25:01,220 --> 00:25:07,500
미니 배치당 하나의 추가 전달을 희생하여 감소하면 33% 정도입니다.

239
00:25:07,500 --> 00:25:16,780
적어도 변환기의 경우 계산 증가는 매우, 매우, 매우

240
00:25:16,780 --> 00:25:20,580
매우 좋은.

241
00:25:21,580 --> 00:25:35,740
이제 우리는 0으로갑니다. Zero는 제로 중복 최적화 프로그램이며 Microsoft에서 제공합니다. 그것은 기본적으로

242
00:25:35,740 --> 00:25:43,140
지금까지 이야기한 대부분의 기능을 사용하는 최적화 프로그램입니다.

243
00:25:43,140 --> 00:25:53,140
메모리 소비를 기본적으로 상각하여 실제로 좋아하게 만드는 영리한 방법으로

244
00:25:53,140 --> 00:25:56,820
원래 메모리 소비량을 장치 수로 나눈 것과 같습니다.

245
00:25:56,820 --> 00:26:03,380
장치 수는 120GB에서 1.9GB로 즉시 줄어듭니다.

246
00:26:03,380 --> 00:26:08,420
그럼 어떻게 되나요? 본질적으로 다음과 같이 진행됩니다. 따라서 우선 얼마나 많은 메모리가

247
00:26:08,420 --> 00:26:17,580
모델 테이크? 음, 모든 매개변수를 사용합니다. FP16처럼 2바이트를 사용하므로

248
00:26:17,580 --> 00:26:24,460
매개변수에 2바이트, 그래디언트에 2바이트, 그리고 일부 최적화 프로그램이 있습니다.

249
00:26:24,460 --> 00:26:33,780
이러한 최적화 프로그램 상태는 32비트와 같기 때문에 4 더하기 4입니다.

250
00:26:34,220 --> 00:26:40,580
바이트이므로 실제로는 12입니다. 그런 다음 매개변수 개수를 곱하면 됩니다.

251
00:26:40,580 --> 00:26:51,220
이것이 당신이 얻는 것입니다. 따라서 0에는 세 가지 수준의 병렬 처리가 있습니다. 첫번째

252
00:26:51,220 --> 00:27:00,860
옵티마이저 상태 병렬성, 그리고 옵티마이저 상태에 그라디언트 병렬성을 더한 것입니다.

253
00:27:00,980 --> 00:27:12,060
최적화 상태, 그래디언트, 매개변수 병렬성. 여기서는 속도를 볼 수 있습니다.

254
00:27:12,060 --> 00:27:25,460
0이 되면 매우 높습니다. GPU당 처리량은 일관적이라는 것을 알 수 있습니다.

255
00:27:26,060 --> 00:27:32,340
매우 높은 모델 매개변수에서만 감소하기 시작하지만 속도 증가는 정말 빠릅니다.

256
00:27:32,340 --> 00:27:43,580
모델이 더 이상 단일 노드에 적합하지 않을 때 속도 향상은 매우 중요해집니다.

257
00:27:43,580 --> 00:27:52,260
여기서 기준은 다음과 같은 것을 볼 수 있는 단일 장치에서의 모델 병렬성입니다.

258
00:27:52,260 --> 00:27:58,900
상대적으로 가깝지만 이 시점 이후에는 더 이상 단일 노드에 들어갈 수 없습니다. 그래서 당신은 필요합니다

259
00:27:58,900 --> 00:28:04,820
이를 맞추기 위해 노드 간 모델 병렬성을 갖습니다. GPU당 처리량은 다음과 같습니다.

260
00:28:04,820 --> 00:28:11,580
정말 빠르게 다운되는데, 제로 옵티마이저가 정말 빛나는 이유는 확장성이 너무 크기 때문입니다.

261
00:28:11,580 --> 00:28:20,940
매개변수가 높은 상황에서는 더 좋습니다. 좋습니다. 여기서는 4개의 장치에서 0의 예를 볼 수 있습니다.

262
00:28:21,540 --> 00:28:30,660
각 장치에는 모델의 일부가 있으므로 장치 1에는 이 부분이 있고 그 다음에는

263
00:28:30,660 --> 00:28:37,460
포워드 배치, 모델의 특정 섹션에 해당하는 각 당사자를 모두에게 보냅니다.

264
00:28:37,460 --> 00:28:43,220
클라이언트가 각자의 데이터를 전달할 수 있도록 합니다. 그래서 이것은 데이터의 한 형태입니다.

265
00:28:43,220 --> 00:28:49,060
파이프라인 병렬과 결합된 병렬. 포워드는 다음과 같은 경우 손실로 계산됩니다.

266
00:28:49,060 --> 00:28:56,100
당신은 데이터의 당신 부분, 모델의 당신 부분, 모든 당사자에서 역방향 손실을 겪고 있습니다.

267
00:28:56,100 --> 00:29:02,620
모델의 이 부분에 대한 그라데이션을 제공합니다. 그래서 일어나는 의사소통은 단지 일어난다.

268
00:29:02,620 --> 00:29:09,460
필요한 경우 모든 당사자는 필요한 데이터만 전달합니다. 그러니까 완전체같아

269
00:29:09,460 --> 00:29:15,300
모델과 데이터를 동시에 분할합니다. 그리고 때가 되면 모델을 업데이트하세요.

270
00:29:15,300 --> 00:29:21,300
최적화 상태를 사용하여 새로운 모델 매개변수를 얻은 다음 다음 배치로 이동합니다.

271
00:29:21,300 --> 00:29:29,940
이것이 본질적으로 전체 아이디어입니다. 이제 데이터 병렬 접근 방식으로 0을 자체적으로 사용할 수 있습니다.

272
00:29:29,940 --> 00:29:39,340
3D 병렬 처리에 사용되며 이것이 깊은 속도 라이브러리입니다. 따라서 0을 사용하여 데이터 병렬을 가질 수 있습니다.

273
00:29:39,340 --> 00:29:46,180
그 안에 각 데이터가 병렬로 있고 파이프라인 병렬이 있으며 각 파이프라인 병렬은

274
00:29:46,180 --> 00:29:52,740
내부 모델은 평행합니다. 따라서 최적화가 0인 3D 병렬성을 갖게 됩니다.

275
00:29:52,740 --> 00:30:01,180
성능이 크게 향상됩니다. 작동 방식은 다음과 같습니다. 모델 병렬성은

276
00:30:01,180 --> 00:30:06,460
가장 큰 통신 오버헤드. 따라서 우리는 노트 내에 모델 병렬 그룹을 배치하는 것을 우선시합니다.

277
00:30:06,580 --> 00:30:14,620
더 큰 노드 간 대역폭을 활용합니다. 그런 다음 텐서 슬라이싱을 위한 Megatron LM이 있습니다.

278
00:30:14,620 --> 00:30:23,700
여기 안에서 일어나고 있어요. 제가 설명한 것과 똑같이 내부에서 파이프라인 단계를 수행합니다.

279
00:30:23,700 --> 00:30:38,220
예, 그리고 0을 사용하면 기본적으로 모든 접근 방식의 모든 이점을 결합합니다.

280
00:30:38,220 --> 00:30:45,100
우리가 얘기했던 것처럼, 우리는 Trillium 매개변수 모델 이상의 것을 얻을 수 있습니다. 우리는 그것을 할 수 있고, 병렬화를 할 수 있습니다.

281
00:30:45,100 --> 00:30:51,900
1000x 이상으로 계산 효율성이 매우 좋습니다. 그리고 제로의 가장 좋은 점은

282
00:30:51,900 --> 00:30:56,260
코드를 다시 작성할 필요가 없습니다. 왜냐하면 이는 단지 최적화 도구일 뿐이기 때문입니다.

283
00:30:56,260 --> 00:31:05,060
PyTorch에 적용해보세요. 당신은 단지 당신을 위해 그것을 할 수 있는 깊은 속도 라이브러리를 사용할 수 있습니다. 응, 그럼 넌 할 수 있어

284
00:31:05,060 --> 00:31:14,660
GPT3 규모의 작업을 수행하려면 3D 병렬성이 실제로 필요하다는 것을 확인하십시오. 예를 들어,

285
00:31:14,660 --> 00:31:19,420
모델 병렬을 1로 동일하게 수행하는 경우 문자 그대로 성능을 얻을 수 있는 방법이 없습니다.

286
00:31:19,420 --> 00:31:28,140
GPT3 규모, 최소한 모델 병렬이 2와 같고 매우 높은 파이프라인 병렬이 필요합니다.

287
00:31:28,140 --> 00:31:37,340
에. 파이프라인 병렬이 20이고 모델 병렬이 20일 때 가장 좋은 3D 병렬성을 얻을 수 있습니다.

288
00:31:37,340 --> 00:31:42,820
2이고 데이터 병렬은 20입니다. 아이디어는 모델 병렬이 매우 높을수록 낮아야 한다는 것입니다.

289
00:31:42,860 --> 00:31:48,180
성능은 메모리 효율성을 위해서만 사용되기 때문에 성능 향상을 위해 사용되지는 않습니다.

290
00:31:48,180 --> 00:31:58,240
왜냐하면 우리가 이야기하고 있는 것은 성능 측면에서 매우 효율적이지 않기 때문입니다. 의사소통 때문에

291
00:31:58,240 --> 00:32:06,340
그것이 도입하는 오버헤드. 응, 내 참고자료 봤어? 들어주셔서 감사합니다.

292
00:32:06,340 --> 00:32:09,500
이것은 당신에게 유용했습니다.