1
00:00:00,000 --> 00:00:08,280
안녕하세요. 오늘 우리는 주제 매개변수 효율적인 변환기에 대해 논의할 것입니다. 그럼

2
00:00:08,280 --> 00:00:13,240
시작하다. 매개변수 효율적인 변압기는 표준 변압기를 수정한 것입니다.

3
00:00:13,240 --> 00:00:19,480
성능을 유지하면서 훈련 가능한 매개변수의 수를 줄이도록 설계된 모델입니다.

4
00:00:19,480 --> 00:00:24,920
이러한 방법은 변환기를 보다 계산적으로 효율적이고 배포하기 쉽게 만드는 것을 목표로 합니다.

5
00:00:24,920 --> 00:00:31,360
자원이 제한된 환경에서. 발표 전반부에는

6
00:00:31,360 --> 00:00:37,360
전제 조건, 개념, 실험을 논의하는 4개의 연구 논문을 다루어야 합니다.

7
00:00:37,360 --> 00:00:43,160
그리고 각각의 결과. 논문 1에서는 매개변수 효율적인 전이 학습을 수행했습니다.

8
00:00:43,160 --> 00:00:49,480
NLP, 우리는 물 전달 학습, 교사 기반 대 미세 조정, 어댑터를 다룰 것입니다.

9
00:00:49,480 --> 00:00:56,160
튜닝과 아키텍처. 논문 2에서 Laura는 언어 모델의 낮은 순위 적응, 대규모

10
00:00:56,160 --> 00:01:02,600
언어 모델에서는 미세 조정, LLM 및 매개변수 효율적인 접근 방식, 어댑터에 대해 이야기하겠습니다.

11
00:01:02,600 --> 00:01:11,480
모듈 및 접두사 조정, 고유 차원 및 고유 순위 및 Laura. 세 번째 논문에서는

12
00:01:11,480 --> 00:01:16,480
중요한 것은 크기만이 아닙니다. 소규모 언어 모델은 짧은 학습자도 거의 없습니다.

13
00:01:16,960 --> 00:01:22,880
우리는 짧은 학습이 없는 것과 몇 가지 짧은 학습, 애완동물 패턴 활용에 대해 논의할 것입니다.

14
00:01:22,880 --> 00:01:30,920
훈련, IPET, 애완동물의 반복 변형 및 여러 마스크를 사용한 애완동물. 종이 4개를 만들기 위해

15
00:01:30,920 --> 00:01:37,400
사전 훈련된 언어 모델은 짧은 학습자가 적을수록 좋음, 주제는 LMBFF, 짧은 학습자가 적을수록 좋음

16
00:01:37,400 --> 00:01:43,640
언어 모델 미세 조정, 프롬프트 기반 미세 조정, 자동 프롬프트 생성 및

17
00:01:43,640 --> 00:01:51,360
시연을 통해 미세 조정. 논문 1에서는 매개변수 효율적인 전이 학습

18
00:01:51,360 --> 00:01:57,880
NLP의 경우 시작해 보겠습니다. 그렇다면 전이학습이란 무엇인가? 머신러닝 방식입니다.

19
00:01:57,880 --> 00:02:03,920
하나의 작업을 위해 개발된 모델이 다른 작업의 모델을 위한 출발점으로 사용됩니다.

20
00:02:03,920 --> 00:02:09,920
하지만 관련 작업. 기본적으로 개념은 바퀴를 재발명할 필요가 없다는 것입니다.

21
00:02:09,920 --> 00:02:16,280
하지만 바퀴를 사용하여 자동차를 만드는 것입니다. 이 접근 방식은 더 효율적이고 다재다능합니다.

22
00:02:16,280 --> 00:02:23,920
많은 유사한 작업에 사용되며 대부분 더 높은 성능을 발휘합니다. 핵심을 이해하려면

23
00:02:23,920 --> 00:02:29,560
기능 기반과 미세 조정의 차이점에 대해 예를 들어 보겠습니다. 누군가를 위해

24
00:02:29,560 --> 00:02:34,520
나처럼 여러 언어를 사용하는 인도에서 왔기 때문에 배우기가 더 쉽습니다.

25
00:02:34,520 --> 00:02:40,760
많은 언어가 유사한 기능을 공유하므로 새로운 언어입니다. 그리고 나도 같은 이해를 사용할 수 있습니다

26
00:02:40,760 --> 00:02:47,640
그리고 나는 이미 배운 개념을 나에게 유익하게 사용하고 새로운 언어를 배웁니다.

27
00:02:47,640 --> 00:02:53,000
한 가지 언어만 구사하는 친구에게는 배우기가 비교적 어려울 수 있습니다.

28
00:02:53,000 --> 00:02:58,560
두 언어 간의 유사성이 제한될 수 있으므로 완전히 새로운 언어

29
00:02:58,560 --> 00:03:06,360
새로운 언어를 능숙하게 익히려면 더 많은 자원과 시간이 필요합니다. 기능 있음

30
00:03:06,360 --> 00:03:13,520
기반으로, 원래 함수 phi w는 새로운 매개변수 V에 대해 학습하기 위한 기반으로 사용됩니다.

31
00:03:13,520 --> 00:03:20,200
sky V phi w X로 새로운 기능을 만듭니다. 원본에는 거의 변화가 없습니다.

32
00:03:20,200 --> 00:03:26,040
함수와 그 매개변수. 미세 조정에서는 기능 후반부가 수정됩니다.

33
00:03:26,040 --> 00:03:31,880
새 매개변수의 경우 새 매개변수가 도입되고 이전 매개변수가 사용됩니다.

34
00:03:31,880 --> 00:03:38,360
모델을 조정합니다. 여기에 새로운 개념의 어댑터 튜닝이 도입되었습니다.

35
00:03:38,360 --> 00:03:43,600
함수와 해당 매개변수는 학습 중에 동결되며 새 매개변수는

36
00:03:43,600 --> 00:03:49,320
어댑터 모듈 형태로 기능 사이에 추가되었습니다. 시간을 보내는 것과 같습니다

37
00:03:49,320 --> 00:03:56,000
기계를 사용하고 평생 동안 매일 새로운 언어를 배우는 데 몇 분만 추가하면 됩니다.

38
00:03:56,240 --> 00:04:01,200
당신의 삶의 과정과 당신의 핵심 이해와 학습을 수정하지 않고

39
00:04:01,200 --> 00:04:07,520
원어. 왼쪽에는 우리가 가지고 있는 기본 변환기 레이어 아키텍처가 있습니다.

40
00:04:07,520 --> 00:04:13,760
사이에 어댑터 모듈을 추가하여 수정되었습니다. 오른쪽에는 어댑터 모듈의 모습이 있습니다.

41
00:04:13,760 --> 00:04:20,960
우리가 사용한 것. 각각에 대해 더 자세히 살펴보겠습니다. 트랜스포머 레이어에서는

42
00:04:20,960 --> 00:04:26,480
기본 하위 레이어는 입력을 처리하는 다중 헤드 주의 레이어입니다.

43
00:04:26,480 --> 00:04:32,000
모델에 대한 데이터입니다. 그런 다음 피드 포워드 계층이 나오며, 여기서 데이터는 다음을 사용하여 처리됩니다.

44
00:04:32,000 --> 00:04:38,080
주의 메커니즘. 우리는 어댑터 계층에 대해 조금 논의할 것입니다. 그런 다음 더하기 기호가 표시됩니다.

45
00:04:38,080 --> 00:04:43,680
건너뛰기 연결입니다. 여기서는 어댑터의 출력인지 피드의 출력인지 결정됩니다.

46
00:04:43,680 --> 00:04:49,840
순방향 레이어는 원래 입력 레이어 또는 하위 레이어에 추가됩니다. 이는 보존에 도움이 됩니다.

47
00:04:49,840 --> 00:04:56,000
이전 단계의 정보를 수집하고 이 새로운 프로세스 데이터가 적합한지 여부를 결정합니다.

48
00:04:56,000 --> 00:05:03,280
네트워크에 추가할지 여부를 결정합니다. 마지막으로, 정규화하는 레이어 정규화가 있습니다.

49
00:05:03,280 --> 00:05:11,040
이전 하위 레이어에서 제공한 결합된 출력입니다. 추가된 어댑터 모듈

50
00:05:11,040 --> 00:05:16,880
변환기 계층에 병목 현상이 있는 아키텍처가 있습니다. 이 특정 아키텍처

51
00:05:16,880 --> 00:05:23,360
0.5~8% 범위의 몇 가지 매개변수만 추가하여 총

52
00:05:23,360 --> 00:05:31,680
매개변수는 2MD + D + M입니다. 추가되는 새 매개변수의 수는 다음과 같습니다.

53
00:05:31,680 --> 00:05:38,080
이 어댑터 모듈의 크기를 늘려 제어됩니다. 마지막으로 건너뛰기 연결이 있습니다.

54
00:05:38,080 --> 00:05:44,560
새로 추가된 매개변수의 영향은 거의 0으로 초기화하여 제어됩니다.

55
00:05:44,560 --> 00:05:51,680
처음에는 훈련을 진행하면서 점차적으로 증가시킵니다. 이 문서의 목표는 도움을 주는 것입니다.

56
00:05:51,680 --> 00:05:58,320
여러 유사한 다운스트림 작업에서 좋은 결과를 제공하는 컴팩트하고 확장 가능한 시스템을 구축합니다.

57
00:05:58,320 --> 00:06:03,120
훈련 시점에는 시스템에 들어오는 새로운 데이터를 알 수 없습니다.

58
00:06:04,880 --> 00:06:10,640
이를 위해 저자는 결과를 벤치마킹하기 위해 26개의 서로 다른 분류 작업을 사용했습니다.

59
00:06:10,720 --> 00:06:17,200
BERT를 사용합니다. Gluebenchmark는 교육에 널리 사용되는 중요한 벤치마킹 도구입니다.

60
00:06:17,200 --> 00:06:23,120
그러한 자연어 이해 시스템을 평가합니다. 가장 큰 특징은 모델이라는 점입니다.

61
00:06:23,120 --> 00:06:28,400
불가지론적이며 결과를 벤치마킹하고 보여줄 수 있는 리더보드와 대시보드가 ​​있습니다.

62
00:06:30,160 --> 00:06:37,200
본 논문에서는 Stanford 질의응답 데이터셋인 Squad도 활용하였다. 그것은 다음과 같이 구성됩니다

63
00:06:37,760 --> 00:06:44,560
1.1 버전의 Wikipedia 기사에 있는 107,785개의 질문 답변 쌍입니다.

64
00:06:46,640 --> 00:06:52,720
글루벤치마킹 도구에서 어댑터 모듈이 포함된 BERT 모델은 정밀한 성능과 일치합니다.

65
00:06:52,720 --> 00:07:00,080
튜닝하고 새 매개변수 중 약 3%만 활용하면 AutoML 벤치마크 결과는 다음과 같습니다.

66
00:07:00,080 --> 00:07:10,080
또한 좋다. 어댑터 기반 BERT는 1.14%의 새로운 매개변수만 활용하고 73.3%의 정확도에 도달합니다.

67
00:07:10,080 --> 00:07:16,640
BERT 기반 미세 조정은 100% 새로운 매개변수를 사용하고 73.7% 정확도에 도달합니다.

68
00:07:17,600 --> 00:07:22,880
오른쪽 그래프는 또한 어댑터 모듈의 검증 정확도가 다음과 같을 수 있음을 보여줍니다.

69
00:07:22,880 --> 00:07:27,840
100배 적은 수의 새로운 매개변수를 활용하면서 미세 조정과 유사합니다.

70
00:07:30,800 --> 00:07:35,280
논문 2는 Laura, 대규모 언어 모델의 하위 적응입니다.

71
00:07:36,080 --> 00:07:42,320
이 방정식은 구문의 다음 단어를 예측하는 데 사용됩니다. 여기, yt가 다음 단어이고 그

72
00:07:42,320 --> 00:07:50,080
확률은 이전 단어 y0에서 yt 빼기 1과 매개변수 5를 기반으로 계산됩니다.

73
00:07:50,720 --> 00:07:57,600
미세 조정에서 이 방정식은 5를 새로운 매개변수 가중치 y0+로 대체하여 변경됩니다.

74
00:07:57,600 --> 00:08:04,880
델타 5와 델타 5의 차원은 동일하지 않습니다. 매개변수 효율적인 접근법에서는

75
00:08:04,880 --> 00:08:12,560
이는 델타 5와 세타를 곱하여 추가로 변경됩니다. 세타는 다음보다 훨씬 작은 차원을 갖습니다.

76
00:08:12,560 --> 00:08:18,800
5와 5는 없습니다. 이는 50보다 약 10,000배 더 작습니다.

77
00:08:19,600 --> 00:08:26,880
방금 어댑터 모듈에 대해 배웠듯이, 우리가 사용할 수 없는 질문이 생길 수도 있습니다.

78
00:08:26,880 --> 00:08:33,600
GPT-3과 같은 LLM의 어댑터 모듈? 대답은 '예'입니다. 가능합니다. 하지만 어댑터 모듈은

79
00:08:33,600 --> 00:08:39,040
순차적으로 처리되므로 추론 지연 시간이 길어지고 데이터에 깊이가 더해집니다.

80
00:08:39,040 --> 00:08:45,920
GPT-3과 같은 대규모 모듈에서 접근합니다. 여기서 접두사 조정에 관한 또 다른 질문을 다루겠습니다.

81
00:08:45,920 --> 00:08:50,800
영상 후반부에는 프리픽스 튜닝에 대해 살펴보겠지만 간략하게만 말씀드리자면

82
00:08:50,800 --> 00:08:56,160
지금은. 접두사 조정에서는 학습 프로세스 시작 시 지침을 추가하고 동결합니다.

83
00:08:56,160 --> 00:09:02,960
그러나 이로 인해 일관되지 않은 결과가 발생하고 최적화가 어려워집니다.

84
00:09:02,960 --> 00:09:08,720
전체 접근 방식. 또한 각 입력 앞에 프롬프트를 추가해야 합니다.

85
00:09:08,720 --> 00:09:17,440
작업의 길이. 모델로 처리하고 싶습니다. 따라서 이 두 가지 접근 방식이 가능합니다.

86
00:09:17,440 --> 00:09:26,240
그러나 그 자체의 결함도 있습니다. 이 접근 방식에 필요한 두 가지 주요 개념을 이해해 보겠습니다.

87
00:09:26,240 --> 00:09:31,680
첫 번째, 본질적인 차원은 다음을 얻는 데 필요한 최소 매개변수 수를 나타냅니다.

88
00:09:31,680 --> 00:09:37,440
전체 모델과 유사한 성능. 기본적으로 모델의 가장 간단한 버전입니다.

89
00:09:37,520 --> 00:09:41,920
비슷한 수준의 작업을 수행합니다. 두 번째 개념은 본질적인 순위입니다.

90
00:09:41,920 --> 00:09:46,400
이는 신경망의 특정 구성 요소에 대한 자유의 복잡성입니다.

91
00:09:46,960 --> 00:09:52,320
기본적으로 중요한 내용을 표현하는 데 필요한 최소 차원 수를 나타냅니다.

92
00:09:52,320 --> 00:09:58,320
신경망의 해당 구성 요소에 있는 정보입니다. 이러한 개념은 일반적으로 서로 짝을 이루며,

93
00:09:58,320 --> 00:10:03,600
여기에서 전체 아키텍처의 본질적인 차원과 본질적인 순위를 찾을 수 있습니다.

94
00:10:03,600 --> 00:10:07,440
각 단계는 최소한의 기능을 사용하도록 접근 방식을 최적화합니다.

95
00:10:09,440 --> 00:10:15,280
GARA에서 활용되고 있는 주요 컨셉입니다. 여기서는 사전 훈련된 가중치가 고정되어 있습니다.

96
00:10:15,280 --> 00:10:20,400
학습 중에 모델에 추가되는 주요 매개변수

97
00:10:20,400 --> 00:10:26,160
여기서는 A 교차 B 행렬로 표시되는 행렬 곱셈 형식으로 추가됩니다.

98
00:10:26,960 --> 00:10:35,680
방금 논의한 고유 순위인 R은 원래의 D와 K보다 훨씬 낮습니다.

99
00:10:35,680 --> 00:10:42,800
신경망에서 활용되고 있던 차원 길이입니다. 여기서 B는 0으로 초기화되고,

100
00:10:42,800 --> 00:10:49,600
A는 임의의 가우스 방식으로 초기화되며 이는 모델에 매우 많은 정보를 제공하기 위해 수행됩니다.

101
00:10:49,600 --> 00:10:56,160
다시 실행해야 할 필요성을 줄이기 위해 시작 및 이스케이프 시 새 매개변수 수가 줄어듭니다.

102
00:10:56,160 --> 00:11:06,320
하이퍼파라미터에서. 마지막으로 W0x + BAx인 LARA를 사용한 정방향 패스는 순위가 더 낮습니다.

103
00:11:06,320 --> 00:11:14,320
평소보다 훨씬 적은 수의 매개변수를 사용합니다. 본 논문에서 수행된 실험은

104
00:11:14,320 --> 00:11:20,960
최종적으로 Robberta, Debberta, GPT-2, GPT-3을 사용하여 비교하였다. 이건 끝났어

105
00:11:20,960 --> 00:11:30,800
글루 벤치마킹 도구 및 Vicky STL, SAMSUM, A2A, NLG, Dart 및 Web과 같은 다양한 데이터 세트

106
00:11:30,800 --> 00:11:36,960
에너지. 다음은 자연어를 테스트하기 위해 일반적으로 사용되는 데이터 세트 및 벤치마킹 도구입니다.

107
00:11:36,960 --> 00:11:45,520
시스템 이해. 접착제 벤치마크에서 우리는 Robberta를 기반으로 한 접근 방식을 볼 수 있습니다.

108
00:11:45,520 --> 00:11:53,840
LARA는 30만 개의 새로운 매개변수만 활용하여 최소한의 사용으로 87.2%의 정확도를 얻습니다.

109
00:11:53,840 --> 00:11:58,800
비트 폭을 사용하는 접근 방식과 달리 매개변수 수가 훨씬 적습니다.

110
00:11:58,800 --> 00:12:07,360
정확도 85.2%. 그리고 GPT-2를 사용한 A2E NLG 챌린지에서는 GPT-2 매체를 사용한 접근 방식이

111
00:12:07,360 --> 00:12:14,800
LARA는 비교 시 모든 경우에 대해 가장 높은 정확도를 제공하기 위해 35만 개의 새로운 매개변수만을 사용합니다.

112
00:12:14,800 --> 00:12:22,400
정확도를 낮추기 위해 354.9~100만 개의 새로운 매개변수를 사용하는 미세 조정을 수행했습니다. 비슷한

113
00:12:22,400 --> 00:12:28,560
LARA가 우수한 성능을 보이는 GPT-2의 대규모 훈련 가능한 매개변수와 정확성에서 추세를 볼 수 있습니다.

114
00:12:28,640 --> 00:12:37,040
대부분의 경우 다른 접근 방식. LARA를 GPT-3와 함께 활용하면

115
00:12:37,040 --> 00:12:44,240
3,770만 개와 470만 개의 새로운 매개변수로 비교했을 때 약간 더 높은 정확도에 도달

116
00:12:44,240 --> 00:12:51,040
1,750억 개의 새로운 매개변수로 미세 조정이 가능합니다. 이러한 추세는 다음 그래프에 표시됩니다.

117
00:12:51,040 --> 00:12:56,240
LARA가 정확성 측면에서 다른 모든 접근 방식보다 뛰어난 성능을 발휘하는 경우도 마찬가지입니다.

118
00:12:59,120 --> 00:13:06,400
Paper 3은 크기만 중요한 것이 아니며, 작은 언어 모델도 짧은 학습자가 거의 없습니다.

119
00:13:07,040 --> 00:13:12,080
먼저 제로 단기 학습에 대해 알아보겠습니다. 이는 모델을 훈련할 때의 접근 방식입니다.

120
00:13:12,080 --> 00:13:19,120
다양한 작업을 수행하고 나중에 새로운 작업에 적용되며 배울 수 있는 새로운 예제가 제공되지 않습니다.

121
00:13:19,680 --> 00:13:24,960
이 모델은 학습 과정에서 얻은 일반적인 이해와 지식에 의존합니다.

122
00:13:24,960 --> 00:13:31,040
새로운 작업에 대해 예측하기 위한 원래의 훈련입니다. 짧은 학습은 거의 없습니다.

123
00:13:31,040 --> 00:13:36,800
제로 단기 학습과 유사하지만 새로운 작업과 관련된 몇 가지 예를 사용하여 미세 조정합니다.

124
00:13:36,800 --> 00:13:43,360
이 모델. 이는 일반적으로 새로운 예제로 인해 제로 단기 학습보다 더 나은 정확도를 가져옵니다.

125
00:13:45,200 --> 00:13:50,160
영어 데이터 세트를 학습하여 감성에 적용할 때 모델을 고려해 보겠습니다.

126
00:13:50,160 --> 00:13:55,760
프랑스어 데이터를 직접 분석하면 학습 시간이 제로이지만 동일한 모델을 사용하면

127
00:13:55,760 --> 00:14:01,360
프랑스어가 포함된 문서가 포함된 몇 가지 예를 정밀하게 조정한 다음

128
00:14:01,360 --> 00:14:09,680
짧은 학습이 거의 없습니다. 여기서 입력 x1 쉼표 x2가 변환되는 애완동물 또는 패턴 활용 훈련

129
00:14:09,680 --> 00:14:17,520
닫힌 질문 px에 대해 x에 의한 확률 qp y는 가능한 출력 선택에 대해 파생됩니다.

130
00:14:17,520 --> 00:14:26,320
대량 언어 모델 또는 mlm을 사용하여 닫힌 질문을 완성하기 위한 입력에 맞는 것입니다.

131
00:14:26,320 --> 00:14:35,440
여기의 예에서 x1은 유가 하락이고 x2는 유가 상승입니다. 예 또는 아니오에 맞춰야 합니다.

132
00:14:35,440 --> 00:14:41,440
x1과 x2 사이의 공백을 사용하여 이러한 입력을 연결하여 닫힌 질문을 완성하세요.

133
00:14:42,000 --> 00:14:50,080
이는 입력을 출력으로 매핑하는 데 도움이 되는 한 쌍의 패턴 언어변환기 쌍 또는 pvps를 사용하여 수행됩니다.

134
00:14:50,640 --> 00:14:57,040
여기서 p는 단일 질량을 포함하는 닫힌 질문에 입력을 매핑하고 v는 각 출력을 매핑하는 데 사용됩니다.

135
00:14:57,040 --> 00:15:05,840
패턴에서 작업별 의미를 나타내는 토큰입니다. qpy by x는 다음을 사용하여 계산됩니다.

136
00:15:05,840 --> 00:15:13,040
대량 위치에서 닫힌 질문을 완료하기 위해 가능한 출력인 vy의 원시 위치

137
00:15:13,040 --> 00:15:19,680
모델을 미세 조정하기 위해 더 적은 수의 예를 사용하는 미래 학습을 사용하므로 px 단위입니다.

138
00:15:19,680 --> 00:15:25,360
여러 pvp가 필요하며 이들의 조합을 사용하여 출력을 결정합니다.

139
00:15:27,200 --> 00:15:32,880
이러한 pvps는 레이블이 지정되지 않은 예제에 최종 확률을 할당하는 데 사용됩니다.

140
00:15:32,880 --> 00:15:38,400
그 결과 레이블이 지정되지 않은 각 입력이 가능한 출력에 대한 확률 분포가 생성됩니다.

141
00:15:40,720 --> 00:15:48,000
ipet 또는 pet의 반복 변형은 흐름이 세 가지 주요 단계로 구성되는 pet과 유사합니다.

142
00:15:48,000 --> 00:15:54,880
방금 논의한 것처럼 애완동물을 사용하여 mlms 앙상블을 훈련하는 첫 번째 초기 훈련입니다.

143
00:15:54,880 --> 00:16:01,440
두 번째는 이 앙상블의 각 모델이 무작위 하위 집합을 사용하는 새로운 훈련 세트를 만드는 것입니다.

144
00:16:02,400 --> 00:16:09,920
레이블이 지정되지 않은 예제 집합에 레이블을 지정하는 모델 세 번째는 각 모델이 다음과 같은 재교육 모델입니다.

145
00:16:09,920 --> 00:16:16,800
새로 생성된 훈련 세트로 재훈련되고 2단계와 3단계가 여러 번 반복됩니다.

146
00:16:16,800 --> 00:16:24,800
훈련 세트에 더 많은 예제를 추가하여 훈련 세트의 크기를 변경합니다.

147
00:16:24,800 --> 00:16:30,800
마스크는 예측 단어가 더 길다는 점만 제외하면 단일 마스크를 사용하는 애완동물과 유사합니다.

148
00:16:30,800 --> 00:16:36,000
예제에 나온 것과 같이 여러 개의 마스크로 나누어야 합니다.

149
00:16:38,160 --> 00:16:47,440
여기서 입력 x는 끔찍한 피자입니다. 그 뒤에는 닫힌 질문을 형성하는 여러 마스크가 있습니다.

150
00:16:47,440 --> 00:16:53,600
여기에서는 입력을 출력으로 매핑하는 대신 가능한 출력으로 다시 매핑합니다.

151
00:16:53,600 --> 00:16:59,600
그런 다음 완전한 닫힌 질문에 맞는 최상의 세트를 찾기 위해 프로세스가 반복됩니다.

152
00:17:01,440 --> 00:17:06,800
레스토랑 리뷰의 감정 분류를 예로 들면,

153
00:17:06,800 --> 00:17:13,920
리뷰가 플러스 1인 긍정적인 리뷰와 마이너스 1인 부정적인 리뷰로 평가될 확률을 계산합니다.

154
00:17:13,920 --> 00:17:21,280
각 세트에 대한 마스크 토큰의 각 부분에 맞을 수 있는 각 단어를 찾아 최종적으로 가장 좋은 토큰을 다시 계산합니다.

155
00:17:21,280 --> 00:17:29,120
닫힌 질문 애완동물에 맞는 것은 Albert를 기반으로 하며 저자는 고급 방법을 사용했습니다.

156
00:17:29,120 --> 00:17:35,360
superglue로 알려진 접착제 벤치마크 버전 이 벤치마킹 도구에는 더 어려운 작업이 포함되어 있습니다.

157
00:17:35,360 --> 00:17:44,800
bool Cube popa 등과 같이 저자는 몇 가지 접착제로 알려진 슈퍼 접착제의 변형을 개발했습니다.

158
00:17:44,800 --> 00:17:51,360
pet 및 gpt3와 같은 언어 모델을 평가합니다. 소수 접착제는 무작위로 32개를 선택하여 개발되었습니다.

159
00:17:51,360 --> 00:17:57,040
일관되고 통제된 데이터 선택을 보장하기 위한 각 초강력 접착제 작업의 예

160
00:17:57,840 --> 00:18:04,800
또한 라벨을 제거하여 각 작업에 대해 최대 20,000개의 라벨이 지정되지 않은 예시 세트를 만들었습니다.

161
00:18:04,800 --> 00:18:10,720
비지도 및 준지도 학습 모델을 평가하기 위해 원래 훈련 세트에서

162
00:18:10,720 --> 00:18:20,400
무작위로 선택된 32개의 애완동물 및 아이패드 모델이 성능을 능가하는 결과를 보여줍니다.

163
00:18:20,400 --> 00:18:27,840
Albert xx Large v2 uh 대부분의 gpt3 모델보다 몇 가지 접착제로 훈련한 후의 예

164
00:18:27,840 --> 00:18:35,920
초강력 접착제 이것은 애완동물이 정확도를 능가하거나 거의 일치하는 확장된 예입니다.

165
00:18:35,920 --> 00:18:45,040
오늘날 가장 많이 사용되는 모델 중 네 번째 논문은 사전 훈련된 언어 모델을 더 좋게 만드는 것입니다.

166
00:18:45,040 --> 00:18:52,240
짧은 학습자를 위해 이 문서는 우리가 논의한 이전 문서를 기반으로 하며 더 나은 lmbf를 소개합니다.

167
00:18:52,240 --> 00:18:58,800
언어 모델의 몇 가지 짧은 미세 조정은 다음을 포함하는 3단계 프로세스입니다.

168
00:18:58,800 --> 00:19:05,200
새 작업을 자동으로 프롬프트로 변환합니다.

169
00:19:05,200 --> 00:19:13,200
훈련 데이터가 프롬프트에 추가된 다음 작품에 입력되면 이를 다음과 같은 작업으로 볼 수 있습니다.

170
00:19:13,200 --> 00:19:20,160
이 프롬프트의 기본 형식에 맞는 프롬프트로 변환될 수 있는 입력입니다.

171
00:19:20,160 --> 00:19:26,720
또한 각 수업의 예가 포함되어 있으며 긍정적이고 긍정적인 각 수업의 한 예로 볼 수 있습니다.

172
00:19:26,720 --> 00:19:34,800
레스토랑에 대한 리뷰에서 부정적인 등급 첫 번째 단계는 신속한 기반 미세 조정입니다.

173
00:19:34,800 --> 00:19:41,440
입력 작업은 예를 들어 uh의 cls 토큰을 사용하여 lm에 의해 올바른 형식에 맞게 변환됩니다.

174
00:19:41,440 --> 00:19:47,840
서로 다른 입력 사이에 시작 및 설정 토큰 이 입력에는 마스크된 단어도 포함됩니다.

175
00:19:47,840 --> 00:19:55,120
이것은 마스킹된 토큰입니다. 이 프롬프트는 가능한 직접 입력에 대한 확장된 답변입니다.

176
00:19:55,120 --> 00:20:01,200
예를 들어, 어 어디에서 프롬프트가 나올지 지켜볼 이유가 없듯이 훌륭했거나 끔찍했습니다

177
00:20:03,520 --> 00:20:09,840
이러한 작업은 단일 확률을 분류하기 위한 분류 또는 회귀 작업일 수 있습니다.

178
00:20:09,840 --> 00:20:16,640
작업 레이블 공간을 개별 단어로 분류하는 데 도움이 되도록 마스크된 단어에 대해 계산됩니다.

179
00:20:16,640 --> 00:20:22,640
회귀 프롬프트에 기초한 어휘에서 최종 확률은 모든 확률의 합입니다.

180
00:20:22,640 --> 00:20:31,360
가능한 세트의 확률 예 긍정 및 부정 두 번째 단계 어 자동

181
00:20:31,360 --> 00:20:36,960
프롬프트 생성은 라벨 단어 자동 선택과 자동 선택 두 부분으로 나뉩니다.

182
00:20:36,960 --> 00:20:44,320
레이블 단어 자동 선택에서 템플릿 생성 상위 k 어휘 단어 세트

183
00:20:44,320 --> 00:20:52,240
저자가 대규모로 사용한 템플릿의 자동 생성에서 각 클래스에 대해 구성됩니다.

184
00:20:52,240 --> 00:20:59,280
훈련 세트에서 문장을 입력하고 채우기 위한 사전 훈련된 텍스트-텍스트 변환기 t5

185
00:20:59,280 --> 00:21:05,680
프롬프트에 대한 초기 템플릿을 생성하기 위한 x 및 y와 같은 템플릿 토큰 어 이 모든 것

186
00:21:05,680 --> 00:21:12,720
템플릿은 라벨 매핑 구성표를 사용하여 구축되었습니다. 어 이거 어 최종 출력 템플릿을 찾는 데 도움이 됩니다.

187
00:21:12,720 --> 00:21:19,520
그게 가장 잘 맞습니다. 음 이것은 데이터 세트 d def에서 테스트하여 수행됩니다.

188
00:21:21,520 --> 00:21:27,520
마지막 단계에서는 이전에 생성된 프롬프트를 사용하여 llm의 미세 조정이 수행됩니다.

189
00:21:27,520 --> 00:21:34,560
그리고 이를 각 클래스의 예로 확장하면 이유가 없는 예에서 볼 수 있습니다.

190
00:21:34,560 --> 00:21:42,480
보는 것은 입력이고 마스크였던 템플릿이 마지막에 추가되었습니다.

191
00:21:42,480 --> 00:21:48,480
재미있는 라이딩에 대한 긍정적인 시연은 훌륭했고 부정적인 시연은 훌륭했습니다.

192
00:21:48,480 --> 00:21:54,240
드라마는 아무것도 공개하지 않았습니다. 끔찍한 일도 프롬프트에 추가되었습니다.

193
00:21:54,240 --> 00:22:00,400
몇 가지 짧은 학습을 돕기 위해 전체 데이터 세트에 대해 LLM을 미세 조정하기 위해 반복되었습니다.

194
00:22:00,720 --> 00:22:10,320
어 평가를 위해 저자는 8개의 단일 문장과 7개의 문장에 대해 roberta Large를 사용합니다.

195
00:22:10,320 --> 00:22:17,200
문장 쌍 uh 쌍 영어 작업에는 접착제 벤치마크의 8개 작업이 포함됩니다 uh 스탠드

196
00:22:17,200 --> 00:22:23,680
언어 자연어 어 추론 snli 외 6개의 인기 문장 분류

197
00:22:23,680 --> 00:22:30,160
작업 어 그리고 평균 성능은 무작위로 샘플링된 5개의 서로 다른 디트레인에서 측정됩니다.

198
00:22:30,160 --> 00:22:36,960
그리고 테이블의 dd def 분할을 통해 이에 사용된 템플릿을 볼 수 있습니다.

199
00:22:40,160 --> 00:22:46,080
결과는 훌륭합니다. 이 방법은 숏이 전혀 없는 프롬프트보다 성능이 뛰어나다는 것을 알 수 있습니다.

200
00:22:46,080 --> 00:22:53,120
대부분의 경우 gpd 3의 미세 조정을 통한 상황 학습 음 이 결과는 또한 보여줍니다

201
00:22:53,120 --> 00:22:59,040
자동 프롬프트 생성과 수동 프롬프트 생성 모두 다른 모델보다 성능이 뛰어남

202
00:23:00,880 --> 00:23:07,600
이러한 결과는 앞서 논의한 대로 애완동물과의 3단계 접근 방식이 공정하게 비교되었음을 보여줍니다.

203
00:23:07,600 --> 00:23:15,120
그래프는 정확도 대 표준 미세 조정 인스턴스 대 lmbff를 보여줍니다.

204
00:23:15,120 --> 00:23:21,840
후자가 전자보다 성능이 뛰어납니다. 모든 사람이 코드를 확인하도록 적극 권장합니다.

205
00:23:21,840 --> 00:23:26,240
github에 있는 네 가지 논문 모두 잘 문서화되어 있고 사용하기 쉽습니다.

206
00:23:26,480 --> 00:23:33,040
이제 저는 이 프리젠테이션의 후반부를 다루겠습니다.

207
00:23:33,040 --> 00:23:38,320
프리픽스 튜닝 프롬프트 튜닝과 ia 큐브 등 매개변수 효율적인 미세 조정 방법

208
00:23:38,320 --> 00:23:42,720
그런 다음 peft를 더 광범위하게 이해하는 방법을 살펴보겠습니다. 이는 peft에 대한 통일된 관점을 통해 이루어집니다.

209
00:23:43,760 --> 00:23:50,800
그리고 peft를 전체 모델 미세 조정과 비교하기 전에 먼저 소개하는 이 문서를 살펴보겠습니다.

210
00:23:50,800 --> 00:23:56,080
프롬프트 튜닝 및 프롬프트 튜닝을 다루기 전에 접두사 튜닝에 대한 배경 지식이 필요합니다.

211
00:23:56,480 --> 00:24:01,760
접두사 조정에서는 접두사인 작은 고정 추가 매개변수 세트를 학습합니다.

212
00:24:01,760 --> 00:24:08,400
따라서 접두사 조정을 수행하려면 조정 가능한 접두사 벡터를 준비하거나 조정 가능한 접두사를 l개 준비합니다.

213
00:24:08,400 --> 00:24:12,320
모든 레이어에서 다중 헤드 어텐션의 키와 값에 대한 벡터

214
00:24:14,560 --> 00:24:20,880
보다 공식적으로 수학적으로 쿼리 벡터 x와 m개의 벡터 시퀀스 c가 주어지면

215
00:24:20,880 --> 00:24:25,360
일반적으로 Attention을 수행하고 싶은 다중 헤드 Attention은 다음과 같은 형식을 취합니다.

216
00:24:25,440 --> 00:24:31,120
관련 벡터를 가져와 매핑하는 쿼리 키와 값이 있는 곳

217
00:24:31,840 --> 00:24:34,400
이러한 가중치 행렬을 통해 키와 값을 쿼리합니다.

218
00:24:36,720 --> 00:24:41,680
하지만 이제 접두사가 있는 다중 헤드 주의는 이제 키가 실제로 다음과 같이 보입니다.

219
00:24:41,680 --> 00:24:47,200
이전과 마찬가지로 키를 연결하지만 이제는 접두사가 붙은 키도 연결합니다.

220
00:24:48,160 --> 00:24:52,080
값과 동일하므로 이전과 마찬가지로 값을 연결합니다.

221
00:24:52,080 --> 00:24:58,080
그러나 이제 접두사 값을 사용하면 접두사 벡터 p sub k 및 p sub v의 두 세트는 다음과 같습니다.

222
00:24:58,080 --> 00:25:03,440
모든 주의 헤드와 모든 레이어에서 원래 키 및 값과 연결되어 있으며 이제

223
00:25:03,440 --> 00:25:08,720
우리는 모든 모델 매개변수 대신 p sub k와 p sub v를 학습하면 됩니다.

224
00:25:13,040 --> 00:25:17,600
이제 접두사 튜닝의 후손인 프롬프트 튜닝에 들어갑니다. 접두사 튜닝에서 우리는

225
00:25:17,600 --> 00:25:22,400
각 어텐션 헤드의 각 레이어에 있는 키와 값에 붙은 활성화 앞에 접두사를 붙였습니다.

226
00:25:22,400 --> 00:25:27,920
프롬프트 조정에서는 키 토큰 k 토큰을 입력 텍스트에 접두사로 붙인 다음 방금 배웠습니다.

227
00:25:27,920 --> 00:25:34,240
토큰 임베딩 우리 모두가 어텐션 토큰화에 익숙하다고 가정하지만 여기에 예가 있습니다.

228
00:25:34,240 --> 00:25:39,600
이것을 입력 텍스트로 가지고 있는 곳에서는 다음 토큰으로 분류됩니다. 시퀀스 토큰의 시작

229
00:25:39,600 --> 00:25:45,120
cls를 사용하면 기본적으로 단어가 토큰이 됩니다. 하위 단어 토큰화도 있지만

230
00:25:45,120 --> 00:25:50,000
여기에는 표시되지 않지만 기본적으로 이러한 토큰은 토큰 ID에 매핑되고 다음으로 매핑됩니다.

231
00:25:50,000 --> 00:25:57,120
임베딩 레이어를 통한 임베딩이지만 접두사가 붙은 토큰의 경우 실제로는 그렇지 않습니다.

232
00:25:57,120 --> 00:26:02,480
어휘의 토큰에 해당하며 우리가 학습하는 관련 임베딩이 있습니다.

233
00:26:03,040 --> 00:26:11,280
신속한 조정에서 이러한 방법은 어떻게 잘 평가됩니까? t5 lm 적응 모델을 사용하여 평가됩니다.

234
00:26:11,360 --> 00:26:17,600
모든 크기가 있으므로 작은 모델이 있는 곳에 일종의 규모가 표시됩니다.

235
00:26:18,240 --> 00:26:23,360
약 6천만 개의 매개변수가 있고 xxl 모델과 더 높은 범위에는 110억 개의 매개변수가 있습니다.

236
00:26:23,360 --> 00:26:29,120
매개변수와 모델은 컬렉션인 superglue 벤치마크를 사용하여 평가됩니다.

237
00:26:29,120 --> 00:26:35,920
8가지 도전적인 영어 이해 과제 중 하나는 boolq입니다.

238
00:26:35,920 --> 00:26:45,280
부울 답변을 믿는 것과 관련된 질문 응답 데이터 세트이므로 첫 번째 질문은

239
00:26:45,280 --> 00:26:50,960
이 문서에서 대답하려고 하는 것은 프롬프트를 어떻게 초기화해야 하는지와 세 가지 다른 대안이 있다는 것입니다.

240
00:26:50,960 --> 00:26:56,160
그 중 하나는 프롬프트가 무작위로 초기화되어 임베딩의 각 숫자가 다음과 같다는 것입니다.

241
00:26:56,160 --> 00:27:03,200
임의의 숫자나 프롬프트는 모델 어휘나 모델 어휘에서 무작위로 샘플링될 수 있습니다.

242
00:27:03,280 --> 00:27:09,280
어휘에서 가장 일반적인 5000개의 토큰을 무작위로 선택합니다. 어쩌면 개 하늘색과 보라색일 수도 있습니다.

243
00:27:09,280 --> 00:27:14,240
모두 자체 토큰을 형성하고 해당 무작위 토큰의 임베딩을 사용하여 초기화합니다.

244
00:27:14,240 --> 00:27:22,880
이를 사용하여 우리는 거기에서 대안으로 임베딩이 열거되는 것일 수도 있음을 배웁니다.

245
00:27:22,880 --> 00:27:27,600
출력 클래스 그래서 내가 구절이 있는 뉴스 분류 작업을 배우려고 한다면

246
00:27:27,600 --> 00:27:32,800
세상과 관련된 뉴스, 스포츠 관련 뉴스, 비즈니스 뉴스로 분류하려고 합니다.

247
00:27:33,360 --> 00:27:40,640
그런 다음 해당 출력 클래스를 사용하여 관련 토큰 임베딩을 찾고 초기화하면 됩니다.

248
00:27:40,640 --> 00:27:46,400
이를 사용하여 알림 메시지와 마찬가지로 거기에서 임베딩을 학습합니다.

249
00:27:46,400 --> 00:27:52,320
크기가 p 곱하기 e인 행렬인 매개변수 p sub e로 표시됩니다. 여기서 e는 차원입니다.

250
00:27:52,320 --> 00:27:59,920
임베딩 공간의 길이이고 p는 프롬프트 오른쪽의 길이이며 그들이 찾은 것은

251
00:27:59,920 --> 00:28:05,040
그들은 클래스 레이블 초기화가 가장 잘 수행된다고 말하며 또한 더 작은 모델 크기에서도 그렇게 말합니다.

252
00:28:05,040 --> 00:28:09,520
서로 다른 초기화 사이에는 큰 간격이 있지만 일단 모델이 확장되면

253
00:28:09,520 --> 00:28:15,840
xxl 사이즈를 사용하면 이러한 차이가 사라지지만 차트에 동일한 내용이 나와 있는지 묻고 싶었습니다.

254
00:28:15,840 --> 00:28:21,760
여기에 x축이 모델 매개변수이고 y축이 평균인 차트가 있습니다.

255
00:28:21,760 --> 00:28:26,080
superglue 점수를 얻은 다음 세 가지 방법으로 무작위로 균일한 샘플 어휘와 클래스 레이블을 얻습니다.

256
00:28:27,040 --> 00:28:30,880
그리고 클래스 라벨 초기화가 가장 잘 수행된다고 말하는 것이 먼저지만 차트에서는 다음과 같이 제안한다고 생각합니다.

257
00:28:30,880 --> 00:28:36,160
기본적으로 클래스 레이블 초기화와 샘플 어휘는 매우 유사하게 수행되며

258
00:28:36,160 --> 00:28:43,920
손을 잡고 걷고 무작위 유니폼의 경우 모델 크기가 작을수록 더 크다고 말합니다.

259
00:28:43,920 --> 00:28:49,920
격차가 있지만 일단 모델을 성장시키면 격차가 줄어들고 확실히 가장 큰 모델에서는 세 가지 방법 모두

260
00:28:49,920 --> 00:28:55,520
매우 유사하게 수행되지만 가장 작은 모델에서도 유사하게 수행됩니다.

261
00:28:55,520 --> 00:29:05,680
유니폼은 노이즈가 더 많고 두 번째 질문에 더 많은 분산이 있는 방법입니다.

262
00:29:05,680 --> 00:29:10,640
대답은 프롬프트가 얼마나 길어야 하는지이며 모든 모델에서 그 이상으로 증가한다는 것을 발견했습니다.

263
00:29:10,640 --> 00:29:16,480
20개의 토큰은 미미한 이익만 얻을 수 있으며 실제로 xxl 모델을 사용하면 여전히 강력한 결과를 얻을 수 있습니다.

264
00:29:16,480 --> 00:29:21,200
더 큰 모델의 경우 프롬프트가 그렇게 클 필요가 없음을 제안하는 단일 토큰 프롬프트

265
00:29:21,200 --> 00:29:26,400
실제로 흥미로운 점은 많은 매개변수를 학습할 필요가 없다는 것입니다.

266
00:29:26,400 --> 00:29:34,160
t5 xxl 모델의 단일 토큰 임베딩에는 차원이 1024이므로 1024개의 매개변수만 학습합니다.

267
00:29:34,720 --> 00:29:38,640
그리고 그것은 잘 작동하는 것 같아서 꽤 흥미롭습니다.

268
00:29:42,480 --> 00:29:47,360
흥미롭게도 다른 방법과 비교하면 논문에서는 효율성을 비교하지 않습니다.

269
00:29:47,360 --> 00:29:52,000
다른 경로 방법의 경우 효율성을 위해 전체 모델 미세 조정과의 비교만 수행됩니다.

270
00:29:52,560 --> 00:29:59,120
그들은 매개변수 사용을 다른 방법과 비교하고 있으며 신속한 조정이 가장 중요하다고 주장합니다.

271
00:29:59,120 --> 00:30:05,440
모델 크기 범위 전체에서 매우 적은 수의 매개변수를 학습하므로 매개변수 효율적인 방법입니다.

272
00:30:07,600 --> 00:30:14,080
그러나 효율성에 대해 다른 경로 방법과 비교하지 않았다는 점이 흥미롭습니다.

273
00:30:14,160 --> 00:30:18,240
한편으로 이러한 비교를 하려면 다른 한편으로는 이러한 비교를 재현해야 했을 것입니다.

274
00:30:18,240 --> 00:30:23,440
더 많은 작업이 필요한 방법이지만 반면에 나는 그들의 작업이 격려에 있다고 주장합니다.

275
00:30:23,440 --> 00:30:27,760
다른 경로 방법과의 효율성 비교가 없으면 방법이 불완전합니다.

276
00:30:31,200 --> 00:30:35,280
그들은 전체 모델의 미세 조정과 다시 비교하므로 우리는 이를 바로 보여줍니다.

277
00:30:35,280 --> 00:30:41,440
프롬프트 튜닝은 규모가 커질수록 더 효과적입니다. 더 강할수록 프롬프트 튜닝 일치를 보여줍니다.

278
00:30:41,440 --> 00:30:46,560
다중 작업 모델 튜닝 기준은 단일 모델이 모든 작업에 대해 튜닝되는 기준입니다.

279
00:30:46,560 --> 00:30:51,200
동시에 초강력 접착제 작업을 수행하므로 이것이 가장 강력한 기준이 되고 가장 강력한 결과를 얻습니다.

280
00:30:51,200 --> 00:30:57,760
결과는 충분하지만 t5 모델 이상의 경우 프롬프트 튜닝이 전체 성능과 동일하게 수행됩니다.

281
00:30:57,760 --> 00:31:05,040
모델 미세 조정 또는 다중 작업 전체 모델 미세 조정 네 번째 라인이 있어 신속한 설계가 가능합니다.

282
00:31:05,120 --> 00:31:11,920
파란색 선은 상황 학습 예시의 몇 장면에서 gpt3의 성능을 측정합니다.

283
00:31:13,440 --> 00:31:19,600
그래서 그들이 훈련하는 방법은 실제로 t5 모델을 사용하는 gpt3보다 더 낫습니다.

284
00:31:23,520 --> 00:31:27,840
그래서 제가 살펴볼 이 문서의 마지막 부분은 도메인 이동과 도메인에 대한 탄력성입니다.

285
00:31:27,840 --> 00:31:32,400
변화는 상황에 따른 데이터 특성의 변화입니다. 아마도 하나의 상황에서 훈련했을 것입니다.

286
00:31:32,400 --> 00:31:36,800
그런 다음 모델이 훈련된 이 예에서는 다른 맥락에서 모델을 평가하려고 합니다.

287
00:31:36,800 --> 00:31:42,800
스탠포드 질문 답변 데이터 세트는 다른 데이터 세트에 대해 평가됩니다.

288
00:31:42,800 --> 00:31:48,400
배포 권한이 있으므로 스탠포드 질문 답변 데이터 세트에는 위키피디아의 구절이 있습니다.

289
00:31:48,400 --> 00:31:54,240
그런 다음 해당 구절이 주어진 질문에 답하려고 시도하지만 실제로 교과서의 QA 데이터 세트는

290
00:31:54,960 --> 00:31:59,680
중학교 수준 교과서의 구절이 있는 것 같아요. 질문도 중학교 수준인 것 같아요

291
00:31:59,680 --> 00:32:06,800
레벨 지식 질문과 바이오 질문은 데이터 세트에 응답하는 생물 의학 질문이라고 생각합니다.

292
00:32:06,800 --> 00:32:14,080
도메인은 제가 옳다고 생각하는 생의학 논문이므로 즉각적인 튜닝 수학이 가능하다는 것을 알 수 있습니다.

293
00:32:14,080 --> 00:32:20,880
실제로 전체 모델 미세 조정에 비해 이러한 설정에서 매우 잘 수행되지만 이 표에서는 다시 한 번 언급합니다.

294
00:32:20,880 --> 00:32:26,000
전체 모델 미세 조정이 더 나은 방법이 있지만 어쨌든 저자는 주장합니다.

295
00:32:26,000 --> 00:32:30,720
핵심 언어 모델 매개변수를 동결함으로써 신속한 조정으로 인해 모델이 수정되는 것을 방지할 수 있습니다.

296
00:32:30,720 --> 00:32:35,040
언어에 대한 일반적인 이해로 인해 모델이 데이터에 과적합되는 능력이 감소합니다.

297
00:32:35,040 --> 00:32:39,520
나쁜 설정이고 우리가 원하는 것이 아니기 때문에 그들은 즉각적인 조정이 필요할 수 있다고 주장합니다.

298
00:32:39,520 --> 00:32:47,520
도메인 이동에 대한 견고성을 향상하지만 이 표를 있는 그대로 제시하므로 동의하지 않을 수도 있습니다.

299
00:32:47,520 --> 00:32:52,880
결과가 확실하지 않으면 명확한 프롬프트 튜닝이 더 좋아 보입니다.

300
00:32:56,800 --> 00:33:01,440
이제 뷰 샷 매개변수 효율적인 미세 조정이라는 제목의 두 번째 논문으로 넘어가겠습니다.

301
00:33:01,440 --> 00:33:07,600
새로운 경로 방법 iacubed를 도입하는 상황 학습보다 더 좋고 저렴합니다.

302
00:33:07,600 --> 00:33:14,640
이는 이 이름에서 유래되었으며 이 방법의 아이디어는 그들이 l sub kl sub v 및 l을 학습한다는 것입니다.

303
00:33:14,720 --> 00:33:21,840
l과 y의 곱셈을 통해 키와 키를 다시 스케일링하는 작업을 수행하는 벡터인 sub ff

304
00:33:21,840 --> 00:33:28,560
피드포워드 신경망이나 네트워크에서의 가치와 관심, 상호작용

305
00:33:30,400 --> 00:33:37,760
여기 주의 메커니즘이 있고 값은 lv 키를 사용하여 재조정됩니다.

306
00:33:37,760 --> 00:33:44,080
lk를 사용하여 재조정한 다음 조밀한 레이어와 비선형성 이후의 피드포워드 네트워크에서

307
00:33:44,800 --> 00:33:51,040
다음 조밀한 레이어로 들어가는 출력의 크기를 조정하는 l sub ff가 있습니다.

308
00:33:54,160 --> 00:33:58,640
그래서 다시 이 문서에서는 이 방법을 소개하고 pft가 icl보다 낫다고 주장합니다.

309
00:33:58,640 --> 00:34:04,160
몇 가지 샷 설정과 몇 가지 샷 학습은 몇 가지 예에서만 학습한다는 점을 상기시켜 드립니다.

310
00:34:04,960 --> 00:34:12,240
icl은 입력에 대한 다운스트림 작업을 수행하도록 모델을 유도하는 컨텍스트 학습에 있습니다.

311
00:34:12,240 --> 00:34:17,440
프롬프트된 예이므로 여기에 몇 개의 샷과 icl의 예가 있으므로 4개의 샷 입력이 됩니다.

312
00:34:17,440 --> 00:34:23,360
문맥상 문자를 단어로 풀어서 모델에 풀어달라고 요청합니다.

313
00:34:23,360 --> 00:34:29,840
이 문자를 입력하고 단어를 쓰세요. 예를 들어 asinoc이 얻는 네 가지 예는 다음과 같습니다.

314
00:34:29,840 --> 00:34:38,880
카지노로 다시 출격해서 우리는 마지막 예인 astedro를 풀어보고 싶었습니다.

315
00:34:38,880 --> 00:34:46,000
이제 상황에 맞게 이점을 잘 익히면서 문제가 무엇인지 다시 확인하시기 바랍니다.

316
00:34:46,000 --> 00:34:51,440
저자는 icl이 단일 모델이 별도의 작업 없이 많은 작업을 수행할 수 있게 해준다고 말합니다.

317
00:34:51,440 --> 00:34:58,000
하지만 단점은 모델이 필요하기 때문에 예측하는 데 비용이 많이 든다는 것입니다.

318
00:34:58,000 --> 00:35:01,920
예측이 이루어질 때마다 모든 상황 내 예시를 처리합니다.

319
00:35:02,160 --> 00:35:08,000
따라서 상황에 맞게 학습하는 대신 peft가 유망한 대안입니다.

320
00:35:10,640 --> 00:35:15,680
그래서 그들이 방법을 평가하는 방법은 미세 조정을 통해 생성된 t0 모델을 사용하는 것입니다.

321
00:35:15,680 --> 00:35:20,880
각 데이터세트에 대해 제로샷 일반화를 가능하게 하기 위해 데이터세트를 혼합한 t5 모델

322
00:35:20,880 --> 00:35:25,920
그들은 사용된 몇 가지 샷 예제의 수를 20에서 70까지 다양하게 테스트하며 이는 임의적이지 않습니다.

323
00:35:25,920 --> 00:35:30,880
종이 언어 모델의 평가와 동일한 숫자를 일치시키기 위해 그렇게 합니다.

324
00:35:30,880 --> 00:35:36,960
소수의 샷 학습자 그리고 그것은 gp3 모델을 사용한 소수의 샷 학습을 살펴보는 논문입니다.

325
00:35:36,960 --> 00:35:49,200
openai의 논문과 그들이 고려하는 데이터 세트는 다음 네 가지가 있다는 것을 알고 있습니다.

326
00:35:50,000 --> 00:35:54,320
영역 문장 완성 자연어 추론 상호 참조 해결 및 단어

327
00:35:54,320 --> 00:36:00,160
명확성을 매우 빠르게 감지 문장 완성 당신은 한 문장이 주어진 것을 알고 있습니까?

328
00:36:00,160 --> 00:36:07,840
그냥 완료하세요 자연어 추론에는 전제가 주어집니다.

329
00:36:07,840 --> 00:36:14,000
또 다른 전제는 첫 번째 문장이나 첫 번째 전제에 논리적으로 수반됩니다.

330
00:36:14,000 --> 00:36:20,480
너도 알잖아, 도트 도트 도트 그녀가 Ron과 골프를 치고 있을 때 그녀의 전화가 울렸는데 그것은 Liz의 어머니 전화였어

331
00:36:20,480 --> 00:36:27,600
친구, Liz가 Taylor에게 전화했다고 말하면 Ron이 Taylor에게 전화했다고 말하면 논리적으로 수반됩니다.

332
00:36:27,600 --> 00:36:31,360
그건 말도 안 되는 모순인데, 테일러라는 의사를 말한다면

333
00:36:32,640 --> 00:36:41,600
아마도 리즈는 의사이기 때문에 논리적으로 상호 참조 해결이 인식되는지 알 수 없습니다.

334
00:36:41,600 --> 00:36:47,520
엔터티가 일부 텍스트 및 단어에서 다른 엔터티와 동일한 것을 참조하는지 여부

335
00:36:47,520 --> 00:36:52,960
의미 명확성은 우리가 뭔가를 말할 때 그 단어가 어떤 의미로 의미되는지 알아내는 것입니다.

336
00:36:52,960 --> 00:36:57,680
창문을 열어서 신선한 공기가 들어오게 해주세요. 어떤 종류인지 아시나요?

337
00:36:57,680 --> 00:37:02,640
우리가 밖을 보기 위해 내다보는 창, 아니면 운영체제에 대해 이야기하는 걸까요?

338
00:37:06,000 --> 00:37:12,400
그래서 여기에 그들이 iac 세제곱을 다른 경로 방법과 비교한 결과가 있습니다.

339
00:37:13,840 --> 00:37:20,960
iac는 icl을 사용하여 tfue 모델에서 세제곱되므로 실제로는 꽤 혼란스럽습니다.

340
00:37:20,960 --> 00:37:26,320
먼저 설명해야 할 것 중 하나는 peft 방법과의 비교입니다.

341
00:37:26,320 --> 00:37:31,360
모델의 30억 매개변수 버전과 icl과의 비교는 11로 이루어졌습니다.

342
00:37:31,360 --> 00:37:35,280
30억으로 평가하는 것이 더 빠르다고 말하는 모델의 10억 매개변수 버전

343
00:37:36,800 --> 00:37:39,520
매개변수 모델은 더 작고 작업하기 쉽기 때문에

344
00:37:42,400 --> 00:37:48,000
따라서 왼쪽 그림부터 시작하면 iac Cubed가 실제로 아주 잘 작동한다는 것을 알 수 있습니다.

345
00:37:48,000 --> 00:37:52,160
점선으로 표시된 모델 미세 조정은 다른 방법보다 우수합니다.

346
00:37:53,280 --> 00:37:58,240
어댑터 프롬프트 튜닝 접두사 튜닝 및 Laura와 같이 우리가 다룬 이들 중 일부

347
00:37:59,600 --> 00:38:04,640
상대적으로 적당한 양으로 업데이트된 매개변수의 양이 많지 않은 경우에도 마찬가지입니다.

348
00:38:06,160 --> 00:38:11,920
이제 icl과 비교하면 이러한 기준선은 직관적이지 않으므로 tfue는 단지

349
00:38:11,920 --> 00:38:17,200
T0를 iac 입방체로 미세 조정한 다음 이를 평가하면 가장 좋은 성능을 발휘합니다.

350
00:38:17,760 --> 00:38:22,960
정확도 측면에서 x축은 플롭 수입니다. 예를 들어 플로팅 양은

351
00:38:22,960 --> 00:38:32,240
예측을 올바르게 하려면 포인트 연산이 필요하므로 t0은 t0 110억 매개변수를 평가합니다.

352
00:38:32,240 --> 00:38:36,480
제로샷 설정에서 모델을 구축하므로 교육이나 대면 학습 없이도 가능합니다.

353
00:38:37,280 --> 00:38:41,840
그리고 그것은 실제로 상당히 잘 수행됩니다. 접촉 학습 설정에서는 gpt3보다 더 낫습니다.

354
00:38:41,920 --> 00:38:48,000
이는 t5 plus lm을 포함하는 관련 기준입니다. 실제로 t5 lmadapt 11을 평가합니다.

355
00:38:48,000 --> 00:38:55,920
앙상블 icl이 포함된 10억 매개변수 모델이므로 icl을 사용하고 있으며 컨텍스트 창이

356
00:38:55,920 --> 00:39:04,640
t5의 각 예는 기본적으로 자체 컨텍스트이고 예측은 너무 작습니다.

357
00:39:04,640 --> 00:39:09,440
평균이므로 4개의 예가 있으면 t5 모델을 사용하여 4개의 예측이 이루어지고

358
00:39:09,440 --> 00:39:15,280
이러한 예측은 평균화되었으며 다음을 보여주기 때문에 t0 모델로 icl을 수행하지 않습니다.

359
00:39:15,280 --> 00:39:21,040
실제로 결과는 icl을 사용하지 않는 것보다 나빴으므로 t5에 대한 icl의 기본 모델은

360
00:39:21,040 --> 00:39:27,520
모델은 실제로 해당 그림에 있는 모든 방법 중 최악의 방법을 수행하는 것입니다.

361
00:39:27,520 --> 00:39:34,320
비교적 잘 수행되는 강력한 icl 기준인 gpt3가 있지만 시간이 많이 걸립니다.

362
00:39:34,320 --> 00:39:38,640
계산할 부동 소수점 연산이 있지만 실제로는 여전히 tfue만큼 잘 수행되지 않습니다.

363
00:39:42,160 --> 00:39:48,160
그래서 지금은 효율성에 대한 논쟁을 벌이고 있는데 tfue 방법은 추론 측면에서 빛을 발합니다.

364
00:39:48,160 --> 00:39:53,600
부동 소수점 연산은 다른 방법에 비해 예측하는 것이 매우 저렴합니다.

365
00:39:54,160 --> 00:40:01,840
특히 icl 방법은 다른 모델과 달리 훈련된 모델이라고 합니다.

366
00:40:01,840 --> 00:40:10,400
메서드를 사용하므로 비용이 발생하므로 플롭은 다시 부동 소수점 연산입니다.

367
00:40:10,400 --> 00:40:17,360
단일 예에 대해 예측을 하고 다시 tfue 방법이 낮은 수준에서 최선을 다합니다.

368
00:40:17,360 --> 00:40:23,600
부동 소수점 연산을 추론하지만 훈련하는 데 약간의 비용이 들고 디스크 공간이 충분하지 않습니다.

369
00:40:23,600 --> 00:40:29,920
너무 흥미롭지만 기본적으로 모델의 매개변수를 저장해야 하므로 추가 비용이 발생합니다.

370
00:40:29,920 --> 00:40:35,360
해당 모델을 저장하려면 기본적으로 아무 것도 아닌 4MB의 디스크 공간 사용량을 알고 있어야 합니다.

371
00:40:35,360 --> 00:40:41,600
왜냐하면 이 모델과 그 무게는 이미 110억 개의 데이터를 저장하는 데 기가바이트가 걸리기 때문입니다.

372
00:40:41,600 --> 00:40:50,320
매개변수 모델은 저장 공간이 30GB 정도이고 gpt3 모델은 170 정도인 것 같아요.

373
00:40:50,320 --> 00:40:55,040
10억 개의 매개변수를 저장하려면 많은 양이 필요하지만 4MB는

374
00:40:55,040 --> 00:41:00,880
추가 매개변수이며 16킬로바이트는 각 작업에 대한 입력 프롬프트를 저장하는 데 드는 비용입니다.

375
00:41:01,680 --> 00:41:09,120
16킬로바이트는 기본적으로 지금은 아무것도 아닙니다. 이 작업에 대한 비판을 공유하고 싶었습니다.

376
00:41:09,760 --> 00:41:14,800
하나는 실험이 내가 보고 싶은 최적의 분류 작업에 대해서만 수행되었다는 것입니다.

377
00:41:15,600 --> 00:41:19,520
생성적 질문 답변과 같은 다른 작업에 대한 실험

378
00:41:20,400 --> 00:41:24,320
다른 비판은 가치 평가가 t0 모델을 사용하여 수행되었다는 것입니다.

379
00:41:24,320 --> 00:41:28,240
최적의 매개변수 효율적인 미세 조정 방법으로서 다른 모델에서 평가되어야 합니다.

380
00:41:28,240 --> 00:41:35,920
모델 역시 t0 모델은 인코더 디코더 모델일 수 있습니다. 아마도 몇 가지 실험을 수행해야 할 것입니다.

381
00:41:35,920 --> 00:41:43,120
디코더 전용 모델이 많은 개방형 모델인 Burp 모델 제품군과 같은 인코더 전용 모델

382
00:41:43,120 --> 00:41:46,000
오늘 우리가 보는 것은 라마나 미스트랄이에요

383
00:41:48,320 --> 00:41:52,560
마지막 비판은 페프트 방식의 평가가 소수샷 설정에서만 이루어진다는 것이다.

384
00:41:52,560 --> 00:41:58,400
그리고 그것이 내가 알고 있는 논문의 요점이지만 평가를 보고 싶었습니다.

385
00:41:58,400 --> 00:42:03,280
많은 교육 데이터를 사용할 수 있는 보다 높은 리소스 설정에서 이 방법을 사용합니다.

386
00:42:07,360 --> 00:42:11,920
이제 매개변수 효율성에 대한 통합된 관점을 지향하는 이 문서로 이동했습니다.

387
00:42:11,920 --> 00:42:17,840
접두사 조정을 어댑터와 연관시킨 다음 어댑터에서 새로운 통찰력을 얻으려고 시도하는 전이 학습

388
00:42:17,840 --> 00:42:25,520
그 관계는 주로 내가 방금 언급한 것처럼 어댑터의 접두사를 낮출 수 있도록 대답하려고 합니다.

389
00:42:25,520 --> 00:42:29,840
튜닝은 서로 관련되어 있으며 실제로 이러한 튜닝에 대한 통일된 이해가 가능합니다.

390
00:42:29,840 --> 00:42:34,640
훨씬 더 나은 매개변수 효율적인 미세 조정 방법을 생성할 수 있습니다.

391
00:42:35,600 --> 00:42:41,760
우리는 어댑터라고 부릅니다. 어 우리는 이미 이것을 다뤘지만 기본적으로 어댑터는

392
00:42:42,560 --> 00:42:48,480
접근 방식은 변환 레이어 사이에 작은 모듈을 삽입하므로 하향 투영 행렬이 있습니다.

393
00:42:48,480 --> 00:42:52,720
투영 행렬과 일반적인 선형 활성화 함수는 매우 빠르게 진행됩니다.

394
00:42:52,720 --> 00:42:58,160
하지만 기본적으로 어댑터가 수행하는 작업에 대한 새로운 기능적 형식을 파생할 수 있습니다.

395
00:42:58,160 --> 00:43:05,360
여기서 새로운 숨겨진 상태는 이전 숨겨진 상태에 어댑터의 출력을 더한 것과 같습니다.

396
00:43:05,360 --> 00:43:11,440
이전 은닉 상태 활성화에 하향 투영을 곱한 다음 모든 행렬을 곱합니다.

397
00:43:11,440 --> 00:43:18,720
상향 투영을 사용하므로 나중에 이것을 기억해 두겠습니다. 비슷한 함수를 파생해 보겠습니다.

398
00:43:18,720 --> 00:43:25,840
접두사 튜닝을 위한 양식이므로 다시 접두사 튜닝을 수행하기 위해 이에 대해 이야기했지만 두 세트를 기억해 보겠습니다.

399
00:43:25,920 --> 00:43:30,720
의 접두사 벡터는 모든 주의에서 원래 키 및 값과 연결됩니다.

400
00:43:30,720 --> 00:43:35,360
모든 레이어의 머리에 벡터 x와 m개의 벡터 c 시퀀스에 대한 쿼리가 제공됩니다.

401
00:43:35,360 --> 00:43:42,160
우리는 접두사 튜닝이 이러한 형식을 올바르게 취한다는 점에 주목하고 싶습니다.

402
00:43:44,400 --> 00:43:50,480
우리는 pk와 키를 연결한 키에 대한 쿼리를 가지고 있습니다.

403
00:43:50,480 --> 00:43:56,640
그렇지 않으면 pv와 값을 연결한 값입니다.

404
00:43:56,640 --> 00:44:03,280
그렇지 않으면 접두사 조정 없이 이제 이 명령문은 실제로 다음과 같이 다시 작성될 수 있으며 건너뛰었습니다.

405
00:44:03,280 --> 00:44:10,240
몇 단계만 거치면 되지만 어 기본적으로 람다 x가 다음을 나타내는 스칼라인 이 형식을 취합니다.

406
00:44:10,240 --> 00:44:16,800
접두사에 대한 정규화된 주의 가중치의 합과 이 공식에서 i 및 j 인덱스 값

407
00:44:16,800 --> 00:44:23,440
벡터는 기본적으로 람다 x는 스칼라이며 접두사 조정을 다음 형식으로 다시 작성했습니다.

408
00:44:24,080 --> 00:44:29,920
접두사가 있는 쿼리 키와 값이 있는 이 양식에만 관심이 있었기 때문에

409
00:44:29,920 --> 00:44:38,400
튜닝은 일반적으로 주의 사항이므로 분리했지만 어 추가 사항도 추가되었습니다.

410
00:44:38,400 --> 00:44:45,760
접두사 조정을 통한 수정으로 기능적 측면에서 접두사 조정에 대한 대체 보기

411
00:44:45,760 --> 00:44:53,440
우리가 생각해 왔던 형태는 이런 형태입니다. 맞습니다. h는 이전과 마찬가지로 1 - 람다 xh를 취합니다.

412
00:44:53,440 --> 00:45:00,080
플러스 람다 x 시간은 h에 대한 수정이고, 델타 h는 원본에 대한 수정입니다.

413
00:45:00,080 --> 00:45:06,080
머리 주의 출력 h는 선형 보간을 통해 이루어지며 이로 인해 선형 보간이 됩니다.

414
00:45:06,160 --> 00:45:08,320
스칼라 1 - 람다 x 및 람다 x

415
00:45:12,720 --> 00:45:18,320
이제 다시 어댑터의 접두사 조정과 관련하여 접두사에 대한 형식의 기능이 있습니다.

416
00:45:18,320 --> 00:45:25,920
조정하고 w sub one을 다음과 같이 정의하면 이제 어댑터에 대한 기능적 형식이 있습니다.

417
00:45:25,920 --> 00:45:31,520
그리고 w sub 2는 이것으로, f는 Softmax 함수로 사용하면 우리가 가지고 있는 것을 다시 작성할 수 있습니다.

418
00:45:32,080 --> 00:45:37,440
이를 제외하고는 어댑터와 매우 유사해 보이는 접두사 조정의 기능적 형태가 있습니다.

419
00:45:37,440 --> 00:45:40,800
접두사 조정은 람다 때문에 가중치 추가를 수행합니다.

420
00:45:48,000 --> 00:45:52,960
이제 우리는 매우 유사한 기능적 형태를 가지게 되었지만 몇 가지 중요한 차이점이 있습니다.

421
00:45:52,960 --> 00:45:59,120
하나는 접두사 조정이 x를 사용하여 h에 대한 변경 사항을 계산하는 반면 어댑터는 h를 사용하여 계산한다는 것입니다.

422
00:45:59,200 --> 00:46:04,240
h로 변경되므로 병렬 계산과 순차 계산에 차이가 있습니다.

423
00:46:04,240 --> 00:46:07,840
어댑터에 있는 이 그래픽을 보면 더 쉽게 이해할 수 있습니다.

424
00:46:08,480 --> 00:46:16,000
먼저 plm 모듈을 통해 입력 x를 전달하여 h를 얻은 다음 해당 h를 두 번째 모듈로 가져옵니다.

425
00:46:16,000 --> 00:46:25,040
접두사 조정에서는 새로운 숨겨진 상태를 계산하는 어댑터 분기를 알고 있습니다.

426
00:46:25,600 --> 00:46:30,400
실제로 전달되는 두 번째 분기로 이동하기 위해 plm 모듈을 통해 x를 전달하지 않습니다.

427
00:46:30,400 --> 00:46:35,360
바로 두 번째 브랜치로 들어가므로 기본적으로 접두사 조정 없이 h를 갖게 됩니다.

428
00:46:35,360 --> 00:46:42,640
h를 접두사 튜닝으로 추가한 다음 이를 함께 추가할 수 있습니다. 또 다른 차이점은 다음과 같습니다.

429
00:46:42,640 --> 00:46:46,880
어댑터는 삽입 위치와 관련하여 더 유연합니다. 어댑터는 두 가지 모두를 수정할 수 있습니다.

430
00:46:46,880 --> 00:46:53,120
주의뿐만 아니라 피드 포워드 순 출력도 있지만 접두사 튜닝은 주의만 수정합니다.

431
00:46:53,120 --> 00:46:58,720
각 헤드의 출력이므로 이 변압기 블록에서 어댑터가 있는 것을 볼 수 있습니다.

432
00:46:58,720 --> 00:47:04,320
주의 모듈 바로 뒤에 여기에 삽입하거나 어댑터를 주의 모듈 바로 뒤에 여기에 삽입할 수 있습니다.

433
00:47:04,320 --> 00:47:10,640
피드 포워드 네트워크인 반면 접두사 튜닝에서는 p sub k와 p sub b만

434
00:47:11,920 --> 00:47:15,040
기본적으로 주의 모듈 내에서 내용을 변경합니다.

435
00:47:15,120 --> 00:47:24,240
이제 접두사 튜닝 어댑터와 Laura의 차이점을 요약하겠습니다.

436
00:47:25,040 --> 00:47:30,960
유사하고 관련성이 높습니다. 어 먼저 방금 어댑터에 대해 언급한 것처럼 삽입 양식을 살펴보겠습니다.

437
00:47:30,960 --> 00:47:36,480
순차적이지만 접두사 조정이 가능하고 Laura도 어 병렬이므로 알 수 있을 것입니다.

438
00:47:36,480 --> 00:47:45,680
방금 언급한 것처럼 Laura가 어떤 표현을 수정하는지에 대한 이 그래픽은 어댑터가 할 수 있습니다.

439
00:47:45,680 --> 00:47:51,040
피드 포워드 네트워크 부분과 주의를 모두 수정하는 반면 접두사 튜닝은 수정만 할 수 있습니다.

440
00:47:51,040 --> 00:47:57,600
머리 주의와 Laura는 주의에 있는 키와 값만 수정할 수 있습니다. 그래서 Laura는

441
00:47:57,600 --> 00:48:02,640
다시 변환기 블록에서 Laura는 여기서 쿼리에 대한 가중치 행렬을 수정하고

442
00:48:02,640 --> 00:48:09,120
키에 대한 가중치 행렬 그리고 우리가 찾고 있던 이러한 구성 함수가 있습니다.

443
00:48:09,120 --> 00:48:13,680
어 그리고 기본적으로 Laura를 위한 구성 기능이 있는 Laura를 위한 새로운 기능이 있습니다.

444
00:48:13,680 --> 00:48:20,640
어댑터와 매우 비슷해 보이지만 기본적으로 확장 가능한 하이퍼 매개변수가 추가되어 있습니다.

445
00:48:20,640 --> 00:48:25,600
h에 대한 변경이고 여기에 델타 h에 대한 함수 형식이 있습니다.

446
00:48:26,080 --> 00:48:33,760
이제 이러한 방법을 사용하여 새로운 경로 방법을 공식화하는 방법에는 여러 가지가 있습니다.

447
00:48:34,400 --> 00:48:41,200
이러한 방법들 사이의 차이점이 있으므로 이들 중 어떤 변형이 있는지 테스트하고 비교할 수 있습니다.

448
00:48:41,200 --> 00:48:47,600
차이점이 더 잘 형성되므로 병렬 삽입 수정과 순차 삽입 수정을 비교할 수 있습니다.

449
00:48:47,600 --> 00:48:51,280
주의 표현과 피드 포워드 네트워크 표현 수정 비교

450
00:48:51,840 --> 00:48:54,640
구성 기능의 크기를 조정하거나 크기를 조정하지 않은 상태로 둡니다.

451
00:49:05,520 --> 00:49:10,240
그래서 평가에서 이것은 뉴스인 x 합계 데이터 세트를 사용하는 데이터 세트입니다.

452
00:49:10,240 --> 00:49:16,240
기사 요약 데이터 세트 영어에서 루마니아어 번역 m 및 자연어인 li

453
00:49:16,320 --> 00:49:21,360
감정 분류 데이터 세트인 sst2의 추론 데이터 세트는 다음을 평가합니다.

454
00:49:21,360 --> 00:49:26,800
이러한 모델을 사용하는 데이터 세트는 인코더-디코더 생성 모델인 대규모 모델입니다.

455
00:49:27,760 --> 00:49:33,280
요약 및 번역 작업을 위한 인코더 모델인 reberda base

456
00:49:33,280 --> 00:49:38,640
단지 분류 작업으로 표시될 수 있는 mnli 및 sst2 작업

457
00:49:42,080 --> 00:49:44,880
이제 첫 번째 질문은 순차 또는 병렬 중 어느 것이 더 낫습니까?

458
00:49:46,240 --> 00:49:52,320
병렬 삽입을 사용하는 접두사 튜닝은 순차적 주의 주의보다 성능이 뛰어납니다.

459
00:49:52,320 --> 00:50:02,160
순차 어댑터이므로 이 표에서 병렬 어댑터는 기본적으로

460
00:50:02,800 --> 00:50:07,760
순차 튜닝과 병렬 방식인 프리픽스 튜닝보다 성능이 좋습니다.

461
00:50:07,760 --> 00:50:09,680
순차 어댑터보다 더 나은 성능을 발휘합니다.

462
00:50:10,320 --> 00:50:19,040
어 한 가지 주목할 점은 매개변수 변경 횟수가 다음을 사용하여 비교 가능하다는 것입니다.

463
00:50:19,040 --> 00:50:24,960
하이퍼파라미터 l 및 r 여기서 r은 lora 및 어댑터의 경우 병목 현상 차원입니다.

464
00:50:24,960 --> 00:50:28,000
l은 프리픽스 튜닝의 경우 프리픽스의 길이입니다.

465
00:50:30,880 --> 00:50:36,240
좋습니다. 따라서 이 작업의 병렬 어댑터는 모든 경우에 순차 어댑터를 능가할 수 있습니다.

466
00:50:36,320 --> 00:50:38,640
병렬 어댑터는 여기에서 소개하는 것입니다.

467
00:50:42,880 --> 00:50:46,480
좋습니다. 그러면 두 번째 질문은 네트워크에 대한 관심이나 피드를 수정하는 것입니까?

468
00:50:47,600 --> 00:50:51,120
맞습니다. 일부 관찰에 따르면 네트워크 피드를 사용하는 모든 방법은

469
00:50:51,120 --> 00:50:55,600
수정은 모든 경우에 주의 수정을 통해 모든 방법을 형성합니다.

470
00:50:56,160 --> 00:51:01,360
여기에 네트워크 피드를 수정하는 방법이 빨간색으로 표시된 그래픽이 있습니다.

471
00:51:01,920 --> 00:51:07,680
파란색에서는 빨간색 방법을 포함하여 주의를 수정하는 방법이 전체에서 더 나은 성능을 발휘합니다.

472
00:51:07,680 --> 00:51:15,440
매개변수의 수를 미세 조정했지만 머리 주의를 수정하면 최상의 결과를 얻을 수 있습니다.

473
00:51:15,440 --> 00:51:22,880
매개변수 예산은 매우 작으므로 이 표에 나와 있습니다.

474
00:51:22,880 --> 00:51:28,800
실제로 주의 형태를 수정하는 방법은 매우 작게 유지되면서 변경된 매개변수입니다.

475
00:51:28,800 --> 00:51:33,280
최고이며 주의를 수정하는 동시에 접두사 튜닝도 수행하는 병렬 어댑터입니다.

476
00:51:33,280 --> 00:51:43,360
또한 주의력을 수정하는 것은 이러한 다른 방법보다 더 나은 결과를 낳습니다. 세 번째 질문에 대답해 보세요.

477
00:51:43,360 --> 00:51:48,880
우리가 다룬 내용은 구성 함수의 크기를 조정해야 하는지, 크기를 조정하지 않은 상태로 두어야 하는지에 대한 것입니다.

478
00:51:48,880 --> 00:51:54,080
스케일링 구성 기능이 방금 바닐라 첨가제 기능보다 낫다는 것을 알았습니다.

479
00:51:54,160 --> 00:52:01,280
올바른 s 값을 찾아야 하므로 여기 표에는 s가 4인 Laura가 있습니다.

480
00:52:01,280 --> 00:52:06,640
대 s는 1과 같습니다. 여기서 1은 기본적으로 기본값이고 s는 4와 같으며 더 좋습니다.

481
00:52:06,640 --> 00:52:11,680
당신은 s가 4와 같고 훈련 가능한 s가 그만큼 좋지 않은 병렬 어댑터를 가지고 있습니다.

482
00:52:13,040 --> 00:52:17,440
s는 4와 같지만 기본적으로 s가 1인 병렬 어댑터보다 성능이 더 좋습니다.

483
00:52:18,720 --> 00:52:23,120
맞습니다. 따라서 학습 스칼라는 스칼라를 수동으로 조정하는 것보다 더 나은 결과를 제공하지 않습니다.

484
00:52:23,120 --> 00:52:31,840
흥미롭지만 여기에 일종의 통합된 보기가 나오는 곳이 있습니다.

485
00:52:31,840 --> 00:52:37,680
최고의 디자인 요소를 결합하여 더욱 효율적인 매개변수를 생성할 수 있습니다.

486
00:52:37,680 --> 00:52:42,880
그래서 그들은 확장된 병렬 어댑터가 수정하기에 가장 좋은 변형이라는 것을 발견했습니다.

487
00:52:42,880 --> 00:52:47,760
그들은 네트워크 피드에서 수정 사항을 더 잘 활용할 수 있다는 사실도 발견했습니다.

488
00:52:47,760 --> 00:52:52,960
그러나 매개변수 예산이 작은 경우에는 매개변수 수가 더 많아지므로 수정하려는 경우에만 가능합니다.

489
00:52:53,040 --> 00:52:58,160
접두어 튜닝으로 머리 주의를 실제로 수정하는 소수의 매개변수는

490
00:52:58,160 --> 00:53:03,920
강력한 성능을 얻을 수 있으므로 이 문서에서는 믹스 앤 매치 어댑터에 대한 방법을 제시합니다.

491
00:53:03,920 --> 00:53:10,160
이러한 모든 발견을 결합하면 실제로 강력한 결과를 얻을 수 있다는 점을 알 수 있습니다.

492
00:53:10,160 --> 00:53:16,800
여기 표에서는 우리가 지금까지 수행한 많은 방법을 전체 모델 미세 조정과 비교합니다.

493
00:53:16,800 --> 00:53:24,240
이번 작품에서 소개한 프롬프트 튜닝 프리픽스 튜닝 로라 병렬 어댑터에 대해 이야기했습니다.

494
00:53:24,960 --> 00:53:31,760
그리고 기본적으로 많은 결과를 결합하는 믹스 앤 매치 어댑터가 가장 좋습니다.

495
00:53:36,160 --> 00:53:40,320
전체 모델 미세 조정에 비해 히트 또는 실패이므로 때로는 더 잘할 수 있습니다.

496
00:53:40,320 --> 00:53:41,280
가끔은 상황이 더 나빠질 때도 있을 거야

497
00:53:48,000 --> 00:53:50,960
그게 흥미롭군요. 마지막 논문은 다음으로 넘어가겠습니다.

498
00:53:51,520 --> 00:53:56,000
매개변수 효율적인 미세 조정과 전체 모델 미세 조정을 비교하려고 합니다.

499
00:53:58,000 --> 00:54:02,000
당신은 지금까지 전체 모델의 미세 조정이 때로는 더 나은 것처럼 보인다는 것을 알고 있습니다.

500
00:54:02,000 --> 00:54:06,000
매개변수 효율적인 미세 조정이 때때로 더 나은 결과를 가져올 수 있다는 이점이 있습니다.

501
00:54:06,080 --> 00:54:12,080
올바른 것을 선택하면 관련 관찰이 많이 제공됩니다.

502
00:54:13,760 --> 00:54:17,600
그들은 매개변수 효율적인 미세 조정 방법이 가장 좋고 낮거나 중간 정도의 성능을 발휘한다는 것을 발견했습니다.

503
00:54:17,600 --> 00:54:22,480
리소스 시나리오에서는 전체 모델 미세 조정이 리소스가 많은 시나리오에서 더 잘 수행되는 반면

504
00:54:22,480 --> 00:54:27,520
이제 그것은 임의적이므로 기본적으로 최대 100의 리소스가 부족하기 때문에 이것이 무엇을 의미합니까?

505
00:54:27,520 --> 00:54:32,720
데이터 포인트 중간 리소스 최대 1000개의 데이터 포인트(100~1000개 범위)

506
00:54:32,800 --> 00:54:37,840
높은 자원의 경우 최대 10,000개, 현실적으로는 1,000개 이상

507
00:54:39,520 --> 00:54:46,960
하지만 테스트에서는 1000에서 10,000이 맞으므로 리소스가 부족할 때 절반의 방법이 더 좋습니다.

508
00:54:46,960 --> 00:54:51,600
낮은 리소스에서 알고 있는 테이블을 보면 모델 미세 조정이 높은 리소스에서 더 좋습니다.

509
00:54:51,600 --> 00:54:58,560
여기의 리소스는 모든 매개변수 효율적인 미세 조정 방법이 중간 리소스 가득 참으로 이어지고 있습니다.

510
00:54:58,560 --> 00:55:03,520
모델 미세 조정은 실제로 이 데이터 세트를 다른 데이터 세트로 유도합니다. 이는 경로 방법입니다.

511
00:55:03,520 --> 00:55:08,160
이기고 리소스가 많은 시나리오에서는 전체 모델 미세 조정이 두 가지에서 더 잘 수행됩니다.

512
00:55:08,160 --> 00:55:13,040
데이터 세트이지만 여전히 다른 두 가지에서 더 나은 성능을 발휘하는 매개변수 효율적인 미세 조정 방법입니다.

513
00:55:18,080 --> 00:55:23,360
이제 그들은 또한 매개변수 효율적인 미세 조정 방법이 낮은 수준에서 수렴하는 속도가 더 느리다는 것을 보여줍니다.

514
00:55:23,360 --> 00:55:29,520
리소스 시나리오는 리소스가 많은 시나리오에서 수렴하는 것이 더 빠르므로 기본적으로 일반적입니다.

515
00:55:29,520 --> 00:55:34,560
조언은 훈련 데이터가 많으면 매개변수를 효율적으로 미세 조정하는 방법이라는 것입니다.

516
00:55:34,560 --> 00:55:38,560
더 빠르지만 성능이 좋지 않으며 훈련 데이터가 많지 않으면

517
00:55:38,560 --> 00:55:41,760
매개변수 효율적인 미세 조정 방법은 더 나은 성능을 발휘하지만 훈련 속도는 느려집니다.

518
00:55:43,200 --> 00:55:46,080
모델을 훈련하는 경우 두 가지 모두 고려 사항입니다.

519
00:55:46,800 --> 00:55:56,640
따라서 응용 프로그램에서 포옹 얼굴 peft는 우리가 논의한 많은 미세 조정 방법을 지원합니다.

520
00:55:56,640 --> 00:56:01,760
최신 모델은 즉시 사용 가능하므로 이미 이러한 방법으로 작동하는 것으로 나타났으며 실제로

521
00:56:01,760 --> 00:56:06,800
일부 수동 수정 구성을 사용하면 이러한 모델 또는 이러한 방법을 모든 모델에 사용할 수 있습니다.

522
00:56:09,200 --> 00:56:15,760
오른쪽 또는 대부분의 모델이므로 여기에 그들이 할 수 있었던 일을 보여주는 표가 있습니다.

523
00:56:16,080 --> 00:56:23,120
따라서 mistro 모델이나 라마 모델의 경우 이러한 방법이 작동하는 것으로 나타났습니다.

524
00:56:23,680 --> 00:56:29,200
다시 구성을 사용하면 대부분의 모델은 매개변수 효율적인 미세 조정 방법으로 작동합니다.

525
00:56:29,200 --> 00:56:37,200
선택하면 이미 구현되었습니다. 감사합니다. 프레젠테이션을 마치겠습니다.