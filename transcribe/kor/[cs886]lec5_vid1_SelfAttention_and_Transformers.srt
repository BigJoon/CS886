1
00:00:00,000 --> 00:00:07,200
안녕하세요, 제 이름은 Evelyn Riddell이고 저와 함께 공동 발표자인 James Riddell이 있습니다.

2
00:00:07,200 --> 00:00:11,640
우리는 self-attention과 Transformers에 대해 프레젠테이션을 할 예정입니다.

3
00:00:11,640 --> 00:00:16,040
이 프레젠테이션은 원래 대학의 컴퓨터 과학 과정을 위해 준비되었습니다.

4
00:00:16,040 --> 00:00:21,640
우리 둘 다 대학원생이고 Wenhu 교수가 강좌를 진행한 워털루의 대학입니다.

5
00:00:21,640 --> 00:00:22,640
첸.

6
00:00:22,640 --> 00:00:26,320
좋습니다. 이제 시작하겠습니다.

7
00:00:26,320 --> 00:00:32,440
따라서 이 프레젠테이션의 목표는 관심이 이전에 어떤 모습이었는가에 대해 이야기하는 것입니다.

8
00:00:32,440 --> 00:00:35,880
트랜스포머 아키텍처가 도입되었습니다.

9
00:00:35,880 --> 00:00:40,040
특히 이 Attention 메커니즘은 RNN 구조와 밀접하게 결합되어 있으므로

10
00:00:40,040 --> 00:00:41,320
그 문제를 살펴보세요.

11
00:00:41,320 --> 00:00:46,320
그런 다음 우리는 이 분야를 근본적으로 변화시킨 논문이 필요한 전부라는 점에 주목하겠습니다.

12
00:00:46,320 --> 00:00:53,320
RNN에서 이것을 분리하고 변환기 아키텍처를 도입한 연구에서

13
00:00:53,320 --> 00:00:57,040
그들의 논문에 상당한 영향을 미쳤습니다.

14
00:00:57,040 --> 00:01:03,160
또한 다른 영역의 변환기, 특히 조기 채택에 대해서도 다룰 것입니다.

15
00:01:03,160 --> 00:01:06,600
컴퓨터 비전의 변환기.

16
00:01:06,600 --> 00:01:12,800
그리고 목표는 그 과정에서 몇 가지 오해를 해결하는 것입니다.

17
00:01:12,800 --> 00:01:19,120
따라서 머신러닝과 딥러닝에서 컨텍스트를 활용한다는 개념은 새로운 것이 아닙니다.

18
00:01:19,120 --> 00:01:25,880
따라서 CNN을 사용하면 공간적 지역성의 컨텍스트를 활용하며 이는 특히 유용합니다.

19
00:01:25,880 --> 00:01:35,240
이미지의 경우 이미지의 패치 주변 영역이 해당 특정 항목에 대한 컨텍스트를 제공합니다.

20
00:01:35,240 --> 00:01:36,520
반점.

21
00:01:36,520 --> 00:01:42,840
RNN을 사용하면 시간적 지역성에서 컨텍스트가 활용되며 이는 시퀀스에 유용합니다.

22
00:01:42,840 --> 00:01:44,720
또는 시계열 데이터.

23
00:01:44,720 --> 00:01:50,720
그러나 이러한 작업 방식, 즉 컨텍스트를 활용하는 방식은 다음을 포함하는 것입니다.

24
00:01:50,720 --> 00:01:56,800
이는 모델에 대한 관련 기능에 주의를 기울이게 만듭니다.

25
00:01:56,800 --> 00:01:58,360
주어진 문제.

26
00:01:58,360 --> 00:02:01,960
그리고 주어진 문제는 아키텍처 및 이러한 방식과 밀접한 관련이 있습니다.

27
00:02:01,960 --> 00:02:04,240
사전이 포함되어 있습니다.

28
00:02:04,240 --> 00:02:09,640
현재 딥러닝에서 셀프 어텐션(self-attention)이라고 부르는 것은 여전히 ​​주의를 기울이는 아이디어입니다.

29
00:02:09,640 --> 00:02:17,760
주어진 시간 단계에서 입력의 가장 관련성이 높은 부분이나 중요한 부분에

30
00:02:17,760 --> 00:02:22,640
이러한 내장된 사전 변수가 있으며 이상적으로 우리는 무엇에 주의를 기울여야 하는지 배우고 싶습니다.

31
00:02:22,640 --> 00:02:29,000
이는 시퀀스 간 모델링에 매우 유용합니다.

32
00:02:29,000 --> 00:02:34,360
이제 주의 메커니즘은 일반적으로 모델이 참석할 수 있도록 하는 기능을 나타냅니다.

33
00:02:34,360 --> 00:02:37,800
다양한 맥락과 내용으로.

34
00:02:37,800 --> 00:02:40,680
그리고 제가 전에 말했듯이 이상적으로 우리는 이것을 배우고 싶습니다.

35
00:02:40,680 --> 00:02:45,760
그래서 우리가 여기서 배우는 것은 설정된 주의 함수 또는 이 함수의 매개변수입니다.

36
00:02:45,760 --> 00:02:48,920
이제 주의력을 계산할 수 있는 다양한 방법이 있습니다.

37
00:02:48,920 --> 00:02:54,360
내적처럼 덧셈이나 곱셈이 가능합니다.

38
00:02:54,360 --> 00:02:58,800
그리고 우리는 존재하는 것에 따라 주의를 구별하기 위해 다른 이름을 가질 수도 있습니다.

39
00:02:58,800 --> 00:03:00,400
참석.

40
00:03:00,400 --> 00:03:06,080
따라서 self-attention을 위해 콘텐츠는 그 자체에 집중됩니다.

41
00:03:06,080 --> 00:03:10,080
우리는 일반적으로 이것을 인트라어텐션(intra-attention)이라고도 합니다.

42
00:03:10,080 --> 00:03:15,160
교차주의를 통해 콘텐츠는 다른 콘텐츠에 집중됩니다.

43
00:03:15,160 --> 00:03:21,720
우리는 이것을 종종 인코더-디코더 어텐션 또는 인트라 어텐션이라고 부릅니다.

44
00:03:21,720 --> 00:03:28,400
이제 우리는 주의력을 계산하는 방법과 다양한 주의력 유형에 대해 이야기하겠습니다.

45
00:03:28,400 --> 00:03:34,040
나중에 좀 더 자세히 다루겠지만 이는 다양한 기능에 대한 높은 수준의 개요를 제공합니다.

46
00:03:34,040 --> 00:03:38,560
주의를 말할 때 사용하는 용어.

47
00:03:38,560 --> 00:03:44,520
앞서 말했듯이 주의 기능이나 주의 메커니즘은 다음과 같은 경우에 정말 유용합니다.

48
00:03:44,520 --> 00:03:46,400
시퀀스 간 모델링.

49
00:03:46,400 --> 00:03:50,320
이것이 의미하는 바는 시퀀스 간 모델링에 관해 이야기할 때 목표는

50
00:03:50,320 --> 00:03:57,840
입력 시퀀스 x를 가져와 의미 있는 시퀀스 y 또는 출력 시퀀스를 생성하는 것입니다.

51
00:03:57,840 --> 00:03:58,840
그리고.

52
00:03:58,840 --> 00:04:02,920
이제 이는 입력 순서가 텍스트이고 출력이 텍스트인 텍스트 대 텍스트처럼 보일 수 있습니다.

53
00:04:02,920 --> 00:04:04,400
순서는 텍스트입니다.

54
00:04:04,400 --> 00:04:10,500
질문 답변, 번역, 텍스트 요약과 같은 작업을 생각해 보세요.

55
00:04:10,500 --> 00:04:15,640
이제 이 입력 시퀀스는 텍스트일 필요는 없으며 이미지일 수도 있습니다.

56
00:04:15,640 --> 00:04:20,680
이미지를 촬영하고 캡션을 생성하는 이미지 캡션을 생각해 보세요.

57
00:04:20,680 --> 00:04:22,480
출력 순서.

58
00:04:22,480 --> 00:04:28,160
이제 우리는 일종의 마법 같은 방식으로 입력 시퀀스에서 출력 시퀀스로 이동하는 것이 아닙니다.

59
00:04:28,160 --> 00:04:33,840
블랙박스에서는 인코더 디코더 모델을 자주 사용합니다.

60
00:04:33,840 --> 00:04:39,360
이것이 작동하는 방식은 입력 시퀀스가 ​​인코더에 의해 인코딩되어 공유되는 것입니다.

61
00:04:39,360 --> 00:04:46,160
상태 및 컨텍스트 벡터는 정보와 모든 컨텍스트를 캡처합니다.

62
00:04:46,160 --> 00:04:53,080
입력 시퀀스는 이를 디코더와 공유하여 이 정보를 의미 있게 디코딩합니다.

63
00:04:53,080 --> 00:04:55,560
출력 시퀀스를 생성합니다.

64
00:04:55,560 --> 00:05:01,040
그리고 그것은 물론 작업이 무엇인지, 그리고 이러한 순서가 무엇인지에 따라 달라집니다.

65
00:05:01,040 --> 00:05:08,240
그리고 일반적으로 이러한 인코더 디코더 모델은 RNN을 사용하여 구현되며 매우 유용합니다.

66
00:05:08,240 --> 00:05:10,440
시퀀스 시퀀스 모델링을 위한 것입니다.

67
00:05:10,440 --> 00:05:15,440
또한 변환기 아키텍처는 이러한 용도에 유용한 일종의 인코더 디코더 모델입니다.

68
00:05:15,440 --> 00:05:16,440
작업 유형.

69
00:05:16,440 --> 00:05:18,320
물론 이에 대해서는 나중에 다루겠습니다.

70
00:05:18,320 --> 00:05:23,520
하지만 먼저 이야기를 시작하겠습니다. 먼저 RNN이 어떻게 작동하는지에 대한 설명부터 시작하겠습니다.

71
00:05:23,520 --> 00:05:27,960
시퀀스 간 모델링에 사용할 수 있습니다.

72
00:05:27,960 --> 00:05:34,840
여기에 신경망을 이용한 Sequence to Sequence Learning이라는 2014년 논문이 있습니다.

73
00:05:34,840 --> 00:05:41,320
이 인코더 디코더 구조를 사용하여 RNN으로 시퀀스 대 시퀀스 모델링을 수행합니다.

74
00:05:41,320 --> 00:05:47,720
인코더와 디코더는 모두 순환 알고리즘의 일종인 LSDM을 사용하여 인스턴스화됩니다.

75
00:05:47,720 --> 00:05:48,720
신경망.

76
00:05:49,720 --> 00:05:55,160
이것이 작동하는 방식은 인코더가 입력 시퀀스로 시작하는 것입니다.

77
00:05:55,160 --> 00:06:01,160
지금은 이 입력 시퀀스에 X1, X2, X3, X4라는 4개의 토큰이 있다고 가정해 보겠습니다.

78
00:06:01,160 --> 00:06:08,080
이러한 입력 토큰을 사용하여 인코더의 숨겨진 상태를 계산합니다.

79
00:06:08,080 --> 00:06:12,680
이제 각 토큰에 대해 해당 숨겨진 상태를 계산합니다.

80
00:06:12,680 --> 00:06:17,600
토큰 X2에 대해 숨겨진 상태 2를 계산한다고 가정해 보겠습니다.

81
00:06:17,600 --> 00:06:22,720
하지만 해당 숨겨진 상태를 알리려면 이전 숨겨진 상태도 필요합니다.

82
00:06:22,720 --> 00:06:28,440
따라서 H2의 경우 H1을 사용하여 숨겨진 상태가 어떻게 생겼는지 알려줍니다.

83
00:06:28,440 --> 00:06:32,280
그리고 우리는 다음 단계에 도달할 때까지 모든 숨겨진 상태와 모든 토큰에 대해 이 작업을 수행합니다.

84
00:06:32,280 --> 00:06:34,120
시퀀스의 끝.

85
00:06:34,120 --> 00:06:39,400
하지만 여기서 주목하세요. H4를 계산할 때 이전 숨겨진 상태가 필요하며

86
00:06:39,400 --> 00:06:41,600
그 전의 것과 그 전의 것.

87
00:06:41,600 --> 00:06:51,200
따라서 우리는 이전 토큰과 숨겨진 토큰이 무엇이든 이러한 종속성을 갖게 됩니다.

88
00:06:51,200 --> 00:06:55,400
상태는 다음 숨겨진 상태가 무엇인지 알려줍니다.

89
00:06:55,400 --> 00:07:01,360
그러나 시퀀스가 ​​길어질수록 이 정보는 약해집니다.

90
00:07:01,360 --> 00:07:07,800
그리고 다시 말하지만, RNN은 공간적 지역성을 내장된 사전 정보로 사용한다는 점을 기억하세요.

91
00:07:07,800 --> 00:07:13,320
숨겨진 상태 또는 토큰에 가장 가까운 상태 또는 토큰이 가장 큰 영향을 미칩니다.

92
00:07:13,320 --> 00:07:15,520
가장 큰 효과.

93
00:07:15,520 --> 00:07:21,600
그러나 장거리 종속성으로 인해 시퀀스 초기에 발생한 입력이나 토큰은

94
00:07:21,600 --> 00:07:25,840
더 이상 큰 영향을 미치지 않습니다.

95
00:07:25,840 --> 00:07:32,080
이제 이것이 진행되는 방식은 이 토큰이 인코딩된 후입니다.

96
00:07:32,080 --> 00:07:38,520
이 입력 시퀀스는 인코딩되었으며 마지막 숨겨진 상태는 컨텍스트를 계산하는 데 사용됩니다.

97
00:07:38,520 --> 00:07:42,760
벡터와 디코더의 초기 상태.

98
00:07:42,760 --> 00:07:48,880
컨텍스트 벡터는 입력 시퀀스의 정보를 캡처합니다.

99
00:07:48,880 --> 00:07:57,800
거기에서 우리는 이전 숨겨진 상태와 이전 숨겨진 상태를 사용하는 매우 유사한 프로세스를 수행합니다.

100
00:07:57,800 --> 00:08:01,920
출력 토큰을 사용하여 다음 출력 토큰을 계산합니다.

101
00:08:01,920 --> 00:08:09,240
따라서 우리는 반복적인 자기회귀 방식으로 이 작업을 수행합니다. 이 경우에는 반복됩니다. 왜냐하면 다시 말하지만,

102
00:08:09,240 --> 00:08:14,520
이전 숨겨진 상태를 사용하여 다음 상태를 계산합니다.

103
00:08:14,520 --> 00:08:20,240
출력에 도달할 때까지 생성하는 각 출력 토큰에 대해 이 작업을 수행합니다.

104
00:08:20,240 --> 00:08:21,960
순서.

105
00:08:21,960 --> 00:08:29,720
이제 다시 숨겨진 상태에 대한 정보는 물론 이전 출력 토큰을 통해 알 수 있을 뿐만 아니라

106
00:08:29,720 --> 00:08:32,160
그 앞에 오는 모든 숨겨진 상태.

107
00:08:32,160 --> 00:08:37,160
다시 말하지만, 이는 공간적 지역성을 활용하는 개념입니다.

108
00:08:37,160 --> 00:08:40,400
RNN 악용.

109
00:08:40,400 --> 00:08:47,920
그러나 우리는 출력 시퀀스에서 이전에 발생한 일이 무엇이든 물에 젖어 있다는 것을 다시 알 수 있습니다.

110
00:08:47,920 --> 00:08:51,200
이 순서대로 점점 더 나아가면 아래로 내려갑니다.

111
00:08:51,200 --> 00:08:58,080
이제 우리가 볼 수 있는 또 다른 문제는 이 컨텍스트 벡터가 모든 숨겨진 상태에 재사용된다는 것입니다.

112
00:08:58,080 --> 00:08:59,320
디코더.

113
00:08:59,320 --> 00:09:03,840
우리는 모든 정보를 캡처하는 단일 컨텍스트 벡터를 사용합니다.

114
00:09:03,840 --> 00:09:06,600
입력에서.

115
00:09:06,600 --> 00:09:15,080
그러나 이는 고정 길이 컨텍스트로 인해 정보 병목 현상이 발생하는 경향이 있습니다.

116
00:09:15,080 --> 00:09:16,680
벡터.

117
00:09:16,680 --> 00:09:22,680
여기서 문제는 입력이 너무 길어지면 이 컨텍스트 벡터가 그렇지 않을 수도 있다는 것입니다.

118
00:09:22,680 --> 00:09:28,000
모든 정보를 캡처할 수 있지만 더 중요한 것은 새로운 정보를 생성하고 있다는 것입니다.

119
00:09:28,360 --> 00:09:33,720
출력 토큰, 입력의 특정 부분이 더 관련성이 높거나 더 중요할 수 있습니다.

120
00:09:33,720 --> 00:09:38,560
그리고 이 컨텍스트 벡터는 단일 컨텍스트 벡터가 이를 캡처하지 못할 수도 있습니다.

121
00:09:38,560 --> 00:09:46,800
2015년의 또 다른 논문인 Neural Machine Translation by Jointly Learning to Align

122
00:09:46,800 --> 00:09:51,480
그리고 Translate는 이러한 정보 병목 현상을 해결했습니다.

123
00:09:51,480 --> 00:09:57,560
그들은 디코더의 각 시간 단계에 대해 서로 다른 컨텍스트 벡터를 사용하는 아이디어를 도입했습니다.

124
00:09:58,000 --> 00:10:03,360
따라서 프로세스는 매우 유사하지만 단일 컨텍스트 벡터를 사용하는 대신

125
00:10:03,360 --> 00:10:09,320
각 디코더의 숨겨진 상태에 사용되므로 각 디코더에 대해 새로운 상태를 계산하겠습니다.

126
00:10:09,320 --> 00:10:16,240
숨겨진 상태를 보고 가장 중요한 것을 보거나 포착하도록 만들 수 있습니다.

127
00:10:16,240 --> 00:10:20,680
해당 시간 단계에 대한 입력 시퀀스의 일부입니다.

128
00:10:20,680 --> 00:10:27,800
그래서 우리가 하는 방식은 인코딩하고 초기 상태를 생성하는 것과 동일한 설정을 보는 것입니다.

129
00:10:27,800 --> 00:10:32,880
디코더의 경우 단일 컨텍스트 벡터를 계산하는 대신

130
00:10:32,880 --> 00:10:37,880
디코더의 모든 단계에서 사용되어 새로운 것을 계산하고 있습니다.

131
00:10:37,880 --> 00:10:47,320
이것이 작동하는 방식은 먼저 각 숨겨진 상태 사이의 정렬을 계산하는 것입니다.

132
00:10:47,320 --> 00:10:52,080
이전 디코더 상태를 갖는 인코더.

133
00:10:52,080 --> 00:10:58,120
따라서 이 경우 S1에 대해 컨텍스트 벡터 1을 계산하려면 이전 숨겨진 상태가 필요합니다.

134
00:10:58,120 --> 00:11:00,080
이 경우 S0.

135
00:11:00,080 --> 00:11:04,240
그래서 우리는 attention 함수를 사용하여 정렬 점수를 계산합니다.

136
00:11:04,240 --> 00:11:07,480
지금은 이 추상적인 내용을 유지해 보겠습니다.

137
00:11:07,480 --> 00:11:13,120
이것이 어떻게 보이는지는 나중에 살펴보겠지만 정렬이나 호환성을 계산합니다.

138
00:11:13,120 --> 00:11:17,000
숨겨진 상태와 해당 디코더 상태 사이.

139
00:11:17,000 --> 00:11:21,800
그런 다음 이러한 정렬 점수를 정규화하여 주의 가중치를 생성한 다음

140
00:11:21,800 --> 00:11:26,200
가중 합계를 계산합니다.

141
00:11:26,200 --> 00:11:34,720
여기서 무슨 일이 일어나고 있는지는 주의 가중치를 사용하여 각 숨겨진 상태가 얼마나 되는지 확인하는 것입니다.

142
00:11:34,720 --> 00:11:40,520
이 컨텍스트 벡터가 무엇인지 알려주는 데 사용할 것입니다.

143
00:11:40,520 --> 00:11:46,640
그래서 우리는 주의 가중치가 높은 숨겨진 상태를 말하는 것입니다.

144
00:11:46,640 --> 00:11:54,760
가중치가 높은 도시는 다음 숨겨진 상태를 계산할 때 더 많은 영향을 미칠 것입니다.

145
00:11:54,760 --> 00:11:59,400
따라서 다음 출력 토큰입니다.

146
00:11:59,400 --> 00:12:05,600
따라서 본질적으로 이러한 주의 가중치가 의미하거나 나타내는 것은

147
00:12:05,600 --> 00:12:12,640
대상 단어(이 경우 Y1)가 소스에 정렬되거나 소스에서 번역되었는지 측정합니다.

148
00:12:12,640 --> 00:12:20,200
각 은닉 상태 HI에 대응되는 워드 XI.

149
00:12:20,200 --> 00:12:25,640
또한 주석의 중요성을 반영하므로 관련 숨겨진 상태가 표시됩니다.

150
00:12:25,640 --> 00:12:31,200
이전 숨겨진 상태로 전환합니다. 왜냐하면 우리는 이전 숨겨진 상태와 모든 항목을 사용하고 있기 때문입니다.

151
00:12:31,200 --> 00:12:36,840
해당 컨텍스트를 계산하기 위한 인코더의 다양한 주석 벡터 또는 숨겨진 상태

152
00:12:36,840 --> 00:12:42,080
벡터를 생성하고 다음 출력 토큰을 생성합니다.

153
00:12:42,080 --> 00:12:50,120
그래서 우리는 디코더의 각 단계나 숨겨진 상태에 대해 이 작업을 수행합니다.

154
00:12:50,120 --> 00:12:55,360
디코더의 이전 숨겨진 상태를 사용하여 새로운 컨텍스트 벡터를 계산합니다.

155
00:12:55,360 --> 00:13:02,600
다음 숨겨진 상태와 상태를 알리기 위해 디코더에 의해 생성된 이전 출력 토큰

156
00:13:02,600 --> 00:13:05,040
다음 출력 토큰이 될 것입니다.

157
00:13:05,040 --> 00:13:12,840
이것이 우리가 하는 반복적인 자기회귀 과정입니다.

158
00:13:12,840 --> 00:13:21,600
그래서 우리가 한 일은 이제 가장 중요한 부분을 포착하는 문제를 해결한 것입니다.

159
00:13:21,600 --> 00:13:26,000
출력에서 각 토큰을 생성하기 위한 입력입니다.

160
00:13:26,000 --> 00:13:33,200
이제 우리가 실제로 해결하지 못한 한 가지는 순환 구조가

161
00:13:33,200 --> 00:13:41,660
우리의 디코더가 점점 더 많은 출력 토큰과 이전의 각 디코더를 생성함에 따라

162
00:13:41,660 --> 00:13:46,440
상태는 다음 디코더 상태가 무엇인지 알려주지만 여전히 일부 정보를 잃습니다.

163
00:13:46,440 --> 00:13:55,200
이전에 어떤 숨겨진 상태나 출력 토큰이 생성되었는지에 대해 설명합니다.

164
00:13:55,200 --> 00:13:59,800
이제 이 경우 인코더는 양방향이므로 각 단어에 주석을 달 수 있습니다.

165
00:13:59,800 --> 00:14:03,080
앞의 단어와 뒤의 단어를 모두 요약합니다.

166
00:14:03,080 --> 00:14:05,240
그래서 우리는 이 정보를 앞뒤로 얻습니다.

167
00:14:05,240 --> 00:14:10,880
입력의 가장 중요한 부분을 포착하여 그 부분을 이런 관심으로 해결합니다.

168
00:14:10,880 --> 00:14:11,880
기구.

169
00:14:11,880 --> 00:14:16,080
이제 모든 단계가 미분 가능하므로 모든 단계를 역전파하고 업데이트할 수 있습니다.

170
00:14:16,080 --> 00:14:19,760
모델 매개변수.

171
00:14:19,760 --> 00:14:29,200
따라서 이러한 주의 가중치에 대한 시각화를 제공하기 위해 이 논문의 저자는

172
00:14:29,200 --> 00:14:32,000
번역 작업을 위한 시각화입니다.

173
00:14:32,000 --> 00:14:37,480
이 경우 상단에는 영어 문장이 있고 왼쪽에는 프랑스어 문장이 있습니다.

174
00:14:37,480 --> 00:14:38,480
옆.

175
00:14:38,480 --> 00:14:47,560
여기서 픽셀은 주의에 해당하는 밝기로 색상이 지정되거나 픽셀화됩니다.

176
00:14:47,560 --> 00:14:48,560
무게.

177
00:14:48,560 --> 00:14:50,760
따라서 밝을수록 주의 가중치가 높아집니다.

178
00:14:50,760 --> 00:14:59,520
이것이 의미하는 바는 모델이 생성할 때 특정 입력에 주의를 기울였다는 것입니다.

179
00:14:59,520 --> 00:15:00,840
출력 토큰.

180
00:15:00,840 --> 00:15:02,800
그럼 합의라는 단어를 살펴보겠습니다.

181
00:15:02,800 --> 00:15:08,880
모델이 프랑스어로 합의라는 단어를 생산할 때,

182
00:15:08,880 --> 00:15:14,560
영어 문장의 단어 일치는 픽셀화되거나 밝은 부분을 보는 이유입니다.

183
00:15:14,560 --> 00:15:18,400
이 시각화의 픽셀은 여기입니다.

184
00:15:18,400 --> 00:15:27,880
따라서 밝은 픽셀의 멋진 대각선을 보면 좋은 픽셀이 있다는 것을 나타냅니다.

185
00:15:27,880 --> 00:15:30,960
영어와 프랑스어 단어의 일대일 대응.

186
00:15:30,960 --> 00:15:34,920
물론 이제 우리는 그것을 모든 곳에서 볼 수는 없습니다.

187
00:15:34,920 --> 00:15:41,520
이는 일반적으로 다른 언어에서 발생하는 단어의 순서가 다르기 때문입니다.

188
00:15:41,520 --> 00:15:46,200
예를 들어 유럽 경제 지역에서는 대각선이 반대 방향으로 보입니다.

189
00:15:46,200 --> 00:15:52,840
프랑스어 단어 순서가 다르거나, 일종의 반전이 있거나, 모호함을 볼 수 있습니다.

190
00:15:52,840 --> 00:15:57,040
이는 서로 다른 단어의 의미론적 의미가 다르기 때문에 발생합니다.

191
00:15:57,040 --> 00:16:04,680
예를 들어 영어로 was라는 단어와 프랑스어로 aité라는 단어는 서로 다른 의미를 나타낼 수 있습니다.

192
00:16:04,680 --> 00:16:10,800
이는 모델이 생성할 때 다른 단어에도 주의를 기울였다는 것을 의미합니다.

193
00:16:10,800 --> 00:16:19,620
영어로 된 해당 단어의 번역된 표현을 나타내는 프랑스어 단어

194
00:16:19,620 --> 00:16:22,620
문장.

195
00:16:22,620 --> 00:16:27,400
하지만 이는 주의 가중치가 어떻게 생겼는지, 모델이 지불한 금액에 대한 좋은 아이디어를 제공합니다.

196
00:16:27,400 --> 00:16:38,320
이 특정 작업에 대한 출력을 생성할 때 주의하세요.

197
00:16:38,320 --> 00:16:45,840
저자는 자신의 접근 방식을 RNN과 일반 관심 프로세스와 비교했습니다.

198
00:16:45,840 --> 00:16:54,680
RNN 인코더-디코더 구조, 두 개의 훈련 패러다임(하나는 30단어 문장 포함)을 사용했습니다.

199
00:16:54,680 --> 00:16:57,440
또 다른 하나는 50단어 문장입니다.

200
00:16:57,440 --> 00:17:04,880
여기서 볼 수 있는 것은 주의 메커니즘이 50개 단어에 내장된 RNN이라는 것입니다.

201
00:17:04,880 --> 00:17:11,480
훈련 패러다임에서는 더 긴 문장 길이에 대한 성능 분리가 관찰되지 않습니다.

202
00:17:11,480 --> 00:17:18,720
이제 이는 디코더가 주의를 기울여야 할 중요한 것이 무엇인지 알고 있는지 확인했기 때문입니다.

203
00:17:18,720 --> 00:17:24,960
입력으로부터 각 디코더 시간 단계에 대해 특화된 컨텍스트 벡터를 갖게 됩니다.

204
00:17:24,960 --> 00:17:28,320
각각의 새로운 출력 토큰을 생성할 때.

205
00:17:28,320 --> 00:17:34,640
이제 우리는 50~60개의 문장 길이만 보고 있으므로 아마도

206
00:17:34,640 --> 00:17:42,440
물을 주었기 때문에 문장 길이가 훨씬 더 늘어나면서 여전히 약간의 저하가 보입니다.

207
00:17:42,440 --> 00:17:48,680
디코더 시퀀스의 다운 효과.

208
00:17:48,680 --> 00:17:56,240
이제 Show, Attend, and Tell, Neural Image Caption이라는 2015년의 다른 논문이 있습니다.

209
00:17:56,240 --> 00:18:03,080
시각적 주의를 갖춘 생성에서는 주의를 기울여 이 RNN을 사용하지만 다른 용도로 사용됩니다.

210
00:18:03,080 --> 00:18:11,400
작업에서는 텍스트 입력 시퀀스를 사용하지 않고 이미지 캡션을 위해 이미지 시퀀스를 사용합니다.

211
00:18:11,400 --> 00:18:15,680
그것은 이 이전 논문을 바탕으로 직접 구축되었으며, 그들이 이를 수행하는 방식은 매우

212
00:18:15,680 --> 00:18:21,480
유사한 설정이지만 텍스트를 입력 시퀀스로 사용하는 대신 이미지를 사용합니다.

213
00:18:21,480 --> 00:18:26,440
숨겨진 상태를 생성하기 위해 컨볼루셔널 신경망을 사용하여 특징을 추출합니다.

214
00:18:26,440 --> 00:18:27,440
인코더의.

215
00:18:27,800 --> 00:18:36,960
기본적으로 이는 다양한 이미지 영역을 나타내는 이미지의 특징 벡터입니다.

216
00:18:36,960 --> 00:18:45,760
따라서 디코더를 사용하여 출력 시퀀스를 생성할 때 새로운 컨텍스트 벡터는 다음과 같습니다.

217
00:18:45,760 --> 00:18:53,760
각 디코더의 숨겨진 상태에 대해 계산되며 서로 다른 이미지 영역이 되는 경향이 있습니다.

218
00:18:53,760 --> 00:18:58,960
특징 벡터 또는 인코더의 숨겨진 상태로 캡처됩니다.

219
00:18:58,960 --> 00:19:04,440
우리는 여전히 정렬 점수, 주의 가중치를 계산하고 가중치를 적용하여 계산합니다.

220
00:19:04,440 --> 00:19:11,720
컨텍스트 벡터가 무엇인지 알려주는 합계이며 여기서 아이디어는 주의 가중치라는 것입니다.

221
00:19:11,720 --> 00:19:18,440
이미지를 생성할 때 이미지의 다양한 영역이 얼마나 중요한지 나타냅니다.

222
00:19:18,440 --> 00:19:20,680
그 출력 토큰.

223
00:19:20,680 --> 00:19:26,440
이제 우리는 자동 회귀 방식으로 이 작업을 수행합니다.

224
00:19:26,440 --> 00:19:31,440
이전 디코더 상태는 다음 디코더 상태가 무엇인지 알려줍니다.

225
00:19:31,440 --> 00:19:36,520
다음 출력 토큰이 될 예정이지만 각각에 대해 새로운 컨텍스트 벡터를 계산하고 있습니다.

226
00:19:36,520 --> 00:19:37,560
시간 단계.

227
00:19:41,160 --> 00:19:44,920
모든 단계는 미분 가능하므로 모든 단계를 역전파하고 모델을 업데이트할 수 있습니다.

228
00:19:44,920 --> 00:19:50,520
매개변수, 그리고 이 경우 각 컨텍스트 벡터는 서로 다른 이미지 영역에 사용됩니다.

229
00:19:50,680 --> 00:19:55,480
이 토큰과 숨겨진 상태에 대해 여기 모델에 알려주는 것은 다음과 같습니다.

230
00:19:55,480 --> 00:20:00,600
출력을 위해 해당 토큰이 무엇인지 알려주기 위해 이 지역 또는 해당 지역을 사용합니다.

231
00:20:01,800 --> 00:20:08,600
이제 시각화를 제공하기 위해 여기서는 관심이 집중된 위치와 대상에 대한 통찰력을 제공합니다.

232
00:20:08,600 --> 00:20:13,000
각 단어를 생성할 때 켜집니다. 그래서 여기에 우리는 여전히 새의 이미지를 가지고 있습니다.

233
00:20:13,960 --> 00:20:20,920
모델이 언제 Bird라는 단어를 생성했는지 살펴보겠습니다. 그 당시 모델은 다음과 같습니다.

234
00:20:20,920 --> 00:20:26,600
새가 있던 지역에주의를 기울이십시오. 그래서 모델이 생산하는 방법을 알았습니다.

235
00:20:26,600 --> 00:20:35,880
이미지의 해당 영역에 대한 새라는 단어입니다. 이제 아래로 내려가 좀 더 자세히 살펴보면

236
00:20:35,880 --> 00:20:41,080
물이 생성되는 순서, 그 시점에서 모델이 주의를 기울인 부분

237
00:20:42,040 --> 00:20:47,000
물이 있었던 새 주변 지역이므로 물이라는 단어가

238
00:20:47,000 --> 00:20:53,800
그런 다음 모델에 의해 생산됩니다. 이제 논문에서는 주의력을 계산하는 두 가지 방법을 소개했습니다.

239
00:20:53,800 --> 00:21:00,600
결정론적인 소프트 어텐션(soft attention)이 있는데, 이것이 바로 제가 지금까지 설명드린 것입니다. 또 있어요

240
00:21:00,600 --> 00:21:06,200
이를 수행하는 방법은 강화가 필요한 확률론적 하드 주의(stochastic hard attention)라고 합니다.

241
00:21:06,200 --> 00:21:11,400
학습. 이것은 여러분이 관심을 갖고 있는 내용입니다. 해당 논문을 읽어 보시기를 적극 권장합니다.

242
00:21:11,400 --> 00:21:17,960
이는 이 프레젠테이션의 범위를 벗어납니다. 이제 몇 가지 시각화가 더 있습니다.

243
00:21:19,080 --> 00:21:24,680
이 아이디어를 실제로 강화하는 것입니다. 여자가 던지는 첫 번째 사진을 보자

244
00:21:24,680 --> 00:21:31,400
프리스비. 모델이 프리스비가 있던 이 부위에 주목하던 당시,

245
00:21:31,400 --> 00:21:34,440
캡션에 frisbee라는 단어가 생성되었습니다.

246
00:21:38,280 --> 00:21:45,480
좋아요, 이제 우리는 주의가 인코더-디코더 구조에 어떻게 통합되었는지에 대해 이야기했습니다.

247
00:21:45,480 --> 00:21:54,040
RNN으로. 2017년에 Vaswani와 그의 동료들은 Attention is All You Need 논문을 발표했습니다.

248
00:21:54,760 --> 00:22:00,520
이 논문은 그 당시에는 상당히 급진적인 논문이었으며 지금도 그렇습니다. 매우 임팩트가 있었고

249
00:22:00,520 --> 00:22:08,040
오늘날 우리가 알고 사용하는 많은 AI 기술의 기본입니다. 그들은 주의를 분리시켰습니다

250
00:22:08,040 --> 00:22:12,840
RNN 구조에서 변환기 아키텍처를 도입했습니다.

251
00:22:15,480 --> 00:22:21,880
그럼 바로 본론으로 들어가겠습니다. 하지만 그 전에 잠시 여러분과 이야기를 나누고 싶습니다.

252
00:22:21,880 --> 00:22:28,440
특징 중첩과 다형성에 대해. 심층 신경망으로 작업한 것을 기억하세요.

253
00:22:28,440 --> 00:22:35,160
신경 활성화는 단일한 것을 나타내지 않는 경우가 많으며, 이는 신경 네트워크 때문에 발생합니다.

254
00:22:35,160 --> 00:22:41,240
뉴런이 가지고 있는 것보다 더 많은 특징을 표현하고 싶어합니다. 그리고 우리는 이것을 심층 신경으로 봅니다.

255
00:22:41,240 --> 00:22:49,400
트랜스포머 아키텍처와 같은 네트워크. 이를 통해 우리는 종종 관련되지 않은 많은

256
00:22:50,200 --> 00:22:56,840
개념은 단일 뉴런으로 압축됩니다. 이로 인해 특성이 중첩됩니다.

257
00:22:58,760 --> 00:23:04,440
이는 토큰과 입력 및 출력에 대해 이야기할 때 중요합니다.

258
00:23:05,960 --> 00:23:14,120
딥 러닝 네트워크 계층 또는 해당 계층의 일부를 생각하는 것은 의미가 없습니다.

259
00:23:14,120 --> 00:23:20,760
이전에 해왔던 것처럼 토큰이나 단어로 입력과 출력을 수행합니다. 이는 의미를 잃는 경향이 있습니다.

260
00:23:20,760 --> 00:23:25,720
우리는 네트워크에 점점 더 깊이 들어가게 됩니다. 대신, 우리는 그들을 다음과 같이 생각할 것입니다.

261
00:23:26,680 --> 00:23:34,680
추상적이거나 일반적인 벡터라고 가정해 보겠습니다. 그리고 다시 말하지만, 이것은 다원성 때문입니다.

262
00:23:34,680 --> 00:23:43,480
심층 신경망에서 발생합니다. 이제 이것의 불행한 부작용은 감소입니다.

263
00:23:43,480 --> 00:23:50,760
심층 신경망의 설명 가능성. 이 분야에서 몇 가지 연구가 진행되고 있습니다.

264
00:23:50,760 --> 00:23:58,920
Anthropic에서 수행 중인 LLM과 같은 심층 신경망에 설명 기능을 다시 추가합니다.

265
00:24:00,760 --> 00:24:07,880
하지만 현재로서는 여전히 해결되지 않은 문제이므로 작업할 때 주의하면 됩니다.

266
00:24:07,880 --> 00:24:13,400
입력과 출력을 연관시키지 않는 심층 신경망 또는 이에 대해 이야기하거나 생각하는 것

267
00:24:13,400 --> 00:24:19,880
단어나 토큰과 같은 유형의 것에는 그런 일이 일어나지 않기 때문입니다. 때문이에요

268
00:24:19,880 --> 00:24:28,440
중첩과 다중성이 발생한다. 좋아요, 그 점을 염두에 두고 다음으로 넘어가겠습니다.

269
00:24:30,360 --> 00:24:33,640
지금까지 우리가 보았던 관심에 대해 간략하게 개요를 말씀드리겠습니다.

270
00:24:34,840 --> 00:24:40,680
지금까지 우리가 본 것은 이제 부가적 반복 주의(additive recurrent attention)라고 부르는 것입니다. 그래서 첨가물

271
00:24:41,240 --> 00:24:47,240
주의 기능을 선택했기 때문입니다. 두 경우 모두 이전에 말씀드렸듯이

272
00:24:48,200 --> 00:24:56,920
간단한 피드포워드 네트워크입니다. 이것은 MLP 네트워크이고 덧셈 계산입니다.

273
00:24:58,360 --> 00:25:05,080
이 주의 메커니즘의 반복되는 부분은 이전 디코더가 필요하다는 사실입니다.

274
00:25:05,080 --> 00:25:11,480
다음 상태를 계산하는 숨겨진 상태. 이 경우, 이 이미지는 ST-1이 이전의

275
00:25:11,560 --> 00:25:18,280
디코더 숨겨진 상태. 모든 인코더가 숨겨진 상태에서 이 숨겨진 상태의 정렬을 계산합니다.

276
00:25:18,280 --> 00:25:25,640
H는 컨텍스트 벡터가 무엇인지 알려주고 숨겨진 다음 디코더를 생성한다고 명시합니다.

277
00:25:25,640 --> 00:25:31,080
상태. 그래서 이것은 다음을 생산하기 위해 이전이 필요하다는 반복되는 개념입니다.

278
00:25:32,760 --> 00:25:39,480
이제 이 반복 구조에는 몇 가지 문제가 있습니다. 첫 번째는 확장성 문제입니다.

279
00:25:39,480 --> 00:25:47,240
이것으로. 단어 사이의 거리가 멀어질수록 성능이 저하됩니다. 자, 우리는 이미 그것에 대해 이야기했습니다.

280
00:25:48,520 --> 00:25:55,640
주의 메커니즘이 해결하는 컨텍스트 벡터 병목 현상입니다. 우리는 이 문제를 해결합니다

281
00:25:56,440 --> 00:26:02,120
입력에서 정보를 캡처하는 인코더의 정보 희석

282
00:26:02,120 --> 00:26:11,160
디코더. 그러나 여전히 더 긴 출력을 생성함에 따라 디코더의 출력 시퀀스는

283
00:26:11,160 --> 00:26:18,360
이전 디코더 숨겨진 상태 및 출력 토큰의 정보는 희석되어

284
00:26:18,360 --> 00:26:29,400
희석. 우리는 이러한 장거리 의존성을 보고 있으며 이것이 바로 순환 신경망이 존재하는 이유입니다.

285
00:26:29,400 --> 00:26:37,160
잘 확장되지 않습니다. 병렬화에도 한계가 있습니다. 반복되는 프로세스에는 능력이 부족합니다.

286
00:26:37,160 --> 00:26:43,400
병렬화됩니다. 이전 숨겨진 항목이 필요하기 때문에 병렬화할 수 있는 것이 실제로 너무 많습니다.

287
00:26:43,400 --> 00:26:48,440
다음 숨겨진 상태와 다음 출력 토큰이 무엇인지 알려주는 상태 및 이전 출력 토큰

288
00:26:48,440 --> 00:26:56,120
될 것. 그리고 메모리 제약이 있습니다. RNN은 제한적인 것으로 악명이 높습니다.

289
00:26:56,120 --> 00:27:02,280
기억, 기억, 장거리 의존성에 대한 투쟁. 희석된 영향이 있습니다.

290
00:27:02,280 --> 00:27:09,880
시퀀스가 진행됨에 따라 출력 시퀀스의 이전 요소. 그렇다면 이러한 문제를 어떻게 해결할 수 있을까요?

291
00:27:09,880 --> 00:27:16,520
문제? 이를 수행하는 한 가지 방법은 RNN 구조에서 주의를 분리하는 것입니다. 우리는 원한다

292
00:27:16,520 --> 00:27:22,120
분리, 목표는 어텐션 메커니즘을 더 작은 자체 포함 구성 요소로 분리하는 것입니다.

293
00:27:22,120 --> 00:27:30,840
우리는 희망적으로 병렬화할 수 있습니다. 이제 관심은 순전히

294
00:27:31,720 --> 00:27:37,960
모델에서 전달될 요소의 중요성을 결정하는 행위. 우리는 이것을 통해

295
00:27:37,960 --> 00:27:43,320
모델이 가장 중요한 부분에 주의를 기울일 수 있도록 주의 가중치를 계산합니다.

296
00:27:44,040 --> 00:27:49,320
그리고 RNN에서 분리하는 목표는 보다 일반적인 주의 메커니즘을 생성하는 것입니다.

297
00:27:49,320 --> 00:27:54,680
그것은 반복되는 구조에만 국한되지 않습니다. 따라서 수정된 절차가 필요합니다. 우리는 여전히 필요합니다

298
00:27:54,680 --> 00:27:59,160
주의를 기울여야 할 요소를 나타내는 컨텍스트의 가중치 공간을 결정한 다음

299
00:27:59,160 --> 00:28:06,280
수행자 기능을 향상하려면 이러한 가중치를 적용해야 합니다. 이제 우리는 새로운 세트가 필요합니다.

300
00:28:06,280 --> 00:28:10,760
왜냐하면 우리가 사용해왔던 표기법이 다음과 밀접하게 연관되어 있기 때문입니다.

301
00:28:12,760 --> 00:28:18,440
오른쪽, 숨겨진 상태와 컨텍스트 벡터 등이 포함된 RNN 구조입니다. 어디에서

302
00:28:18,440 --> 00:28:24,920
대신 사용할 것은 키 값 쿼리 표기법이며, 이 표기법도 사용됩니다.

303
00:28:24,920 --> 00:28:32,440
주의를 기울이는 것은 종이가 필요한 전부이며 우리가 작업하고 있는 개념을 강화합니다.

304
00:28:32,440 --> 00:28:37,480
특징의 중첩을 관찰하는 심층 신경망. 그래서 우리는 생각해 볼 것입니다

305
00:28:37,480 --> 00:28:45,080
이러한 키, 값, 쿼리 및 출력은 우리가 아닌 일반 또는 추상 벡터로 표시됩니다.

306
00:28:45,080 --> 00:28:51,720
지금 당장에 의미를 연관시키겠습니다. 이것이 바로 지금 우리가 하려는 일입니다.

307
00:28:51,720 --> 00:28:58,600
단지 그것들을 추상적인 벡터로 생각하면 됩니다. 좋습니다. 여기에 주의 메커니즘을 그렸습니다.

308
00:28:59,320 --> 00:29:05,640
하지만 키 값과 쿼리 표기법을 사용합니다. 여기서 아이디어는 우리가 정렬을 찾는 것입니다.

309
00:29:05,640 --> 00:29:14,520
또는 키의 호환성이므로 쿼리와 함께 k가 여기에 있습니다. 우리는 다음을 사용하여 정렬을 계산합니다.

310
00:29:14,520 --> 00:29:19,880
주의 기능. 우리는 이전에 피드포워드 네트워크를 사용하여 이 작업을 수행할 수 있다는 것을 보았습니다.

311
00:29:19,880 --> 00:29:23,880
하지만 이를 수행하는 방법에는 여러 가지가 있습니다. 지금은 이것을 일반적인 것으로 유지하겠습니다.

312
00:29:24,440 --> 00:29:28,600
그런 다음 이러한 정렬 점수를 정규화하여 주의 가중치를 생성합니다.

313
00:29:28,600 --> 00:29:34,120
그런 다음 해당 주의 가중치를 사용하여 다양한 값을 조정하여 출력을 생성합니다.

314
00:29:34,120 --> 00:29:43,640
벡터. 이제 우리는 반복되는 쿼리에서 벗어나려고 하기 때문에 여러 쿼리로 이 작업을 수행할 수 있습니다.

315
00:29:43,720 --> 00:29:49,640
이전 쿼리가 다음 쿼리에 종속되지 않는 구조

316
00:29:49,640 --> 00:29:55,480
이것이 바로 RNN 구조의 경우였습니다. 맞습니다. 말하자면 이러한 쿼리는

317
00:29:55,480 --> 00:30:01,560
디코더의 숨겨진 상태가 있는 곳에서 우리는 그것으로부터 멀어지고 있습니다. 그래서 우리는

318
00:30:02,200 --> 00:30:08,200
주의 기능을 올바르게 설정하는 한 쿼리와 키가 동시에 동시에 발생할 수 있습니다.

319
00:30:08,280 --> 00:30:14,920
또는 이를 활용할 수 있는 방식으로. 따라서 여기서도 목표는 여전히 정렬을 찾는 것입니다.

320
00:30:14,920 --> 00:30:21,640
또는 키와 쿼리 간의 호환성을 통해 값을 확장할 수 있으며 이제 세 가지를 생성하고 있습니다.

321
00:30:21,640 --> 00:30:29,320
우리가 수행 중인 세 가지 쿼리에 해당하는 출력입니다. 이제 이 시점에서 이야기해보자.

322
00:30:29,320 --> 00:30:35,320
해당 키, 값 및 쿼리가 어디에서 나오는지, 여기서 self-attention이라는 용어가 사용됩니다.

323
00:30:35,400 --> 00:30:42,120
그리고 Cross-Attention이 들어옵니다. Self-Attention의 경우 키, 값, 쿼리는 모두 다음에서 파생됩니다.

324
00:30:42,120 --> 00:30:49,960
동일한 소스이므로 임의의 입력 x가 임의의 변환을 거친다고 가정해 보겠습니다.

325
00:30:49,960 --> 00:30:56,280
이러한 키, 값 및 쿼리를 생성합니다. 이제 교차주의를 위해 키와 값은 다음에서 나옵니다.

326
00:30:56,280 --> 00:31:03,960
하나의 소스이며 쿼리는 별도의 소스에서 파생됩니다. 이제 그들이 어디에 있든 상관없어

327
00:31:03,960 --> 00:31:11,480
왜냐하면 그것들은 모두 어텐션 함수로 전달되기 때문입니다.

328
00:31:12,600 --> 00:31:17,240
키, 값, 쿼리가 무엇인지 또는 어디서 왔는지. 그러나 이것이 바로 이 개념이 있는 곳입니다.

329
00:31:17,240 --> 00:31:25,560
Self-attention과 Cross-attention은 소스가 분리되어 있음을 나타냄으로써 발생합니다.

330
00:31:25,640 --> 00:31:34,680
키, 값 및 쿼리의 경우. 이제 이 분리된 구현 측면에서

331
00:31:34,680 --> 00:31:41,000
어텐션 메커니즘에서는 어텐션만 두 가지 속성으로 구현하면 됩니다. 첫 번째는

332
00:31:41,000 --> 00:31:47,400
숙련된 내적인 주의 함수에 대한 선택입니다. 이제 이건 좋은거야

333
00:31:47,400 --> 00:31:54,120
호환성을 표현하며 빠르고 해석이 가능하며 병렬화도 가능합니다.

334
00:31:54,680 --> 00:32:05,640
모든 쿼리를 처리하므로 GPU를 활용할 수 있습니다. 우리는 또한 이 내적을 안정적으로 확장합니다.

335
00:32:05,640 --> 00:32:13,480
높은 차원의 자체 최대 그래디언트. 이제 다른 속성은 키에 대한 공통 차원입니다.

336
00:32:13,480 --> 00:32:18,920
값, 쿼리. 이제 이것은 내적에 대한 요구사항입니다. 우리는 내적을 계산할 수 없습니다.

337
00:32:18,920 --> 00:32:25,480
다양한 크기의 벡터를 사용할 수 있지만 트랜스포머 아키텍처도 단순화합니다.

338
00:32:25,480 --> 00:32:30,840
대략 나중에 예측 가능한 주의 출력 형태를 사용하여 보다 일관성 있는 결과를 제공합니다.

339
00:32:30,840 --> 00:32:40,680
더 쉬운 모델 분석을 위한 숨겨진 상태 치수. 이제 이 주의 다이어그램으로 돌아가서,

340
00:32:41,560 --> 00:32:45,880
우리는 이 두 가지 속성을 추가했습니다. 먼저 키의 공유 차원을 확인하고

341
00:32:45,960 --> 00:32:51,960
값, 쿼리 및 출력 벡터 o에 대한 선택 사항을 지정했습니다.

342
00:32:51,960 --> 00:32:58,200
Attention 함수는 각 쿼리와 각 키 사이의 내적입니다.

343
00:32:58,200 --> 00:33:10,200
공유 차원의 제곱근, dk. 이제 우리는 이러한 내적을 병렬로 계산할 수 있습니다

344
00:33:10,200 --> 00:33:16,920
행렬 곱셈을 사용하면 모든 키를 행렬 k로 그룹화하고 모든 쿼리를 그룹화할 수 있습니다.

345
00:33:16,920 --> 00:33:22,680
행렬 q에 넣고 단순히 행렬 곱셈을 하면 됩니다. 왜냐하면 이것은 다음과 똑같은 일을 하기 때문입니다.

346
00:33:22,680 --> 00:33:29,160
각 키와 각 쿼리 사이에 내적을 수행합니다. 값에 대해서도 동일한 작업을 수행할 수 있습니다.

347
00:33:29,160 --> 00:33:33,880
그리고 모든 attention 가중치를 행렬에 넣습니다. 그러면 또 다른 행렬 곱셈을 할 수 있습니다.

348
00:33:33,960 --> 00:33:41,240
출력인 행렬 o를 생성합니다. 이것이 정말 유용한 이유는 다음과 같습니다.

349
00:33:41,240 --> 00:33:46,760
최신 하드웨어에서는 동시성이 높으며 각 쿼리를 독립적으로 계산합니다.

350
00:33:53,880 --> 00:33:58,040
안녕하세요. 이 프리젠테이션의 다음 부분을 진행하기 위해 Aveline의 일을 제가 대신하겠습니다.

351
00:33:59,000 --> 00:34:03,640
나는 다중 머리 주의에 대해 매우 빠르게 이야기하겠습니다.

352
00:34:03,640 --> 00:34:07,800
메커니즘 Aveline이 방금 설명했습니다. 그럼 내용에 대해 조금 이야기해보겠습니다.

353
00:34:07,800 --> 00:34:13,240
트랜스포머가 하고 있는 일과 그 아키텍처가 어떤 모습인지 알아보겠습니다. 그런 다음 조금 더 진행하겠습니다.

354
00:34:13,240 --> 00:34:17,160
구현에 좀 더 중점을 두고 구현 방법을 안내합니다.

355
00:34:17,160 --> 00:34:21,560
자신을 변화시키고 이 과정을 통해 몇 가지 오해를 바로잡고 싶습니다.

356
00:34:21,560 --> 00:34:26,120
구어체로 나오는 변압기에 대한 사람들의 이해 때문에

357
00:34:26,120 --> 00:34:32,200
표기법과 그것이 일반적으로 어떻게 표시되는지. 그럼 저는 첫 번째로 뛰어들겠습니다.

358
00:34:32,200 --> 00:34:37,000
변압기에 대해 발생하는 오해는 주의 메커니즘과 관련이 있습니다.

359
00:34:37,000 --> 00:34:43,400
Aveline이 방금 설명했습니다. 따라서 이것은 트랜스포머가 하는 일에 대한 오해입니다.

360
00:34:43,400 --> 00:34:49,640
쿼리, 키, 값이라는 용어 때문에 사람들이 생각하는 일반적인 생각은

361
00:34:49,720 --> 00:34:56,520
이것은 일종의 벡터 유사성 검색을 수행하고 있다는 것입니다. 그리고 이 표기법은 매우 중요하기 때문에

362
00:34:56,520 --> 00:35:01,800
편리하게도 많은 사람들이 데이터베이스나 사전이 수행할 수 있는 작업에 대한 참고 자료를 갖게 될 것입니다.

363
00:35:01,800 --> 00:35:06,360
비슷한 것을 찾은 다음 해당 값을 가져오는 곳입니다. 사람들은

364
00:35:06,360 --> 00:35:11,640
이것에 지나치게 집착하고 있으며 사용자에게 '나는 추측한다'라는 개념을 소개하는 것이 정말 좋습니다.

365
00:35:11,640 --> 00:35:17,720
이 정의를 사용한 유사성 내적. 그러고 보면 사람들은 외모를 모른다.

366
00:35:17,720 --> 00:35:24,200
이를 지나쳐 사람들이 우리의 가치를 어디서 얻는지 이해하지 못하는 문제로 인해 발생합니다.

367
00:35:24,200 --> 00:35:28,680
그런 다음 우리가 학습할 때 실제로 이 변환기 또는 주의 메커니즘은 무엇입니까?

368
00:35:28,680 --> 00:35:34,360
학습. 그래서 질문이 생깁니다. 우리는 의미가 있는 매개변수적인 것을 배우고 있습니까?

369
00:35:34,360 --> 00:35:40,280
미리 설정된 수의 매개변수나 가중치로 증류됩니다. 아니면 보시다시피 비모수적인가요?

370
00:35:40,280 --> 00:35:45,240
나중에 조사할 수 있는 내장된 표현을 저장하는 데이터베이스를 사용합니다.

371
00:35:46,200 --> 00:35:50,920
제가 말씀드리고 싶은 것은 변환기가 철저하게 파라메트릭 프로세스라는 것입니다.

372
00:35:50,920 --> 00:35:56,120
RAG처럼 제안된 시스템이나 추가 구현이 있습니다.

373
00:35:56,120 --> 00:36:00,600
이런 종류의 혼란스러운 정보를 검색하여 변환기 프로세스를 강화합니다.

374
00:36:00,600 --> 00:36:05,240
실제로 해당 정보를 얻기 위해 사전을 사용합니다. 하지만 오늘은 그냥 얘기만 하자

375
00:36:05,240 --> 00:36:09,400
바닐라 형식의 변압기에 대해. 그리고 이 상황에서 우리는 단지 배우고 있는 중입니다.

376
00:36:09,400 --> 00:36:15,720
매개변수적으로. 따라서 학습되는 내용에 대해 이야기하면서 Aveline은 주의력이 중요하다고 언급했습니다.

377
00:36:15,720 --> 00:36:20,680
Vaswani와 그의 동료들이 제시한 메커니즘은 이 스케일링된 내적 관심입니다.

378
00:36:21,320 --> 00:36:25,000
하지만 우리가 이것을 보면, 우리가 어디에서 왔는지, Self-Attention이 어디에서 왔는지에 관계없이

379
00:36:25,000 --> 00:36:29,400
당신은 자신에게 주의를 기울이는 단일 입력에서 모든 것을 얻거나 교차합니다.

380
00:36:29,400 --> 00:36:34,520
다른 입력을 생성하기 위해 임의의 입력이 있는 경우 주의하세요.

381
00:36:34,600 --> 00:36:40,680
쿼리, 키, 값. 그럼에도 불구하고 내적 주의는 단지 연산일 뿐입니다.

382
00:36:40,680 --> 00:36:45,720
그 일부로 어떤 매개변수도 학습하지 않습니다. 즉, 다른 곳에서 학습을 시도해야 함을 의미합니다.

383
00:36:45,720 --> 00:36:49,240
우리가 무엇을 배울 것인지 알아내세요. 그리고 이것이 바로 변압기의 진정한 힘이 되는 곳입니다.

384
00:36:49,240 --> 00:36:56,280
들어와서, 우리는 임의의 값을 취할 선형 맵이나 가중치 행렬을 배우고 싶습니다.

385
00:36:56,280 --> 00:37:01,480
입력이 어디서 나오든 상관없이 이를 독립된 부분으로 변환하여

386
00:37:01,480 --> 00:37:05,240
이 주의 메커니즘에 영향을 미칠 것입니다. 이는 쿼리, 키 및 값입니다.

387
00:37:06,200 --> 00:37:11,640
쿼리를 생성하기 위해 self-attention 예제를 살펴보겠습니다. 우리는 입력 x를 가지고 있습니다.

388
00:37:11,640 --> 00:37:16,920
다시 말하지만, 우리는 심층 신경망에서 표현의 다의미성을 얻었기 때문에

389
00:37:16,920 --> 00:37:22,520
우리는 상황에 따라 임의의 벡터 또는 임의의 벡터 시퀀스를 고려해 보겠습니다.

390
00:37:23,640 --> 00:37:27,800
x를 모두 보는 곳입니다. 그리고 우리가 하는 일은 그것에 가중치 행렬을 곱하는 것입니다.

391
00:37:27,800 --> 00:37:32,920
유용한 쿼리를 생성할 수 있도록 네트워크에 대해 학습 중인 매개변수입니다.

392
00:37:32,920 --> 00:37:36,760
그런 다음 관련 정보를 가져옵니다. 마찬가지로 키가 무엇인지 알려주기 위해 이 작업을 수행합니다.

393
00:37:37,320 --> 00:37:41,240
그리고 다시, 우리는 키와 쿼리 사이의 유사성을 살펴보고 있으며, 우리는 이러한 유사성을 원합니다.

394
00:37:41,240 --> 00:37:46,760
무엇을 처리해야 하는지에 따라 구분됩니다. 그리고 마지막으로 가치관입니다. 그래서 우리가 할 때

395
00:37:46,760 --> 00:37:51,880
실제로 주의에 대해 여러 번 이야기합니다. 왜냐하면 이 표기법은 과부하가 걸리기 때문입니다.

396
00:37:51,880 --> 00:37:55,800
키, 쿼리,

397
00:37:55,800 --> 00:38:00,040
하지만 호출되는 주의 메커니즘은 키, 값 및 쿼리도 받아들입니다.

398
00:38:01,240 --> 00:38:05,400
너무 과부하가 걸리기 때문에 사람들은 상황에 대한 입력이

399
00:38:06,200 --> 00:38:13,560
값과 완전히 동일합니다. 따라서 쿼리와 값이 있는 경우에는 동일할 수 있습니다.

400
00:38:13,560 --> 00:38:17,080
이론적으로는 그럴 수 있지만 실제로는 다음 과정을 통해 이를 배울 수 있습니다.

401
00:38:17,080 --> 00:38:21,720
최적화를 통해 이러한 가중치 행렬은 다양한 것을 학습하게 됩니다. 그래서 우리의 키와 쿼리는

402
00:38:21,720 --> 00:38:27,000
네, 그렇습니다. 하지만 우리가 self-attention에 관해 이야기할 때, 그것은 단순히 주의를 기울이는 것이 아닙니다.

403
00:38:27,000 --> 00:38:32,760
입력과 자체 사이. 이들은 독립적입니다. 또는 죄송합니다. 서로 의존하고 있습니다.

404
00:38:32,760 --> 00:38:39,160
왜냐하면 그들은 같은 근원에서 나왔지만 서로 다른 표현을 갖도록 학습되었기 때문입니다.

405
00:38:39,160 --> 00:38:43,640
그런 다음 서로에게 질문할 수 있습니다. 내가 사용하고 싶은 중요한 기능이 있습니까?

406
00:38:44,360 --> 00:38:50,040
그 정보를 전달하기 위해? 이제 조금 더 남았으니

407
00:38:50,040 --> 00:38:55,480
우리의 가치가 어디에서 왔는지, 키와 쿼리도 어디에서 왔는지에 대한 직관,

408
00:38:55,480 --> 00:39:00,120
우리는 이 과정에서 무엇을 배우고 있는지에 대해 이야기했습니다. 이제 아이디어로 넘어가겠습니다.

409
00:39:00,120 --> 00:39:04,280
원래의 관심을 바탕으로 여러 사람의 주의를 기울이는 것이 논문의 내용입니다.

410
00:39:04,280 --> 00:39:10,440
실제로 이러한 향상된 성능을 얻기 위해 활용합니다. 그래서 제가 몇 번 언급했듯이,

411
00:39:10,440 --> 00:39:15,640
저자가 구현한 스케일링된 도트 제품 주의 위에 있습니다. 이것의 연장선이군요

412
00:39:15,640 --> 00:39:21,320
이전 프레젠테이션에서 설명한 일반화된 주의 메커니즘과 실제 아이디어는 다음과 같습니다.

413
00:39:21,320 --> 00:39:25,720
여러 머리를 활용하여 다양한 일을 처리하게 될 것입니다. 그래서 우리가 본 것처럼

414
00:39:25,720 --> 00:39:30,840
이전의 반복 신경망 상황은 하나만 취하는 단일 컨텍스트를 가졌습니다.

415
00:39:30,840 --> 00:39:35,480
매번 그렇게 하더라도 중요한 것이 무엇인지에 대한 관점을 소개했습니다.

416
00:39:35,480 --> 00:39:40,200
이 주의 메커니즘을 사용하면 여전히 정보 병목 현상이 발생합니다. 그래서 우리가 그랬음에도 불구하고

417
00:39:40,200 --> 00:39:44,360
우리가 관심을 도입했기 때문에 더 좋아졌습니다.

418
00:39:44,360 --> 00:39:48,760
다양한 하위 표현을 학습하고 이러한 다양한 표현을 학습하는 방법에 대해 알아보겠습니다.

419
00:39:48,760 --> 00:39:54,200
잠시 후. 하지만 주요 아이디어는 데이터를 다른 각도에서 볼 수 있다면 다음과 같은 결과를 얻을 수 있다는 것입니다.

420
00:39:54,200 --> 00:39:59,480
모델은 해당 데이터의 다양한 관점에 공동으로 주의를 기울이는 방법을 학습하므로 디코더가 있을 때

421
00:39:59,480 --> 00:40:05,480
마침내 해당 정보에 액세스하면 해당 정보를 볼 수 있고 편안해질 수 있습니다.

422
00:40:05,480 --> 00:40:09,080
즉, 임의 시퀀스의 다음 토큰이나 요소를 디코딩해야 합니다.

423
00:40:09,960 --> 00:40:13,720
그래서 왜 다중 헤드인지에 대해 약간의 직관을 제공하겠습니다.

424
00:40:13,720 --> 00:40:19,720
처음부터 주의가 필요합니다. Attention 블록의 위쪽 절반을 통과하면

425
00:40:19,720 --> 00:40:23,000
그래서 우리는 어텐션 가중치를 취하고 여기에 값을 곱하는 것에 대해 이야기하고 있습니다.

426
00:40:23,000 --> 00:40:27,720
여러분이 주목하게 될 것은 우리가 이것을 덧셈의 곱셈이나 행렬로 하고 있기 때문입니다.

427
00:40:27,720 --> 00:40:32,200
곱셈 모든 것을 함께 모아서 병렬화하면 값을 곱합니다.

428
00:40:32,200 --> 00:40:37,000
그런 다음 값의 차원을 통해 합산합니다. 그래서 이것은 키와 일치합니다

429
00:40:37,720 --> 00:40:43,480
그리고 실제 값 자체. 따라서 하나의 시퀀스에서 단수를 기반으로 합산합니다.

430
00:40:43,480 --> 00:40:48,840
보다. 그리고 요약을 해보니 무엇이 중요한지에 대한 결정력을 잃어버렸습니다. 본질적으로,

431
00:40:48,840 --> 00:40:54,520
우리는 하나의 관점을 고수했습니다. 그리고 Multi-head attention의 목표는 여러 가지를 배우는 것입니다.

432
00:40:54,520 --> 00:40:59,080
가중치 세트. 그래서 우리는 키를 생성하기 위한 하나의 가중치 세트를 가진 하나의 어텐션 헤드를 가지고 있습니다.

433
00:40:59,080 --> 00:41:04,040
입력이 제공된 값 및 쿼리, 그리고 다른 별도의 헤드를 가진 다른 헤드

434
00:41:04,040 --> 00:41:09,800
다양한 쿼리, 키, 값 등 세트를 학습하기 위한 가중치 세트. 논문

435
00:41:09,800 --> 00:41:14,200
실제로 이것을 확장하여 8개의 개별 헤드와 최신 아키텍처를 학습합니다.

436
00:41:14,200 --> 00:41:21,080
변압기에는 훨씬 더 많은 수의 헤드가 있습니다. 여기서 진짜 목표는 배우는 것입니다.

437
00:41:21,080 --> 00:41:25,640
데이터에 대한 다양한 관점을 제공하는 하위 표현을 통해 우리는

438
00:41:25,640 --> 00:41:31,000
더 풍부한 표현을 제공하고 시퀀스의 각 요소가 어떻게 작동하는지에 대한 모든 컨텍스트를 하나로 묶습니다.

439
00:41:31,080 --> 00:41:35,400
입력 시퀀스인지 여부에 관계없이 해당 시퀀스의 다른 요소와 관련됩니다.

440
00:41:35,400 --> 00:41:40,200
이는 토큰에서 직접적인 의미를 갖거나 우리가 네트워크에서 더 높은 위치에 있고 다음을 처리하는 경우

441
00:41:40,200 --> 00:41:46,120
위치 정보나 의미 또는 문법 정보를 가진 임의의 시퀀스

442
00:41:46,120 --> 00:41:53,240
그것에 포장. 그렇다면 여러분은 스스로에게 물어볼 수도 있습니다. 음, 우리가 다양한 기능을 모두 추가할 것인지

443
00:41:53,240 --> 00:41:57,720
주목하세요, 계산 비용이 극적으로 증가하지 않을까요? 그리고 당신은

444
00:41:57,720 --> 00:42:03,800
추가 예방 조치를 취하지 않았다면 정확합니다. 이제 저자가 일종의 타협점으로 찾은 것은

445
00:42:03,800 --> 00:42:09,240
이는 하위 표현의 크기를 축소하는 경우이므로 더 작은 크기로 이동합니다.

446
00:42:09,240 --> 00:42:15,560
각 주의 머리에 대한 차원 공간을 사용하여 특정 주어진 정보를 줄이거나 압축할 수 있습니다.

447
00:42:15,560 --> 00:42:21,080
렌즈를 사용하면 해당 관점에 더 유용하지만 계산 비용은 더 낮습니다. 그리고 그들은 무엇을

448
00:42:21,080 --> 00:42:25,880
find는 고정된 차원을 부과했기 때문에 모델의 전체 차원을 나누는 경우입니다.

449
00:42:25,880 --> 00:42:30,360
앞서 Aveline이 언급한 것처럼 머리 수에 따라 차원이 더 작아집니다.

450
00:42:30,360 --> 00:42:35,880
마치 머리가 하나인 것처럼 대략 같은 시간 안에 계산할 수 있는 부분 공간입니다.

451
00:42:36,440 --> 00:42:41,080
하지만 압축이 실제로는 그렇지 않기 때문에 훨씬 더 풍부한 정보를 얻을 수 있습니다.

452
00:42:41,080 --> 00:42:47,800
큰 요인입니다. 여기서 일어나는 일을 시각적으로 표현하면 다음과 같습니다.

453
00:42:48,680 --> 00:42:52,360
가중치 행렬로 유입될 소스가 있는데 아직 렌더링하지 않았습니다.

454
00:42:52,360 --> 00:42:56,360
여기로 슬라이드하세요. 왼쪽에서 나온다고 상상해 보세요. 그러면 이 무게를 통해 흐를 것입니다.

455
00:42:56,360 --> 00:43:01,880
각 헤드에 대한 행렬은 키, 값 및 쿼리를 생성한 다음 그로부터 각각

456
00:43:01,880 --> 00:43:07,400
attention head는 별도의 출력 매트릭스를 생성하며 이는 다음을 나타냅니다.

457
00:43:07,400 --> 00:43:14,360
그것은 t x dk의 차원이 될 것입니다. t는 디코딩 시퀀스의 요소 수를 나타냅니다.

458
00:43:14,360 --> 00:43:21,240
또는 쿼리가 처음에 어디서 나오는지. 이제 우리에게 이것이 있다는 것을 기억한다면

459
00:43:21,240 --> 00:43:26,280
모델에 일정하게 부과된 차원이 있는 경우, 우리가 여러 가지 차원을 가지고 있다는 사실을 볼 수 있습니다.

460
00:43:26,280 --> 00:43:30,200
지금 출력하고 모델이

461
00:43:30,200 --> 00:43:35,000
전체적으로 일관된 차원. 모델의 나머지 부분이 사용하도록 설정된 것이 필요합니다.

462
00:43:35,000 --> 00:43:40,920
그리고 이해합니다. 따라서 그 가정은 옳을 것입니다. 그리고 우리가 최종적으로 하고 싶은 것은

463
00:43:40,920 --> 00:43:46,120
일종의 팩업을 위한 매개변수 세트로도 학습되는 최종 가중치 행렬을 도입합니다.

464
00:43:46,120 --> 00:43:50,040
문제에 대한 다양한 관점에서 얻은 풍부한 맥락,

465
00:43:50,040 --> 00:43:54,840
단일 출력으로. 따라서 우리는 모든 머리를 연결한 다음 행렬 곱셈을 수행할 수 있습니다.

466
00:43:54,840 --> 00:44:01,640
마지막으로 이 출력 변환의 가중치를 나타내는 가중치 0을 사용하여

467
00:44:01,640 --> 00:44:06,120
그리고 그것은 우리를 모델의 적절한 차원으로 다시 데려갈 것입니다. 그래서 dk 시대부터

468
00:44:06,120 --> 00:44:12,040
d 모델의 헤드 수. 이제 우리는 다중 헤드 어텐션(multi-head attention)에 대해 이야기했습니다.

469
00:44:12,040 --> 00:44:16,760
우리가 방금 겪은 이해와 그것이 왜 그렇게 중요한지. 이제 나는 이야기하고 싶다.

470
00:44:16,760 --> 00:44:22,520
변환기 아키텍처가 이를 사용하여 RNN의 문제를 탈출하는 방법. 그래서 나는 갈거야

471
00:44:22,520 --> 00:44:28,680
즉, 다중 헤드 아이디어인 트랜스포머의 어텐션 아키텍처가 활용된다고 말할 수 있습니다.

472
00:44:28,680 --> 00:44:32,840
변압기에서는 세 가지 다른 방식으로 작동합니다. 하나는 인코더에서 self-attention을 수행한다는 것입니다.

473
00:44:33,560 --> 00:44:38,760
다음은 디코더에서 마스킹된 self-attention을 수행한다는 것입니다. 마지막으로 우리는 자동을 갖게 되었습니다.

474
00:44:38,760 --> 00:44:43,880
인코더-디코더 교차 주의를 수행하는 회귀 요소입니다. 우리는 겪고 있으니까

475
00:44:43,880 --> 00:44:50,200
시퀀스를 디코딩하는 프로세스가 필요하고 한 번에 토큰을 생성해야 하는데, 결국 이렇게 됩니다.

476
00:44:50,200 --> 00:44:55,080
자동 회귀 요소는 여전히 존재하지만 목표는 이러한 새로운 self-attention 요소를 사용하는 것입니다.

477
00:44:55,080 --> 00:44:59,720
많은 작업을 오프로드하여 컴퓨터가

478
00:44:59,720 --> 00:45:05,080
병렬로 실행할 수 있으며 하나에서 시작하여 가능한 한 최소한의 시간을 수행합니다.

479
00:45:05,080 --> 00:45:09,320
시퀀스를 다음 시퀀스로 이동하거나 시퀀스의 한 요소에서 다음 요소로 이동합니다.

480
00:45:10,120 --> 00:45:15,640
따라서 우리는 레이어마다 사용자 정의 컨텍스트를 가지고 있으므로 레이어마다 다른 헤드를 가지고 있다는 것을 기억하십시오.

481
00:45:15,640 --> 00:45:20,920
아키텍처에 소요되는 시간을 줄이고 싶습니다. 그래서 당신은 무엇을 할 수 있습니까?

482
00:45:20,920 --> 00:45:25,720
순환 신경망 아키텍처에는 이전 입력이 필요하고 우리는

483
00:45:25,720 --> 00:45:30,920
모두 순서대로 실행하는 대신 이러한 반복에 대한 아이디어를 없애겠습니다. 대신 우리는

484
00:45:30,920 --> 00:45:37,160
우리가 보고 있는 전체 입력에 대해 self-attention이라는 아이디어를 실행할 것입니다. 그래서 주어진 시간에

485
00:45:37,160 --> 00:45:43,640
그런 다음 시간 단계, 모델의 기록, 이미 본 토큰을 다음과 같이 인코딩합니다.

486
00:45:43,640 --> 00:45:48,440
이 self-attention 블록의 일부이며, 이를 통해 모델은 공동으로 모든 위치에 주의를 기울일 수 있습니다.

487
00:45:48,440 --> 00:45:53,240
인코더. 따라서 이전 숨겨진 상태의 경로를 따라 흐를 필요는 없으며,

488
00:45:53,240 --> 00:45:57,960
하지만 각 영역은 처음에 약간의 위치 정보를 추가하기 때문에

489
00:45:58,600 --> 00:46:03,320
그러면 다른 모든 요소를 ​​살펴보고 그것이 다른 요소와 어떻게 관련되어 있는지 이해할 수 있습니다.

490
00:46:03,320 --> 00:46:09,000
거기에 있는 맥락입니다. 그리고 이는 요소가 하나의 요소와 어떻게 관련되는지에 대한 맥락을 포함합니다.

491
00:46:09,000 --> 00:46:16,840
다른 것이지만 다운스트림에 유용할 수 있습니다. 따라서 각 레이어 또는 각 주의 블록이 최적화됩니다.

492
00:46:16,840 --> 00:46:21,480
그래서 가장 중요한 정보를 네트워크에 전달합니다. 이제 질량

493
00:46:21,480 --> 00:46:27,720
디코더의 self-attention은 이전 토큰에 대한 종속성 아이디어도 제거합니다.

494
00:46:27,720 --> 00:46:32,360
아키텍처의 일부로. 우리는 여전히 이러한 자기회귀 과정을 갖고 있지만 어느 시점에서든

495
00:46:33,320 --> 00:46:38,360
자동 회귀 프로세스를 반복하면 이전에 디코딩된 모든 토큰을 사용하여 실행됩니다.

496
00:46:39,000 --> 00:46:45,480
그리고 해당 정보를 얻기 위해 복잡한 경로 순회를 수행할 필요 없이 이를 처리합니다.

497
00:46:45,480 --> 00:46:50,120
인코더의 self-attention과 완전히 동일한 방식입니다. 그래서 우리는 각 요소가 모든 것을 살펴보도록 합니다.

498
00:46:50,120 --> 00:46:55,480
그 외에는 제가 공유해야 할 맥락이 무엇인지 알리기 위해 이전에 있었던 것입니다.

499
00:46:55,480 --> 00:47:00,200
이 토큰을 생성하려면? 그리고 이 시점에서 우리는 순환 구조에서 분리되었으므로

500
00:47:00,200 --> 00:47:03,720
우리는 마치 단일 출력인 것처럼 생성될 다음 토큰에 대해서만 이야기하고 있습니다.

501
00:47:04,760 --> 00:47:09,800
이제 특정 요소가 생산되지 않았기 때문에 미래 지향적 편견 문제가 발생합니다.

502
00:47:09,800 --> 00:47:14,680
실제로 볼 수 있는 순간이겠죠? 인코딩된 텍스트에서 왼쪽에서 오른쪽으로 이동하면

503
00:47:14,680 --> 00:47:18,920
시퀀스 또는 디코딩하려는 시퀀스는 토큰 3에 도달할 때까지

504
00:47:18,920 --> 00:47:24,680
토큰 4, 5 등에 대한 정보나 컨텍스트를 본 적이 없습니다.

505
00:47:24,680 --> 00:47:31,480
임베딩 방법에 영향을 미치는 요소입니다. 그래서 이런 상황에서 우리가 하는 일은 저자입니다.

506
00:47:31,480 --> 00:47:39,240
마스킹 또는 상황에 맞는 마스킹이라는 개념을 도입하여 미래를 차단하여

507
00:47:39,240 --> 00:47:45,400
미래의 정보를 바탕으로 표현하고자 하는 중요한 정보를 알려드립니다.

508
00:47:45,400 --> 00:47:49,320
그리고 이를 통해 우리는 이 작업을 병렬로 수행할 수 있습니다. 전체 시퀀스를 얻었기 때문입니다.

509
00:47:49,320 --> 00:47:54,920
한 번에. 그리고 모델이 보게 될 모든 쿼리는

510
00:47:54,920 --> 00:47:58,440
왜냐하면 아키텍처가 반복적인 구조를 갖도록 강요하지 않기 때문입니다.

511
00:48:00,360 --> 00:48:03,800
그리고 이것이 여기서 일어나고 있는 일의 진정한 강점은 우리가 그 많은 작업을 덜어냈다는 것입니다.

512
00:48:04,680 --> 00:48:08,600
비록 우리가 자기 관심 부분에 있어서 공격적으로 이 일을 해야 한다고 해도 말이죠.

513
00:48:09,720 --> 00:48:14,520
마지막으로 인코더-디코더 교차 주의가 있습니다. 그리고 이를 통해 우리가 할 수 있는 일은

514
00:48:14,600 --> 00:48:19,080
우리의 디코더는 인코더의 정보를 보고 관련 정보를 추출하고 가져옵니다.

515
00:48:19,080 --> 00:48:23,880
디코딩을 계속하는 방법을 알려주는 컨텍스트입니다. 그래서 우리는 여전히 인코더를 가지고 있습니다.

516
00:48:23,880 --> 00:48:28,120
모든 정보를 수집하여 잠재 공간의 유용한 컨텍스트에 적용할 것입니다.

517
00:48:28,120 --> 00:48:33,240
그리고 그것을 활용할 디코더입니다. 하지만 우리가 있는 둘 사이의 영역은

518
00:48:33,240 --> 00:48:37,720
실제로 계산을 수행하는 것은 회귀적으로 최소화됩니다. 그리고 그것이 바로 그 이유의 진짜 본질입니다

519
00:48:37,720 --> 00:48:43,880
변압기는 훨씬 더 효율적입니다. 그래서 저자들은 일종의 동기를 부여합니다.

520
00:48:43,880 --> 00:48:48,840
왜 self-attention 섹션인가요? 빨리 요약하자면 그들은 분석을 다음과 같이 분류합니다.

521
00:48:48,840 --> 00:48:54,280
세 가지 구성 요소. 하나는 레이어당 복잡성이고, 하나는 순차적인 작업 수입니다.

522
00:48:54,280 --> 00:49:00,040
실행해야 하며 마지막은 최대 경로 길이입니다. 그래서 오른쪽에서 왼쪽으로 작업하고,

523
00:49:00,040 --> 00:49:04,840
최대 경로 길이에 대해 이야기하겠습니다. 첫째, 이는 반복이라는 개념에서 비롯됩니다.

524
00:49:04,840 --> 00:49:09,640
종속성에서 숨겨진 상태에 대한 정보를 얻을 수 있는 신경망

525
00:49:09,640 --> 00:49:16,280
실제로 이전 숨겨진 상태를 통과해야 했습니다. 그러니 언제든지, 당신의 시간

526
00:49:16,280 --> 00:49:20,840
복잡성은 해당 경로의 길이 또는 본 요소 수에 따라 달라집니다. 에서

527
00:49:20,840 --> 00:49:26,360
self-attention 블록은 위치에 대한 이해와 아키텍처를 인코딩했기 때문입니다.

528
00:49:26,360 --> 00:49:30,680
여기서는 어떤 유형의 종속성도 적용하지 않으므로 해당 정보를 찾을 필요가 없습니다.

529
00:49:30,680 --> 00:49:34,760
다른 곳. 실제로 제가 언급하고 있는 위치로 인코딩됩니다.

530
00:49:34,760 --> 00:49:40,280
계산을 하고 있어요. 따라서 최대 경로 길이는 일정한 것으로 간주됩니다. 순차적인 만큼

531
00:49:40,280 --> 00:49:45,080
작업이 진행되는 경우 이는 병렬화할 수 있는 양을 나타냅니다. 다시 말하지만, 우리는 제거했기 때문에

532
00:49:45,080 --> 00:49:50,920
반복되는 작업은 단일 반복에서 반복을 수행하더라도

533
00:49:50,920 --> 00:49:55,400
이는 고정된 또는 일정한 시간입니다. 왜냐하면 모델 상태를 통과하는 흐름에 의존하지 않기 때문입니다.

534
00:49:55,400 --> 00:50:01,160
시간. 마지막으로, 레이어당 복잡성과 self-attention에 대해 이야기했습니다.

535
00:50:01,160 --> 00:50:05,000
이것은 행렬 곱셈으로 사용할 수 있는 스케일 내적의 길이입니다.

536
00:50:05,560 --> 00:50:09,240
우리는 이 행렬을 수행하기 때문에 n 제곱 d의 시간 복잡도를 갖게 됩니다.

537
00:50:09,240 --> 00:50:14,760
네트워크를 통한 반복당 곱셈은 레이어별로 이루어집니다. 그래서 그게 뭔지 설명해준다

538
00:50:14,760 --> 00:50:18,760
여기서 우리가 주목하는 것은 최신 하드웨어의 self-attention 메커니즘입니다.

539
00:50:18,760 --> 00:50:23,560
행렬 곱셈을 수행하는 데 최적화된 동시성을 실제로 사용할 수 있는 곳은

540
00:50:23,560 --> 00:50:28,120
우리는 계산 복잡도를 낮추고 더 많은 양의 계산을 할 수 있게 됩니다.

541
00:50:28,120 --> 00:50:32,840
실제로 병렬화되고 위치를 인코딩하는 표현으로 끝납니다.

542
00:50:32,840 --> 00:50:38,680
그리고 시퀀스 내에서 서로 관계가 있는 각 요소에 대한 참조 정보를 포함합니다.

543
00:50:40,440 --> 00:50:44,520
그래서 이것은 이것이 훨씬 더 저렴하다는 생각을 강조할 뿐입니다. 그리고 물건이 더 저렴할 때,

544
00:50:44,520 --> 00:50:49,960
우리는 실제로 동일한 비용으로 더 깊이 들어갈 수 있는 능력을 갖고 있으며, 이를 통해 우리는 더욱 강력해질 수 있습니다.

545
00:50:50,600 --> 00:50:57,000
고정된 수의 매개변수가 주어졌습니다. 또한 우리에게는 능력이 있기 때문에 훈련하는 것이 더 빠릅니다.

546
00:50:57,000 --> 00:51:01,240
우리는 반복되는 관계가 없으므로 훈련 과정을 거칠 때

547
00:51:01,240 --> 00:51:05,880
우리는 연속적으로 각 토큰을 반복적으로 예측할 필요가 없습니다. 단일 시퀀스를 취하고

548
00:51:05,880 --> 00:51:11,320
다음 토큰을 예측한다고 표현하고, 이 입력이 주어지면 이를 기반으로 일괄 처리하여 다음 토큰을 예측합니다.

549
00:51:11,320 --> 00:51:17,640
단일 다음 토큰을 사용하면 모든 토큰을 병렬로 실행할 수 있습니다. 마지막으로 약간의 대화를 나누면,

550
00:51:17,640 --> 00:51:22,040
표현하려는 어휘나 시퀀스 길이 n이

551
00:51:23,000 --> 00:51:27,000
다른 시퀀스로 변환하려는 특정 입력에 대해

552
00:51:27,000 --> 00:51:31,800
모델의 실제 치수보다 작습니다. 따라서 숨겨진 표현 상태는

553
00:51:31,800 --> 00:51:36,200
시퀀스를 다룰 때 자주 발생하는 일입니다.

554
00:51:36,200 --> 00:51:40,200
반복 메커니즘보다 self-attention 메커니즘을 사용합니다. 그래서 우리도 이익을 얻습니다

555
00:51:41,080 --> 00:51:43,720
사람들이 이것을 사용하는 문제의 성격상.

556
00:51:46,840 --> 00:51:51,000
이제 우리는 변환기가 이러한 요소를 어떻게 사용할 것인지 동기를 부여했습니다.

557
00:51:51,000 --> 00:51:54,680
Self-Attention의 요소에 대해 아키텍처에 대해 조금 이야기하겠습니다.

558
00:51:54,680 --> 00:51:58,680
이것은 높은 수준의 개요가 될 것입니다. 구현 세부 사항에 대해 더 자세히 알아 보겠습니다.

559
00:51:58,680 --> 00:52:02,200
하지만 이 프리젠테이션에서 무엇인가 취하고 싶다면, 그렇게 되기를 바랍니다.

560
00:52:02,200 --> 00:52:06,760
다중 헤드 주의 및 변환기가 RNN에서 분리될 수 있는 이유와 그 이유

561
00:52:06,760 --> 00:52:12,600
좋은 일이며, 전체적인 트랜스포머 아키텍처를 구축하기 위해 어떻게 협력하는지도 설명합니다.

562
00:52:12,600 --> 00:52:17,160
따라서 문헌이나 프리젠테이션 또는 일반적인 프로젝트에서 이에 대해 들으면

563
00:52:17,160 --> 00:52:23,240
당신은 건축학적으로 무슨 일이 일어나고 있는지 이해합니다. 화면에 표시된 이 이미지를 보면

564
00:52:23,240 --> 00:52:27,640
이 작업이 무엇인지 이야기해 봅시다. 우리는 임의의 입력 시퀀스에서 임의의 입력 시퀀스로 가고 있습니다.

565
00:52:28,360 --> 00:52:34,760
디코딩된 시퀀스 또는 대상 시퀀스이며 이 모델이 작동하려면

566
00:52:34,760 --> 00:52:43,160
부동 소수점 차원 시스템으로 전환해야 합니다. 이 신경망은 작동하도록 설계되지 않았습니다.

567
00:52:43,160 --> 00:52:48,360
토큰이나 기호를 명시적으로 사용하는 대신 수치적 접근 방식을 사용합니다. 그래서 우리는 우리의 것을 가져갈 것입니다

568
00:52:48,360 --> 00:52:53,240
입력과 출력 모두 이 인코더와 디코더를 통해 흐르기 때문입니다.

569
00:52:53,240 --> 00:53:00,600
스택을 쌓아서 숫자 공간에 삽입할 것입니다. 그렇죠? 그래서 이것은 다음과 같은 과정을 거칩니다.

570
00:53:00,600 --> 00:53:08,680
임베딩 입력 임베딩 공간, 죄송합니다. 입력 임베딩 레이어, 예를 들어 출력 임베딩

571
00:53:08,680 --> 00:53:13,720
그런 다음 모델의 나머지 부분으로 흘러들어갑니다. 다음으로 이러한 시퀀스는

572
00:53:13,720 --> 00:53:21,720
위치 인코딩. 이제 이 아키텍처의 위치 인코딩 요소가 엮어질 것입니다.

573
00:53:21,720 --> 00:53:27,160
또는 모든 요소를 ​​참조하여 요소가 어디에 있는지에 대한 신호 정보를 중첩합니다.

574
00:53:27,160 --> 00:53:31,960
다른 요소들과 이것이 우리가 반복되는 관계를 제거할 수 있는 이유입니다. 우리는 필요하지 않습니다

575
00:53:31,960 --> 00:53:36,680
이전 또는 이후에 오는 내용을 정의하는 경로입니다. 왜냐하면 해당 정보는 직접적으로

576
00:53:36,680 --> 00:53:44,120
시퀀스의 해당 요소를 표현하는 데 사용할 수 있습니다. 그런 다음 공통 인코더가 있습니다.

577
00:53:44,120 --> 00:53:49,000
앞서 이야기한 디코더 아키텍처에 대해서는 잠시 후에 확대해 보겠습니다.

578
00:53:49,000 --> 00:53:53,160
주요 아이디어는 해당 정보를 인코더에서 압축한 다음 디코더에서 압축한다는 것입니다.

579
00:53:53,160 --> 00:53:57,560
이를 확장하여 예측 헤드 또는 생성기를 통해 흐르는 다음 토큰을 생성합니다.

580
00:53:57,560 --> 00:54:04,280
맨 위. 해당 예측 헤드는 토큰이 무엇일 수 있는지에 대한 일련의 확률을 출력합니다.

581
00:54:04,360 --> 00:54:09,720
이러한 시퀀스를 표현하는 데 사용하는 미리 설정된 보컬 카포럼 또는 어휘,

582
00:54:10,520 --> 00:54:15,720
그런 다음 다음 토큰이 무엇인지 선택하는 디코딩 절차를 통해 흐릅니다.

583
00:54:15,720 --> 00:54:21,240
확률입니다. 여기서 자동 회귀하는 부분은 이전 금액이 주어졌을 때입니다.

584
00:54:21,240 --> 00:54:26,280
디코딩된 토큰 중에서 다음 토큰이 무엇인지 예측하고 일부 디코딩을 사용할 것입니다.

585
00:54:26,280 --> 00:54:33,880
가능성이 가장 높은 것을 선택하는 절차입니다. 인코더 블록을 확대하면

586
00:54:36,600 --> 00:54:41,480
인코더는 본질적으로 이 self-attention을 적용하여 어떻게 진행되는지에 대한 컨텍스트를 추가할 것입니다.

587
00:54:41,480 --> 00:54:47,560
요소는 서로 관련되어 있습니다. 따라서 언어 대 언어 또는 텍스트 대 텍스트 작업에서는

588
00:54:47,560 --> 00:54:53,000
이는 실제로 표현 자체가 흐름에 따라 문법적 또는 순차적 맥락을 추가하는 것입니다.

589
00:54:53,000 --> 00:54:58,040
모델을 통해 중첩된 컨텍스트가 있는 특징을 추출하는 것이 전부입니다.

590
00:54:58,040 --> 00:55:04,520
디코더가 데이터에 대해 다양한 관점을 얻을 수 있도록 하는 것은 엄청나게 강력합니다.

591
00:55:04,520 --> 00:55:08,120
그래서 모든 토큰에 대해 가장 중요한 컨텍스트를 파악할 수 있습니다.

592
00:55:11,000 --> 00:55:15,160
반면에 디코더는 비슷한 프로세스를 거치게 되지만 마스크를 사용하게 됩니다.

593
00:55:15,160 --> 00:55:19,400
우리가 미래 지향적 편견을 가지지 않도록 자기 관심을 기울이는 것입니다.

594
00:55:19,400 --> 00:55:24,200
이전에 유용한 정보를 얻기 위해 인코더에 추가 교차 주의를 기울입니다.

595
00:55:25,000 --> 00:55:29,240
이제 이 두 가지 상황 모두에서 구현에 대해 더 자세히 설명하겠습니다.

596
00:55:29,800 --> 00:55:34,920
하지만 우리는 이 블록에 추가된 최종 피드포워드 네트워크를 가지고 있으며, 이 피드포워드는 무엇을 의미합니까?

597
00:55:34,920 --> 00:55:39,800
네트워크가 할 일은 다중 헤드 주의가 중요한 것에 대한 컨텍스트를 추가하는 것입니다.

598
00:55:39,800 --> 00:55:43,080
피드포워드 네트워크를 통해 우리는 해당 컨텍스트를 활용할 수 있습니다.

599
00:55:44,200 --> 00:55:48,600
앞으로 더 풍부한 표현을 얻을 수 있도록 말이죠. 따라서 다중 헤드 주의는 다음과 같이 말합니다.

600
00:55:48,600 --> 00:55:52,280
주의해야 할 사항은 피드포워드 네트워크입니다.

601
00:55:52,280 --> 00:55:57,640
단일 상황에서는 '좋아, 이 관심을 바탕으로 이것이 내가 하고 싶은 일'이라고 말할 것입니다.

602
00:55:57,640 --> 00:56:04,280
내 신호로. 이러한 모든 종류의 하위 계층 또는 하위 주의 블록 사이에는

603
00:56:05,240 --> 00:56:09,320
이러한 연결, 이러한 하위 계층 연결에 대해서는 나중에 더 자세히 설명하겠습니다.

604
00:56:09,320 --> 00:56:14,280
하지만 실제로는 신호를 정규화하고 문제가 없는지 확인하기 위해 존재합니다.

605
00:56:14,360 --> 00:56:18,360
그라디언트가 폭발하거나 사라지거나 우리가 본 데이터에 과적합되지 않습니다.

606
00:56:20,520 --> 00:56:26,200
생성기는 아키텍처에 부착되는 최종 헤드이며 매우 간단합니다. 그것은

607
00:56:26,200 --> 00:56:30,280
선형 레이어를 가져와서 소프트맥스로 흐르게 하고, 이는 다음을 기반으로 생성됩니다.

608
00:56:30,280 --> 00:56:35,240
모델이 생산한 로짓. 이를 취하여 출력 확률로 변환합니다.

609
00:56:35,240 --> 00:56:41,480
당신이 훈련하고 있는 어휘에서. 이제 디코딩 프로세스가 진행되는 한,

610
00:56:41,480 --> 00:56:46,280
이것은 일반적으로 탐욕스럽게 수행되므로 일종의 argmax와 같은 작업을 수행합니다. 당신은 가장 가능성이 높은 것을 선택합니다

611
00:56:46,280 --> 00:56:52,360
확률을 확인하고 이를 다음 토큰으로 직접 디코딩하면 됩니다. 모델을 훈련할 때,

612
00:56:52,360 --> 00:56:57,160
우리는 실제로 디코딩된 토큰을 다루지 않습니다. 우리는 항상 인코딩된 상태로 유지합니다. 그런 식으로,

613
00:56:57,160 --> 00:57:03,400
우리는 훈련 과정을 계속해서 진행할 수 있습니다. Aveline이 앞서 언급했듯이 Veswanian은

614
00:57:03,400 --> 00:57:12,440
동료들은 고정 차원 구현을 사용합니다. 그들은 아키텍처 전반에 걸쳐 D 모델을 사용합니다.

615
00:57:12,440 --> 00:57:17,960
적어도 레이어 사이에 전달되는 것까지. 그 이유는 그것이 매우 성공할 것이기 때문이다.

616
00:57:17,960 --> 00:57:22,840
이러한 블록을 쉽게 쌓을 수 있으므로 인코더 블록과 디코더 블록을 통해 깊이를 얻을 수 있습니다.

617
00:57:22,840 --> 00:57:28,040
모델. 매개변수가 많을수록 신경망은 무엇이든 더 잘 근사할 수 있습니다.

618
00:57:28,040 --> 00:57:33,480
당신이 하고 있는 일은. 이제 이 다이어그램이 화면에 표시되는 동안 빨리 정리하고 싶습니다.

619
00:57:34,120 --> 00:57:38,680
변압기 블록을 쌓는 개념에 대해 알아보겠습니다.

620
00:57:38,680 --> 00:57:44,440
나중에 또 다른 오해가 생기겠지만, 여기서 n 번을 보는 것은 실제

621
00:57:44,440 --> 00:57:49,000
자체를 차단합니다. 따라서 인코더 블록을 쌓으면 다른 블록 위에 인코더가 됩니다.

622
00:57:49,000 --> 00:57:54,360
다른 디코더 위에 인코더와 디코더가 있습니다. 저자가 직접 제작한 논문

623
00:57:54,360 --> 00:58:00,760
6개의 스택된 인코더 블록과 스택된 디코더 블록을 사용했지만 이 그림에 따르면

624
00:58:00,760 --> 00:58:05,080
제 생각에는 n번이 여기서 의미하는 바에 대해 약간 오해의 소지가 있을 수 있다고 생각합니다.

625
00:58:05,080 --> 00:58:09,480
전체 변압기를 쌓는 것이 아니라 인코더와 디코더를 쌓는 것으로 생각하십시오.

626
00:58:11,800 --> 00:58:15,160
좋아요, 그럼 그건 제쳐두고 좀 더 자세히 들어가 보겠습니다.

627
00:58:15,160 --> 00:58:19,000
구현 세부 사항을 설명하고 주석이 달린 변환기를 활용하겠습니다.

628
00:58:19,000 --> 00:58:23,000
이는 훌륭한 리소스이므로 이 내용을 살펴보기 위해 아래 슬라이드에 링크를 걸어 두었습니다.

629
00:58:23,240 --> 00:58:27,400
내 설명의 일부로, 실제로 가서 직접 읽어 보시기 바랍니다.

630
00:58:27,400 --> 00:58:31,320
코드를 살펴보고 이것이 무엇을 의미하는지 생각해 볼 시간이 더 있을 것입니다.

631
00:58:31,320 --> 00:58:35,560
실제 논문과의 관계. 여기에는 일부 내용이 포함되어 있으며 기본적으로는 그냥 걸어다닙니다.

632
00:58:35,560 --> 00:58:40,520
종이가 의미하는 바를 단계별로 설명합니다. 그래서 훌륭한 자원입니다. 오늘 발표를 위해,

633
00:58:40,520 --> 00:58:45,480
저는 트랜스포머를 조립하는 것을 마치 레고 하우스를 짓는 것처럼 다루고 싶습니다. 우리는 시작할거야

634
00:58:45,480 --> 00:58:50,200
작은 블록으로 정말 작은 다음 구성할 수 있는 다양한 모듈을 구축합니다.

635
00:58:50,280 --> 00:58:54,680
함께 사용하거나 구현할 수 있는 최종 변환기 아키텍처를 구축합니다.

636
00:58:54,680 --> 00:58:58,760
실생활. 그리고 나는 주석이 달린 순서와 약간 다른 순서로 그것을 제시할 것입니다.

637
00:58:58,760 --> 00:59:03,000
트랜스포머는 그렇습니다. 우리가 여러분에게 보여주고자 하는 이야기에 적합하다고 생각하기 때문입니다.

638
00:59:03,000 --> 00:59:09,560
조금 더 나은. 그래서 제가 이 트랜스포머인 레고 하우스를 짓는다면,

639
00:59:09,560 --> 00:59:14,360
우리가 이것에 대해 이야기하고 싶을 때, 반복해서 많이 하지 않도록 도움이 될 수 있습니다.

640
00:59:14,360 --> 00:59:19,320
일종의 복제 또는 복사 기능을 도입하는 코드입니다. 따라서 하위 구성요소를 설계하면

641
00:59:20,280 --> 00:59:25,080
이것이 이 가상의 집의 벽이라고 가정해 보겠습니다. 저는 그런 벽을 잔뜩 만들 수 있기를 원합니다.

642
00:59:25,080 --> 00:59:29,320
인프라를 구축하는 데 사용할 수 있도록 동일하게 보이거나 이 상황에서

643
00:59:29,320 --> 00:59:35,640
변압기의 아키텍처. 여기서 주된 동기는 요소를 반복할 수 있는지,

644
00:59:35,640 --> 00:59:42,760
그런 다음 실제 설계 비용 없이 더 깊게 쌓을 수 있습니다. VGG 구현과 마찬가지로

645
00:59:42,760 --> 00:59:48,360
이는 컴퓨터 비전 구현이었으며 꼭 이와 관련이 있는 것은 아니지만

646
00:59:48,360 --> 00:59:52,200
쌓을 수 있는 구성 가능한 블록이 있다는 아이디어를 추가했습니다. 그래서 우리는 그 일을 하고 싶습니다

647
00:59:52,200 --> 00:59:56,840
우리 트랜스포머도 마찬가지야. 그래서 이 기능을 클론이라고 부르겠습니다. 그리고 본질적으로 무엇

648
00:59:56,840 --> 01:00:02,680
할 일은 모듈을 제공하는 것입니다. 이 경우에는 PyTorch에서 이 작업을 수행하고 n 개의 복사본을 만듭니다.

649
01:00:02,680 --> 01:00:08,520
그것. 이제 PyTorch 또는 PyTorch에서 매개변수를 학습한 무언가의 복제본을 생성할 때마다

650
01:00:08,520 --> 01:00:14,520
TensorFlow에서는 명시적으로 그렇게 설계하지 않는 한 이러한 클론이

651
01:00:14,520 --> 01:00:17,960
동일한 매개변수를 공유하지 마십시오. 그리고 변압기를 사용하여 우리는 그것들이 분리되어 있는지 확인하고 싶습니다.

652
01:00:17,960 --> 01:00:23,320
서로 다른 헤드가 별개의 것을 배우거나, 서로 다른 스택이 학습하도록

653
01:00:23,960 --> 01:00:30,120
또는 인코더 또는 디코더의 서로 다른 스택 블록이 별도의 매개변수를 학습하고 있습니다.

654
01:00:30,120 --> 01:00:34,040
그렇게 하면 그들은 네트워크의 특정 부분을 최적화할 수 있는 모든 능력을 갖게 됩니다.

655
01:00:34,680 --> 01:00:38,520
그래서 우리가 실제로 이 모델을 만들러 갈 때, 여러분은 이 모델의 마지막 부분을 기억해야 합니다.

656
01:00:38,520 --> 01:00:42,760
모든 모델 매개변수를 다시 초기화하여 클론별로 분리되고 독립적이 되도록 합니다.

657
01:00:43,560 --> 01:00:51,080
좋아요, 이해한 바에 따라 건축물의 첫 번째 부분인 레고 하우스를 만들어 봅시다.

658
01:00:51,080 --> 01:00:54,440
그리고 이런 상황에서 우리는 어떻게 정보를 얻을 수 있는지 이해해야 합니다.

659
01:00:54,440 --> 01:00:58,440
우선 우리 변압기에 들어가세요. 그래서 앞서 우리가 사용할 것이라고 언급한 바 있습니다.

660
01:00:58,440 --> 01:01:04,840
입력 어휘의 일종의 수치 시스템에 임베딩합니다. 이제 이것이 토큰이라면,

661
01:01:04,840 --> 01:01:09,640
단어와 관련된 것과 같이 우리는 본질적으로

662
01:01:09,640 --> 01:01:14,280
I am James와 같은 문구의 독립적인 부분이나 개별적인 부분,

663
01:01:14,840 --> 01:01:19,080
그리고 이를 숫자로 변환할 수 있는 작은 하위 조각으로 나눕니다.

664
01:01:20,120 --> 01:01:24,520
고정 길이의 벡터를 모델에 입력할 수 있습니다. 그리고 이것은 주로 신경 때문입니다.

665
01:01:24,520 --> 01:01:31,800
아키텍처는 숫자에 대해 작업합니다. 게다가, 가능한 많은 어휘를 다룰 때,

666
01:01:31,800 --> 01:01:37,000
영어 전체나 현대의 대규모 언어 모델처럼 어쩌면 전체

667
01:01:37,000 --> 01:01:44,920
모든 언어의 어휘에는 다음을 나타내는 수많은 단어나 기호가 있습니다.

668
01:01:44,920 --> 01:01:49,720
해당 시퀀스의 의미를 전달합니다. 그리고 우리는 신경망이 다음과 같이 되는 것을 원하지 않습니다.

669
01:01:49,720 --> 01:01:54,440
차원이 가는 한 매우 넓습니다. 왜냐하면 비효율적이고 시간이 많이 걸리기 때문입니다.

670
01:01:54,440 --> 01:01:59,640
각 레이어를 계산하는 데 오랜 시간이 걸립니다. 따라서 이러한 임베딩 프로세스의 일부로 다음과 같은 이점이 있습니다.

671
01:01:59,640 --> 01:02:03,720
이 수치 시스템으로 변환하면 차원이 줄어들 것입니다.

672
01:02:03,720 --> 01:02:09,800
그리고 계산을 더 효율적으로 만듭니다. 이제 이러한 임베딩을 구축하는 방법은 무엇입니까?

673
01:02:09,800 --> 01:02:13,960
학습된 매핑을 사용하고 있습니다. 이것을 표준 선형 투영처럼 생각할 수 있습니다.

674
01:02:13,960 --> 01:02:19,960
신경망에서는 선형 레이어처럼 수행하는 대신 사전을 갖게 됩니다.

675
01:02:19,960 --> 01:02:27,720
이는 어휘를 조회하거나 매핑하고 해당 고유 벡터를 갖게 됩니다. 그래서 우리는

676
01:02:27,720 --> 01:02:31,320
본질적으로 앞으로 나아갈 가중치를 배우고 그 결과가 무엇인지 알아보기 위해 검색을 수행합니다.

677
01:02:31,320 --> 01:02:37,880
해당 단어는 입니다. 하지만 여전히 선형 레이어를 수행하고 있습니다. 일단 숫자로 입력하면

678
01:02:37,880 --> 01:02:40,920
공간이 있는 경우 최종 임베딩에 도달하기 위해 여전히 선형 레이어를 사용합니다.

679
01:02:42,760 --> 01:02:47,000
이제 변환기로 데이터를 가져오는 방법의 다음 요소는 위치입니다.

680
01:02:47,000 --> 01:02:52,840
부호화. 그리고 이것이 하는 일은 요소에 대한 정보를 추가하는 것입니다.

681
01:02:52,840 --> 01:02:58,920
다른 요소를 참조하여 시퀀스 위치를 지정합니다. 그리고 우리는 이것을 명시적으로 수행합니다.

682
01:02:58,920 --> 01:03:03,400
반복이나 컨벌루션 구조를 피할 수 있습니다. 왜냐하면 계산을 할 때

683
01:03:03,400 --> 01:03:08,760
표현 자체에는 그 위에 계층화되거나 얽혀 있는 정보가 있습니다.

684
01:03:08,760 --> 01:03:14,680
그 신호로. 그리고 우리가 이것을 할 수 있는 방법은 우리가 고정된 차원을 만들어냈기 때문입니다.

685
01:03:14,680 --> 01:03:21,480
입력 시퀀스에 대한 기능의 내장된 표현(고정 차원이므로)

686
01:03:21,480 --> 01:03:28,040
실제로 위치 요소나 정보를 추가하기 위해 요소별 추가를 수행할 수 있습니다.

687
01:03:28,120 --> 01:03:35,160
해당 위치 정보가 포함된 기능과 동일한 차원을 공유하는 한.

688
01:03:37,080 --> 01:03:41,240
그래서 저자들은 이에 대해 몇 가지 방법을 사용합니다. 그들이 논문에서 강조한 점은

689
01:03:41,240 --> 01:03:47,640
정현파 위치 인코딩을 활용합니다. 그리고 그들은 다른 접근법을 선호하여 이렇게 합니다.

690
01:03:47,640 --> 01:03:52,840
실험한 MLP를 사용하여 학습된 위치 인코딩과 같이

691
01:03:53,320 --> 01:03:57,800
정현파 표현이 1위이기 때문에 조금 더 직관적입니다.

692
01:03:57,800 --> 01:04:03,160
이해하다. 두 번째, 사인파는 반복되는 방식을 갖고 있으며 이를 표현할 수 있기 때문입니다.

693
01:04:03,160 --> 01:04:11,400
선형 함수로서 한 위치에서 다른 위치로의 오프셋을 가정하면 다음과 같습니다.

694
01:04:11,400 --> 01:04:16,440
컨텍스트 길이가 점점 길어지면 모델이 컨텍스트에 주의를 기울이는 방법을 쉽게 배울 수 있습니다.

695
01:04:16,440 --> 01:04:21,320
서로 다른 요소 간의 관련 위치 및 더 긴 시퀀스 길이로 일반화 가능

696
01:04:21,400 --> 01:04:27,160
훈련받은 것보다 조금 더 나아졌습니다. 따라서 이 문제에 관한 한 그들이 하는 일은 본질적으로 다음과 같습니다.

697
01:04:27,160 --> 01:04:34,040
코사인 신호와 사인 신호를 엮습니다. 나는 이것에 대한 방정식을 옆에 가지고 있습니다.

698
01:04:34,600 --> 01:04:41,560
여기서 나는 시퀀스의 위치를 ​​나타내고 L은

699
01:04:43,240 --> 01:04:48,520
시퀀스의 해당 요소에 대한 표현의 차원에 대한 색인입니다.

700
01:04:49,240 --> 01:04:57,080
따라서 신호는 인덱스 L을 기반으로 한 요소 표현에 포함됩니다.

701
01:04:57,080 --> 01:05:02,120
그런 다음 시퀀스의 각 토큰 또는 이 임의 시퀀스의 각 요소에 대해

702
01:05:02,120 --> 01:05:06,360
우리는 그것이 어디에 있는지 또는 인덱스 I를 기반으로 위치 정보를 추가할 것입니다.

703
01:05:07,560 --> 01:05:11,880
우리가 사용하는 방식을 개선하기 위해 변압기 위에 구축된 추가 작업이 있습니다.

704
01:05:11,880 --> 01:05:16,840
이 위치 인코딩을 얻는 중입니다. 로프는 사후에 만들어진 훌륭한 방법이다.

705
01:05:17,560 --> 01:05:21,000
요즘에는 많이 사용됩니다. 나는 당신이 가서 그것을 살펴 보는 것이 좋습니다. 만약 당신이

706
01:05:21,000 --> 01:05:25,560
이를 직접 구현하려면 기초가 어디에 있는지 이해하는 것이 중요합니다.

707
01:05:25,560 --> 01:05:29,800
원래의 변압기 종이가 사용되었으며 무슨 일이 일어나고 있는지 머리를 감싸는 것이 쉽습니다.

708
01:05:29,800 --> 01:05:35,480
여기서 이 위치 인코딩을 추가하거나 삽입합니다. 그러나 회전식 위치 지정을 사용하는 경우

709
01:05:35,480 --> 01:05:39,800
좀 더 현대적인 기능인 인코딩을 사용하면 동일한 수준의 표현력을 얻을 수 있습니다.

710
01:05:39,800 --> 01:05:44,120
하지만 두 개의 별도 신호가 필요하지 않습니다. 그래서 저는 여러분이 가서 살펴보시기를 권하는 것입니다.

711
01:05:45,080 --> 01:05:48,840
그럼에도 불구하고, 주요 아이디어는 구현이 진행되는 한, 우리는

712
01:05:48,840 --> 01:05:54,840
실제 위치 정보에 추가할 함수

713
01:05:54,840 --> 01:05:58,680
모델이 취할 특징 표현이므로 정보가 거기에 있을 것입니다.

714
01:05:58,680 --> 01:06:03,640
그리고 옆에 있는 하이 토치 코드를 보면 요소가 강조 표시된 것을 볼 수 있습니다.

715
01:06:03,640 --> 01:06:09,000
각 요소에 대한 사인파와 비용파를 계산한 다음 인터리브합니다.

716
01:06:09,000 --> 01:06:14,680
그것들을 함께 사용한 다음 선형 투영을 수행합니다. 따라서 우리는 선형 투영이 아니라 다음을 수행하고 있습니다.

717
01:06:15,320 --> 01:06:19,560
실제 기능 표현과 함께 해당 신호를 제자리에 추가합니다.

718
01:06:22,280 --> 01:06:25,480
이를 이해한 상태에서 정보를 얻는 방법에 대해 이야기했습니다.

719
01:06:25,480 --> 01:06:30,040
이 문제는 인코더에서 발생하지만 이전 모델에서도 발생한다는 점을 기억하세요.

720
01:06:30,040 --> 01:06:36,120
디코딩된 토큰. 따라서 우리는 임베딩 및 위치 인코딩을 통해 다음 단계로 들어갈 준비를 합니다.

721
01:06:36,120 --> 01:06:40,760
디코더 레이어. 하지만 일단 이 인코더-디코더가 가져올 수 있는 정보를 얻었으면

722
01:06:40,760 --> 01:06:45,320
구조에 관해서는 인코더-디코더 구조가 실제로 어떻게 보이는지에 대해 이야기해야 합니다.

723
01:06:46,280 --> 01:06:53,000
따라서 인코더와 디코더 모두에는 다음과 같은 여러 하위 계층이 있습니다.

724
01:06:53,000 --> 01:06:58,840
구조를 구축하는 데 유용하며 그 중 하나는 다중 헤드 주의 하위 계층입니다.

725
01:06:58,840 --> 01:07:03,240
그리고 이것은 다중 헤드 주의를 수행하여 우리에게 허용되는 가중치를 학습할 것입니다.

726
01:07:03,240 --> 01:07:08,120
앞서 이야기한 것처럼 키, 값, 쿼리를 생성합니다. 그리고 이것의 주요 아이디어는,

727
01:07:08,120 --> 01:07:12,040
앞서 이야기한 것처럼 입력 시퀀스에서 관련 컨텍스트를 추출하는 것입니다.

728
01:07:12,760 --> 01:07:16,840
여러 헤드를 활용하여 더 높은 해상도나 다양한 뷰를 제공합니다.

729
01:07:17,400 --> 01:07:23,960
해당 컨텍스트를 다양한 각도에서 사용할 수 있도록 문제에 접근합니다. 그리고 이것이 어떻게 구현되는지

730
01:07:23,960 --> 01:07:28,840
우리가 이전에 설명했던 바로 그 방법입니다. 이에 대한 코드 표현을 보려면

731
01:07:28,920 --> 01:07:34,200
우리는 여전히 확장된 내적 구현을 ​​사용하고 있습니다. 그래서 우리가 단지

732
01:07:34,200 --> 01:07:39,000
여기에 주의를 기울이고 이를 수행한 후의 키, 값 및 쿼리라는 점을 명심하세요.

733
01:07:39,000 --> 01:07:44,280
선형 투영이죠? 그래서 이것은 단지 어텐션 함수 그 자체입니다. 우리는 매트릭스를 수행합니다

734
01:07:44,280 --> 01:07:52,440
쿼리와 키를 곱하고 dk 차원으로 크기를 조정한 다음 다음과 같이 표시됩니다.

735
01:07:52,440 --> 01:07:55,960
여기 맨 아래에서 정규화한 후 주의 가중치를 취하겠습니다.

736
01:07:56,040 --> 01:08:00,520
소프트맥스에 값을 곱하여 우리가 사용하는 각 값의 양을 추출하거나

737
01:08:00,520 --> 01:08:04,600
우리가 그것에 얼마나 주의를 기울여야 하는지. 이제 여러분이 만들 수 있는 몇 가지 유용한 것들이 있습니다.

738
01:08:04,600 --> 01:08:10,120
당신이 그것을 할 때 당신의 구현에. 하나는 마스킹 아이디어이고 다른 하나는 드롭아웃입니다.

739
01:08:10,760 --> 01:08:16,680
자신만의 구현을 설계할 때 이러한 기능을 통합하는 주요 아이디어는 다음과 같습니다.

740
01:08:16,680 --> 01:08:21,800
사용자 정의 관심을 디자인할 필요가 없기 때문에 이러한 모델을 구성하는 능력을 가속화합니다.

741
01:08:21,800 --> 01:08:26,360
인코더에 대한 함수 또는 디코더에 대한 사용자 지정 함수

742
01:08:26,360 --> 01:08:30,840
이 마스킹을 사용하세요. 정확한 정보를 사용할 수 있도록 이 함수의 매개변수로 설정할 수 있습니다.

743
01:08:30,840 --> 01:08:36,760
인코더와 디코더 모두에 동일한 주의 기능이 있습니다. 여기서의 드롭아웃은 단지 이점일 뿐입니다.

744
01:08:37,400 --> 01:08:42,040
하위 계층 연결과 일반적으로 우리가 사용할 정규화 전략

745
01:08:42,040 --> 01:08:48,760
모델 전반에 걸쳐. Dropout은 기본적으로 우리 내부의 일부 임의 기능을 삭제합니다.

746
01:08:48,760 --> 01:08:53,960
모델이 보고 있는 데이터에 과적합되는 경향이 없도록 표현합니다. 그래서 이것은

747
01:08:53,960 --> 01:08:58,520
더 깊이 들어가면서 네트워크에 구축하는 데 유용한 패턴인 암시적 정규화의 한 형태입니다.

748
01:08:58,520 --> 01:09:04,760
과적합을 피하기 위해 더 깊게 만듭니다. 이제 이 주의 기능을 기반으로 하는 아이디어는 다음과 같습니다.

749
01:09:04,760 --> 01:09:10,440
다중 주의. 자, 다시 말하지만, 다중 헤드 어텐션이 무엇인지에 대한 모호함이 있습니다.

750
01:09:10,440 --> 01:09:15,960
표기법은. 여기서 쿼리, 키, 값을 취하는 정방향 전달을 살펴보면,

751
01:09:15,960 --> 01:09:21,400
나는 이것을 다른 이름으로 지정하고 싶지만 이에 상응하는 일반적인 구현은

752
01:09:21,400 --> 01:09:26,120
종이에 쿼리, 키, 값의 이름을 지정합니다. 이것들을 실제라고 생각하세요

753
01:09:26,120 --> 01:09:30,120
선형 투영을 통과하기 전에 스스로에게 신호를 보냅니다.

754
01:09:31,880 --> 01:09:38,200
그 다른 공간, 학습되고 있는 하위표상. 그래서 이 상황에서,

755
01:09:38,200 --> 01:09:42,200
self-attention을 수행한다면 쿼리, 키, 값은 모두 동일할 것입니다.

756
01:09:42,760 --> 01:09:47,560
교차 어텐션 구현을 수행할 때 쿼리는 다음 중 하나에서 발생합니다.

757
01:09:47,560 --> 01:09:54,520
소스, 우리가 디코딩하고 있는 디코딩된 시퀀스, 키와 값은 메모리에서 나옵니다.

758
01:09:54,520 --> 01:10:00,920
인코딩된 소스의 모습. 어느 쪽이든 여기서는 관련 부분을 강조했습니다.

759
01:10:01,480 --> 01:10:05,400
즉, 우리는 이 행렬 곱셈을 얻었고 우리가 원하는 것은 다음과 같습니다.

760
01:10:05,400 --> 01:10:10,040
최신 하드웨어에서 이를 가속화하기 위해 일괄 처리 개념을 구축하여 실제로

761
01:10:10,120 --> 01:10:14,680
self-attention을 활용하여 동시적이고 빠르게 수행합니다. 그러니 우리가 우리의 모든 일을 한다면

762
01:10:14,680 --> 01:10:19,640
선형 투영을 한 번에 수행할 수 있으며 이것이 바로 이 블록에서 다음과 같은 주석과 함께 볼 수 있는 것입니다.

763
01:10:19,640 --> 01:10:24,520
첫 번째부터 시작해서 우리는 이 모든 예측을 진행하고 수행할 것입니다.

764
01:10:24,520 --> 01:10:28,600
그리고 이를 우리의 주의 메커니즘에 적용할 수 있는 지점에 도달하게 될 것입니다.

765
01:10:28,600 --> 01:10:34,840
그 바로 아래에서 매우 빠르고 평행하게 따라옵니다. 그런 다음 우리는 다음과 같은 과정을 거칩니다.

766
01:10:34,840 --> 01:10:39,480
다양한 다중 어텐션 헤드를 모두 연결한 다음 마지막으로

767
01:10:39,480 --> 01:10:45,000
하단에서 학습된 가중치 행렬을 사용하여 선형 투영을 수행하여 다음으로 돌아갑니다.

768
01:10:45,000 --> 01:10:48,680
나머지 부분에 입력할 수 있는 모델의 차원입니다. 그래서 이것은 정말로 우리를 데려가는 것입니다

769
01:10:48,680 --> 01:10:52,360
앞서 설명했던 다중 머리 주의 과정을 통해

770
01:10:52,360 --> 01:10:57,480
구현 세부 사항은 여기에서 확인하세요. 초기화 중인 위치를 왼쪽에서 볼 수 있습니다.

771
01:10:57,480 --> 01:11:04,200
하위 레이어에 대한 다중 헤드 어텐션 블록은 실제 레이어를 복제하고 있음을 의미합니다.

772
01:11:04,200 --> 01:11:09,320
매트릭스 보기 또는 모델의 매개변수를 학습하는 데 사용됩니다. 그렇게 하면 우리는 할 수 있다

773
01:11:09,960 --> 01:11:17,640
기본적으로 각 항목을 코드 블록으로 복사할 필요 없이 모듈 자체만 복사하면 됩니다.

774
01:11:17,640 --> 01:11:21,800
이 모델이 기대하는 8개의 주목 헤드를 구축하려면 다음을 명심하세요.

775
01:11:21,800 --> 01:11:25,800
우리는 모든 값이 서로 독립적이도록 다시 초기화해야 합니다.

776
01:11:25,800 --> 01:11:33,800
훈련을 시작합니다. 좋아요, 첫 번째 하위 레이어를 이해했으니 두 번째 하위 레이어로 넘어가겠습니다.

777
01:11:33,800 --> 01:11:40,040
하나는 위치별 피드포워드 네트워크입니다. 그리고 여기서 무슨 일이 일어나고 있는지, 아니면 직관이

778
01:11:40,040 --> 01:11:44,520
그 이유는 학습된 변환을 입력의 각 위치에 적용한다는 것입니다.

779
01:11:44,520 --> 01:11:49,080
표현하지만 별도로 동일하게 적용됩니다. 그래서 우리가 그 과정을 겪을 때

780
01:11:49,080 --> 01:11:54,200
Multi-head attention, 우리는 본질적으로 가치 차원을 쿼리로 변환할 것입니다.

781
01:11:54,200 --> 01:12:00,200
차원이죠? 따라서 쿼리당 값을 합산합니다. 우리가 포지션 전체로 하고 싶은 것

782
01:12:00,200 --> 01:12:05,720
피드포워드 네트워크는 쿼리별로 해당 컨텍스트에 추가했습니다. 이제 쿼리별로 실행하고 싶으므로 이를 유지하세요.

783
01:12:06,760 --> 01:12:11,560
각 쿼리 레인의 독립성을 유지하지만 이제는 해당 컨텍스트를 활용합니다. 그래서 우리는 그것을 폭파시킬 것입니다.

784
01:12:12,280 --> 01:12:17,080
좀 더 유용한 것으로 매핑한 다음 차원에서 다시 압축하는 방법을 알아보세요. 그래서 우리는

785
01:12:17,080 --> 01:12:22,760
이전 하위 계층에 의해 이미 추가된 컨텍스트를 활용합니다. 그리고 주요 아이디어는

786
01:12:22,760 --> 01:12:27,640
우리는 더 큰 복잡성을 근사화할 수 있도록 네트워크에 더 많은 매개변수를 추가하고 있습니다.

787
01:12:27,640 --> 01:12:31,800
그리고 결국 신경망은 보편적인 근사치입니다.

788
01:12:31,800 --> 01:12:36,760
임의로 더 깊거나 더 넓어집니다. 그리고 주요 아이디어는 우리가 하나의 시퀀스에서 다음 시퀀스로 이동하는 방법을 배우고 싶다는 것입니다.

789
01:12:36,760 --> 01:12:41,080
시퀀스 번역을 시퀀싱하는 작업이 포함된 또 다른 시퀀스입니다. 따라서 매개변수가 많을수록

790
01:12:41,080 --> 01:12:47,080
모델이 잘 훈련되었다고 가정하면 모델이 더 좋아질 것입니다. 그리고 여기의 주요 아이디어는,

791
01:12:47,080 --> 01:12:52,600
또는 이 컨텍스트를 어떻게 활용할 것인지, 해상도를 높일 것인가?

792
01:12:53,160 --> 01:12:58,040
모델이 가지고 있는 것을 시뮬레이션할 수 있도록 각 위치를 순간적으로

793
01:12:58,920 --> 01:13:03,080
또는 이러한 맥락의 중첩을 활용합니다. 그럼 조금 풀어서,

794
01:13:03,800 --> 01:13:09,960
몇 가지 계산을 수행한 다음 모델의 고정된 차원으로 다시 매핑합니다. 그래서 그들은 이렇게 해요

795
01:13:09,960 --> 01:13:17,000
선형 MLP 또는 ReLU 활성화가 있는 완전히 연결된 피드포워드 네트워크를 통해.

796
01:13:17,000 --> 01:13:21,080
그리고 핵심은 숨겨진 공간이 더 높은 차원성을 갖게 될 것이라는 점입니다.

797
01:13:22,040 --> 01:13:25,240
그래서 이것을 조금 설명하기 위해 옆에 그림을 그렸습니다. 당신이 가지고있을 때

798
01:13:25,240 --> 01:13:30,520
컨텍스트가 중첩되면 위치 노드에 다른 요소가 있음을 알 수 있습니다.

799
01:13:30,520 --> 01:13:35,400
맥락에 추가되는 것입니다. 이제 신경망에 대해 더 깊이 들어갈수록 우리는

800
01:13:35,400 --> 01:13:40,360
그 식별 가능한 의미는 여전히 존재하지만 논쟁의 여지가 많이 있습니다.

801
01:13:40,360 --> 01:13:44,440
단일 노드 표현으로 요약됩니다. 그래서 그것을 분리하는 것이 어렵습니다. 그래서 방금 왔어

802
01:13:44,440 --> 01:13:48,040
내가 지정한 색상과 비슷해졌습니다. 아마도 그 중 하나가 무엇에 관한 것인지 생각할 수 있습니다.

803
01:13:48,040 --> 01:13:53,240
실제 단어 의미는 인코딩된 순서로 되어 있었습니다. 그 중 일부는 다른 요소와의 관계입니다.

804
01:13:53,240 --> 01:13:58,520
보라색은 시퀀스의 다른 요소나 그 이웃 요소 중 하나와 관련이 있을 수 있습니다.

805
01:13:58,520 --> 01:14:03,720
하지만 주요 아이디어는 이 피드포워드 또는 위치별 피드포워드 하위 계층이 진행되는 동안

806
01:14:03,720 --> 01:14:07,880
우리는 특징의 다른 부분이 나타낼 수 있도록 차원을 확대할 것입니다.

807
01:14:07,880 --> 01:14:13,320
다른 것들. 그런 다음 이를 재결합하면 다른 방식으로 재결합할 수 있습니다.

808
01:14:13,320 --> 01:14:18,680
그러면 해당 컨텍스트에 더 쉽게 접근할 수 있고 더 유용해집니다. 구현까지

809
01:14:18,680 --> 01:14:23,080
이 모델의 피드포워드 부분은 매우 간단합니다. 우리는 두 개의 선형 레이어를 갖게 될 것입니다

810
01:14:23,080 --> 01:14:27,960
쌓여 있는 것. 우리는 정규화 능력을 높이기 위해 일부 드롭아웃을 추가할 것입니다.

811
01:14:27,960 --> 01:14:31,640
그리고 순방향 패스에서는 선형 투영을 하나만 수행하겠습니다.

812
01:14:31,640 --> 01:14:34,360
ReLU를 수행하고 다음 선형 투영을 수행합니다.

813
01:14:37,720 --> 01:14:42,120
완벽한. 그리고 이를 이해하면 하위 블록이 조립되므로 이러한 하위 레이어가 있습니다.

814
01:14:42,920 --> 01:14:48,280
하지만 이를 연결하는 방법도 중요합니다. 구현이 진행되는 한,

815
01:14:49,640 --> 01:14:53,480
한 슬라이드를 너무 멀리 이동했습니다. 이제 시작합니다. 구현이 진행되는 한,

816
01:14:54,120 --> 01:14:58,840
변압기는 이러한 하위 레이어 사이에 하위 레이어 연결을 통합합니다.

817
01:15:00,040 --> 01:15:04,440
또는 각 하위 레이어를 둘러싸는 것입니다. 그리고 이것이 할 일은 정규화를 추가하는 것입니다.

818
01:15:05,320 --> 01:15:12,600
모델 전반에 걸쳐 열악한 그라데이션 동작과 싸우는 것 같아요. 그래서 이건 그냥 만드는 거야

819
01:15:12,600 --> 01:15:17,560
안정적으로 훈련할 수 있고 데이터에 과적합되지 않도록 하기 위해서입니다. 그들은 이것을 통해 이것을 한다

820
01:15:17,560 --> 01:15:22,360
세 가지 다른 메커니즘. 하나는 잔여 연결입니다. 다음은 이 드롭아웃입니다.

821
01:15:22,360 --> 01:15:28,120
이미 설명했기 때문에 조금 얼버무렸습니다. 마지막으로 레이어 표준을 일부로 사용합니다.

822
01:15:28,120 --> 01:15:32,440
하위 계층 연결의 모습입니다. 그리고 저는 이것이 무엇인지에 대한 공식을 아래에 놓았습니다. 그래서 그들은 레이어를 만듭니다

823
01:15:32,440 --> 01:15:38,680
잔여 연결이 무엇인지 원래 신호에 대한 표준입니다. 본질적으로 당신은 단지

824
01:15:39,560 --> 01:15:44,120
모델을 통과하여 잔여 요소를 표현하는 방법을 배우는 것이 훨씬 저렴하기 때문입니다.

825
01:15:44,120 --> 01:15:51,400
잔차는 전체 신호를 학습하고 학습된 신호에 원래 신호를 추가하는 것입니다.

826
01:15:51,400 --> 01:15:56,600
잔여. 그리고 이는 건너뛰기 연결이라는 아이디어를 통해 원래 신호를 더 많이 보존합니다.

827
01:15:56,600 --> 01:16:01,960
그래서 x에 당신이 배운 것을 추가합니다. 또한 이는 다음과 같은 문제 중 일부를 완화합니다.

828
01:16:01,960 --> 01:16:07,480
그래디언트 소실. 왜냐하면 우리는 많은 양의 숫자를 계산하지 않기 때문입니다. 그래서 그라데이션에는 없습니다

829
01:16:07,480 --> 01:16:13,640
숫자가 너무 많아 제대로 동작하지 않는 문제. 그래서 일반적으로,

830
01:16:13,640 --> 01:16:18,120
그것이 잔여 연결입니다. 상황에 따라 레이어 표준에 포함되어 있음을 알 수 있습니다.

831
01:16:18,120 --> 01:16:22,200
레이어 노름이 할 일은 값을 축소하여 각 값이 잘 작동하도록 하는 것입니다.

832
01:16:22,200 --> 01:16:27,080
층. 따라서 해당 단일 레이어에 있는 모든 숫자를 기반으로 크기를 조정해 보겠습니다.

833
01:16:27,080 --> 01:16:31,880
그들은 잘 행동하고 우리는 사라지거나 폭발하는 그라디언트가 없습니다. 한

834
01:16:31,880 --> 01:16:35,640
구현이 진행되면 레이어 표준을 옆으로 치워두었습니다. 왜냐하면 제 생각에는 그것이 좀 흥미롭기 때문입니다.

835
01:16:35,640 --> 01:16:41,240
이는 예를 들어 a와 같은 것이 아닙니다. 배치 표준을 수행하는 경우

836
01:16:41,240 --> 01:16:46,600
전체 배치, 배치당이 아니라 상황에 따라 레이어당입니다. 그래서 우리는 본질적으로 계산합니다.

837
01:16:46,600 --> 01:16:52,120
평균과 표준편차를 구한 다음 해당 레이어의 활성화가 어떻게 보이는지 정규화합니다.

838
01:16:52,120 --> 01:16:57,160
그것을 바탕으로. 그런 다음 실제 하위 레이어 연결에 대한 사진을 올려 놓았습니다.

839
01:16:57,160 --> 01:17:01,080
권리이며 이것이 더 중요한 요소입니다. 이것이 우리가 이 모든 것을 구성하거나 추가하는 방법입니다.

840
01:17:01,080 --> 01:17:06,200
이러한 하위 계층 간의 연결에 대한 다양한 유형의 정규화.

841
01:17:07,800 --> 01:17:12,520
보시다시피 순방향 패스에서 우리는 하위 레이어의 드롭아웃을 패스할 것입니다.

842
01:17:13,640 --> 01:17:19,720
신호의 레이어 정규 버전에 있고 하위 레이어 자체는 다중 헤드입니다.

843
01:17:19,720 --> 01:17:24,760
주의 또는 포인트 위치별 피드포워드 네트워크. 이제 이것이

844
01:17:24,760 --> 01:17:30,120
제가 앞서 말한 실제 방정식과는 별개의 순서입니다. 방정식이 도출됩니다.

845
01:17:30,120 --> 01:17:34,440
이는 주석이 달린 변환기가 소스에서 벗어나는 영역 중 하나입니다.

846
01:17:34,440 --> 01:17:40,040
문자를 조금만 보내면 상대적으로 동일하지만 조금 더 저렴하다는 것을 알게 됩니다.

847
01:17:40,040 --> 01:17:45,720
단계를 재정렬하기 위해 이것을 적용하거나 배우는 과정을 거칠 때

848
01:17:45,720 --> 01:17:50,440
이러한 정규화를 이러한 방식으로 적용합니다. 구현 시간이 되면

849
01:17:50,440 --> 01:17:55,960
이것으로 조금 놀 수 있습니다. 나는 논문을 구현하는 것을 선호합니다

850
01:17:55,960 --> 01:18:01,480
우리는 단지 잔차 성분 대신에 잔차와 함께 신호에 대한 레이어 표준을 갖기 때문에 제안합니다.

851
01:18:04,920 --> 01:18:09,880
이제 인코더와 디코더를 구축하는 데 필요한 모든 부분이 준비되었습니다. 얘기하자

852
01:18:09,880 --> 01:18:15,640
먼저 인코더에 대해. 인코더 자체 또는 인코더 레이어는 구성 가능한 블록입니다.

853
01:18:15,640 --> 01:18:20,280
이 self-attention을 사용하여 입력 시퀀스 표현을 인코딩하는 작업에 대해

854
01:18:20,280 --> 01:18:24,360
우리는 이야기했습니다. 임의의 입력이 무엇이든 받아들일 것입니다. 어쩌면 그것은

855
01:18:24,360 --> 01:18:29,720
원래 신호. 어쩌면 우리가 이 네트워크에 점점 더 깊이 들어가면서 숨겨진 상태 중 하나일 수도 있습니다.

856
01:18:29,720 --> 01:18:33,880
피드포워드 네트워크에서 해당 맥락에서 레이어에 대한 셀프 어텐션을 수행합니다.

857
01:18:33,880 --> 01:18:40,040
해당 컨텍스트를 활용합니다. 이 반복 가능한 블록으로 구성하려는 이유에 대한 주요 아이디어

858
01:18:40,040 --> 01:18:44,920
고정된 입력 및 출력 크기를 사용하면 모델을 구성하는 것이 매우 쉽습니다.

859
01:18:44,920 --> 01:18:50,040
우리는 기본적으로 이것을 쌓아서 얼마나 쌓고 싶은지에 따라 임의의 깊이를 얻을 수 있습니다.

860
01:18:50,040 --> 01:18:56,360
그리고 이를 통해 인코더는 반복적인 다중 헤드 주의를 가질 수 있으며, 이는 우리에게 더 많은 매개변수를 제공합니다.

861
01:18:56,360 --> 01:19:02,040
그리고 우리가 경험한 본질적으로 더 복잡한 위치 상호 작용을 모델링하는 능력이 있습니다.

862
01:19:02,040 --> 01:19:07,560
훨씬 더 깊어요. 우리는 관계를 추출한 다음 '그래, 이것이 기여하고 있다'고 말할 시간이 더 많았습니다.

863
01:19:07,560 --> 01:19:12,040
이만큼. 다음에 레이어를 더 추상화할 때, 그것이 기여했다고 말할 수 있습니다.

864
01:19:12,040 --> 01:19:16,840
그만큼, 또 다른 요소가 이만큼 기여하고 있었습니다. 함께 그 만큼 기여하면

865
01:19:16,840 --> 01:19:22,040
이것이 일어나는 일입니다. 다음 단계를 수행하면 모델에 더 복잡한 조건을 적용할 수 있습니다.

866
01:19:22,040 --> 01:19:27,560
깊은. 이를 달성하는 방법은 다음을 사용하여 인코더 블록을 구현하는 것입니다.

867
01:19:27,560 --> 01:19:31,720
하위 계층 연결. 우리는 다중 헤드 셀프 어텐션을 갖고 있고 다음으로 피드포워드를 사용합니다.

868
01:19:31,720 --> 01:19:38,840
그들 각각 사이의 하위 계층 연결. 구현에 관한 한, 그것은 정확히 무엇입니까?

869
01:19:38,840 --> 01:19:46,520
설명했습니다. 핵심은 하위 레이어의 복제본을 생성하여 여러 레이어를 갖게 된다는 것입니다.

870
01:19:46,520 --> 01:19:51,160
그런 다음 필요한 만큼 쌓아 올리겠습니다. 이러한 상황에서,

871
01:19:53,400 --> 01:19:59,560
먼저 이 하위 계층 연결을 사용하여 피드포워드 또는 다중 헤드 셀프 어텐션으로 이동합니다.

872
01:19:59,560 --> 01:20:03,880
그런 다음 해당 하위 계층 연결의 출력에서 ​​피드포워드로 흘러 들어가게 됩니다.

873
01:20:03,880 --> 01:20:08,520
여기서는 잔차를 모델링하고 나머지 하위 계층 연결은

874
01:20:08,520 --> 01:20:12,520
해당 신호의 나머지 부분을 처리한 다음 이를 임의의 깊이로 쌓을 수 있습니다.

875
01:20:12,520 --> 01:20:18,120
반면에 디코더는 인코더와 매우 유사하지만 다음이 필요합니다.

876
01:20:18,120 --> 01:20:23,160
여러 머리 주의를 위한 마스킹 요소이므로 이 인과적 마스킹은

877
01:20:23,160 --> 01:20:28,680
미래를 내다보려면 정확한 작업을 수행할 추가 하위 계층이 필요합니다.

878
01:20:28,680 --> 01:20:33,560
Cross-Attention을 수행한다는 점을 제외하면 동일한 작업입니다. 우리가 신호를 받는 곳은 어디로 가는지

879
01:20:33,560 --> 01:20:40,040
지금까지 디코딩된 시퀀스가 ​​아닌 키와 값에 대한 인코더에서 나옵니다.

880
01:20:41,000 --> 01:20:45,000
마스킹을 하려는 동기에 대해서는 조금 설명드린 것 같습니다.

881
01:20:45,000 --> 01:20:48,600
조금 전에도 그랬지만 주요 아이디어는 부정행위를 방지하는 것입니다. 모델은

882
01:20:49,240 --> 01:20:53,480
내 생각에는 다음 토큰을 예측하는 방법을 배우려고 할 때 출력을 미래 지향적으로 보는 것 같습니다.

883
01:20:53,480 --> 01:20:57,720
따라서 훈련 작업을 진행하고 미래 데이터를 볼 수 있다면 모델이 적합할 수 있습니다.

884
01:20:57,720 --> 01:21:03,000
다음 토큰을 보려고요. 사실, 그것이 무엇을 예측하기 위해 할 수 있는 최선의 방법입니까?

885
01:21:03,000 --> 01:21:07,320
토큰은 있어야 하며 이전 토큰 간의 인과 관계를 학습하지 않습니다.

886
01:21:07,400 --> 01:21:12,440
다음에는 무엇이 흘러야 할까요? 따라서 우리는 해당 정보로부터 보호하여 순전히 주의를 기울일 수 있습니다.

887
01:21:12,440 --> 01:21:19,640
과거 맥락. 구현에 관한 한, 우리는 다음을 사용하여 매우 유사한 구조를 갖게 될 것입니다.

888
01:21:19,640 --> 01:21:24,440
아래에서 강조한 내용을 제외하고 이전에 이야기한 하위 레이어 블록

889
01:21:24,440 --> 01:21:31,560
마스크된 self-attention을 사용하면 x, x, x에서 이동하므로 하나의 시퀀스에서

890
01:21:32,440 --> 01:21:35,960
미래를 보지 못하게 하는 가면을 쓰고 자기 자신에게 주의를 기울이는 것입니다.

891
01:21:37,800 --> 01:21:41,240
그리고 우리가 Cross-Attention을 수행하는 다음 레이어에는

892
01:21:41,240 --> 01:21:47,720
쿼리는 하나의 소스에서 나오지만 키와 값은 이 메모리에서 나옵니다.

893
01:21:47,720 --> 01:21:55,080
멀티는 인코더에 대한 관심에서 나오므로 인코딩된 컨텍스트는 사라집니다.

894
01:21:55,080 --> 01:22:01,080
통해 개발되었습니다. 마지막으로 피드포워드 블록을 통과하여 이를 활용하게 됩니다.

895
01:22:01,080 --> 01:22:05,080
해당 컨텍스트를 다시 한번 살펴보고 이 구성 가능한 구조를 갖고 있으므로 이를 하나씩 쌓을 수 있습니다.

896
01:22:05,080 --> 01:22:11,240
우리가 여기에 얽혀 있는 모든 맥락을 활용하고 활용하기 위해 서로 협력합니다.

897
01:22:11,240 --> 01:22:17,720
다의미적 표현. 따라서 인코더와 디코더가 무엇을 차단하는지 설정한 후에는

898
01:22:17,720 --> 01:22:24,360
실제 인코더-디코더 구조 자체를 조립하기 전에 블록은 다음과 같습니다.

899
01:22:24,360 --> 01:22:28,920
예측 헤드를 정의해야 합니다. 아까도 말씀드렸지만 이게 마지막이에요

900
01:22:28,920 --> 01:22:32,440
출력 확률을 알아보는 방법은 매우 간단합니다. 우리는

901
01:22:32,440 --> 01:22:37,560
선형 레이어를 로지트로 변환한 다음 소프트맥스를 통해 생성합니다.

902
01:22:37,560 --> 01:22:42,440
다음 토큰이 무엇이어야 하는지에 대한 확률을 알려주고 이는 우리를 다시 다음 토큰으로 매핑할 것입니다.

903
01:22:43,400 --> 01:22:51,480
실제 토큰의 출처인 어휘 카포라입니다. 따라서 이미지 공간에서 이는 다음과 같습니다.

904
01:22:51,480 --> 01:22:56,120
이미지 패치. 우리가 텍스트 공간에 대해 이야기하고 있는 곳에서는 이것이 토큰이 될 것입니다.

905
01:22:56,840 --> 01:23:02,600
순서가 유지된다는 것입니다. 구현은 매우 간단합니다. 원한다면 일시중지할 수 있습니다.

906
01:23:02,600 --> 01:23:09,560
지금 프레젠테이션을 보고 저것 좀 보세요. 난 그냥 실제 의회에 계속 갈거야

907
01:23:09,560 --> 01:23:13,880
인코더-디코더 구조에 대해 이야기하는 것이 중요하다고 생각합니다.

908
01:23:13,880 --> 01:23:17,560
무슨 일인지 오해를 풀겠습니다. 그래서 여기 사진을 아주 빨리 보여 드리겠습니다.

909
01:23:18,760 --> 01:23:22,600
우리는 본질적으로 인코더 블록 전체를 서로 쌓고

910
01:23:22,600 --> 01:23:27,960
디코더 블록 전체가 서로 겹쳐져 있습니다. 그런 다음 해당 생성기로 흘러들어가

911
01:23:27,960 --> 01:23:34,120
다음 토큰이 무엇이어야 하는지에 대한 가장 가능성 있는 확률, 그리고 우리는 라인을 알고 있습니다.

912
01:23:34,120 --> 01:23:38,440
그것은 실제로 해당 토큰을 생성할 디코딩 프로세스로 흘러 들어갈 것입니다.

913
01:23:40,600 --> 01:23:45,160
구현 측면에서는 매우 간단합니다. 우리는 그 클론을 사용할 것입니다. 우리는

914
01:23:45,160 --> 01:23:50,200
인코더 및 디코더 블록의 복사본을 생성하기 위해 만든 복제 도우미 기능을 활용합니다.

915
01:23:50,200 --> 01:23:53,800
우리는 그것들을 서로 쌓아올린 다음 예측할 때가 되면

916
01:23:53,800 --> 01:23:58,680
정방향 패스에서 볼 수 있듯이, 우리는 단지 블록 수를 통과합니다. 그들은 레이어를 호출합니다

917
01:23:58,680 --> 01:24:03,800
여기서는 구현을 하겠지만 첫 번째 인코더 블록에 신호를 보낼 것입니다.

918
01:24:03,800 --> 01:24:09,880
해당 출력을 다음 디코더 블록에 넣는 식으로 진행됩니다. 아니면 인코더가 인코더, 디코더로 들어갑니다.

919
01:24:09,880 --> 01:24:13,720
인코더에서도 메모리를 학습하여 디코더로 유입됩니다.

920
01:24:15,800 --> 01:24:19,720
이것이 우리에게 변압기에 대한 오해를 불러일으켰고, 제 생각에는 이것이 실제로

921
01:24:19,800 --> 01:24:23,160
n이라는 시간으로 계속해서 보여주고 있는 그 모습에서 비롯된 것인데,

922
01:24:23,800 --> 01:24:27,000
레이어를 쌓는다는 것이 무엇을 의미하는지에 대한 잘못된 이해입니다.

923
01:24:28,600 --> 01:24:34,440
따라서 전체 변압기 블록이 서로 쌓여 있다는 개념은 올바르지 않습니다.

924
01:24:34,440 --> 01:24:40,360
주된 이유는 크로스 어텐션을 위해 인코더가 디코더로 흘러 들어가는 경우,

925
01:24:41,960 --> 01:24:46,440
누군가는 그것들을 서로 쌓아두기만 한다고 생각할 수도 있습니다. 그러면 악용할 수 없습니다.

926
01:24:46,440 --> 01:24:53,960
디코더의 초기 레이어에서는 죄송합니다. 전체에 걸쳐 축적된 풍부한 컨텍스트

927
01:24:53,960 --> 01:24:58,360
인코더. 따라서 인코더가 디코더로 흘러 들어간 다음

928
01:24:58,360 --> 01:25:03,960
디코더가 다음 블록으로 흐르거나 그와 비슷한 것, 스태킹 트랜스포머에 관해 이야기할 때,

929
01:25:03,960 --> 01:25:09,240
우리는 인코더 자체를 쌓아서 더욱 풍부한 표현을 구축하는 것에 대해 이야기하고 있습니다.

930
01:25:09,240 --> 01:25:14,600
신호에 포함된 관계형 컨텍스트를 사용하거나

931
01:25:14,600 --> 01:25:21,320
동일한 작업을 수행하는 디코더, 어딘가에서 정보를 가져오기 위해 교차 주의를 수행할 수도 있음

932
01:25:21,320 --> 01:25:25,400
하지만 주요 아이디어는 우리가 자기 관심을 갖고 포괄적인 시스템을 구축하고 싶다는 것입니다.

933
01:25:25,400 --> 01:25:29,160
신호를 통해 표현하지만, 이 경우 토큰을 생성할 목적이 있습니다.

934
01:25:29,720 --> 01:25:33,560
따라서 변압기 적층에 관해 이야기할 때는 모든 인코더를 적층하는 것입니다.

935
01:25:33,560 --> 01:25:39,880
서로 다른 내용에 대한 여러 비트의 컨텍스트를 갖는 단일 표현을 생성합니다.

936
01:25:39,880 --> 01:25:44,120
문제를 볼 수 있는 방법이 있으므로 디코더는 쿼리할 수 있는 풍부한 표현을 갖게 됩니다.

937
01:25:45,320 --> 01:25:49,160
그런 다음 각 디코더를 흐름으로 서로 쌓은 후,

938
01:25:49,160 --> 01:25:53,800
하지만 그들은 모두 최종 출력에서 ​​교차주의를 기울이는 동안 정보를 얻습니다.

939
01:25:53,800 --> 01:26:01,320
최종 인코더 블록. 이것이 바로 우리가 인코더-디코더를 조립할 때 하는 일입니다. 우리는 짓는다

940
01:26:01,320 --> 01:26:05,560
인코더는 인코더 블록을 쌓아서 만들고, 디코더는 디코더 블록을 쌓아서 만들고,

941
01:26:05,560 --> 01:26:09,800
우리는 생성기를 맨 위에 놓고 이를 모두 모아 이 인코더-디코더를 얻습니다.

942
01:26:09,800 --> 01:26:14,840
입력은 인코딩 프로세스 내부에서 볼 수 있는 인코더를 통해 유입됩니다.

943
01:26:14,840 --> 01:26:17,800
순방향 프로세스에서는 소스에서 인코딩을 호출하는 것을 볼 수 있습니다.

944
01:26:18,600 --> 01:26:25,400
그런 다음 해당 출력은 이전 출력과 함께 메모리로서 디코더 프로세스로 흘러 들어갑니다.

945
01:26:25,400 --> 01:26:33,480
디코딩된 시퀀스. 이 모든 것을 종합하면 매우 마비되기 쉬운 아키텍처를 얻을 수 있습니다.

946
01:26:33,480 --> 01:26:37,080
그런 다음 한 반복에서 다른 반복으로 이동하려면 디코딩하면 됩니다.

947
01:26:37,640 --> 01:26:42,600
다음 토큰을 예측한 다음 이를 이전 토큰에 추가하여 자동 회귀적으로

948
01:26:42,600 --> 01:26:49,080
목록을 디코딩하고 해당 프로세스를 계속 진행합니다. 실제로 최종 구현은 다음과 같습니다.

949
01:26:49,080 --> 01:26:54,200
이 모델을 만드는 중입니다. 가장 중요한 부분은 여기 맨 아래에 있습니다.

950
01:26:54,200 --> 01:27:00,920
모델의 모든 매개변수를 독립적으로 초기화하는 과정을 통해

951
01:27:00,920 --> 01:27:05,400
이 복제 도우미 기능을 광범위하게 사용했습니다. 실수로 발생하지 않도록 하고 싶습니다.

952
01:27:05,400 --> 01:27:10,840
너무 많은 작업을 학습하려고 하거나 하나의 양식으로 수렴하려는 공유 매개변수가 있습니다.

953
01:27:10,840 --> 01:27:17,160
예측. 이를 통해 우리는 지상에서 변압기를 구현하는 과정을 거쳤습니다.

954
01:27:17,160 --> 01:27:21,880
마치 레고 하우스처럼 건축물을 조립하는 것과 같습니다. 지금부터 간략하게 이야기하겠습니다.

955
01:27:21,880 --> 01:27:28,360
훈련 변압기. 내 생각에 모델의 개념은 우리가 단지 다음과 같이 이야기한다면 불완전하다고 생각합니다.

956
01:27:28,360 --> 01:27:35,000
자연과 양육이 모두 우리가 표현하고 표현하는 방식에 중요한 역할을 하는 사람들과 마찬가지로 건축도 마찬가지입니다.

957
01:27:35,080 --> 01:27:39,960
우리 세상에서 진화하세요. 모델은 두 아키텍처의 산물입니다.

958
01:27:39,960 --> 01:27:45,400
가지고 있는지, 어떻게 훈련받았는지. 훈련은 모델의 매개변수가 어떻게 결정되는지에 영향을 미칩니다.

959
01:27:45,400 --> 01:27:51,320
표현한 것이나 배운 것. 이것은 쓰레기가 들어오고 쓰레기가 나온다는 개념에 이르게 됩니다. 당신이 가면

960
01:27:51,320 --> 01:27:55,560
끔찍한 훈련 과정을 거치면 아키텍처가 아무리 훌륭하더라도 모델은 발전할 것입니다.

961
01:27:55,560 --> 01:27:59,960
결국 모델이나 아키텍처가 교육에 적합하기 때문에 성능이 저하됩니다.

962
01:27:59,960 --> 01:28:06,440
이 훈련 과정을 통해 데이터를 수집합니다. 우리가 알고 있는 트랜스포머 아키텍처에 다음과 같은 기능이 있다면

963
01:28:06,440 --> 01:28:11,800
표시된 경우 시퀀스의 모든 요소에 대한 self-attention을 통해 양방향으로 참석합니다.

964
01:28:11,800 --> 01:28:17,400
양방향 주의를 장려하는 예를 통해 이를 학습하게 됩니다. BERT와 같은 후속 논문

965
01:28:17,400 --> 01:28:23,800
대규모 사전 훈련을 사용하여 양방향 표현을 학습하고 장려합니다.

966
01:28:23,800 --> 01:28:29,880
변압기 매개변수. 그러나 대안적인 측면에서 단지 필요한 예가 표시된다면

967
01:28:29,880 --> 01:28:34,840
과거만을 바라보는 인과적 관심이

968
01:28:34,840 --> 01:28:38,840
보다 단방향적인 동작을 표현할 수 있으며 일반화되지도 않습니다.

969
01:28:40,280 --> 01:28:45,320
따라서 이것은 변환기 아키텍처가 있을 때 우리가 원하는 사실을 간단히 요약한 것입니다.

970
01:28:45,320 --> 01:28:50,360
우리가 좋은 훈련 절차를 거치도록 하기 위해서입니다. 하지만 Vaswani와 그의 동료들은

971
01:28:50,360 --> 01:28:57,320
종이는 대중의 관심을 이용해 훈련한다는 것입니다. 그래서 그들은 다음과 같은 주의 메커니즘을 구축합니다.

972
01:28:57,320 --> 01:29:02,680
사전 토큰 세트를 제공하고 예측하려는 미래 예측 편견을 방지합니다.

973
01:29:02,680 --> 01:29:09,400
이전 입력만을 기반으로 하는 출력 대상 토큰입니다. 그리고 그들이 이것을 대규모로 수행하는 방법은 다음과 같습니다.

974
01:29:09,400 --> 01:29:15,800
그들은 다음과 같이 말함으로써 모든 훈련 작업을 일괄 처리합니다. 이것은 이전 동영상에서 볼 수 있는 것입니다.

975
01:29:15,800 --> 01:29:20,040
창이며 이것이 예측하려는 단일 토큰입니다. 전체 시퀀스의 경우

976
01:29:20,040 --> 01:29:23,960
반복을 통해 첫 번째 항목을 생성한 다음 두 번째 항목을 생성할 필요가 없습니다.

977
01:29:23,960 --> 01:29:29,560
토큰, 세 번째 토큰 등입니다. 과거에 대한 유한한 창을 제공할 수 있습니다.

978
01:29:29,560 --> 01:29:33,720
다음 토큰이 무엇인지 예측하면 기본적으로 모든 토큰에 대해 슬라이딩 창을 구축할 수 있습니다.

979
01:29:33,720 --> 01:29:40,680
모든 창과 모든 대상에 대해 동시에 데이터를 수집하고 훈련합니다. 그래서 우리는 이것에 대해 이야기했습니다.

980
01:29:40,680 --> 01:29:45,000
조금, 실제로 구현되는 방식은 후속 마스크 또는 이 인과 마스크를 만드는 것입니다.

981
01:29:45,960 --> 01:29:53,240
이는 창에 대해 말하기만 하면 됩니다. 따라서 토큰 4에 있다고 가정하면 토큰을 볼 수 있습니다.

982
01:29:53,240 --> 01:29:58,680
3, 2, 1, 0이지만 미래는 없습니다. 이것이 바로 옆에 있는 그림이 보여주는 것입니다.

983
01:29:59,560 --> 01:30:04,520
우리는 해당 마스크를 생성할 수 있으며, 주의를 끌기 위해 마스크의 매개변수를 구웠기 때문입니다.

984
01:30:04,520 --> 01:30:10,200
함수를 구현하고 사용하는 것은 실제로 매우 쉽습니다. 따라서 다음 마스크를 전달합니다.

985
01:30:10,280 --> 01:30:15,000
우리가 훈련할 때 디코더 블록에 있는 대량의 self-attention에 대해

986
01:30:15,000 --> 01:30:20,840
그리고 우리는 그것이 과거만 볼 수 있도록 합니다. 이제 우리가 이것을 제출할 때 저자들은 어떻게

987
01:30:21,560 --> 01:30:27,640
이 마스킹을 적용하면 소프트맥스 이전에 수행하지만 소프트맥스가 진행될 것이기 때문입니다.

988
01:30:27,640 --> 01:30:34,280
이것을 확률로 변환하기 위해 현재로 정규화하면 숫자를 음수로 설정해야 합니다.

989
01:30:34,280 --> 01:30:39,400
우리가 할 수 있는 대로. 그래서 저자들은 음수 1의 음수 9승을 합니다. 이는 매우 음수입니다.

990
01:30:39,400 --> 01:30:49,080
숫자. 따라서 이러한 키와 쿼리, 키와 쿼리에 대한 소프트맥스를 통과하면

991
01:30:49,080 --> 01:30:53,320
쿼리에서 정보를 가져서는 안 되는 키를 언급할 때

992
01:30:53,320 --> 01:30:58,200
그 값은 너무 음수가 되어 0이 되거나 기다리지 않게 됩니다.

993
01:30:58,760 --> 01:31:04,360
이 자동 회귀 생성 동안 반복의 해당 단계가 어떻게 보이는지 살펴보겠습니다.

994
01:31:04,360 --> 01:31:09,320
이것이 바로 여기의 주요 아이디어입니다. 매우 부정적으로 설정하십시오. 그런 다음 이를 우리에게 전달할 수 있습니다.

995
01:31:09,320 --> 01:31:13,880
소프트맥스는 이를 0 또는 임팩킹되지 않는 용어로 변환합니다.

996
01:31:13,880 --> 01:31:18,280
체중이 있어서 주의를 기울일 수 없기 때문에 주의를 기울이지 않고 있습니다. 저것들

997
01:31:18,280 --> 01:31:24,280
토큰은 아직 확인되지 않았습니다. 그렇게 말하면 나는 Aveline에게 다시 넘겨 줄 것입니다. 그녀는 간다

998
01:31:24,280 --> 01:31:29,080
약간의 결과와 그 영향에 대한 대화로 프레젠테이션을 마무리합니다.

999
01:31:30,120 --> 01:31:32,440
다른 도메인에 대한 변환기의 초기 채택.

1000
01:31:34,440 --> 01:31:42,280
네, James가 말했듯이, 저는

1001
01:31:42,280 --> 01:31:49,800
필요한 모든 것에 주의를 기울이십시오(Attention to All You Need) 보고서가 보고되었습니다. 그들은 이전 주의 논문과 유사하게,

1002
01:31:49,800 --> 01:31:54,280
그들의 실험은 텍스트 번역을 기반으로 했기 때문에 영어와 독일어,

1003
01:31:54,280 --> 01:31:59,960
영어와 프랑스어. 실험을 위해 그들은 두 가지 다른 측정항목을 보고했습니다. 하나는

1004
01:31:59,960 --> 01:32:05,160
성능 지표이고 또 다른 하나는 모델 훈련에 필요한 훈련 비용이었습니다.

1005
01:32:06,120 --> 01:32:15,000
여기서 볼 수 있는 것은 트랜스포머 모델에 대한 것이며, 큰 모델은 파란색 점수에서 꽤 좋은 성능을 발휘합니다.

1006
01:32:16,760 --> 01:32:24,040
이는 당시의 다른 최신 모델 대부분을 능가합니다. 훈련비용의 경우,

1007
01:32:24,040 --> 01:32:33,480
여기서 우리가 알 수 있는 것은 큰 모델과 기본 모델 모두 훈련 비용이 더 낮다는 것입니다.

1008
01:32:33,480 --> 01:32:39,880
다른 최신 모델. 큰 모델의 훈련 비용은 약 10~19 정도였고,

1009
01:32:39,880 --> 01:32:46,440
반면 다른 것들은 모두 10의 20만큼 작았습니다.

1010
01:32:47,960 --> 01:32:53,000
성과는 향상되지만 훈련량은 감소하는 혁명적인 현상입니다.

1011
01:32:53,000 --> 01:33:02,920
비용. 이제 이 논문 자체가 큰 영향력을 미쳤습니다. 이번 주 인용 횟수

1012
01:33:02,920 --> 01:33:09,560
3월 말에는 113,000회의 인용이 있었고 시간이 지나면서 그 수치도 바뀔 것이라고 확신합니다.

1013
01:33:10,840 --> 01:33:17,000
그러나 트랜스포머 아키텍처는 많은 최첨단 모델의 기초로 사용되었습니다.

1014
01:33:17,000 --> 01:33:25,160
이는 GPT-4와 같이 오늘날 우리가 알고 사용하는 모든 LLM의 기본 백본 또는 빌딩 블록입니다.

1015
01:33:25,160 --> 01:33:32,200
라마-2, 쌍둥이자리. 그런 의미에서 AI 도구와 기술은 상당히 혁명적이었습니다.

1016
01:33:32,760 --> 01:33:40,360
이 논문이 2017년에 출판된 이후 실제로 나타났습니다.

1017
01:33:41,000 --> 01:33:50,840
따라서 다른 영역, 특히 컴퓨터 비전의 조기 채택에 대해서는 다음과 같은 이미지를 볼 수 있습니다.

1018
01:33:50,840 --> 01:33:58,360
2018년에 Transformer가 등장했습니다. 이제 그들은 Transformer 아키텍처를 직접 사용하지 않았습니다.

1019
01:33:58,360 --> 01:34:04,760
하지만 그들은 텍스트의 이러한 아키텍처에서 영감을 얻었고 이를 이미지에 적용하고 싶었습니다. 그래서,

1020
01:34:04,760 --> 01:34:11,720
그들이 이 아이디어를 사용한 방식은 이미지 픽셀을 위치적으로 인코딩한 다음 일종의

1021
01:34:11,720 --> 01:34:20,040
쿼리 블록의 여러 픽셀과 주변의 다른 픽셀에 대한 로컬 자체 주의 메커니즘

1022
01:34:20,040 --> 01:34:28,200
해당 쿼리 블록이 메모리 블록을 구성합니다. 따라서 지역 인근의 픽셀을 고려합니다.

1023
01:34:28,200 --> 01:34:35,320
쿼리 위치 주변. 요즘에는 이 방법이나 프로세스가 많이 사용되지 않습니다.

1024
01:34:35,320 --> 01:34:41,800
사람들이 발견한 것은 이미지에 대한 전 세계적인 관심이 더 효과적이며 전 세계적이라는 것입니다.

1025
01:34:41,800 --> 01:34:49,000
이를 통해 우리가 원하는 결과와 성과를 얻으려면 주의가 필요합니다.

1026
01:34:49,000 --> 01:34:54,760
이러한 세계적인 관심을 활용하는 도구 및 후속 모델의 유형은 훨씬 더 많은 것으로 밝혀졌습니다.

1027
01:34:55,320 --> 01:35:04,600
강한. 이에 대한 예는 인코딩을 사용하는 비전 변환기입니다.

1028
01:35:04,600 --> 01:35:11,320
트랜스포머 아키텍처를 직접적으로 구현합니다. 특히, 이미지 분류를 위해 인코더를 사용합니다.

1029
01:35:11,320 --> 01:35:18,680
16 x 16 단어 상당의 이미지를 호출한 2020년 원본 논문입니다.

1030
01:35:18,680 --> 01:35:23,720
대규모 이미지 인식을 위한 변환기. 자, 이것은 전체 연구 분야입니다.

1031
01:35:23,720 --> 01:35:27,160
하지만 이것이 제가 여기서 간단히 언급하려는 원본 논문이었습니다.

1032
01:35:28,520 --> 01:35:36,200
제가 말했듯이 그들은 바닐라 트랜스포머 인코더만 사용하고 그들이 하는 일은 이미지를 찍는 것입니다.

1033
01:35:36,760 --> 01:35:43,640
그들은 이것을 일종의 시퀀스를 생성하기 위해 다른 패치로 분할한 다음 삽입합니다.

1034
01:35:44,520 --> 01:35:52,280
위치 인코딩을 사용하면 이러한 패치를 임베디드 및 인코딩된 시퀀스로 변환할 수 있습니다.

1035
01:35:52,360 --> 01:36:00,200
그런 다음 이를 변압기 인코더에 전달하고 분류를 위해 MLP 헤드를 추가합니다.

1036
01:36:00,200 --> 01:36:08,840
그런 식으로 이미지 분류기를 구축하세요. 이는 매우 강력한 것으로 확인되었으며,

1037
01:36:08,840 --> 01:36:16,280
제가 말했듯이 최근 몇 년 동안 이를 기반으로 많은 것들이 만들어졌습니다.

1038
01:36:22,760 --> 01:36:27,640
좋습니다, 이것으로 프레젠테이션을 마치겠습니다. 나는 당신이 즐겼기를 바랍니다

1039
01:36:29,880 --> 01:36:34,600
여기에 제시된 self-attention 및 변환기에 대한 모든 정보입니다.

1040
01:36:35,160 --> 01:36:39,720
원하시면 언제든지 의견이나 질문을 남겨주세요.

1041
01:36:39,720 --> 01:36:43,800
네, 그리고 유튜브 댓글도요. 몇 가지 논의만으로 추가 표시도 있습니다.

1042
01:36:43,800 --> 01:36:49,240
제가 여기에 올리려는 질문이 있습니다. 우리는 이 문제에 대해 길게 논의하지 않을 것입니다.

1043
01:36:49,240 --> 01:36:53,960
하지만 이 시점에서는 영상을 잠시 멈추고 생각해 보세요.

1044
01:36:53,960 --> 01:36:58,920
당신의 이해 또는 우리를 통해 변압기에 대해 더 많이 알게 된 방법에 대해

1045
01:36:58,920 --> 01:37:04,200
프레젠테이션. 추가적으로 더 궁금하신 점은 댓글로 남겨주세요

1046
01:37:04,200 --> 01:37:09,480
시간이 있으면 위에서부터 토론을 시작하고 싶습니다.