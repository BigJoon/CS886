1
00:00:00,000 --> 00:00:08,000
안녕하세요 여러분, 제 이름은 Xu Yixue입니다. 대규모 다중 모드 모델에 대한 논의를 계속하겠습니다.

2
00:00:08,000 --> 00:00:15,000
LMM 제품군의 정식 모델을 소개하겠습니다.

3
00:00:15,000 --> 00:00:24,000
그래서 우리가 살펴볼 첫 번째 모델은 Polly 제품군의 또 다른 제품군인 Polly 3입니다.

4
00:00:24,000 --> 00:00:31,000
따라서 Polly 3의 동기는 매우 간단합니다. 더 작은 규모의 모델에 중점을 둡니다.

5
00:00:31,000 --> 00:00:39,000
Polly에서 언급했듯이 비전 언어 모델을 수백억, 심지어 수천억 개의 매개변수로 확장합니다.

6
00:00:39,000 --> 00:00:46,000
800억으로 플라밍고를 논한 것처럼 계속해서 증가하는 성과를 보여줬습니다.

7
00:00:46,000 --> 00:00:55,000
그리고 LMM의 언어 부분을 확장하는 데 중점을 둔 Polly도 있습니다.

8
00:00:55,000 --> 00:01:00,000
그러나 Polly 3에서는 더 작은 규모의 모델이 여전히 중요하다고 지적합니다.

9
00:01:00,000 --> 00:01:05,000
따라서 Polly 3에는 50억 개의 매개변수만 제공됩니다.

10
00:01:05,000 --> 00:01:13,000
그리고 Polly 3에는 여전히 경쟁력 있는 성능을 달성하는 데 도움이 되는 세 가지 구성 요소가 있습니다.

11
00:01:13,000 --> 00:01:20,000
첫 번째는 WebScale 이미지 텍스트 데이터에 대한 이미지 인코더의 대조 사전 학습입니다.

12
00:01:20,000 --> 00:01:24,000
그런 다음 Polly에서 상속받은 데이터 세트 혼합을 개선합니다.

13
00:01:24,000 --> 00:01:29,000
또한 Polly 3를 더 높은 해상도로 훈련시킵니다.

14
00:01:29,000 --> 00:01:33,000
이미지 인코더를 사전 훈련하는 데는 두 가지 주요 방법이 있습니다.

15
00:01:33,000 --> 00:01:40,000
그리고 이 논문의 저자는 실제로 이 두 가지 주요 방법을 컴파일하는 데 중점을 두었습니다.

16
00:01:40,000 --> 00:01:47,000
대규모 주간 레이블이 지정된 데이터 세트를 사용하여 분류, 사전 학습인 폴리프레임 작업을 사용합니다.

17
00:01:47,000 --> 00:01:51,000
WebScale 시끄러운 데이터에 대한 대조 사전 훈련.

18
00:02:10,000 --> 00:02:33,000
먼저 대조적인 사전 훈련된 탐색 도약을 살펴보겠습니다.

19
00:02:34,000 --> 00:02:42,000
따라서 먼저 언어 이미지 사전 훈련에 대한 표준 소프트맥스 손실을 요약해야 합니다.

20
00:02:42,000 --> 00:02:54,000
이 언어 이미지 사전 훈련의 목적은 일반적으로 이미지 텍스트 파일의 I1, T1, I2, T2 등과 같은 미니 배치 B가 제공됩니다.

21
00:02:54,000 --> 00:03:05,000
학습 목표는 일치하는 파일의 임베딩을 서로 정렬하는 동시에 일치하지 않는 파일의 임베딩을 분리하도록 권장합니다.

22
00:03:05,000 --> 00:03:14,000
그리고 마지막 함수를 살펴보면 실제로 두 개의 용어로 구성되어 있음을 알 수 있습니다. 하나는 이미지에서 텍스트까지의 소프트맥스입니다.

23
00:03:14,000 --> 00:03:17,000
또 하나는 텍스트에서 이미지 소프트맥스로의 변환입니다.

24
00:03:17,000 --> 00:03:37,000
그리고 분모의 비대칭성으로 인해 일반적으로 정규화 항을 두 번 계산해야 하는데 이는 실제로 계산적으로 비효율적입니다.

25
00:03:37,000 --> 00:03:47,000
그리고 이러한 종류의 손실 함수는 정규화를 위해 쌍별 유사성에 대한 전역적인 관점이 필요한 모달형 클립에서 일반적으로 사용됩니다.

26
00:03:47,000 --> 00:04:00,000
배치 수준 소프트맥스 기반 대비 손실을 두 번 적용하여 모든 데이터 쌍에 걸쳐 쌍별 유사성 점수를 정규화합니다.

27
00:04:01,000 --> 00:04:08,000
자, 표준 소프트맥스 손실을 요약한 후 탐색 도약이 무엇인지 살펴보겠습니다.

28
00:04:08,000 --> 00:04:23,000
그래서 Seek Leap은 사실 소프트맥스와 달리 모든 이미지 텍스트 파일을 독립적으로 처리하는 Seek-Moid 기반 손실을 제안하는 논문입니다.

29
00:04:23,000 --> 00:04:30,000
그래서 공식을 살펴보면 이전보다 훨씬 간단해졌습니다.

30
00:04:30,000 --> 00:04:38,000
우리는 stat-ig라는 텍스트 쌍의 특정 이미지에 대한 추가 레이블만 소개합니다.

31
00:04:38,000 --> 00:04:47,000
그리고 이 학습 문제를 모든 쌍 조합의 데이터 세트에 대한 표준 이진 분류로 전환합니다.

32
00:04:47,000 --> 00:04:53,000
따라서 일치하는 쌍이 있을 때마다 관련된 긍정적인 레이블이 있습니다.

33
00:04:53,000 --> 00:04:57,000
그리고 일치하지 않는 쌍이 있으면 음수 레이블이 서명됩니다.

34
00:04:57,000 --> 00:05:11,000
따라서 이는 이전 공식에서 언급한 분모인 전역 정규화 요소를 계산할 필요가 없기 때문에 실제로 다중 레이블 분류 문제에 더 좋습니다.

35
00:05:12,000 --> 00:05:22,000
탐색 도약에 대해 살펴본 후 언어 학습 패러다임을 통합하는 UL2 프레임워크를 살펴보겠습니다.

36
00:05:22,000 --> 00:05:29,000
따라서 기존의 사전 추세 모델은 일반적으로 특정 종류의 문제에 맞춰져 있습니다.

37
00:05:29,000 --> 00:05:40,000
그리고 이것이 왜 사전 추세의 선택이 다운스트림 작업에 의존해야 하는지 묻는 이 논문의 동기입니다.

38
00:05:40,000 --> 00:05:49,000
따라서 이 문제를 이해하기 위해 현재 대규모 언어 모델에 대한 사전 학습 목표가 무엇인지 요약해 보겠습니다.

39
00:05:49,000 --> 00:06:00,000
따라서 문화적 LM인 UL3는 이전의 모든 타임스탬프를 모델에 대한 입력으로 사용하여 목표인 다음 토큰을 예측합니다.

40
00:06:00,000 --> 00:06:07,000
그리고 과거 토큰을 입력으로 사용한 접두사 LM도 있는데, 이는 문화적인 LM과 매우 유사합니다.

41
00:06:07,000 --> 00:06:12,000
유일한 차이점은 입력을 양방향으로 소비한다는 점입니다.

42
00:06:12,000 --> 00:06:17,000
그리고 또 하나는 스팬 보정인데 조금 다릅니다.

43
00:06:17,000 --> 00:06:26,000
손상된 범위 목표를 예측하기 위해 과거 및 미래의 입력 세트에서 잘리지 않은 모든 토큰을 활용합니다.

44
00:06:26,000 --> 00:06:36,000
그리고 우리는 이러한 사전 훈련 목표 각각에 대해 항상 하나의 사전 훈련 목표를 다른 사전 훈련 목표로 줄일 수 있다는 것을 알 수 있습니다.

45
00:06:36,000 --> 00:06:49,000
그래서 예를 들어 스팬 수정 대물렌즈에서 대상인 제작물이 전체 시퀀스와 같을 때 문제는 LM 문제가 됩니다.

46
00:06:50,000 --> 00:07:05,000
따라서 각 사전 훈련 목표 간의 공통점을 확인한 후 저자는 실제로 잡음 제거 작업 후 입력과 목표를 생성하는 데 사용되는 기능인 스팬 크롭을 제안합니다.

47
00:07:05,000 --> 00:07:12,000
다음은 세 가지 매개변수를 도입한 이 함수의 매개변수화입니다.

48
00:07:12,000 --> 00:07:21,000
Mu는 주요 스팬 길이, R은 보정율, N은 제작된 스팬 토큰 개수입니다.

49
00:07:21,000 --> 00:07:26,000
이는 스팬 길이 mu에서 입력 길이 L의 함수일 수 있습니다.

50
00:07:26,000 --> 00:07:37,000
따라서 입력 텍스트가 주어지면 이 함수는 평균 mu를 사용하여 정규 균일 분포에서 추출된 렌즈 범위에 대한 수정을 도입합니다.

51
00:07:37,000 --> 00:07:48,000
예를 들어 접두사 LM 목적을 표현하기 위해 mu를 L 빼기 P와 동일하게 설정할 수 있습니다. 여기서 P는 접두사의 길이입니다.

52
00:07:48,000 --> 00:07:59,000
수정율인 R은 1.0에서 POL을 뺀 것과 같고, N은 1이며, 이는 만들어진 범위가 하나만 있음을 의미합니다.

53
00:07:59,000 --> 00:08:11,000
그리고 수정 후 입력된 텍스트는 노이즈 제거 작업에 대한 사실이 되며, 제작된 스팬은 복구 대상으로 사용됩니다.

54
00:08:12,000 --> 00:08:25,000
따라서 사전 훈련 중에 다양한 문제를 해결하려면 강력한 범용 모델이 노출되어야 한다는 사실을 기반으로 스팬 크롭 기능을 학습한 후,

55
00:08:25,000 --> 00:08:37,000
사전 훈련이 자기 초월화를 사용하여 수행된다는 점을 감안할 때 저자는 이러한 다양성이 모델의 목표에 주입되어야 한다고 주장합니다.

56
00:08:37,000 --> 00:08:48,000
그렇지 않으면 모델은 중심 능력 부족으로 어려움을 겪을 수 있으며 이에 동기를 부여하여 현재의 목적 함수 클래스를 제안합니다.

57
00:08:48,000 --> 00:08:59,000
저자는 사전 훈련 중에 사용되는 세 가지 주요 문제인 R denoiser, S denoiser 및 X denoiser를 정의합니다.

58
00:08:59,000 --> 00:09:07,000
따라서 우리는 이러한 각 패러다임을 살펴보면 부패의 다른 패턴을 발견할 수 있습니다.

59
00:09:07,000 --> 00:09:14,000
따라서 R 노이즈 제거의 경우 스팬 손상의 목표를 모방하는 데 더 중점을 두고,

60
00:09:14,000 --> 00:09:21,000
범위는 짧고 학습 대신 지식을 획득하고 유동적인 작업을 생성하는 데만 부분적으로 유용합니다.

61
00:09:21,000 --> 00:09:27,000
반면 S 노이즈 제거는 접두사 LM 대물렌즈와 훨씬 더 유사합니다.

62
00:09:27,000 --> 00:09:33,000
따라서 접두사인 컨텍스트는 양방향 수용 필드를 유지합니다.

63
00:09:33,000 --> 00:09:41,000
마지막으로 입력의 작은 부분을 고려하여 큰 부분을 복구하는 것을 목표로 하는 X 디노이저는

64
00:09:41,000 --> 00:09:49,000
실제로 일반 범위 손상과 목표와 같은 언어 모델 간의 보간입니다.

65
00:09:49,000 --> 00:10:00,000
그리고 그 외에도 저자는 모드 전환 중 패러다임 전환이라는 개념도 도입했습니다.

66
00:10:00,000 --> 00:10:08,000
따라서 사전 훈련 중에 R, X, X라는 추가 상위 토큰이 있는 모델을 갖게 됩니다.

67
00:10:08,000 --> 00:10:20,000
따라서 모델이 다양한 작업 간에 확장을 전환하고 주어진 작업에 더 적합한 모드에서 작동하는 데 도움이 됩니다.

68
00:10:20,000 --> 00:10:27,000
그럼 폴리3의 주요 구성요소를 학습한 후 학습 단계를 살펴보겠습니다.

69
00:10:27,000 --> 00:10:33,000
따라서 폴리3에는 실제로 여러 가지 다른 단계가 있습니다.

70
00:10:33,000 --> 00:10:41,000
따라서 첫 번째 단계에서는 각 구성 요소가 별도로 훈련되는 단위 모델 훈련을 수행합니다.

71
00:10:41,000 --> 00:10:48,000
따라서 이미지 인코더는 웹의 이미지 텍스트 경로에 대해 사전 훈련되어 있습니다.

72
00:10:48,000 --> 00:10:51,000
병약한 훈련 프로토콜을 따릅니다.

73
00:10:51,000 --> 00:11:02,000
그리고 수십억 개의 언어 모델인 텍스트 인코더 디코더는 디노이저의 혼합에 따라 훈련됩니다.

74
00:11:03,000 --> 00:11:16,000
두 번째 단계는 다중 모드 훈련으로, 대비 비전 인코더를 동결하고 다중 모드 작업과 데이터 혼합에 대해서만 훈련합니다.

75
00:11:16,000 --> 00:11:27,000
인코더를 해상도 224로 고정하여 폴리에서 유지됩니다.

76
00:11:27,000 --> 00:11:33,000
그 후에는 해상도가 증가하여 전체 모델이 미세 조정됩니다.

77
00:11:33,000 --> 00:11:50,000
따라서 이 순간 대비 비전 인코더와 언어 모델은 모두 고정 해제되어 있으며 812 또는 1064 해상도로 훈련되었습니다.

78
00:11:50,000 --> 00:12:07,000
따라서 본 논문에서 진행된 실험을 살펴보면, 진행되고 있는 중요한 실험 중 하나는 분류 기반 비전 인코더와 대조 기반 비전 인코더 간의 비교입니다.

79
00:12:08,000 --> 00:12:21,000
따라서 우리는 JFT 데이터 세트에 대한 분류를 위한 하나의 사전 훈련과 실제로 병약한 프로토콜을 사용하는 Wabley 데이터 세트에 대한 대조 사전 훈련임을 알 수 있습니다.

80
00:12:21,000 --> 00:12:37,000
그리고 다음 표에서 우리는 실제로 병약한 모델이 캡션 같은 작업과 여기의 텍스트 WQA 또는 RathCoco와 같은 더 복잡한 작업에서 탁월하다는 것을 알 수 있습니다.

81
00:12:37,000 --> 00:12:43,000
따라서 문제가 어려울수록 병약한 모델이 더 많은 이점을 제공할 수 있습니다.

82
00:12:44,000 --> 00:12:48,000
그리고 또 다른 실험이 진행되고 있습니다.

83
00:12:48,000 --> 00:12:55,000
예를 들어 비디오 캡션 및 비디오 QA를 세부적으로 조정하고 평가합니다.

84
00:12:55,000 --> 00:13:01,000
그리고 폴리 프리는 어떤 비디오 데이터로도 사전 훈련되지 않습니다.

85
00:13:01,000 --> 00:13:06,000
따라서 이는 실제로 대조 VIT 채택의 이점을 강조합니다.

86
00:13:07,000 --> 00:13:19,000
그리고 이 실험에서는 고정된 임시 휴식을 사용하여 최대 16프레임까지 각 프레임의 비전 토큰을 연결합니다. 그렇죠?

87
00:13:19,000 --> 00:13:22,000
그래서 이것은 폴리 프리에 관한 것입니다.

88
00:13:22,000 --> 00:13:33,000
자, 이제 우리가 이야기할 두 번째 모델인 생성 다중 모드 모델(Generative Multimodal Models)인 E-Mode 2로 가보겠습니다.

89
00:13:34,000 --> 00:13:40,000
그리고 GMM이 실제로 상황 내 학습자임을 지적합니다.

90
00:13:40,000 --> 00:13:47,000
따라서 먼저 상황 내 학습이 무엇인지 빠르게 파악하거나 요약해 보겠습니다.

91
00:13:47,000 --> 00:13:59,000
따라서 상황 내 학습은 실제로 몇 가지 시연이나 간단한 지침만으로 상황에 맞게 다중 모드 과제를 해결하는 능력을 의미합니다.

92
00:13:59,000 --> 00:14:07,000
그래서 우리는 여기의 예를 살펴보고 여기에서 맨 아래의 예를 볼 수 있습니다.

93
00:14:07,000 --> 00:14:19,000
따라서 모델에는 실제로 세 가지 예가 제공됩니다. 배경에 도시가 있는 피사체 A와 같은 프롬프트와 이미지가 제공됩니다.

94
00:14:19,000 --> 00:14:33,000
이것은 예시 1, 예시 2, 예시 3입니다. 무지개 모자를 쓴 피험자 A인 모델에게 프롬프트가 주어졌습니다.

95
00:14:33,000 --> 00:14:41,000
그리고 모델은 이전 예제에 따라 이미지를 생성할 것으로 예상됩니다.

96
00:14:41,000 --> 00:14:47,000
그리고 이것이 E-Mode 2가 생성할 수 있는 것입니다.

97
00:14:47,000 --> 00:15:01,000
따라서 E-Mode 2의 동기는 실제로 다중 모드 작업이 단일 또는 다중 모드에서 진화하는 모든 이해와 생성을 포괄한다는 점입니다.

98
00:15:01,000 --> 00:15:12,000
그리고 이전 다중 모드 시스템은 주로 작업 설계, 특정 아키텍처 및 상당한 규모의 감독 훈련 세트 수집에 의존했습니다.

99
00:15:12,000 --> 00:15:22,000
그러나 우리는 인간이 맥락에 따라 새로운 작업을 해결할 수 있다는 것을 알고 있습니다. 즉, 몇 가지 시연이나 간단한 지침만으로 문제를 해결할 수 있습니다.

100
00:15:22,000 --> 00:15:38,000
이것이 바로 이 논문이 실제로 약 370억 개의 매개변수로 구성된 확장 다중 모드 생성적 사전 추세 모델이 유사한 상황 내 학습 능력을 활용할 수 있음을 입증하는 이유입니다.

101
00:15:38,000 --> 00:15:43,000
그럼 먼저 E-Mode 2의 아키텍처를 살펴보겠습니다.

102
00:15:43,000 --> 00:15:54,000
그리고 E-Mode 2의 아키텍처가 무엇인지 이해하려면 먼저 E-Mode 2의 이전 버전인 E-Mode를 이해하는 것이 좋습니다.

103
00:15:54,000 --> 00:15:58,000
E-Mode 2는 E-Mode를 기반으로 구축되었기 때문입니다.

104
00:15:59,000 --> 00:16:04,000
따라서 E-Mode의 모델 아키텍처에는 실제로 4개의 구성 요소가 있습니다.

105
00:16:04,000 --> 00:16:17,000
시각적 인코더, 캐주얼 변환기, LLM인 다중 모드 모델링 구성 요소, 시각적 인코더가 있습니다.

106
00:16:17,000 --> 00:16:39,000
먼저 볼 수 있듯이 이미지는 밀도가 높은 시각적 특징인 시각적 특징으로 인코딩된 다음 인코딩을 캐주얼 변환기처럼 고정된 수의 비시각적 캐주얼 임베딩으로 변환합니다.

107
00:16:39,000 --> 00:17:00,000
그리고 세금 토큰과 결합된 이러한 시각적 임베딩은 LLM을 사용한 다중 모드 모델링에 공급되어 다중 모드 시퀀스를 형성한 다음 통합된 자동 회귀 모델링을 수행합니다.

108
00:17:01,000 --> 00:17:17,000
그리고 추론에는 미세 조정된 시각적 인코더, 이러한 시각적 임베딩을 사실적인 이미지로 디코딩하는 데 사용되는 디코더가 있습니다. 이것이 바로 E-Mode 2가 이미지를 생성할 수 있는 이유입니다.

109
00:17:18,000 --> 00:17:34,000
그리고 우리는 이 캐주얼 트랜스포머의 목표가 이미지의 특성을 더 잘 포착하고 다양한 다중 양식의 통합 모델링을 달성하는 것임을 주목해야 합니다.

110
00:17:34,000 --> 00:17:47,000
그리고 이 모든 캐주얼 트랜스포머 작업은 인코더에서 생성된 임베딩을 고정된 수의 비시각적 캐주얼 임베딩으로 변환하는 것뿐입니다.

111
00:17:47,000 --> 00:17:58,000
그리고 이 캐주얼 트랜스포머 외에도 우리가 실제로 주목해야 할 것은 안정적인 확산에 의해 초기화되는 이 디코더입니다.

112
00:17:58,000 --> 00:18:21,000
그리고 E-Mode에서는 각 훈련 샘플에 대해 이 디코더를 미세 조정하기 위해 이 부분인 다중 모드 모델링 LLM을 사용하여 이 이미지 인코더에 맞게 자동 회귀 방식으로 비가시적 임베딩을 생성합니다. 이미지 생성 훈련의 조건으로.

113
00:18:21,000 --> 00:18:32,000
따라서 매번 이 디코더를 훈련하려면 LLM을 한 번 실행하여 이 비시각적 임베딩을 생성하는 추론을 수행해야 합니다.

114
00:18:32,000 --> 00:18:39,000
그래서 실제로 E-Mode 2가 이 두 부분을 타겟으로 하고 최적화한 것입니다.

115
00:18:39,000 --> 00:18:50,000
따라서 E-Mode 2의 목표와 아키텍처를 살펴보면 아키텍처는 여전히 통합된 자동 회귀 목표이며, 이는 다음 다중 모드 요소를 예측합니다.

116
00:18:50,000 --> 00:18:54,000
시각적 임베딩 또는 텍스처 토큰일 수 있습니다.

117
00:18:54,000 --> 00:19:00,000
이후 슬라이드에서 이에 대해 설명할 때 먼저 모델 아키텍처에 초점을 맞춥니다.

118
00:19:00,000 --> 00:19:11,000
따라서 E-Mode 2에는 시각적 인코더, 자동 회귀 다중 모드 및 시각적 디코더만 있는 더 간단한 아키텍처가 있습니다.

119
00:19:11,000 --> 00:19:26,000
그리고 인코더와 LLM 사이를 연결하기 위해 그들은 각 이미지를 8x8 이미지 패치로 가져온 다음 선형 투영을 통해 이 두 구성 요소를 연결했습니다.

120
00:19:26,000 --> 00:19:32,000
그리고 E-Mode에서 E-Mode 2로 더욱 향상된 것은 바로 이 디코더입니다.

121
00:19:32,000 --> 00:19:38,000
따라서 E-Mode 2의 이 디코더는 실제로 여기처럼 토큰 헤더로 훈련됩니다.

122
00:19:38,000 --> 00:19:41,000
따라서 언어 모델 없이도 학습이 가능합니다.

123
00:19:41,000 --> 00:19:45,000
이에 대해서는 이후 슬라이드에서 살펴보겠습니다.

124
00:19:45,000 --> 00:19:52,000
그럼 먼저 E-Mode 2의 훈련 목표를 살펴보겠습니다.

125
00:19:52,000 --> 00:20:09,000
따라서 교육 목표는 다음 다중 모드 요소를 예측하는 것입니다. 이는 레이블이 지정되지 않은 웹 규모 스코프 플롯(D)이 주어지는 것을 의미하며 인터리브 다중 모드 시퀀스 x는 x1에서 xn까지와 같습니다.

126
00:20:09,000 --> 00:20:23,000
먼저 이 모든 연속 2D 신호를 1D 이후 임베딩 시퀀스로 변환합니다. u는 u1에서 um과 같습니다. 여기서 ui는 설명 텍스트 토큰이거나 시각적 임베딩일 수 있습니다.

127
00:20:23,000 --> 00:20:37,000
그리고 목표는 우리에게 이미 익숙한 로그 가능성을 사용하여 PU가 포함된 웹 규모 구리 APX의 가능성을 근사화하는 것입니다.

128
00:20:37,000 --> 00:20:45,000
따라서 ui는 항상 매개변수를 사용한 이전의 모든 사용을 조건으로 합니다.

129
00:20:45,000 --> 00:20:48,000
그리고 두 가지 유형의 손실이 사용됩니다.

130
00:20:48,000 --> 00:20:53,000
개별 텍스트 토큰의 경우 교차 엔트로피 손실이 발생합니다.

131
00:20:53,000 --> 00:21:02,000
그리고 이것이 연속적인 시각적 임베딩이라면 L2 회귀 손실이 사용됩니다.

132
00:21:02,000 --> 00:21:19,000
따라서 E-Mode 2의 사전 훈련 단계를 살펴보면 E-Mode 2 훈련을 위해 먼저 텍스트 토큰의 캡션 손실만 포함하여 이미지 텍스트와 비디오 텍스트 창백한 데이터를 사전 훈련합니다.

133
00:21:19,000 --> 00:21:32,000
그런 다음 시각적 인코더를 표현하고 텍스트 분류 손실과 이미지 회귀 손실을 모두 사용하여 선형 투영 레이어와 다중 모드 모델링만 최적화합니다.

134
00:21:32,000 --> 00:21:44,000
그리고 시각적 디코딩 부분을 살펴보면, 시각적 디코더는 시각적 인코더에서 생성된 시각적 임베딩을 여기와 같은 이미지로 직접 디코딩하도록 훈련됩니다.

135
00:21:44,000 --> 00:22:01,000
따라서 이 8번째 시각적 디코더의 각 최적화 단계에는 언어 모델의 초회귀적 추론이 필요한 E-Mode와 달리 E-Mode 2 시각적 디코딩은 실제로 토큰으로 간주될 수 있습니다.

136
00:22:01,000 --> 00:22:10,000
따라서 이 디코더의 훈련에는 더 이상 언어 모델이 포함되지 않습니다.

137
00:22:10,000 --> 00:22:16,000
따라서 E-Mode 2에는 실제로 두 가지 훈련 목표를 가진 두 가지 변형이 있습니다.

138
00:22:16,000 --> 00:22:28,000
첫째, 대화 데이터를 미세 조정하여 특정 작업 지침을 따르도록 효율적으로 조정할 수 있는 E-모드 채팅이 있습니다.

139
00:22:28,000 --> 00:22:46,000
따라서 채팅 후 이 지침의 경우 훈련 목표는 여기와 같으며 데이터 유형을 구성하는 데 도움이 되는 사용자 및 보조자라는 두 개의 특수 토큰이 도입됩니다.

140
00:22:46,000 --> 00:22:52,000
따라서 사용자의 경우 지침은 프롬프트와 이미지인 다중 모드 토큰입니다.

141
00:22:52,000 --> 00:23:05,000
따라서 이후 보조 토큰은 모델에 의해 생성될 것으로 예상되는 답변이며 실제로 훈련 중 교차 엔트로피 손실에 의해 감독됩니다.

142
00:23:05,000 --> 00:23:16,000
또한 두 가지 주요 작업 범주(학술 작업 지향 또는 다중 모드 채팅) 간에 달라지는 시스템 메시지가 하나 더 있습니다.

143
00:23:16,000 --> 00:23:31,000
그리고 다른 변형 I E-Mode는 태그, 위치 및 이미지를 조건으로 혼합하고 지정된 텍스트나 주제에 기반한 일반 이미지를 수용할 수 있는 E-Mode Jain입니다.

144
00:23:31,000 --> 00:23:44,000
따라서 이 E-Mode 2 Jain의 경우 훈련 목표는 여전히 다음 다중 모드 요소를 예측하는 통합 생성 사전 훈련 목표입니다.

145
00:23:44,000 --> 00:23:48,000
하지만 입력에는 뭔가 다른 것이 있습니다.

146
00:23:48,000 --> 00:23:58,000
그래서 검은색 이미지 위에 지정된 위치에 경계 상자를 그려서 각 객체의 좌표를 이미지 형식으로 표현합니다.

147
00:23:58,000 --> 00:24:01,000
그래서 우리는 여기에서 한 가지 예를 볼 수 있습니다.

148
00:24:01,000 --> 00:24:05,000
이것이 프롬프트이고 이것이 원본 이미지입니다.

149
00:24:05,000 --> 00:24:16,000
이를 모델에 추가하는 것은 사람, 말, 산, 물과 같은 각 개체에 대한 경계 상자입니다.

150
00:24:16,000 --> 00:24:21,000
이 모든 것이 실제로 이 검은 이미지를 강조하고 있습니다.

151
00:24:21,000 --> 00:24:25,000
그럼 마지막으로 E-Mode 2에 대한 몇 가지 평가를 살펴보겠습니다.

152
00:24:25,000 --> 00:24:29,000
먼저 사전 훈련된 기본 모델의 성능을 살펴보겠습니다.

153
00:24:29,000 --> 00:24:46,000
그리고 매개변수가 370억 개에 불과한 E-Mode 2가 모델 규모가 훨씬 작은 모든 현장 촬영 설정에서 실제로 Flamingo 80억 및 IDE FICS 80억보다 성능이 우수하다는 것을 알 수 있습니다.

154
00:24:46,000 --> 00:24:52,000
또한 이러한 제어 가능한 시각적 생성 예제도 볼 수 있습니다.

155
00:24:52,000 --> 00:25:05,000
실제로 E-Mode 2는 다른 모든 모델에 비해 성능이 훨씬 더 뛰어나다는 것을 알 수 있습니다.

156
00:25:05,000 --> 00:25:12,000
이제 우리는 이 부분의 세 번째 모델인 InterVL로 이동하겠습니다.

157
00:25:12,000 --> 00:25:22,000
따라서 InterVL은 LLM의 매개변수 규모에 맞게 비전 인코더를 평가하는 데 특별합니다.

158
00:25:22,000 --> 00:25:30,000
그래서 InterVL에서는 먼저 기존 비전 대형 언어 모델의 문제를 조사합니다.

159
00:25:30,000 --> 00:25:40,000
따라서 이전에 언급한 대부분의 비전 언어 모델에 대해 비전 모델을 대규모 언어 모델과 연결한다는 점을 알 수 있습니다.

160
00:25:40,000 --> 00:25:47,000
항상 가벼운 접착제 층이 사용되며 이러한 정렬에는 실제로 몇 가지 제한 사항이 있습니다.

161
00:25:47,000 --> 00:25:53,000
하지만 먼저 매개변수 척도에는 항상 차이가 있습니다.

162
00:25:53,000 --> 00:26:02,000
예를 들어 대규모 언어 모델의 매개변수는 최대 1,000억 개에 달할 수 있지만 시각적 인코더는 약 10억 개에 불과합니다.

163
00:26:02,000 --> 00:26:07,000
이로 인해 대규모 언어 모델 용량이 제대로 사용되지 않을 수 있습니다.

164
00:26:07,000 --> 00:26:17,000
그리고 비전 모델은 일반적으로 순수한 비전 데이터에 대해 훈련되기 때문에 표현 간에 불일치가 있습니다.

165
00:26:17,000 --> 00:26:24,000
그리고 이런 종류의 표현은 일반적으로 대규모 언어 모델과 일치하지 않습니다.

166
00:26:24,000 --> 00:26:30,000
그리고 글루 레이어는 일반적으로 가볍고 무작위로 초기화되기 때문에

167
00:26:30,000 --> 00:26:43,000
이는 비효율적인 연결이 될 수 있으며 이제 풍부한 클래스 모델 상호 작용 및 종속성을 캡처할 수 있습니다.

168
00:26:43,000 --> 00:26:55,000
따라서 이러한 관찰을 바탕으로 우리는 먼저 Qformer라는 매우 유명하고 유명한 경량 접착제 레이어를 살펴볼 수 있습니다.

169
00:26:55,000 --> 00:27:01,000
그리고 Qformer는 이전 강의에서도 여러번 소개되거나 언급되었던 것 같습니다.

170
00:27:01,000 --> 00:27:08,000
따라서 Qformer는 양식 격차를 해소하는 데 사용되는 경량 쿼리 변환기일 뿐입니다.

171
00:27:08,000 --> 00:27:16,000
목표는 쿼리가 작업에 대한 대부분의 정보를 제공하는 비전 표현을 추출하는 방법을 학습할 수 있다는 것입니다.

172
00:27:16,000 --> 00:27:27,000
따라서 Qformer는 관련 없는 시각적 정보를 제거하면서 가장 유용한 정보를 LRM에 맞추는 정보 병목 현상 역할을 합니다.

173
00:27:27,000 --> 00:27:34,000
그리고 Qformer가 어떻게 이미지 인코더와 대규모 언어 모델 사이의 가교 역할을 하는지 명확하게 확인할 수 있습니다.

174
00:27:34,000 --> 00:27:42,000
따라서 이미지 인코더에서 생성된 이미지 인코딩에서 정보를 추출하는 Qformer의 기능에 따라

175
00:27:42,000 --> 00:27:47,000
LRM에 얼마나 많은 유용한 시각적 정보를 입력할 수 있는지.

176
00:27:47,000 --> 00:27:53,000
그래서 인터뷰어의 저자는 Qformer가 너무 가볍기 때문에,

177
00:27:53,000 --> 00:28:00,000
이미지 인코딩과 작업 간의 모든 유용한 정보와 종속성을 캡처하지 못할 수도 있습니다.

178
00:28:00,000 --> 00:28:08,000
그러면 면접관이 제안되고, 먼저 면접관의 전반적인 아키텍처를 살펴볼 수 있습니다.

179
00:28:08,000 --> 00:28:11,000
따라서 면접관은 두 가지 주요 구성 요소를 가지고 있습니다.

180
00:28:11,000 --> 00:28:19,000
첫 번째는 60억 개의 매개변수 크기를 가진 내부 현실로 호출되는 대규모 비전 인코더입니다.

181
00:28:19,000 --> 00:28:24,000
그리고 이것은 실제로 바닐라 비전 트랜스포머로 구현되었습니다.

182
00:28:24,000 --> 00:28:33,000
그리고 80억 개의 매개변수 크기로 QLAMA에 의해 초기화되는 대규모 언어 미들웨어도 있습니다.

183
00:28:33,000 --> 00:28:39,000
그리고 전임상 다국어 LRMA를 기반으로 개발되었으며,

184
00:28:39,000 --> 00:28:47,000
새로 추가된 69개의 학습 가능한 쿼리와 무작위로 초기화되는 Cross-Attention 레이어가 있습니다.

185
00:28:47,000 --> 00:28:51,000
따라서 이러한 아키텍처의 장점은 매우 분명합니다.

186
00:28:51,000 --> 00:28:56,000
첫째, 이는 균형 잡힌 시각과 언어 구성요소라는 매개변수이며,

187
00:28:56,000 --> 00:29:01,000
두 가지 주요 구성 요소 모두 약 60억~80억 개의 매개변수를 갖고 있기 때문입니다.

188
00:29:01,000 --> 00:29:11,000
그리고 QLAMA는 Intel VIT 6B에서 생성된 이미지 토큰을 LRM과 일치하는 표현으로 전송할 수 있습니다.

189
00:29:11,000 --> 00:29:15,000
다국어 LRMA의 사전 학습 가중치로 초기화합니다.

190
00:29:15,000 --> 00:29:23,000
그리고 QLAMA에는 Qformer보다 42배나 많은 80억 개의 매개변수가 있으므로

191
00:29:23,000 --> 00:29:33,000
고정된 LRMA 디코더를 사용하더라도 Intel VAR은 다중 모드 대화 테스트에서 매우 유망한 성능을 달성할 수 있습니다.

192
00:29:33,000 --> 00:29:37,000
그래서 인텔 VAR의 훈련 전략을 살펴보면,

193
00:29:37,000 --> 00:29:40,000
폭력적인 실제 건축물 외에도

194
00:29:40,000 --> 00:29:49,000
Intel VAR의 저자는 또한 비전 인코더, 언어, 미들웨어 및 LRMA 간의 더 나은 정렬을 돕기 위한 새로운 교육 전략을 도입했습니다.

195
00:29:49,000 --> 00:29:53,000
따라서 첫 번째 단계는 비전-언어 대조 훈련입니다.

196
00:29:53,000 --> 00:30:03,000
LRMA 7B를 채택하여 STF에서 텍스트를 인코딩하고 Intel VIT를 사용하여 시각적 특징 IF를 추출합니다.

197
00:30:03,000 --> 00:30:12,000
그리고 목표는 일괄적으로 이미지 텍스트 쌍의 유사성 점수에 대한 대칭 교차 엔트로피 손실을 최소화하는 것입니다.

198
00:30:12,000 --> 00:30:22,000
그리고 이 단계에서는 인텔 VAR이 제로샷 이미지 분류, 제로샷 이미지 텍스트 검색과 같은 대조 텍스트에서 탁월한 성능을 발휘할 수 있습니다.

199
00:30:22,000 --> 00:30:26,000
두 번째 단계에서는 시각-언어 생성 훈련입니다.

200
00:30:26,000 --> 00:30:35,000
음, QLAMA는 첫 번째 단계에서 LRMA 7B의 가중치를 상속받고 Intel VIT와 QLAMA가 모두 동결됩니다.

201
00:30:35,000 --> 00:30:45,000
이 단계에서는 새로 추가된 학습 가능한 쿼리와 교차 어텐션 레이어만 필터링된 고품질 데이터로 학습됩니다.

202
00:30:45,000 --> 00:30:57,000
따라서 손실에는 이미지 텍스트 대비 손실인 ITC, 이미지 텍스트 일치 손실인 ITM 및 이미지 기반 텍스트 생성 손실인 ITG인 3K 구성 요소가 있습니다.

203
00:30:57,000 --> 00:31:08,000
이를 통해 쿼리는 강력한 시각적 표현을 추출하고 기능 공간을 대규모 언어 모델과 더욱 일치시킬 수 있습니다.

204
00:31:08,000 --> 00:31:24,000
마지막 단계는 지도 미세 조정 단계로, Intel VIT와 QLAMA가 기성 LM 디코더와 연결되어 지도 미세 조정을 수행합니다.

205
00:31:24,000 --> 00:31:41,000
따라서 Intel VAR의 흥미로운 부분은 실제로 스위스 R&R 모델로 작동할 수 있다는 것입니다. 즉, 희망 인코더와 언어 미들웨어를 유연하게 결합하고 다양한 희망 언어 작업을 지원할 수 있다는 의미입니다.

206
00:31:41,000 --> 00:31:57,000
따라서 대조 작업의 경우 희망 인코더를 사용하거나 Intel VIT와 QLAMA를 조합하여 희망 기능을 인코딩함으로써 Intel VLC 및 Intel VLG를 사용할 수 있습니다.

207
00:31:57,000 --> 00:32:15,000
그리고 작업 생성을 위해 QLAMA는 본질적으로 유망한 이미지 캡션 기능을 가지고 있으며, QLAMA의 능력은 Intel VIT 6B의 희망 표현을 재구성하고 QLAMA의 접두사 텍스트로 재생됩니다.

208
00:32:15,000 --> 00:32:25,000
마지막으로 다중 모달 대화를 위해 QLAMA와 함께 또는 없이 사용할 수 있는 Intel VAR 검사 모션이 있습니다.

209
00:32:26,000 --> 00:32:46,000
그래서 턴웨어에서 진행된 실험을 살펴보면 실험의 일부만 보여주고 있으며, 저자가 진행하고 있는 가장 중요한 실험 중 하나는 Intel VIT 6B의 희망사항이나 지각능력을 검증하는 것이라고 생각합니다.

210
00:32:46,000 --> 00:33:00,000
그리고 이것이 실제로 Intel VAT 6B라고 말할 수 있습니다. 저는 사용 가능한 모든 종류의 방법을 수행하고 모든 종류의 데이터 세트에서 정말 유망한 결과를 얻을 것입니다.

211
00:33:01,000 --> 00:33:28,000
Intel VLG 채팅에 대한 평가와 같은 다른 평가도 있으며 QLAMA의 60억 소원 인코더를 미들웨어로 사용하는 전체 아키텍처와 ALM이 lava와 같은 다른 모든 모델 중에서 가장 유망하고 최상의 결과를 가지고 있음을 알 수 있습니다. , 플라밍고, 블립, 블립을 지시합니다.

212
00:33:28,000 --> 00:33:33,000
내 생각에 우리는 이전 강의에서 꽤 논의한 것 같습니다.

213
00:33:37,000 --> 00:33:42,000
좋습니다. 제가 논의할 마지막 모델은 독일입니다.

214
00:33:42,000 --> 00:33:58,000
그리고 제가 먼저 언급하고 싶은 점은 독일은 실제로 기술 보고서를 온라인으로만 볼 수 있기 때문에 독일에서 사용되는 아키텍처나 교육 전략에 대한 세부 정보가 많지 않다는 것입니다.

215
00:33:58,000 --> 00:34:04,000
그래서 지금 우리가 집중할 것은 품질과 평가에 더 가깝습니다.

216
00:34:04,000 --> 00:34:17,000
따라서 독일은 매우 유능한 다중 모드 제품군이며 이 제품군에는 다양한 시나리오에서 사용할 수 있는 다양한 크기의 세 가지 모델이 있습니다.

217
00:34:17,000 --> 00:34:35,000
그래서 그들은 정말 유용하고 매우 복잡한 작업인 Germany-Altra와 향상된 성능 및 극성 스케일을 사용하는 Germany Pro, 온디바이스 애플리케이션을 위한 Nano를 보유하고 있습니다.

218
00:34:35,000 --> 00:34:43,000
그리고 Germany-Altra는 다양한 벤치마크에서 눈에 띄는 성능을 보이는 정말 강력한 모델입니다.

219
00:34:43,000 --> 00:34:51,000
그리고 실제로 32개 벤치마크 중 30개에서 새로운 수준의 결과를 달성했습니다.

220
00:34:51,000 --> 00:35:01,000
글쎄요, 12개 중 10개는 분류 추론 벤치마크이고 기타 모든 항목은 비디오 이해 음성 인식 및 번역 벤치마크를 이해하는 것입니다.

221
00:35:01,000 --> 00:35:11,000
그리고 한 가지 주목할 점은 Germany-Altra가 MMLU에서 인간 전문가 성능을 달성한 최초의 모델이라는 점입니다.

222
00:35:11,000 --> 00:35:23,000
그래서 MMLU는 초등 수학, 미국 역사, 컴퓨터 과학, 법률 등을 포함한 57개 과제를 실제로 다루는 벤치마크입니다.

223
00:35:23,000 --> 00:35:31,000
따라서 이를 위해서는 실제로 모델이 광범위한 업무 지식과 문제 해결 능력을 보유해야 합니다.

224
00:35:31,000 --> 00:35:47,000
한 가지 예는 다음과 같습니다. 모델에는 실제로 물리적 문제가 주어지고 메모가 작성되며, MMLU는 질문을 단계별로 추론하여 학생들이 올바르게 대답했는지 판단하도록 요청받습니다.

225
00:35:47,000 --> 00:35:53,000
그리고 해결책이 틀리면 잘못된 점을 지적하고 올바른 해결책을 제시합니다.

226
00:35:53,000 --> 00:35:59,000
그리고 대량으로 라텍스를 사용하고 최종 답변을 실행하도록 요청합니다.

227
00:35:59,000 --> 00:36:09,000
따라서 이것은 실제로 모델의 모든 종류의 지식을 다루는 매우 포괄적인 문제입니다.

228
00:36:09,000 --> 00:36:17,000
그래서 독일의 건축을 들여다보면 이용할 수 있는 정보가 많지 않습니다.

229
00:36:17,000 --> 00:36:28,000
따라서 이 기술 보고서에서는 독일이 아키텍처 및 모델 최적화의 개선으로 향상된 변압기 디코더를 기반으로 구축되었다는 점만 언급합니다.

230
00:36:28,000 --> 00:36:40,000
32K 콘택트렌즈에 대해서만 훈련되었으며 다양한 오디오 및 시각적 입력과 상호 연결된 텍스트 입력을 수용하도록 훈련되었습니다.

231
00:36:40,000 --> 00:36:57,000
따라서 독일 모델의 영향력 있는 인코딩은 Flamingo-Kohan-Pali에서 영감을 얻었으며 모델은 처음부터 다중 모드이며 개별 이미지 토큰을 사용하여 기본적으로 이미지를 출력할 수 없다는 중요한 차이점이 있습니다.

232
00:36:57,000 --> 00:37:23,000
그리고 지금까지 다중 모드 모델을 생성하는 표준 접근 방식에는 일반적으로 다양한 양식에 대한 별도의 구성 요소를 훈련한 다음 이러한 기능 중 일부를 대략적으로 모방하기 위해 함께 연결하는 것이 포함되는 반면, 독일은 실제로 기본 다중 모드이며 다양한 모델의 시작부터 사전 훈련입니다. 여기에 표시된 것과 같은 방식이 있습니다.

233
00:37:24,000 --> 00:37:48,000
따라서 독일에 대한 몇 가지 평가를 살펴보면서 우리가 집중할 수 있는 놀라운 성능 중 하나는 독일과 마찬가지로 Altra도 MMLU 데이터 세트에서 90.4%의 정확도에 도달할 수 있다는 것입니다. 우리는 이미 MMLU가 정말 포괄적인 진화 데이터 세트와 같다고 언급했습니다.

234
00:37:48,000 --> 00:37:53,000
그리고 인간 전문가의 성과는 89.8%로 측정되었습니다.

235
00:37:53,000 --> 00:38:15,000
그리고 이 보고서의 또 다른 임무는 이렇게 훌륭한 성능을 달성하기 위해 일종의 프롬프트 기술이 사용된다는 것입니다. 즉, MMLU에 대한 불확실성 라우팅이 있는 CHEVSTAT입니다. 여기서 모델은 KCHEVSTAT 샘플(예: 16 또는 16)을 생성하도록 요청받습니다. 32.

236
00:38:16,000 --> 00:38:26,000
그런 다음 모델이 임계값에 대해 확신을 갖고 있으면 과반수 투표를 할 것입니다. 그렇지 않으면 정말 간단한 선택과 다를 것입니다.

237
00:38:26,000 --> 00:38:35,000
여기서 독일의 Altra가 CHEVSTAT 라우팅 기술을 사용하여 실제로 최고의 성능을 달성한다는 것을 알 수 있습니다.

238
00:38:36,000 --> 00:38:55,000
또한 다국어 이해, 교차 언어 일반화 및 여러 언어로 작업 생성이 필요한 다양한 작업 세트를 사용하여 GENI 모델의 다중 모드 기능에 대한 평가도 있습니다.

239
00:38:55,000 --> 00:39:06,000
그리고 번역 외에도 독일은 모든 중요한 언어에 대한 대규모 요약 벤치마크에서도 평가됩니다.

240
00:39:06,000 --> 00:39:13,000
그리고 또한 독일의 강력한 멀티모달 능력을 보여주려고 합니다.

241
00:39:13,000 --> 00:39:39,000
따라서 예제를 살펴보면 이 예제에서는 실제로 met 플롯을 생성하는 다중 모달 추론 기능, 하위 플롯을 재배열하기 위한 늦은 코드를 보여주며, 이는 더 나쁜 그래픽에서의 인식, 명령 따르기 및 추상 추론과 같은 여러 기능을 보여줍니다. .

242
00:39:40,000 --> 00:39:55,000
그리고 Google이 이 블로그에서 언급한 것처럼 독일도 귀하의 학습 스타일에 맞춰 단계별 지침, 샘플 퀴즈 또는 앞뒤로 토론하는 개인 교사가 될 수 있습니다.

243
00:39:56,000 --> 00:40:11,000
따라서 제로 샷 이미지 이해와 필드 샷 비디오 이해에 대한 일부 평가를 살펴보면 독일이 실제로 사용 가능한 모든 벤치마크에서 SOTI 성능을 달성한 것을 볼 수 있습니다.

244
00:40:12,000 --> 00:40:24,000
그리고 여기에 비디오를 제공하는 위치를 이해하는 비디오의 예가 있으며 그들은 이 비디오에 나오는 사람에게 어떤 종류의 기술이 사용되고 있는지 묻습니다.

245
00:40:24,000 --> 00:40:34,000
따라서 이것은 실제로 비디오를 입력으로 받아들이는 독일의 능력을 테스트하는 것뿐만 아니라 비디오에서의 추론과도 같습니다.

246
00:40:35,000 --> 00:40:47,000
또한 독일의 다중 모드 능력을 검토하기 위해 이미지 생성 및 오디오 이해 성능을 살펴볼 수 있습니다.

247
00:40:48,000 --> 00:41:01,000
그리고 Germany Pro 모델이 실제로 영어와 다국어 작업 모두에 대해 모든 ASR 및 ASC 작업에서 USM 및 속삭임 모델보다 훨씬 뛰어난 성능을 발휘한다는 것을 알 수 있습니다.

248
00:41:02,000 --> 00:41:14,000
마지막으로, 이러한 모든 모델을 논의한 후 오늘 이 LMM 모듈에서 논의한 8개 모델 모두를 전체적으로 비교해 보겠습니다.

249
00:41:15,000 --> 00:41:33,000
그래서 우리는 최초의 대형 멀티모달인 Flamingo, 시각적 명령어 튜닝을 강조한 Laowa, Parva 시각적 명령어 튜닝을 강조한 InstructPlate, Instruction 및 Will 시각적 특징 추출,

250
00:41:33,000 --> 00:41:57,000
비전 멀티모달 다국어 확장에 중점을 둔 Poly, 소규모 대규모 멀티모달의 성능을 보장하기 위해 대조 훈련된 시각적 인코더를 보여주는 Poly3, 모델 확장을 통해 강력한 접촉 학습 능력을 갖춘 Emo2 .

251
00:41:57,000 --> 00:42:10,000
언어 미들웨어를 사용하여 비전 인코더와 대규모 언어 모델 간의 격차를 가져오는 Interwell과 기본적으로 다중 모드인 독일을 사용합니다.

252
00:42:10,000 --> 00:42:36,000
이 표에서 볼 수 있듯이 실제로는 다양한 데이터 세트와 다양한 종류의 데이터 세트에서 각 모델의 일부 성능을 표시합니다. VQA V2는 시각적 질문 변경, 캡 없음, 캡션에 중점을 두고 있으며 GQA는 그래픽 QA, TAX VQA는 TAX 비주얼 QA와 같습니다.

253
00:42:37,000 --> 00:42:59,000
마지막으로 환각효과인 현재의 대형 멀티모달의 한계에 대해 논의하고 싶다. 그럼 먼저 환각이 무엇인지 살펴볼까요? 따라서 환각은 모델이 제공된 소스 콘텐츠에 실제로 올바르지 않거나 무의미한 텍스트를 생성할 수 있다는 것입니다.

254
00:42:59,000 --> 00:43:10,000
그럼 이 두 가지 예를 살펴보세요. 예를 들어, 달에서 최초로 작업한 사람이 누구인지 묻는 질문에 이 모델은 완전히 잘못된 답변을 제공한다는 것을 알 수 있습니다.

255
00:43:10,000 --> 00:43:30,000
그리고 이 경우 사용자는 실제로 컨텍스트를 제공하고 모델이 다음 기사를 요약하도록 허용하지만 요약에는 제공된 컨텍스트와 일치하지 않는 완전히 잘못된 정보가 있습니다.

256
00:43:30,000 --> 00:43:42,000
연도와 마찬가지로 모델 기사에서는 2006년과 같지만 문맥 상으로는 2023년입니다. 따라서 환각의 원인은 다양할 수 있습니다.

257
00:43:43,000 --> 00:44:04,000
우리는 그 중 세 가지, 즉 데이터 훈련과 추론에 대해 논의하고 싶습니다. 따라서 데이터 관련 환각의 경우, 잘못된 정보는 데이터 소스 내에서 편견을 물려받았습니다. 즉, 모델이 가짜 상관관계를 포착하고 지식 회상에서 이러한 어려움을 입증하는 경향이 있음을 의미합니다.

258
00:44:05,000 --> 00:44:28,000
그리고 부적절한 LMS 교육에서는 방향성 표현, 주의, 아키텍처 설계의 결함이 필요하며 정렬 단계에서 기능 정렬 오류와 신념 정렬 오류도 환각 문제로 이어질 수 있습니다.

259
00:44:29,000 --> 00:44:50,000
마지막으로, 디코딩 전략은 LLMMS의 성능에도 영향을 미칠 수 있습니다. 따라서 의존도를 종합화하면 인근 콘텐츠와 소프트맥스 병목 현상으로 인해 다양한 출력 확률을 표현하는 모델의 능력이 제한될 수 있습니다.

260
00:44:51,000 --> 00:45:01,000
그리고 이러한 디코딩 전략의 고유한 무작위성과 불완전한 디코딩 표현도 이러한 환각의 원인일 수 있습니다.

261
00:45:01,000 --> 00:45:19,000
이것이 오늘의 표현에 관한 전부입니다. 우리는 현재 매우 인기 있는 8가지 대규모 다중 모드에 대해 논의하고 이들의 모든 큰 기여와 몇 가지 제한 사항을 지적했습니다. 그리고 들어주셔서 감사합니다.