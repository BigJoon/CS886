1
00:00:00,000 --> 00:00:06,720
이제 인간의 강화 학습(Reinforcement Learning from Human)을 의미하는 RLHF로 초점을 전환하겠습니다.

2
00:00:06,720 --> 00:00:08,800
피드백.

3
00:00:08,800 --> 00:00:14,000
우리가 보게 될 논문은 이 분야의 진정한 선구자입니다.

4
00:00:14,000 --> 00:00:20,560
2022년에 출판된 인간 피드백으로 지침을 따르도록 언어 모델 훈련

5
00:00:20,560 --> 00:00:24,360
오픈AI.

6
00:00:24,360 --> 00:00:30,280
하지만 먼저 RLHF를 고려하는 이유에 대한 몇 가지 동기부터 시작하겠습니다.

7
00:00:30,280 --> 00:00:31,280
LLM.

8
00:00:31,280 --> 00:00:37,840
RLHF는 AI 시스템 정렬을 위한 연구 측면에서 더 넓은 범위의 일부입니다.

9
00:00:37,840 --> 00:00:40,440
인간의 의도로.

10
00:00:40,440 --> 00:00:47,480
기본 LLM 동작은 단지 다음 단어 예측을 최적화하는 것임을 알 수 있습니다.

11
00:00:47,480 --> 00:00:52,600
이것이 목표이지만 모델이 수행하기를 원하는 작업에 대한 하나의 프록시일 뿐입니다.

12
00:00:52,600 --> 00:01:00,520
그리고 이는 꾸며낸 사실, 편파적이거나 유해한 텍스트,

13
00:01:00,520 --> 00:01:06,480
또는 단순히 지침을 따르지 않아 전반적인 정렬 불량이 발생합니다.

14
00:01:06,480 --> 00:01:12,360
이를 설명하기 위해 다음에 몇 가지 예를 들어 보겠습니다.

15
00:01:12,360 --> 00:01:17,200
첫 번째 예는 6세 어린이에게 달 착륙을 설명하라는 메시지입니다.

16
00:01:17,200 --> 00:01:19,240
몇 문장으로요.

17
00:01:19,240 --> 00:01:26,960
우리는 그것이 설명하지 않기 때문에 지침을 따르지 않는다는 것을 꽤 분명하게 알 수 있습니다.

18
00:01:26,960 --> 00:01:27,960
달 착륙.

19
00:01:27,960 --> 00:01:32,680
단지 그 문장의 동일한 구조를 따르고 계속해서 시도할 뿐입니다.

20
00:01:32,680 --> 00:01:37,160
다양한 맥락으로 다가옵니다.

21
00:01:37,160 --> 00:01:40,440
다음 질문은 '왜 새는 진짜가 아닌가?'입니다.

22
00:01:40,440 --> 00:01:50,200
글쎄, 우리는 거의 동일한 주제에 대해 대략적으로 설명하는 완성 출력을 볼 수 있지만

23
00:01:50,200 --> 00:01:54,280
결국 그 자체로 모순되고 실제로 도움이 되지 않습니다.

24
00:01:54,280 --> 00:02:03,360
따라서 우리는 모델이 완전히 진실되지 않거나 도움이 되지 않는 문제를 봅니다.

25
00:02:03,360 --> 00:02:09,080
이 마지막 예에서 프롬프트는 질문입니다. 자유주의자는 왜 그렇게 어리석습니까?

26
00:02:09,080 --> 00:02:14,120
답, 빈 공간, 그리고 완성은 마음 깊은 곳에서 알고 있기 때문이다.

27
00:02:14,120 --> 00:02:20,000
이는 명백한 독성입니다.

28
00:02:20,000 --> 00:02:27,040
전반적인 목표는 결국 사용자의 의도에 따라 행동하도록 LLM을 교육하는 것입니다.

29
00:02:27,040 --> 00:02:29,400
그러므로 우리는 두 가지를 포괄해야 합니다.

30
00:02:29,400 --> 00:02:35,080
첫 번째는 매우 자명합니다. 이는 모델이 따르도록 하는 명시적인 의도입니다.

31
00:02:35,080 --> 00:02:38,000
정확한 지시사항이 있어 도움이 됩니다.

32
00:02:38,000 --> 00:02:42,680
두 번째는 말할 것도 없이 암묵적인 의도입니다.

33
00:02:42,680 --> 00:02:51,720
따라서 인간의 가치에 더 부합하기 위해 진실되고 편견이 없으며 독성이 없거나 유해하지 않은 상태를 유지합니다.

34
00:02:51,720 --> 00:02:57,680
따라서 결국 일어나는 일은 RLHF가 인간의 선호도를 보상 신호로 사용하여 미세 조정하는 것입니다.

35
00:02:57,680 --> 00:03:01,760
LLM.

36
00:03:01,760 --> 00:03:08,120
Instruct GPT는 OpenAI가 논문에 구현한 모델의 이름입니다.

37
00:03:08,120 --> 00:03:15,040
그리고 이것이 GPT 채팅으로 이어진 디딤돌이었다고 주장할 수도 있습니다.

38
00:03:15,040 --> 00:03:18,400
그리고 우리는 이 섹션의 나머지 부분에서 이를 살펴볼 것입니다.

39
00:03:19,400 --> 00:03:28,120
세 번째, 이 연구의 주요 목표 또는 부산물은 인간의 피드백을 처음으로 사용한 것이라고 말할 수 있습니다.

40
00:03:28,120 --> 00:03:33,600
언어 모델을 미세 조정하는 것이 결과적으로 유망한 정렬 방식이 되었습니다.

41
00:03:33,600 --> 00:03:36,840
인간의 의도를 가진 이러한 모델.

42
00:03:36,840 --> 00:03:42,160
두 번째는 훨씬 더 광범위한 종류의 문어를 따를 수 있는 언어 모델을 만드는 것이었습니다.

43
00:03:42,160 --> 00:03:51,240
유해하거나 거짓된 독성 출력을 피하면서 유용하고 안전하게 지침을 제공합니다.

44
00:03:51,240 --> 00:03:57,200
그리고 결국에는 어떤 일이 일어나게 되고, 우리는 세련된 모델이 정말 잘 작동할 수 있다는 것을 알게 됩니다.

45
00:03:57,200 --> 00:04:09,120
원래 GPT3 모델인 1,750억 개의 매개변수와 달리 단 13억 개의 매개변수만 가지고 있습니다.

46
00:04:09,120 --> 00:04:15,320
이제 Instruct GPT 자체에 대해 자세히 알아보기 전에 간단한 배경 정보를 제공하고 싶습니다.

47
00:04:15,320 --> 00:04:19,560
누군가 익숙하지 않은 경우를 대비해 강화 학습에 대해 알아보세요.

48
00:04:19,560 --> 00:04:24,880
우리 모두 알고 있듯이 머신러닝은 세 가지 형태의 학습으로 나눌 수 있습니다.

49
00:04:24,880 --> 00:04:30,080
첫 번째 감독은 분류와 같은 작업에 대해 주로 작업 중심입니다.

50
00:04:30,080 --> 00:04:32,360
또는 회귀.

51
00:04:32,360 --> 00:04:35,800
그 다음은 비지도 방식으로 데이터 중심적입니다.

52
00:04:35,800 --> 00:04:39,880
따라서 레이블이 지정되지 않은 데이터를 클러스터링한다는 개념이 있습니다.

53
00:04:39,880 --> 00:04:47,200
그런 다음 의사 결정을 위한 알고리즘을 포함하는 강화 학습이 있습니다.

54
00:04:47,200 --> 00:04:52,720
따라서 환경에서 상호 작용하여 정책을 학습하려고 시도하는 모델이 있습니다.

55
00:04:52,720 --> 00:04:58,720
탐험과 착취의 개념.

56
00:04:58,720 --> 00:05:05,400
RL에 대한 직관적인 예를 제공하기 위해 우리가 알 수 없는 어둠 속에 있다고 가정해 보겠습니다.

57
00:05:05,400 --> 00:05:11,400
돌도 있고 구덩이도 있는 들판인데 우리의 목표는 이 들판을 건너는 것뿐이다.

58
00:05:11,400 --> 00:05:16,800
규칙은 우리가 구덩이에 빠지거나 바위에 부딪히면 처음부터 끝까지 시작해야 한다는 것입니다.

59
00:05:16,800 --> 00:05:18,520
시작.

60
00:05:18,520 --> 00:05:25,360
그리고 보상에 대한 개념을 갖기 위해서는 각 시도에서 우리가 취하는 단계의 수가

61
00:05:25,360 --> 00:05:29,760
우리의 포인트가 되어주세요.

62
00:05:29,760 --> 00:05:36,840
예를 들어 먼저 x걸음을 걷고 구덩이에 빠졌다고 가정해 보겠습니다.

63
00:05:36,840 --> 00:05:37,840
x 포인트.

64
00:05:37,840 --> 00:05:44,640
우리는 다시 시작하고 x걸음을 걷습니다. 그러나 이제 우리는 앞으로 나아가지 말아야 한다는 것을 알고 방향을 바꿉니다.

65
00:05:44,640 --> 00:05:49,180
우리는 왼쪽이나 오른쪽으로 돌았고, y걸음 후에 돌에 부딪혔습니다.

66
00:05:49,180 --> 00:05:53,880
그래서 우리는 이전보다 더 많은 y점을 얻었습니다.

67
00:05:53,880 --> 00:05:59,600
우리는 다시 시작해서 같은 길을 택하지만 y걸음을 지나면 또 다른 우회로를 택합니다

68
00:05:59,600 --> 00:06:04,360
우리가 돌에 부딪힐 때까지 그것은 z 걸음 이후였습니다.

69
00:06:04,360 --> 00:06:09,040
이제 우리는 이전보다 더 많은 z 포인트를 얻습니다.

70
00:06:09,040 --> 00:06:15,320
한 번 더 다시 시작하고 정확히 동일한 경로를 택하고 z 단계 후에 다른 경로를 선택합니다.

71
00:06:15,320 --> 00:06:20,160
우회해서 이번에는 드디어 들판을 건넜습니다.

72
00:06:20,160 --> 00:06:26,200
이에 대해 시각적으로 더 잘 생각하기 위해 우리가 어떤 개체, 구성 요소라고 상상할 수 있습니다.

73
00:06:26,200 --> 00:06:33,600
들판을 걷고 있는 당신, 우리가 어떻게 걷는지, 어디로 걷는지에 따라,

74
00:06:33,600 --> 00:06:41,040
넘어지거나 부딪힌 후에 점수에 대한 개념이 있으며 필드에서의 위치가 변경됩니다.

75
00:06:41,040 --> 00:06:45,600
우리가 한 발 앞서 나갈 때.

76
00:06:45,600 --> 00:06:53,200
이제 이것을 RL 전체에 대해 좀 더 일반화된 방식으로 생각해 보면 항상 몇 가지 문제가 있습니다.

77
00:06:53,200 --> 00:07:00,880
환경 내에서 작업을 수행하고 일단 해당 환경 내에서 상태를 유지하는 일종의 에이전트입니다.

78
00:07:00,880 --> 00:07:08,360
환경이 변하면 에이전트에게 보상이 돌아간다는 개념도 있습니다.

79
00:07:08,360 --> 00:07:14,280
그 피드백은 학습을 전반적으로 극대화하기 위한 일종의 정책 학습을 촉진하는 데 도움이 됩니다.

80
00:07:14,280 --> 00:07:17,880
보상.

81
00:07:17,880 --> 00:07:22,320
지금까지 많은 용어를 써왔는데, 그냥 한번쯤은 다 찾아보고 싶었을 뿐입니다.

82
00:07:22,520 --> 00:07:24,640
우리는 계속한다.

83
00:07:24,640 --> 00:07:30,960
그래서 RL에는 내가 말했듯이 상호 작용하는 엔터티인 에이전트라는 개념이 있습니다.

84
00:07:30,960 --> 00:07:34,960
환경과 함께 행동을 수행하고 보상을 수집합니다.

85
00:07:34,960 --> 00:07:40,680
환경이라는 개념이 있는데, 이는 에이전트가 수행하는 컨텍스트나 설정일 뿐입니다.

86
00:07:40,680 --> 00:07:43,000
결정을 내리기 위해 활동합니다.

87
00:07:43,000 --> 00:07:47,760
에이전트가 할 수 있는 모든 가능한 결정의 집합인 행동 공간이 있습니다.

88
00:07:47,760 --> 00:07:48,760
가지다.

89
00:07:48,840 --> 00:07:54,480
환경의 가능한 모든 구성 집합인 상태 공간이 있습니다.

90
00:07:54,480 --> 00:07:59,920
이에 대한 환경의 피드백 메커니즘인 보상이 있습니다.

91
00:07:59,920 --> 00:08:03,040
어떤 종류의 조치를 취했습니다.

92
00:08:03,040 --> 00:08:09,680
그리고 PI로 표시되는 정책이 있는데, 이는 에이전트가 수행하는 전략 또는 접근 방식입니다.

93
00:08:09,680 --> 00:08:17,280
현재 상태를 기반으로 다음 작업을 결정하는 데 사용됩니다.

94
00:08:17,320 --> 00:08:23,760
광범위하게 말하면, RL은 가치 기반, 정책 기반의 세 가지 주요 접근 방식으로 분류될 수 있습니다.

95
00:08:23,760 --> 00:08:27,800
기반 및 모델 기반.

96
00:08:27,800 --> 00:08:34,080
가치 기반 방법을 살펴보면, 우리의 목표는 일종의 가치 기능을 극대화하는 것입니다.

97
00:08:34,080 --> 00:08:41,280
이제 우리는 이것을 주어진 미래의 모든 할인된 보상의 예상 합계로 생각할 수 있습니다.

98
00:08:41,280 --> 00:08:45,000
우리가 현재 겪고 있는 특정 상태.

99
00:08:45,000 --> 00:08:49,560
따라서 작성된 방정식에서 T는 우리가 속한 현재 시간 단계입니다.

100
00:08:49,560 --> 00:08:56,880
따라서 우리의 T+1 등은 우리가 행동을 취함으로써 얻을 수 있는 미래의 보상입니다.

101
00:08:56,880 --> 00:08:59,600
S 상태의 A.

102
00:08:59,600 --> 00:09:02,400
이제 감마는 할인 요소입니다.

103
00:09:02,400 --> 00:09:06,600
미래 보상의 강도를 제어하는 ​​하이퍼 매개변수인 계수입니다.

104
00:09:06,600 --> 00:09:13,640
보시다시피 T + 2와 T + 3이 추가됩니다.

105
00:09:13,640 --> 00:09:19,680
정책은 가치함수에 내재된 개념으로, 가치함수를 극대화하기 위해 선택된 행동을 포함합니다.

106
00:09:19,680 --> 00:09:21,080
미래의 보상.

107
00:09:21,080 --> 00:09:29,080
그래서 여러분이 생각할 수 있는 몇 가지 예로는 Q 학습, 심층 QO 네트워크 등이 있습니다.

108
00:09:29,080 --> 00:09:35,280
정책 기반 방법은 더 이상 암시적으로 정책을 찾기 위해 가치 함수를 사용하지 않습니다.

109
00:09:35,280 --> 00:09:42,120
이름에서 알 수 있듯이 정책 PI를 직접 학습하고 지도에서 최적의 조치를 알려줍니다.

110
00:09:42,120 --> 00:09:47,160
이는 결정론적일 수 있습니다. 즉, 모든 상태 S에서 동일한 동작이 다음에 의해 생성됩니다.

111
00:09:47,160 --> 00:09:48,640
정책.

112
00:09:48,640 --> 00:09:54,600
또는 확률론적일 수도 있습니다. 즉, 각 행동이 특정 확률로 선택된다는 의미입니다.

113
00:09:54,600 --> 00:09:58,560
그리고 그것은 주어진 S에 대한 A의 PI로 표현됩니다.

114
00:09:58,560 --> 00:10:04,880
따라서 고려해야 할 몇 가지 예는 강화된 신뢰 영역 정책 최적화와 같은 알고리즘입니다.

115
00:10:04,880 --> 00:10:11,920
또는 근접 정책 최적화에 대해서도 나중에 실제로 살펴보겠습니다.

116
00:10:11,920 --> 00:10:17,680
정책 기반 방법은 모델 학습을 포함하기 때문에 다소 추상적입니다.

117
00:10:17,680 --> 00:10:22,840
그리고 그 모델을 통해 우리는 이제 미래를 예측할 수 있습니다.

118
00:10:22,840 --> 00:10:25,040
상태와 보상.

119
00:10:25,040 --> 00:10:29,680
문제에 크게 의존하기 때문에 모든 솔루션은 맞춤형이며,

120
00:10:29,680 --> 00:10:33,060
범용 알고리즘을 따릅니다.

121
00:10:33,060 --> 00:10:39,160
우리는 근접 정책 최적화(PPO)에 대해 조금 더 깊이 알아볼 것입니다.

122
00:10:39,160 --> 00:10:42,600
이 섹션의 뒷부분과 향후 섹션에서 다루겠습니다.

123
00:10:42,600 --> 00:10:48,240
따라서 2017년 OpenAI에 의해 도입되었으며 최첨단 정책으로 널리 간주됩니다.

124
00:10:48,240 --> 00:10:49,940
기반 알고리즘.

125
00:10:49,940 --> 00:10:55,240
실질적인 이점은 정책 업데이트를 통해 훈련 안정성을 향상시키는 방식입니다.

126
00:10:55,240 --> 00:11:00,100
큰 파괴적인 변화를 받아들이는 대신 규모를 제한하는 데 중점을 둡니다.

127
00:11:00,100 --> 00:11:03,200
오른쪽 사진을 보면 절벽이 보입니다.

128
00:11:03,200 --> 00:11:07,720
우리가 작은 발걸음을 내디딘다면, 우리는 올바른 길을 계속 갈 가능성이 더 커집니다.

129
00:11:07,720 --> 00:11:13,260
하지만 위험한 발걸음과 큰 발걸음을 내딛으면 잠재적으로 절벽에서 떨어질 수도 있습니다.

130
00:11:13,260 --> 00:11:19,080
따라서 전체적인 개념은 탐색과 착취라는 아이디어의 균형을 맞추는 것입니다.

131
00:11:19,080 --> 00:11:23,120
이전 정책과 매우 가깝습니다.

132
00:11:23,120 --> 00:11:29,640
PPO의 핵심 아이디어는 다음과 같은 새로운 목적 함수를 사용하여 정책 업데이트를 제한하는 것입니다.

133
00:11:29,640 --> 00:11:33,080
잘린 대리 목적 함수.

134
00:11:33,080 --> 00:11:38,120
앞서 언급한 것처럼 파괴적인 대규모 가중치 업데이트를 방지하도록 설계되었습니다.

135
00:11:38,120 --> 00:11:44,440
우리는 이것을 본질적으로 세타에 의해 매개변수화되는 신경망으로 생각할 수 있습니다.

136
00:11:44,440 --> 00:11:48,040
이 기능을 더 잘 이해하기 위해 다양한 부분을 살펴보겠습니다.

137
00:11:48,040 --> 00:11:49,880
먼저 비율 함수입니다.

138
00:11:49,880 --> 00:11:56,120
우리는 주어진 시간 간격 t에 대해 상태에서 행동 A를 취할 확률을 생각해 볼 수 있습니다.

139
00:11:56,120 --> 00:12:00,800
이전 정책에 대한 현재 정책의 S.

140
00:12:00,800 --> 00:12:05,480
그래서 그것은 단지 현재와 예전의 비율일 뿐입니다.

141
00:12:05,480 --> 00:12:12,200
이 비율이 1보다 크다면 이는 상태 S에서 행동 A가 다음 단계에서 일어날 가능성이 더 높다는 것을 의미합니다.

142
00:12:12,200 --> 00:12:14,680
현재 정책 대 이전 정책.

143
00:12:14,680 --> 00:12:18,680
0과 1 사이이면 현재 정책에서 반대할 가능성이 낮다는 의미입니다.

144
00:12:18,680 --> 00:12:19,680
예전 것.

145
00:12:19,680 --> 00:12:24,740
따라서 이는 두 정책 간의 차이를 추정하는 쉬운 방법일 뿐입니다.

146
00:12:24,740 --> 00:12:30,080
이제 보수적인 정책 반복의 개념인 잘리지 않은 부분을 살펴보겠습니다.

147
00:12:30,080 --> 00:12:35,440
아무런 수정 없이 확률 비율을 직접 사용하기 때문입니다.

148
00:12:35,440 --> 00:12:39,120
시간 단계 t의 모자가 장점입니다.

149
00:12:39,120 --> 00:12:44,040
그리고 이 스케일러는 정책의 평균과 비교하여 조치가 얼마나 나은지를 정량화합니다.

150
00:12:44,040 --> 00:12:46,680
해당 상태에서 동작합니다.

151
00:12:46,680 --> 00:12:51,680
값이 양수이면 정책 업데이트에서 해당 작업을 고려해야 함을 의미합니다.

152
00:12:51,680 --> 00:12:53,400
앞으로는 더 가능성이 높습니다.

153
00:12:53,400 --> 00:12:58,640
부정적인 경우 정책 업데이트에서는 향후 그러한 조치가 발생할 가능성이 낮다는 점을 고려해야 합니다.

154
00:12:59,360 --> 00:13:05,280
그러나 지나치게 큰 정책 업데이트를 방지할 수 있는 메커니즘은 아직 없습니다.

155
00:13:05,280 --> 00:13:09,440
그런 다음 잘린 부분이 나오며, 이는 잘린 대리 목표의 의미를 가져옵니다.

156
00:13:09,440 --> 00:13:10,880
기능.

157
00:13:10,880 --> 00:13:15,040
이는 기본적으로 지정된 비율을 벗어나지 않도록 비율을 자릅니다.

158
00:13:15,040 --> 00:13:18,240
엡실론으로 매개변수화된 범위.

159
00:13:18,240 --> 00:13:22,040
이 범위 내에 있으면 비율은 변경되지 않습니다.

160
00:13:22,040 --> 00:13:27,640
그러나 비율이 1 마이너스 엡실론보다 작으면 1 마이너스 엡실론으로 잘립니다.

161
00:13:27,640 --> 00:13:32,280
1 더하기 엡실론보다 크므로 1 더하기 엡실론으로 잘립니다.

162
00:13:32,280 --> 00:13:37,560
따라서 이 클리핑은 비율을 조정하지 않도록 하기 위한 가드레일일 뿐입니다.

163
00:13:37,560 --> 00:13:40,040
그것은 단순히 극단을 차단합니다.

164
00:13:40,040 --> 00:13:46,440
잘리지 않은 부분과 비교하여 이 값의 최소값을 취함으로써 기본적으로 다음과 같은 결과를 얻을 수 있습니다.

165
00:13:46,440 --> 00:13:49,840
대규모 정책 업데이트가 한 번에 형성되는 것을 방지합니다.

166
00:13:49,840 --> 00:13:56,880
그래서 Clipped Surrogate 함수를 사용하면 안정적인 학습이 가능해집니다.

167
00:13:56,880 --> 00:14:00,480
이제 GPT를 지시하기 위해 다시 관심을 돌려보겠습니다.

168
00:14:00,480 --> 00:14:07,880
따라서 이 전체 방법은 기본적으로 우리가 살펴볼 세 가지 주요 단계로 고안되었습니다.

169
00:14:07,880 --> 00:14:14,340
첫 번째 단계는 지도 학습을 통해 이러한 기본 정책을 마련하는 아이디어입니다.

170
00:14:14,340 --> 00:14:19,400
이 기본 정책을 감독된 미세 조정 모델(SFT)이라고 합니다.

171
00:14:19,400 --> 00:14:24,760
그들이 한 일은 40명의 계약자를 고용하여 사람이 작성한 데이터 세트를 컴파일하게 한 것입니다.

172
00:14:24,760 --> 00:14:29,840
다양한 프롬프트에서 원하는 출력 동작을 시연합니다.

173
00:14:29,840 --> 00:14:34,240
그리고 이러한 프롬프트는 OpenAI API에 제출되었으며 일부 라벨러가 작성한 프롬프트도 있었습니다.

174
00:14:34,240 --> 00:14:35,440
또한.

175
00:14:35,440 --> 00:14:42,120
따라서 이 데이터 세트는 결국 지도 학습의 소스가 되었습니다.

176
00:14:42,120 --> 00:14:47,960
다음 단계는 보상 모델의 개념입니다.

177
00:14:47,960 --> 00:14:54,720
RM으로 표시된 보상 모델을 훈련하기 위해 인간이 레이블을 붙인 비교 데이터 세트를 수집했습니다.

178
00:14:54,720 --> 00:15:00,000
더 큰 API 프롬프트 컬렉션의 GPT3 출력 사이.

179
00:15:00,000 --> 00:15:05,600
보상 모델은 인간이 어떤 출력을 선호할지 예측하도록 훈련되었습니다.

180
00:15:05,600 --> 00:15:09,280
그리고 그들이 최소화하려고 했던 손실 함수는 아래와 같습니다.

181
00:15:09,280 --> 00:15:13,320
조금 더 자세히 살펴보세요.

182
00:15:13,320 --> 00:15:19,600
첫 번째 X는 프롬프트이고 YW는 선호되는 완성이고 YL은 선호되지 않는 완성입니다.

183
00:15:19,600 --> 00:15:23,120
이 데이터세트에서 D.

184
00:15:23,120 --> 00:15:30,560
라벨러는 4~9 범위의 출력 순위를 지정해야 했습니다.

185
00:15:30,560 --> 00:15:35,480
그리고 이로 인해 K는 두 가지 가능한 비교를 선택하게 되었습니다.

186
00:15:35,480 --> 00:15:40,560
이것이 바로 우리가 처음에 그것을 계수로 사용하는 이유입니다.

187
00:15:40,560 --> 00:15:45,560
이제 이 함수 R은 실제로 비율 함수가 아니며 단지 보상 모델일 뿐입니다.

188
00:15:45,560 --> 00:15:50,360
그리고 선호하는 완성과 선호하지 않는 완성에 대한 스칼라 출력을 생성합니다.

189
00:15:50,360 --> 00:15:52,280
그리고 그 차이를 받아들인다.

190
00:15:52,280 --> 00:15:56,280
이를 사용하여 긍정적인 경우 기본적으로 모델에 피드백을 제공합니다.

191
00:15:56,280 --> 00:15:58,320
올바른 출력을 선택하는 것입니다.

192
00:15:58,320 --> 00:16:03,360
음수이면 바람직하지 않습니다.

193
00:16:03,360 --> 00:16:09,120
마지막으로 훈련의 마지막 단계에 도달하고 PPO가 오는 곳인 GPT를 구성합니다.

194
00:16:09,120 --> 00:16:10,800
플레이에.

195
00:16:10,800 --> 00:16:16,480
이제 필요한 모든 구성 요소가 준비되었으므로 RLHF 프레임워크를 설정할 수 있습니다.

196
00:16:16,480 --> 00:16:21,960
따라서 보상 모델은 보상 기능으로 작동하고 SFT 모델은 기본 정책입니다.

197
00:16:21,960 --> 00:16:26,320
그리고 보상을 극대화하기 위해 PPO 알고리즘을 적용합니다.

198
00:16:26,320 --> 00:16:28,880
실제로는 산적(Bandit) 환경에서 설정되었습니다.

199
00:16:28,880 --> 00:16:35,720
이제 도둑은 RL의 하위 문제이며 이것이 의미하는 바는 모든 상태가 독립적이라는 것입니다.

200
00:16:35,720 --> 00:16:37,320
이전 주에서.

201
00:16:37,320 --> 00:16:44,160
프롬프트 자체를 다음과 같이 생각할 수 있기 때문에 이는 실제로 상황에 맞는 도적입니다.

202
00:16:44,160 --> 00:16:47,880
제공되는 컨텍스트 또는 상태.

203
00:16:47,880 --> 00:16:52,640
그리고 출력이 제공되자마자 프롬프트가 무엇인지 더 이상 신경 쓰지 않습니다.

204
00:16:52,640 --> 00:16:56,200
우리는 완전히 독립적인 새로운 메시지를 받아들입니다.

205
00:16:56,200 --> 00:17:00,800
우리가 최대화하려는 목적 함수는 아래에 표시되어 있으며 조금 살펴보겠습니다.

206
00:17:00,800 --> 00:17:04,080
조금 더 깊이 들어갑니다.

207
00:17:04,080 --> 00:17:10,240
먼저 일부 매개변수, X는 데이터 세트에서 샘플링된 프롬프트이고 Y는 선택된 출력입니다.

208
00:17:10,240 --> 00:17:12,080
정책에 의해.

209
00:17:12,080 --> 00:17:14,080
보상 기능은 다음과 같습니다.

210
00:17:14,080 --> 00:17:18,760
보상 함수의 첫 번째 부분은 생성된 모델에 대한 보상 모델 스칼라 보상입니다.

211
00:17:18,760 --> 00:17:20,880
산출.

212
00:17:20,880 --> 00:17:28,120
두 번째 부분은 기본적으로 다음을 유지하는 데 사용되는 KL 콜백 Liebler 페널티입니다.

213
00:17:28,120 --> 00:17:34,760
원래 SFT 모델에 가까운 미세 조정된 모델의 동작.

214
00:17:34,760 --> 00:17:42,480
이는 정책 그라데이션이 너무 멀리 벗어나는 것을 방지하는 것인데, 이는 우리가 PPO에서 원하는 것입니다.

215
00:17:42,480 --> 00:17:49,120
매개변수 베타는 이 항의 강도를 제어하기 위한 하이퍼 매개변수일 뿐입니다.

216
00:17:49,120 --> 00:17:55,680
마지막으로 원래 GPT-3 LLM의 사전 훈련 그라디언트를 혼합하는 개념이 등장합니다.

217
00:17:55,680 --> 00:17:57,440
PPO 그라데이션.

218
00:17:57,440 --> 00:18:04,080
이를 수행하는 이유는 특정 공개 NLP 데이터세트의 성능 회귀 문제를 해결하기 위한 것입니다.

219
00:18:04,080 --> 00:18:08,800
이제 이는 실제로 두 가지 모델이 있음을 의미합니다.

220
00:18:08,800 --> 00:18:16,000
사전 훈련된 모델의 그래디언트를 혼합하는 PPO-PTX 모델이 있습니다.

221
00:18:16,000 --> 00:18:20,440
그렇지 않은 기본 PPO 모델도 있습니다.

222
00:18:20,440 --> 00:18:26,680
보시다시피 계수 감마가 있고 해당 계수가 0으로 설정되면

223
00:18:26,680 --> 00:18:32,800
본질적으로 믹싱을 수행하지 않도록 마지막 용어를 제거합니다.

224
00:18:32,800 --> 00:18:39,440
평가 세부 사항에는 인간의 선호도 평가 지표인 API 데이터를 사용했습니다.

225
00:18:39,440 --> 00:18:41,880
일련의 프롬프트에 따라.

226
00:18:41,880 --> 00:18:47,160
이를 위해 그들은 실제로 데이터 세트를 훈련 세트와 테스트 세트로 나누고 사용자 ID를 사용했습니다.

227
00:18:47,160 --> 00:18:53,000
동일한 사용자가 양쪽 끝 모두에 프롬프트를 제출하지 않도록 하기 위해 이를 분할합니다.

228
00:18:53,000 --> 00:18:58,440
또한 안전성, 진실성, 독성을 테스트하기 위해 공개 NLP 데이터 세트를 평가하고 싶었습니다.

229
00:18:58,440 --> 00:19:00,040
그리고 편견.

230
00:19:00,040 --> 00:19:06,160
이러한 데이터 세트 중 일부는 T0 및 Flan, 특히 Squad, Drop에서 사용된 데이터에서 나왔습니다.

231
00:19:06,160 --> 00:19:07,960
Heliswag 및 기타.

232
00:19:07,960 --> 00:19:13,320
특히 독성의 경우 실제 독성 프롬프트 데이터 세트를 사용했습니다.

233
00:19:13,320 --> 00:19:19,320
API 프롬프트 배포에 대해 이러한 다양한 모델을 사람이 평가하기 위해

234
00:19:19,320 --> 00:19:23,960
그들은 기본적으로 각 모델의 출력이 얼마나 자주 선호되는지 확인하고 싶었습니다.

235
00:19:23,960 --> 00:19:28,960
1,750억 개의 매개변수 SFT 모델의 결과와 비교됩니다.

236
00:19:28,960 --> 00:19:34,000
체스에서 사용되는 것과 같은 방식으로 ELO 등급으로 생각할 수 있습니다.

237
00:19:34,000 --> 00:19:41,560
두 변형 모두 InstructGPT 모델이 GPT-3 기준보다 훨씬 뛰어난 성능을 발휘한다는 것을 알 수 있습니다.

238
00:19:41,560 --> 00:19:47,840
우리는 13억 개의 매개변수 모델의 출력이 여전히 매우 선호된다는 것을 발견했습니다.

239
00:19:47,840 --> 00:19:53,840
100개의 매개변수가 있음에도 불구하고 1,750억 개의 매개변수 GPT-3 모델의 출력과 비교됩니다.

240
00:19:53,840 --> 00:19:57,600
매개변수가 몇 배 더 적습니다.

241
00:19:57,600 --> 00:20:03,520
이제 비율 또는 비율 측면에서 보급률로 측정된 메타데이터 결과를 살펴보겠습니다.

242
00:20:03,520 --> 00:20:11,880
행동이나 특성이 얼마나 많이 관찰되는지에 대한 빈도를 보면 PPO 모델이 여전히

243
00:20:11,880 --> 00:20:14,920
GPT-3 모델보다 훨씬 더 나은 성능을 발휘합니다.

244
00:20:14,920 --> 00:20:21,040
예를 들어, 올바른 지시를 시도하는 경우에는 훨씬 더 자주 발생합니다.

245
00:20:21,040 --> 00:20:25,640
따라야 할 명시적인 제약 조건이 있다는 측면에서 볼 때 여전히 다음보다 성능이 좋습니다.

246
00:20:25,640 --> 00:20:26,640
GPT-3.

247
00:20:26,840 --> 00:20:35,560
또한 환각은 무게를 줄이고 항상 언어에 적합한 맥락을 사용합니다.

248
00:20:35,560 --> 00:20:42,880
F1 점수와 정확성을 기준으로 평가되는 공개 NLP 데이터 세트를 평가할 때,

249
00:20:42,880 --> 00:20:50,000
PPO-PTX 모델이 성능 회귀 문제를 수정하고 개선한다는 것을 알 수 있습니다.

250
00:20:50,000 --> 00:20:58,000
이러한 그라디언트 혼합의 중요성을 보여주는 PPO 모델을 사용합니다.

251
00:20:58,000 --> 00:21:05,240
1~7점 척도의 Likert 점수를 기준으로 강사 GPT를 Flan NC0과 비교할 때,

252
00:21:05,240 --> 00:21:12,080
우리는 여전히 PPO-PTX 모델이 매우 선호된다는 것을 알 수 있습니다.

253
00:21:12,080 --> 00:21:16,040
이제 몇 가지 질적 결과를 살펴보겠습니다.

254
00:21:16,040 --> 00:21:21,080
먼저 GPT 강사가 외부 작업에 대해 훌륭한 일반화를 보여주었다는 것을 알 수 있습니다.

255
00:21:21,080 --> 00:21:24,480
훈련 분포의.

256
00:21:24,480 --> 00:21:30,120
우리는 대부분의 언어가 서로 다른 언어의 의도에 부합한다는 것을 알 수 있습니다.

257
00:21:30,120 --> 00:21:32,520
훈련 데이터는 영어로 되어 있었습니다.

258
00:21:32,520 --> 00:21:39,440
프로그래밍과 같은 추론 작업에도 잘 맞는다는 것을 알 수 있습니다.

259
00:21:39,440 --> 00:21:46,360
이 예에서 질문은 다양한 객관식 옵션의 출력을 찾는 것이 아닙니다.

260
00:21:46,360 --> 00:21:51,720
단지 코드의 문제에 대한 해결책을 원했을 뿐이며 강사 GPT 모델은 다음과 같습니다.

261
00:21:51,720 --> 00:21:55,680
그 지시사항은 아주 좋습니다.

262
00:21:55,680 --> 00:22:01,040
강사 GPT는 훌륭한 결과를 보여주지만 몇 가지 제한 사항을 인정해야 합니다.

263
00:22:01,040 --> 00:22:05,840
첫째, 행동은 계약업체의 피드백에 영향을 받으며,

264
00:22:05,840 --> 00:22:10,440
라벨링은 그들의 신념, 문화적 배경 및 개인적 영향을 받았을 수 있습니다.

265
00:22:10,440 --> 00:22:16,760
역사뿐만 아니라 계약자가 스펙트럼을 완전히 대표하지 않는다는 사실

266
00:22:16,760 --> 00:22:21,440
이 모델을 사용할 사용자의 수입니다.

267
00:22:21,440 --> 00:22:25,480
그리고 계약자들이 주로 영어로 말하고,

268
00:22:25,480 --> 00:22:28,720
언급한 대로 프롬프트도 주로 영어로 되어 있었습니다.

269
00:22:28,720 --> 00:22:35,800
따라서 이러한 모델은 여전히 ​​생성할 수 있기 때문에 100% 정렬 또는 완전한 안전성을 주장할 수 없습니다.

270
00:22:35,800 --> 00:22:42,560
명시적인 메시지 없이 독성, 편견, 성적인, 폭력적인 콘텐츠를 제공합니다.

271
00:22:42,560 --> 00:22:48,040
명시적인 메시지가 있는 경우 누군가 모델이 지침을 따르도록 만들 수 있습니다.

272
00:22:48,040 --> 00:22:52,880
지침이 유해한 것으로 간주되더라도 마찬가지입니다.

273
00:22:52,880 --> 00:22:58,120
강사 GPT의 일부 제한 사항을 질적으로 더 잘 이해하려면 다음을 참조하세요.

274
00:22:58,120 --> 00:23:00,040
몇 가지 예.

275
00:23:00,520 --> 00:23:05,960
양말을 먹는 것이 왜 중요한가 등 잘못된 전제로 지시를 받은 경우

276
00:23:05,960 --> 00:23:11,760
묵상한 후 이는 명백히 거짓이며 상관관계가 없다고 말하는 대신,

277
00:23:11,760 --> 00:23:16,320
모델은 전제가 참이라고 잘못 가정하고 일종의 답변을 제공하려고 합니다.

278
00:23:16,320 --> 00:23:18,800
그게 합리적인 것 같아요.

279
00:23:18,800 --> 00:23:25,760
또한 모델에 대포를 발사하면 어떻게 되는지와 같은 간단한 질문이 주어지면

280
00:23:25,760 --> 00:23:32,120
대답이 어떻게 될지 말하는 대신 빠른 속도로 호박에 직접 공을 던집니다.

281
00:23:32,120 --> 00:23:35,720
be, 그것은 단지 그것을 꿰뚫을 것이라는 것이 명확하고 간단합니다.

282
00:23:35,720 --> 00:23:41,080
질문에 대한 답은 하나도 없다고 말하고 여러 가지 가능성을 제시하려고 노력합니다.

283
00:23:41,080 --> 00:23:42,080
답변.

284
00:23:42,080 --> 00:23:47,520
마지막으로, 명령을 수행하면 모델의 성능이 저하된다는 점도 언급했습니다.

285
00:23:47,520 --> 00:23:50,680
여러 개의 명시적 제약 조건이 포함되어 있습니다.

286
00:23:50,680 --> 00:23:55,760
예를 들어, 프롬프트가 1930년대 프랑스를 배경으로 제작된 영화 10개를 나열하는 경우

287
00:23:55,760 --> 00:24:00,120
제약 조건이 너무 많아서 모델이 적절한 답변을 찾는 데 어려움을 겪습니다.