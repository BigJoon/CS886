1
00:00:00,000 --> 00:00:07,120
안녕하세요 맥스입니다. 오늘은 이미지 생성에 대해 이야기해보겠습니다.

2
00:00:07,120 --> 00:00:16,080
먼저 작업 정의와 이미지 생성의 패러다임 방법을 살펴보겠습니다.

3
00:00:16,080 --> 00:00:24,240
그런 다음 잠재 확산 모델, 이미지, 이미지 등을 포함하는 6개의 핵심 논문에 대해 이야기하겠습니다.

4
00:00:24,240 --> 00:00:32,400
Gen, Dot e2, Party, Pixel Alpha 및 Stable Diffusion XL Turbo가 있습니다. 그럼 시작해 보겠습니다.

5
00:00:33,600 --> 00:00:40,880
그렇다면 이미지 생성이란 무엇이었나요? 2015년의 맥락에서는 무조건적인 이미지가 전부였습니다.

6
00:00:40,880 --> 00:00:49,120
여기에서 볼 수 있듯이 생성된 인간 얼굴 이미지는 제어할 수 없습니다.

7
00:00:49,760 --> 00:00:58,880
그래서 기여도가 부족해요. 그리고 요즘에는 이미지 생성을 종종 텍스트로 지칭하기도 합니다.

8
00:00:58,880 --> 00:01:08,960
여기에서 볼 수 있는 이미지 합성에는 텍스트가 있습니다. 그런 다음 텍스트를 사용하여 다음을 수행할 수 있습니다.

9
00:01:08,960 --> 00:01:16,160
이미지가 텍스트에 맞춰 정렬되도록 이미지를 합성합니다. 그럼 어떻게 이루어지나요?

10
00:01:19,920 --> 00:01:25,600
따라서 최첨단 이미지 생성 방법의 경우 실제로 유사한 패러다임을 따릅니다.

11
00:01:26,800 --> 00:01:32,960
특히 텍스트 인코더를 포함한 세 가지 주요 구성 요소가 있습니다.

12
00:01:32,960 --> 00:01:41,200
생성 모델 및 대기 시간 디코더. 자세히 살펴보면 먼저 텍스트 형식이 있습니다.

13
00:01:42,160 --> 00:01:46,480
텍스트 형식을 텍스트 인코더에 맞추면 텍스트 벡터를 얻을 수 있습니다.

14
00:01:48,880 --> 00:01:54,240
그런 다음 텍스트 벡터와 입력 대기 시간을 사용하여 생성 모델에 적용합니다.

15
00:01:55,200 --> 00:02:01,280
이는 확산 모델일 수도 있고 다른 종류의 모델일 수도 있습니다. 그리고 그것은 중간체를 생산할 것입니다

16
00:02:01,280 --> 00:02:10,000
인간이 볼 수 있거나 볼 수 없는 잠재성. 그러면 이 중간 잠재성에 적합합니다.

17
00:02:10,000 --> 00:02:17,120
잠재 디코더로. 그러면 다음과 일치하는 합성 이미지를 다시 얻을 수 있습니다.

18
00:02:17,120 --> 00:02:25,280
텍스트 형태. 그래서 우리가 논의할 첫 번째 논문은 잠재 확산 모델입니다.

19
00:02:26,080 --> 00:02:33,760
이는 또한 안정적인 확산입니다. 그것이 하는 일은 기본적으로 잠재 공간을 거부한다는 것입니다.

20
00:02:33,760 --> 00:02:40,880
기존 확산 모델과 비교하여 데이터 공간 대신. 여기 보시다시피,

21
00:02:40,880 --> 00:02:48,160
인코더와 디코더를 포함하는 VAE 자동 인코더 구조를 가지고 있습니다. 그리고 그 동안

22
00:02:48,160 --> 00:02:54,400
훈련 과정에서 잠재성이 압축되었습니다. 그러나 얼마나 압축되어 있습니까?

23
00:02:59,840 --> 00:03:04,080
잠재성에 대한 다운샘플링 인자 8은 실제로 가장 좋은 구성입니다.

24
00:03:04,800 --> 00:03:10,640
잠재성은 64x64가 될 것입니다. 그리고 연구에서 그들은 다음을 발견했습니다.

25
00:03:10,640 --> 00:03:21,040
픽셀 기반 LDM에는 훨씬 더 많은 훈련 시간이 필요하고 지각 압축이 너무 많이 필요합니다.

26
00:03:21,040 --> 00:03:33,840
32와 같은 경우 전체 샘플 품질이 제한됩니다. 그러면 현재 잠재 값이 64 x 64임을 알 수 있습니다.

27
00:03:34,480 --> 00:03:41,520
도메인별 인코더는 어떻습니까? 훈련 과정 중에 다음과 같은 결과가 나타납니다.

28
00:03:42,560 --> 00:03:49,920
도메인별 인코더. 훈련 중에는 교차 어텐션(cross-attention)을 거칩니다.

29
00:03:51,920 --> 00:03:57,520
이미지 생성 방법을 제어합니다. 훈련이 끝난 후,

30
00:03:57,520 --> 00:04:05,440
추론과정에서는 이렇게 됩니다. 도메인별 인코더가 있습니다.

31
00:04:05,440 --> 00:04:11,600
이는 텍스트 인코더입니다. 그리고 세대 모델이 있습니다. 마지막으로 디코더가 있습니다.

32
00:04:13,920 --> 00:04:21,440
이는 우리가 처음에 언급한 패러다임을 따르고 있습니다. 그래서 본문과 함께,

33
00:04:22,000 --> 00:04:28,160
텍스트를 모델에 맞추고, 교차 어텐션을 통해 단위를 제어할 수 있었습니다.

34
00:04:28,160 --> 00:04:37,200
가우스 잡음을 제거하여 여기에서 중간 잠재성을 생성할 수 있습니다.

35
00:04:38,240 --> 00:04:43,360
최종적으로 디코더를 통과하여 이미지를 재구성할 수 있었습니다.

36
00:04:43,600 --> 00:04:54,880
그 내용과 일치합니다. 그렇다면 LDM의 장점은 무엇일까요? 물론 빠른 훈련을 제공하지만,

37
00:04:54,880 --> 00:05:00,080
오른쪽? 낮은 차원에서 시작되기 때문입니다. 그리고 잠재성과 침구의 분포

38
00:05:00,080 --> 00:05:07,040
정규분포에 더 가깝습니다. 그리고 또 다른 큰 장점은 맞춤형 자동차를 가질 수 있다는 것입니다.

39
00:05:07,040 --> 00:05:16,480
인코더는 텍스트뿐만 아니라 그래프, 3D 데이터 등에 적용할 수 있기 때문에

40
00:05:16,480 --> 00:05:24,720
도메인별 인코더에 따라 다릅니다. 따라서 LDM에는 세 가지 핵심 구성 요소가 있습니다.

41
00:05:25,840 --> 00:05:31,680
방금 다룬 내용입니다. 잠재성을 인코딩하고 디코딩하는 변형 자동 인코더입니다.

42
00:05:32,400 --> 00:05:40,720
그런 다음 LDM 문헌에서 새 토크나이저를 사용하는 도메인별 인코더가 있습니다.

43
00:05:40,720 --> 00:05:47,600
또는 텍스트 인코더. 그리고 그들은 확산 모델을 위한 유닛을 가지고 있습니다.

44
00:05:47,600 --> 00:05:55,440
나머지 블록과 기본 변압기 블록도 포함됩니다. 마구간의 단위에 대해 이야기합시다

45
00:05:55,440 --> 00:06:01,520
확산, 내 말은 LDM을 의미합니다. 그래서 그들은 기본적으로 휴식용 그물을 가지고 있고, 그 다음에도

46
00:06:01,520 --> 00:06:07,760
타임 스택 임베딩과 프롬프트 임베딩을 갖춘 어텐션 블록입니다. 그래서 시간은

47
00:06:07,760 --> 00:06:14,240
임베딩은 잡음 제거 프로세스를 지원하는 데 사용되고 프롬프트 임베딩은 노이즈 제거 프로세스를 제어하는 ​​데 사용됩니다.

48
00:06:14,240 --> 00:06:21,760
생성 과정. 그럼 어떻게 이루어지나요? 기본 변압기 블록에서는 교차해야 합니다.

49
00:06:21,840 --> 00:06:28,480
주의 모듈. 그래서 그것은 자기주의(self-attention), 교차주의(cross-attention)를 거쳐 앞으로 나아가는 것입니다.

50
00:06:29,280 --> 00:06:35,120
따라서 공간적 관계를 계산하기 위해 self-attention이 사용되었고, cross-attention은

51
00:06:35,920 --> 00:06:38,560
이미지 생성 시 텍스트 제어를 허용합니다.

52
00:06:42,000 --> 00:06:49,040
그렇다면 안정확산이란 무엇인가? 안정적인 확산은 실제로 안정성 AI로 훈련된 LDM입니다.

53
00:06:49,040 --> 00:06:59,840
텍스트를 이미지로 변환하기 위한 목적입니다. 그들은 총 150,000 GPU 시간 동안 AWS에서 256-800 GPU를 교육했습니다.

54
00:07:00,560 --> 00:07:08,480
그들은 그것에 거의 600,000달러를 썼습니다. 그리고 안정적인 확산은 상대적으로 큰 데이터세트를 대상으로 학습됩니다.

55
00:07:08,480 --> 00:07:14,160
5B 클립 필터 이미지 텍스트 쌍을 포함하는 Lion 5B라고 합니다.

56
00:07:14,880 --> 00:07:21,440
따라서 도메인별 인코더의 경우 클립 텍스트 인코더도 사용하기로 선택합니다.

57
00:07:21,440 --> 00:07:28,720
LDM 문헌의 폭 토크나이저 인코더 대신. 그 이유에 대해서는 나중에 이야기하겠습니다.

58
00:07:28,720 --> 00:07:38,320
섹션. 안정적인 확산 2와 XL의 경우 안정적인 확산 2에서 재훈련되었습니다.

59
00:07:39,200 --> 00:07:47,120
필터링된 Lion 5B 데이터를 처음부터 안정적으로 확산합니다. 그리고 그들은 새로운 텍스트 인코더를 사용했습니다.

60
00:07:47,120 --> 00:07:56,400
클립에서 열린 클립으로 이동했습니다. 그런 다음 SDXL의 경우 자동 인코더를 다시 교육했습니다.

61
00:07:56,400 --> 00:08:05,200
더 큰 배치 크기. 그리고 기존 유닛보다 3.5배 더 큰 유닛을 가지고 있습니다.

62
00:08:05,200 --> 00:08:09,280
그래서 더 큰 출력 해상도를 생성할 수 있었습니다.

63
00:08:12,160 --> 00:08:22,160
그럼 다음 논문으로 넘어가겠습니다. 이제 Image Gen에 대해 이야기해보겠습니다. Image Gen을 살펴보면

64
00:08:22,160 --> 00:08:31,840
아키텍처에는 제가 처음 언급한 패러다임을 따르는 것도 있음을 알 수 있습니다. 거기 보이시죠

65
00:08:32,000 --> 00:08:41,280
텍스트 인코더이자 생성 모델이자 디코더이기도 합니다. 하지만 이번에는 그 대신에

66
00:08:41,280 --> 00:08:48,400
잠재 모델은 다음 슬라이드에서 설명할 계단식 모델을 사용합니다.

67
00:08:49,920 --> 00:08:57,360
그리고 생성된 이미지를 개선하기 위해 동적 임계값이라는 키워드도 있습니다.

68
00:08:57,760 --> 00:09:06,800
그렇다면 이 논문의 특별한 점은 무엇입니까? Image Gen에는 잠재 공간이 포함되지 않습니다.

69
00:09:07,600 --> 00:09:14,480
이것은 계단식 모델입니다. 그리고 동적 임계값 지정을 통해 높은 충실도와

70
00:09:14,480 --> 00:09:21,680
포토리얼리즘 출력. 그리고 이 논문이 가져온 또 다른 중요한 통찰력은 그들이 발견한 것입니다.

71
00:09:21,680 --> 00:09:30,240
큰 텍스트 인코더를 사용하면 이미지 생성 성능이 향상됩니다.

72
00:09:31,040 --> 00:09:37,600
마지막으로 그들은 평가하기 어려운 문제 세트인 드로벤치를 제시합니다.

73
00:09:41,200 --> 00:09:47,680
계단식 확산 모델의 경우 실제로 확산 파이프라인을 사용하는 패러다임입니다.

74
00:09:47,680 --> 00:09:55,040
증가하는 해상도의 이미지를 생성할 수 있는 모델. 세부적으로는 노이즈를 사용합니다.

75
00:09:55,040 --> 00:10:01,040
컨디셔닝 강화는 목표를 달성하는 데 중요하다고 생각하는 기술입니다.

76
00:10:01,760 --> 00:10:11,280
높은 샘플 충실도. 따라서 이 Casc를 조건부로 가져오는 것을 볼 수 있습니다.

77
00:10:11,280 --> 00:10:18,560
이후 확산 모델에서는 샘플을 유지하면서 초해상도를 수행할 수 있었습니다.

78
00:10:18,560 --> 00:10:27,360
이미지의 충실도. Image Gen에서도 비슷한 기술을 사용했지만 발견했습니다.

79
00:10:27,360 --> 00:10:32,640
소음 조절 증강은 저폐기물 모델의 정보를 약화시킵니다.

80
00:10:34,160 --> 00:10:40,800
대신 초해상도 확산 모델에 텍스트 조절 기능을 도입했습니다.

81
00:10:40,800 --> 00:10:45,680
추가 정보로 성능이 향상되는 것을 발견했습니다.

82
00:10:50,560 --> 00:10:56,000
Image Gen이 도입한 다른 중요한 기술은 동적 임계값 지정에 관한 것입니다.

83
00:10:57,040 --> 00:11:02,880
따라서 기본적으로 수행하는 작업은 샘플링 단계에서 샘플의 픽셀 값을 조정하는 것입니다.

84
00:11:03,600 --> 00:11:07,920
현재 샘플의 통계에 대해 동적 범위 계산을 수행하고,

85
00:11:08,720 --> 00:11:16,880
동적 임계값을 사용하면 더 많은 광탄성을 갖는 이미지를 볼 수 있습니다.

86
00:11:16,880 --> 00:11:25,360
더 높은 충실도를 가지고 있습니다. 그럼 분류자가 없는 지침을 다시 살펴보겠습니다. 가입하는 대신

87
00:11:25,360 --> 00:11:32,160
추가 분류자를 훈련하고, 조건부 훈련에 참여하여 암시적 분류자를 얻습니다.

88
00:11:32,240 --> 00:11:38,080
무조건 확산 모델. 이것은 조건문의 매우 중심적인 조합과 같습니다.

89
00:11:38,080 --> 00:11:45,040
그리고 표본 품질과 표본 다양성을 절충하는 데 사용했던 무조건 점수 함수입니다.

90
00:11:46,320 --> 00:11:52,800
따라서 기본적으로 분류기가 없는 지침이 확산 모델에 대한 더 나은 방법입니다.

91
00:11:52,960 --> 00:12:03,520
따라서 각 샘플링 단계에서 출력은 훈련 데이터의 동일한 경계에 따라 잘립니다.

92
00:12:04,480 --> 00:12:11,520
표준 관행인 정적 임계값 지정에서는 이를 음의 일대일로 다시 클립합니다.

93
00:12:12,320 --> 00:12:18,400
이렇게 하면 이미지가 포화된 것처럼 보입니다. 그러나 Image Gen이 제안한 동적 임계값에서는

94
00:12:19,360 --> 00:12:26,080
각 출력에서 ​​먼저 특정 백분위수 절대 픽셀 값으로 임계값이 지정되었습니다.

95
00:12:26,080 --> 00:12:32,080
샘플링 단계를 거친 다음 as로 나누어 기본적으로 다시 정규화합니다.

96
00:12:32,800 --> 00:12:39,120
부정적인 일대일. 채도와 이미지로 인해 미리 프레임이 지정된 픽셀이 더욱 사실적으로 보입니다.

97
00:12:39,440 --> 00:12:49,440
Image Gen이 가져온 또 다른 중요한 통찰력은 강력한 텍스트 인코더가

98
00:12:49,440 --> 00:12:56,080
불필요하며 더 나은 텍스트 인코더를 사용하면 이미지 생성 성능이 향상되었습니다.

99
00:12:56,080 --> 00:13:05,120
단위 크기를 늘리는 것에 비해 크게 증가했습니다. 여기에서 볼 수 있듯이 Image Gen은

100
00:13:05,600 --> 00:13:13,040
매우 큰 텍스트 인코더와도 같은 T5-XXL은 최고의 클립 스코어를 달성했습니다.

101
00:13:13,920 --> 00:13:21,280
가장 낮은 FID 스코어를 달성하면서 FID와 클립 스코어가 무엇인지 설명하겠습니다.

102
00:13:21,280 --> 00:13:28,720
따라서 기본 이미지 생성 측정항목인 FID를 다시 살펴보겠습니다.

103
00:13:28,720 --> 00:13:35,120
시작 거리의 새로운 높이를 측정하여 모델의 이미지 치사율을 측정합니다.

104
00:13:36,080 --> 00:13:40,960
그래서 그것이 하는 일은 실제와 실제 사이의 일종의 거리를 계산하는 것입니다.

105
00:13:41,680 --> 00:13:48,400
생성된 데이터 포인트 분포, 그러면 거리는 범위가 됩니다.

106
00:13:48,400 --> 00:13:53,280
이는 0에서 무한대까지의 값 범위가 됩니다.

107
00:13:53,920 --> 00:14:02,400
거리를 비교하기 때문에 많은 샘플이 필요합니다.

108
00:14:02,400 --> 00:14:07,600
두 분포 사이를 정확하게 계산하려면 많은 샘플이 필요합니다.

109
00:14:09,600 --> 00:14:19,920
하지만 제 생각에는 실제로 10K 이상이면 충분히 사용할 수 있고, 또한 매우 낮습니다.

110
00:14:20,640 --> 00:14:26,880
거리는 이미지의 치사율만 측정하기 때문에 실제로 무언가를 나타낼 수 있습니다.

111
00:14:26,880 --> 00:14:35,360
그러나 모델의 다양성을 측정하지는 않습니다. 그리고 클립 점수가 있는데,

112
00:14:36,560 --> 00:14:43,600
먼저 클립에 대해 이야기해 보겠습니다. Clip은 건설적인 언어 이미지 사전 훈련입니다. 사전 훈련을 받은 것입니다

113
00:14:43,600 --> 00:14:53,360
4천만, 4억 개의 이미지 쌍을 학습한 모델입니다. 그리고 클립 점수의 경우, 단지

114
00:14:53,360 --> 00:14:59,680
클립의 지식을 사용하여 이미지 텍스트 정렬. 그래서 기본적으로 하는 일은

115
00:15:00,640 --> 00:15:09,280
클립에는 텍스트 인코더가 있고 클립에도 이미지 인코더가 있습니다. 그래서 우리는 고정된 텍스트 인코더를 사용합니다.

116
00:15:09,280 --> 00:15:15,760
이미지 인코더를 사용하고 텍스트 이미지 쌍을 각각 인코더에 맞춥니다.

117
00:15:15,760 --> 00:15:26,400
텍스트 벡터와 이미지 벡터 사이의 코사인 유사성을 계산하여 얻으려면

118
00:15:26,400 --> 00:15:36,800
텍스트 및 이미지 벡터에서 값이 높을수록 텍스트가 실제로 텍스트와 더 관련성이 높다는 것을 나타냅니다.

119
00:15:36,800 --> 00:15:43,200
영상. 점수가 낮을수록 텍스트가 이미지와 그다지 관련이 없을 수도 있음을 나타냅니다.

120
00:15:45,200 --> 00:15:53,440
하지만 실제로 클립스코어는 일반적인 클립스코어 범위처럼 0.25~0.35정도 됩니다.

121
00:15:54,560 --> 00:16:01,200
따라서 텍스트 이미지 텍스트 정렬이 실제로 잘 수행되고 있는지 확인하는 것이 상대적으로 어렵습니다.

122
00:16:06,800 --> 00:16:21,680
그런 다음 .e2에 대해 이야기하겠습니다. 보시다시피 .e2는 실제로 클립을 활용하여 이미지를 수행합니다.

123
00:16:21,680 --> 00:16:31,280
세대. 더 자세히 말하자면, 클립 잠재성을 사용하여 이를 수행했습니다. 먼저, 관절을 학습합니다.

124
00:16:31,280 --> 00:16:42,640
텍스트와 이미지를 위한 표현 공간. 그리고 생성 과정에서도 다음과 같습니다.

125
00:16:42,640 --> 00:16:49,920
인코더, 생성 모델, 디코더를 갖는 패러다임입니다.

126
00:16:51,840 --> 00:16:57,920
자세히 말하자면, 텍스트 벡터는 텍스트가 텍스트 인코더에 입력되었다는 의미입니다.

127
00:16:58,560 --> 00:17:06,640
텍스트 벡터를 얻었습니다. 그런 다음 텍스트 벡터가 확산 모델로 전달됩니다.

128
00:17:07,760 --> 00:17:13,440
이 경우 실제로는 단위라기보다는 변환기 디코더 구조에 더 가깝습니다.

129
00:17:14,880 --> 00:17:21,920
확산 모델은 입력 캡션을 조건으로 클립 이미지 임베딩을 생성했습니다.

130
00:17:22,480 --> 00:17:30,960
그런 다음 우리가 이미지 벡터라고 부르는 중간 잠재성을 생성했습니다.

131
00:17:32,720 --> 00:17:39,440
계단식 초해상도 확산 모델인 디코더로 전달되었습니다.

132
00:17:40,320 --> 00:17:46,320
클립 이미지 임베딩과 텍스트를 기반으로 이미지 조건을 생성합니다.

133
00:17:46,560 --> 00:17:59,280
그렇다면 입력 캡션에 클립 이미지 삽입 조건을 생성한 이전 모델은 무엇이었을까요?

134
00:18:00,320 --> 00:18:04,400
문헌에서 그들은 실제로 두 가지 다른 접근 방식을 시도했습니다.

135
00:18:04,400 --> 00:18:10,800
자기회귀적이다. 그들은 기본적으로 이미지를 일련의 개별 코드에 삽입하여 양자화했습니다.

136
00:18:11,360 --> 00:18:18,560
자동회귀적으로 예측하지만 이전에 확산이 실제로 수행되었다는 사실을 발견했습니다.

137
00:18:18,560 --> 00:18:24,240
확산 모델을 통해 연속적인 이미지 임베딩을 모델링했기 때문에 훨씬 더 좋았습니다.

138
00:18:25,200 --> 00:18:31,280
확산 모델은 캡션을 조건으로 한 디코더 전용 변환기입니다.

139
00:18:31,680 --> 00:18:42,720
그래서 디코더에 관해서도 디코더는 클립 이미지 임베딩과 텍스트에 따라 조절됩니다.

140
00:18:43,520 --> 00:18:47,440
클립 이미지 임베딩은 높은 수준의 의미적 의미를 포착합니다.

141
00:18:48,480 --> 00:18:54,720
디코더 모델의 잠재성은 낮은 수준의 세부 사항을 처리합니다.

142
00:18:55,600 --> 00:19:04,320
ImageGen과 매우 유사한 아이디어를 공유합니다. 왜냐하면 그들의 디코더는 기본적으로

143
00:19:04,320 --> 00:19:10,960
이미지를 생성하기 위한 하나의 기본 모델을 포함하는 계단식 초해상도 확산 모델

144
00:19:10,960 --> 00:19:19,520
64 x 64, 그리고 두 개의 초해상도 모델을 사용하여 최대 1000 x 1000까지 확장할 수 있습니다.

145
00:19:20,080 --> 00:19:27,040
따라서 다음 문서에서는 Pathways Autoregressive Text to Image Model의 약어인 PARTY를 소개합니다.

146
00:19:27,040 --> 00:19:32,640
언급된 이전 모델과 달리 PARTY는 확산 모델이 아닙니다. PARTY는 다음을 나타냅니다.

147
00:19:32,640 --> 00:19:37,280
두 가지 계열을 탐색하기 위해 ImageGen과 병행하여 수행된 Google의 연구 노력

148
00:19:37,280 --> 00:19:42,480
생성 모델은 자기회귀 모델과 확산 모델입니다. 자기회귀 접근법

149
00:19:42,480 --> 00:19:47,440
by PARTY는 이전 예측을 입력으로 사용하여 생성하는 원리에 달려 있습니다.

150
00:19:47,520 --> 00:19:52,640
이는 NLP의 언어 번역과 동의어인 방법입니다.

151
00:19:53,600 --> 00:19:58,560
특히 PARTY에서는 텍스트-이미지 생성을 시퀀스-시퀀스 모델링으로 처리합니다.

152
00:19:58,560 --> 00:20:04,240
문제는 이미지 토큰이 시퀀스에 해당하는 계열에서 예측되는 문제입니다.

153
00:20:04,240 --> 00:20:09,600
설명 텍스트 토큰. 이 연구의 동기는 다음과 같은 능력을 입증하는 것입니다.

154
00:20:09,600 --> 00:20:15,280
콘텐츠를 반영하는 충실도가 높고 사실적인 이미지를 생성하는 자동 회귀 모델

155
00:20:15,280 --> 00:20:21,920
텍스트 설명에 의해 제안된 밀도와 복잡성. 주요 기여는 다음과 같습니다.

156
00:20:21,920 --> 00:20:26,480
이 논문의. 첫째, 자동회귀에서 모델의 최첨단 성능을 강조합니다.

157
00:20:26,480 --> 00:20:32,080
텍스트에서 이미지 생성을 위한 모델링을 통해 해당 분야의 새로운 벤치마크를 설정합니다. 둘째, 다음을 표시합니다.

158
00:20:32,080 --> 00:20:36,800
PARTY가 네 가지 다른 규모에 걸쳐 입증한 개선의 일관성

159
00:20:36,800 --> 00:20:44,080
3억 5천만에서 200억 개의 매개변수를 포함하는 아키텍처의 확장성을 강조합니다.

160
00:20:44,080 --> 00:20:50,560
체계. 셋째, 이 연구에서는 다음을 제공하는 개발인 PARTY PROMPS(P2 벤치마크)를 소개합니다.

161
00:20:50,560 --> 00:20:56,400
텍스트를 이미지 모델로 평가하기 위한 포괄적인 프레임워크입니다. 마지막으로 논문에서도 인정합니다.

162
00:20:56,400 --> 00:21:02,240
PARTY 모델의 한계를 식별하여 연구 투명성의 중요성

163
00:21:02,240 --> 00:21:10,560
관찰된 오류 유형과 함께 이를 자세히 설명합니다. 그래서 PARTY는 2단 모델로 제시되고,

164
00:21:10,560 --> 00:21:16,560
자동회귀 모델과 함께 이미지 토크나이저, 디토크나이저가 있습니다. 해독하려면

165
00:21:16,560 --> 00:21:21,840
그 구조를 이해하려면 텍스트를 이미지 회로도로 다시 살펴보겠습니다. 텍스트-이미지 모델은 다음과 같이 시작됩니다.

166
00:21:21,840 --> 00:21:28,000
입력을 일련의 개별 문자로 변환하는 PARTY 2.8 텍스트 인코더인 텍스트 프롬프트

167
00:21:28,000 --> 00:21:34,000
토큰. 그런 다음 이러한 토큰은 생성 모델에 공급되어 다음과 상호 작용합니다.

168
00:21:34,000 --> 00:21:40,000
입력 잠재 공간. 그런 다음 모델은 중간 잠재성을 생성합니다. 마지막으로 잠재 디코더

169
00:21:40,000 --> 00:21:44,720
이러한 중간 표현을 해석하여 출력을

170
00:21:44,720 --> 00:21:51,840
원본 텍스트 프롬프트에 해당하는 사실적인 이미지. 그래서 훈련의 맥락에서

171
00:21:51,840 --> 00:21:57,760
PARTY를 처리하면 이미지와 텍스트가 모두 토큰으로 변환됩니다. 먼저, 이미지 토크나이저

172
00:21:57,760 --> 00:22:02,640
입력 이미지를 본질적으로 시각적인 해당 이미지 토큰으로 나눕니다.

173
00:22:02,640 --> 00:22:07,600
그림의 음절. 그런 다음 이러한 토큰은 자동 회귀 모델에 의해 처리됩니다.

174
00:22:07,600 --> 00:22:12,880
특히 변압기 네트워크의 디코더 부분. 출력은 일련의 새로운 이미지입니다.

175
00:22:12,880 --> 00:22:19,760
토큰. 마지막으로 이미지 해독기는 이러한 토큰을 일관되게 생성된 이미지로 재조립합니다.

176
00:22:19,760 --> 00:22:28,000
초기 텍스트 프롬프트를 시각적으로 나타냅니다. 그래서 여기에 구조의 개략도가 있습니다.

177
00:22:28,000 --> 00:22:33,840
논문에 제시되었습니다. 2단계 구조를 따릅니다. 이미지 토크나이저 및 디토크나이저

178
00:22:33,840 --> 00:22:38,960
다음 이미지 예측을 위한 녹색 단계와 주황색과 파란색의 자동 회귀 단계

179
00:22:38,960 --> 00:22:44,080
토큰, 텍스트 토큰과 이전 토큰을 제공합니다. 이러한 각 구성 요소는 본질적으로

180
00:22:44,080 --> 00:22:48,880
변신 로봇. 따라서 PARTY는 사실상 변환기 기반 시퀀스-시퀀스 모델입니다.

181
00:22:48,880 --> 00:22:52,800
먼저 녹색으로 표시된 이미지 토크나이저 및 디토크나이저 단계에 대해 이야기하겠습니다.

182
00:22:56,240 --> 00:23:00,880
이 첫 번째 단계에는 이미지를 개별 이미지 시퀀스로 변환하는 토크나이저가 포함됩니다.

183
00:23:00,880 --> 00:23:05,680
훈련 중 시각적 토큰 및 이미지를 재구성하는 토큰 해제 장치 훈련

184
00:23:05,680 --> 00:23:11,200
추론 중 토큰. 그러나 선형화하는 방법은 중요한 과제입니다.

185
00:23:11,200 --> 00:23:17,840
2D 이미지를 1D 패치 표현 시퀀스로 변환합니다. 문제는 이미지의 복잡성에 있습니다.

186
00:23:17,840 --> 00:23:24,320
세 개의 RGB 색상 채널이 있는 표준 256x256 픽셀 이미지라도

187
00:23:24,320 --> 00:23:30,240
래스터화된 값 196,000개. 이전 방법에서는 양자화된 표현을 학습하여 이 문제를 해결했습니다.

188
00:23:30,320 --> 00:23:36,160
이미지 패치. 그러나 PARTY 모델은 보다 세련된 솔루션을 도입합니다. 그것은

189
00:23:36,160 --> 00:23:41,280
패치 임베딩을 학습하는 토크나이저를 사용하면 지도에서 시각적 코드북에 대한 임베딩을 볼 수 있습니다.

190
00:23:41,280 --> 00:23:45,760
이 코드북의 각 항목은 잠재 공간의 인덱싱 가능한 위치에 해당합니다.

191
00:23:46,320 --> 00:23:51,440
따라서 이미지 데이터의 복잡성을 줄입니다. 이는 비전을 사용하여 달성됩니다.

192
00:23:51,440 --> 00:23:57,360
벡터 양자화된 생성적 적대 네트워크와 결합된 변환기 또는 VIT 구조,

193
00:23:57,360 --> 00:24:03,680
또는 VIT VQGaN으로 알려진 VQGaN입니다. 이 접근 방식은 이미지 데이터를 간소화할 뿐만 아니라

194
00:24:03,680 --> 00:24:08,160
관리 가능한 순서로 배열할 뿐만 아니라 이미지 정보의 풍부함도 유지합니다.

195
00:24:08,720 --> 00:24:17,200
이제 VIT VQGaN에 대해 자세히 이야기하겠습니다. VIT VQGaN을 이해하기 위해 VQVAE부터 시작하겠습니다.

196
00:24:17,200 --> 00:24:22,400
VAE는 일반적으로 연속 잠재 표현을 학습하여 입력 데이터를

197
00:24:22,400 --> 00:24:27,920
지속적인 배포. 반면, 벡터 양자화(Vector Quantized)를 의미하는 VQVAE는

198
00:24:27,920 --> 00:24:33,440
Variational Autoencoder는 이산적 잠재 표현을 학습합니다. 이러한 변화는 연속에서

199
00:24:33,440 --> 00:24:39,120
개별 인코딩은 VAE의 후방 박수와 같은 문제를 해결하는 데 도움이 될 수 있기 때문에 중요합니다.

200
00:24:39,920 --> 00:24:44,240
디코더의 능력이 너무 뛰어나기 때문에 모델은 잠재 변수를 무시합니다.

201
00:24:44,240 --> 00:24:49,040
슬라이드의 인코더 부분에 표시된 것처럼 개별 코드는

202
00:24:49,920 --> 00:24:53,680
입력 이미지를 일련의 정수로 표현합니다.

203
00:24:53,680 --> 00:24:59,120
미리 정의된 임베딩 공간의 특정 벡터. 그런 다음 디코더는 이러한 개별 코드를 변환합니다.

204
00:24:59,120 --> 00:25:09,440
재구성된 이미지로 돌아갑니다. 이는 VQVAE 아키텍처에 대한 보다 자세한 설명입니다.

205
00:25:09,440 --> 00:25:15,840
CNN으로 표현되는 인코더는 입력 이미지를 일련의 잠재 변수로 변환합니다.

206
00:25:15,840 --> 00:25:22,480
VF, X의 경우 ZE. 이러한 변수는 코드북을 사용하여 개별 잠재 공간에 매핑됩니다.

207
00:25:22,480 --> 00:25:29,040
이는 각각 차원이 D인 k개의 벡터 목록으로 구성됩니다. 이 코드북은 조회 역할을 합니다.

208
00:25:29,040 --> 00:25:33,840
각 임베딩 벡터 EI는 1부터 k까지의 인덱스와 연관되어 있습니다.

209
00:25:38,560 --> 00:25:43,440
연속 잠재 공간에서 이산 코드북으로의 매핑은 다음과 같이 수행됩니다.

210
00:25:43,440 --> 00:25:48,480
코드북인 공유 임베딩 공간에서 가장 가까운 이웃을 검색하여 생성합니다.

211
00:25:48,480 --> 00:25:55,920
이산 잠재 변수 Z. X가 주어진 사후 분포 QZ는 하나의 핫으로 정의됩니다.

212
00:25:55,920 --> 00:26:03,040
여기서 임베딩 벡터가 X의 ZE의 가장 가까운 이웃이면 1이고 그렇지 않으면 0입니다.

213
00:26:06,320 --> 00:26:12,480
그러면 X의 디코더 ZQ에 대한 입력은 해당 임베딩 벡터 EK입니다.

214
00:26:12,480 --> 00:26:15,840
여기서 k는 가장 가까운 이웃 임베딩 벡터의 인덱스입니다.

215
00:26:23,600 --> 00:26:28,160
훈련에는 X의 디코더 입력 ZQ에서 그래디언트를 복사하는 작업이 포함됩니다.

216
00:26:28,160 --> 00:26:34,560
X의 엔코더 출력 ZE에 연결됩니다. 이는 미분 불가능성에도 불구하고 경사 흐름을 허용합니다.

217
00:26:34,560 --> 00:26:40,480
개별 잠재 공간의. 여기서 네트워크를 훈련하는 데 사용되는 이 손실 함수는 다음과 같습니다.

218
00:26:40,480 --> 00:26:45,600
세 부분 중. 첫 번째는 재건 손실(Reconstruction Loss)로, 두 항목 간의 차이를 측정합니다.

219
00:26:45,600 --> 00:26:51,200
원본 이미지와 재구성된 이미지. 두 번째는 코드북 손실입니다.

220
00:26:51,200 --> 00:26:59,680
인코더를 향한 코드북 벡터 E는 X의 ZE를 출력하므로 E가 더 나은 표현입니다.

221
00:26:59,680 --> 00:27:05,520
엔코더 출력의 SG는 여기서 방지하는 정지 그라데이션 작동을 나타냅니다.

222
00:27:05,520 --> 00:27:13,040
인코더 출력으로 흐르는 그라데이션. 세 번째는 불이익을 주는 약속 손실입니다.

223
00:27:13,040 --> 00:27:19,120
인코더는 선택된 코드북 임베딩 E에서 벗어나기 위해 X의 ZE를 출력합니다.

224
00:27:19,760 --> 00:27:28,800
인코더가 코드북 할당을 준수하는지 확인합니다. 베타는 하이퍼파라미터 밸런싱입니다

225
00:27:28,800 --> 00:27:34,240
용어. 이 손실 함수는 인코더 표현이 다음과 유사하다는 것을 집합적으로 보장합니다.

226
00:27:34,240 --> 00:27:39,120
효율적인 재구성을 허용하면서도 코드북 임베딩을 허용합니다. 그리고 그것은

227
00:27:39,120 --> 00:27:43,840
인코더 출력을 대표하는 임베딩을 가짐으로써 코드북의 무결성을 보장합니다.

228
00:27:49,680 --> 00:27:55,040
다음으로 VQVAE 구조를 기반으로 구축되었지만 몇 가지 차이점이 있는 VQGAN을 살펴보겠습니다.

229
00:27:55,920 --> 00:28:02,240
훈련에서 VQVAE는 픽셀에 초점을 맞춘 표준 재구성 손실인 L2 손실을 사용합니다.

230
00:28:02,240 --> 00:28:09,520
현명한 정확성. 그러나 VQGAN은 지각 손실, 특히 학습된 지각 이미지를 선택합니다.

231
00:28:09,520 --> 00:28:16,400
패치 유사성 측정항목. 지각 손실은 지각 손실을 측정하여 L2 손실과 다릅니다.

232
00:28:16,400 --> 00:28:22,720
이미지 패치 간의 유사성. 또한 VQGAN에는 판별자가 포함되어 있습니다.

233
00:28:22,720 --> 00:28:28,320
실제 이미지 패치와 재구성된 이미지 패치를 구별하도록 훈련되었습니다. 샘플링의 경우,

234
00:28:28,320 --> 00:28:33,760
VQVAE는 훈련을 통해 코드북에 있는 잠재 코드보다 먼저라는 범주를 학습합니다.

235
00:28:33,760 --> 00:28:40,720
이미지의 분포를 모델링하기 위해 종종 픽셀 cnn과 같은 자동 회귀 모델을 사용합니다.

236
00:28:40,720 --> 00:28:47,200
잠재 코드. VQGAN은 다음 코드북 인덱스를 예측하여 이 접근 방식을 향상시킵니다.

237
00:28:47,200 --> 00:28:53,600
훈련 이미지의 잠재 코드로 훈련된 자동회귀 변환기 모델, 개선될 가능성이 있음

238
00:28:53,600 --> 00:28:58,320
변환기의 기능을 활용하여 합성된 이미지의 일관성과 품질을 향상합니다.

239
00:28:58,320 --> 00:29:09,680
장거리 종속성을 캡처합니다. VQGAN의 회로도는 다음과 같습니다. 첫 번째 단계에는 다음이 포함됩니다.

240
00:29:09,680 --> 00:29:19,760
벡터 양자화 손실을 사용하여 인코더 e, 디코더 g 및 판별기 d 훈련

241
00:29:19,760 --> 00:29:31,680
그리고 GAN 손실. 손실은 VQVAE와 유사하며 GAN 손실과

242
00:29:31,680 --> 00:29:42,320
L2에서 지각 상실로의 수정. 2단계에서 VQGAN은 변환기를 다음과 같이 교육합니다.

243
00:29:42,560 --> 00:29:47,360
교차 엔트로피 손실을 사용하여 코드북 항목 간의 관계를 학습합니다.

244
00:29:50,000 --> 00:29:55,440
다음으로 VIT VQGAN은 다음 수정을 통해 VQGAN을 더욱 향상시킵니다.

245
00:29:56,560 --> 00:30:02,880
첫째, cnn 인코더와 디코더를 모두 VIT로 대체합니다. 그 다음에는 그것도 소개했다

246
00:30:02,880 --> 00:30:08,960
코드북 사용을 개선하기 위한 코드북 최적화. 먼저, 인수분해된 코드를 사용합니다.

247
00:30:08,960 --> 00:30:13,840
이는 인코더 출력에서 ​​저차원 잠재 공간으로의 선형 투영에 적용됩니다.

248
00:30:13,840 --> 00:30:20,000
코드 인덱스 조회용. 이는 코드 임베딩과 조회를 분리합니다. 둘째, L2를 적용한다.

249
00:30:20,000 --> 00:30:26,400
X의 인코더 출력 잠재 변수 ZE 및 코드북 잠재 변수 E에 대한 정규화.

250
00:30:27,040 --> 00:30:32,880
따라서 결과적인 유클리드 거리는 사실상 X와 E의 ZE의 코사인 유사성입니다.

251
00:30:33,120 --> 00:30:41,200
마지막으로, 손실 함수를 여러 손실의 조합으로 수정했습니다.

252
00:30:41,200 --> 00:30:49,920
지각 손실과 Laplace 손실을 포함하여 VQVAE와 VQGAN 모두에 도입되었습니다.

253
00:30:52,560 --> 00:30:58,480
따라서 이것은 논문에 제시된 VIT VQGAN 구조의 개략도입니다.

254
00:30:59,200 --> 00:31:04,000
여기 오른쪽 부분은 VIT VQGAN의 코드북 최적화 부분을 보여줍니다.

255
00:31:04,000 --> 00:31:08,640
여기에는 고차원 공간에서 저차원 공간으로의 선형 투영이 포함됩니다.

256
00:31:13,120 --> 00:31:20,800
이는 VIT VQGAN 논문의 전체 회로도입니다. VQGAN과 유사하지만 대체만 가능

257
00:31:20,880 --> 00:31:24,480
변환기 인코더와 디코더를 갖춘 CNN.

258
00:31:27,440 --> 00:31:33,200
이제 우리는 파티 1단계로 돌아왔습니다. 이는 이미지 토크나이저의 전체 회로도이며

259
00:31:33,200 --> 00:31:41,360
VIT VQGAN을 사용한 토큰 해독기 교육. 파티에서는 초해상도 업 샘플러를 사용하여

260
00:31:41,360 --> 00:31:47,760
출력 이미지를 256x256에서 1024x1024로 업샘플링합니다.

261
00:31:50,480 --> 00:31:54,800
이제 자기회귀 모델이 포함된 파티 2단계로 넘어갑니다.

262
00:31:55,520 --> 00:31:58,640
본질적으로 인코더-디코더 변환기입니다.

263
00:32:01,920 --> 00:32:07,280
인코더는 텍스트 입력을 처리하고 이를 텍스트 토큰으로 변환하도록 설계되었습니다.

264
00:32:07,360 --> 00:32:13,520
샘플 텍스트 코퍼스를 기반으로 어휘 크기가 16,000인 문장 조각 모델을 활용합니다.

265
00:32:14,240 --> 00:32:20,880
최대 토큰 길이 용량은 128입니다. 한편 디코더는 예측에 중점을 둡니다.

266
00:32:20,880 --> 00:32:27,280
스테이지에서 생성된 이미지 토큰을 입력으로 사용하여 시퀀스의 다음 이미지 토큰

267
00:32:27,280 --> 00:32:33,440
이미지 토크나이저. 디코더의 출력은 1024 이미지의 고정 렌즈로 구성됩니다.

268
00:32:33,440 --> 00:32:41,120
256 x 256 이미지의 32 x 32 토큰 그리드에 해당하는 토큰입니다.

269
00:32:46,240 --> 00:32:53,680
따라서 파티의 네 가지 크기 변형은 3억 5천만, 7억 5천만, 30억, 200억으로 훈련되었습니다.

270
00:32:53,680 --> 00:33:00,960
매개변수. 인코더와 디코더는 표준 변압기를 기반으로 하며 이 구성은

271
00:33:00,960 --> 00:33:06,800
디코더가 더 많은 레이어를 갖도록 이미지 토큰을 생성하기 위해 더 큰 디코더를 선호합니다.

272
00:33:10,960 --> 00:33:15,520
따라서 추가 설계 고려 사항을 위해 저자는 텍스트 인코더 사전 훈련을 사용했습니다.

273
00:33:15,520 --> 00:33:20,080
모델을 웜 스타트할 의도로 시도했지만 도움이 거의 되지 않는 것으로 나타났습니다.

274
00:33:20,800 --> 00:33:26,000
따라서 사전 훈련 후 접착제 벤치마크에서 BERT와 비슷한 성능을 발휘하지만 성능이 저하됩니다.

275
00:33:26,000 --> 00:33:31,840
전체 인코더 디코더 훈련 후. 그들은 이러한 차이점과 통일성을 나열했습니다.

276
00:33:31,840 --> 00:33:36,960
향후 연구를 위한 일반적인 언어 이해와 시각적 기반 언어 이해.

277
00:33:41,040 --> 00:33:47,200
다음은 텍스트 인코더 사전 훈련이 있는 경우와 없는 경우의 훈련 손실 그래프입니다. 본 것처럼,

278
00:33:47,200 --> 00:33:57,040
여기서는 차이가 작습니다. 따라서 샘플링을 위해 저자는 분류자 없는 지침을 적용합니다.

279
00:33:57,040 --> 00:34:02,640
일반 간의 정렬을 장려하는 데 사용되는 자동 회귀 모델의 컨텍스트

280
00:34:02,640 --> 00:34:09,840
샘플 및 텍스트 프롬프트. 이는 Make a Scene의 접근 방식과 유사합니다. 그들은 또한 사용합니다

281
00:34:09,840 --> 00:34:15,200
Dali에서 사용된 접근 방식과 유사한 텍스트 프롬프트당 일괄 샘플 이미지로 순위를 다시 매깁니다.

282
00:34:15,920 --> 00:34:21,280
이미지의 정렬 점수와 텍스트 임베딩을 기반으로 출력의 순위를 다시 매깁니다.

283
00:34:21,280 --> 00:34:28,400
대조 캡션 작성자의 모델. 다음은 PARTY의 교육 파이프라인을 요약한 것입니다.

284
00:34:28,400 --> 00:34:35,040
1단계에서는 3,200만 개의 매개변수 인코더로 구성된 VITB Cougar 훈련에 중점을 둡니다.

285
00:34:35,040 --> 00:34:43,040
토크나이저에서 사용되며 훨씬 더 큰 5억 9900만 개의 매개변수 디코더가

286
00:34:43,040 --> 00:34:49,840
해독기. 이 단계에서는 또한 8192 이미지 토큰 클래스 세트를 학습합니다.

287
00:34:49,840 --> 00:34:57,440
코드북을 만듭니다. 이러한 시각적 어휘를 학습하는 것 외에도 판별자와 슈퍼

288
00:34:57,440 --> 00:35:04,480
해상도 업샘플러는 이미지 품질을 향상하도록 훈련되었습니다. 인코더-디코더 프레임워크가 완료되면

289
00:35:04,480 --> 00:35:12,080
사전 훈련된 Detokenizer는 두 번째 단계 훈련 중에 미세 조정됩니다.

290
00:35:12,080 --> 00:35:17,200
학습된 표현의 일관성을 보장하기 위해 토크나이저 및 코드북이 수정되었습니다.

291
00:35:18,640 --> 00:35:24,000
이 단계의 손실은 VITB Cougar에서 사용된 손실과 동일합니다.

292
00:35:28,400 --> 00:35:34,400
2단계로 이동하여 인코더-디코더 자동회귀 모델을 훈련합니다. 여기,

293
00:35:34,400 --> 00:35:40,640
텍스트 인코더는 사전 훈련되어 있습니다. 그런 다음 인코더-디코더는 다음을 사용하여 함께 훈련됩니다.

294
00:35:40,640 --> 00:35:47,200
모델이 다음 이미지 토큰을 보호하는 방법을 학습하도록 보장하는 Softmax 교차 엔트로피 손실

295
00:35:47,200 --> 00:35:53,200
효과적으로, 1단계에서 토크나이저가 생성한 토큰 시퀀스를 기반으로 합니다.

296
00:35:58,640 --> 00:36:05,120
추론 중에 텍스트 프롬프트는 입력을 텍스트로 변환하는 텍스트 인코더를 통해 전달됩니다.

297
00:36:05,120 --> 00:36:11,680
토큰. 그런 다음 이러한 토큰은 해당 토큰을 생성하는 자동 회귀 변환기 디코더에서 사용됩니다.

298
00:36:11,680 --> 00:36:16,560
분류자가 없는 지침을 통합하여 이미지 토큰 간의 정렬을 향상시킵니다.

299
00:36:16,560 --> 00:36:23,680
생성된 이미지와 프롬프트. 그런 다음 이미지 토큰은 토큰 공간에서 다음으로 변환됩니다.

300
00:36:23,680 --> 00:36:31,920
VITB Cougar의 디코더를 사용하여 토큰화 해제 프로세스를 통해 이미지 공간을 생성합니다. 탈중앙화 이후,

301
00:36:31,920 --> 00:36:38,960
생성된 이미지는 더 높은 해상도를 달성하기 위해 업샘플링될 수 있습니다. 다양성을 보장하고 선택하기 위해

302
00:36:38,960 --> 00:36:45,360
최상의 결과를 얻으려면 모델은 텍스트 프롬프트당 16개의 이미지를 생성합니다. 그런 다음 이러한 이미지가 적용됩니다.

303
00:36:45,360 --> 00:36:51,360
코캠 모델의 이미지 정렬 점수와 생략된 텍스트를 기반으로 순위를 다시 매깁니다.

304
00:36:54,560 --> 00:37:00,960
평가에 사용된 데이터 세트는 MS-Coco 데이터 세트와 Cocoa 하위 세트입니다.

305
00:37:00,960 --> 00:37:06,960
현지화된 서술형 데이터세트. 현지화된 내러티브 데이터세트에는 다음과 같이 훨씬 더 자세한 프롬프트가 있습니다.

306
00:37:07,760 --> 00:37:15,040
여기. 그런 다음 저자는 1600개가 넘는 프롬프트 세트인 파티 프롬프트도 개발합니다.

307
00:37:15,040 --> 00:37:21,040
각 프롬프트는 12개 카테고리의 광범위한 카테고리와 연관되어 있습니다. 예를 들어,

308
00:37:21,040 --> 00:37:26,880
동물, 차량, 세계지식, 초록 등 11개 부문의 도전과제 차원으로 구성되어 있습니다.

309
00:37:27,840 --> 00:37:31,120
예를 들어 기본, 수량, 단어 또는 기호입니다.

310
00:37:33,600 --> 00:37:38,880
제로샷 및 미세 조정된 MS-Coco 및 현지화된 내러티브에 대한 FID 점수 결과는 다음과 같습니다.

311
00:37:39,520 --> 00:37:44,240
Cocoa 데이터세트를 당시의 다른 최첨단 텍스트-이미지 모델과 비교했을 때,

312
00:37:44,240 --> 00:37:50,800
검색 기준선도 마찬가지입니다. 검색 기준선은 단순히 훈련에서 이미지를 검색합니다.

313
00:37:50,880 --> 00:37:57,280
텍스트 프롬프트 임베딩과 이미지 임베딩 간의 정렬 점수를 기반으로 하는 데이터세트입니다.

314
00:37:58,480 --> 00:38:06,800
보시다시피 파티는 제로 샷 및 미세 조정 FID 측면에서 다른 최첨단 모델보다 성능이 뛰어납니다.

315
00:38:06,800 --> 00:38:14,720
점수. 다음은 MS-Coco 데이터 세트에 대한 인간의 평가를 기반으로 한 몇 가지 결과입니다.

316
00:38:14,720 --> 00:38:23,120
XMCGAN 모델과 검색 기준을 통해 당사자가 XMCGAN보다 성능이 우수하다는 것을 보여줍니다.

317
00:38:23,120 --> 00:38:29,600
이미지 사실성과 이미지 텍스트 일치를 비교하고 이미지 텍스트 일치를 기반으로 한 검색 기준보다 성능이 뛰어납니다.

318
00:38:31,920 --> 00:38:37,280
새로운 정당 프롬프트에서 결과는 더 큰 200억 정당 모델이 더 나은 성과를 보인다는 것을 보여줍니다.

319
00:38:38,240 --> 00:38:41,520
30억 모델과 검색 기준선.

320
00:38:43,040 --> 00:38:50,240
다음 논문은 사진처럼 사실적인 텍스트-이미지 변환을 위한 확산 변환기의 빠른 훈련인 Pixar Alpha입니다.

321
00:38:50,240 --> 00:38:56,080
합성. 이 문서에서는 확산 변압기의 기능을 활용하는 것을 목표로 합니다.

322
00:38:56,080 --> 00:39:00,000
최첨단 모델에 필적하는 텍스트-이미지 모델을 훈련합니다.

323
00:39:00,880 --> 00:39:05,520
이 방법은 독립적으로 최적화하기 위한 훈련 전략을 설계함으로써 동기가 부여됩니다.

324
00:39:05,520 --> 00:39:12,400
세 가지 중요한 구성 요소, 픽셀 종속성, 텍스트-이미지 정렬 및 이미지 미적 품질입니다.

325
00:39:13,040 --> 00:39:18,880
Pixar Alpha는 또한 최대 1024픽셀의 고해상도 이미지 합성을 지원하려고 시도합니다.

326
00:39:19,440 --> 00:39:23,520
교육 비용을 대폭 절감하고 효율성을 향상하여 이를 달성합니다.

327
00:39:25,520 --> 00:39:30,720
논문의 핵심 기여는 다음과 같습니다. 첫째, 모델의 훈련 시간

328
00:39:30,720 --> 00:39:37,440
안정적인 확산 버전 1.5에 필요한 시간의 10.8%만 필요하므로 매우 짧습니다.

329
00:39:37,440 --> 00:39:41,120
또한 훈련 데이터의 1.25% 미만을 사용합니다.

330
00:39:42,000 --> 00:39:47,840
둘째, 집중을 통해 프로세스를 향상시키는 3단계 교육 전략을 소개합니다.

331
00:39:47,840 --> 00:39:54,560
모델 초기화에 이어 목표 사전 학습 및 미세 조정 단계가 이어지며, 각 단계는

332
00:39:54,560 --> 00:40:00,880
특정 하위 작업을 간소화하도록 설계된 단계입니다. 셋째, 교차 관심 모듈 도입

333
00:40:00,880 --> 00:40:08,160
확산 변환기 DIT 아키텍처 내에서 텍스트 조건 주입을 개선하는 동시에

334
00:40:08,160 --> 00:40:15,600
특히 계산 집약적인 작업에서 전체 계산 프로세스를 개선합니다.

335
00:40:15,600 --> 00:40:21,840
클래스 조건 분기. 마지막으로 대규모 비전 언어 모델을 활용합니다.

336
00:40:21,920 --> 00:40:24,640
밀도가 높은 의사 캡션에 자동으로 라벨을 지정하는 용암.

337
00:40:27,280 --> 00:40:31,200
훈련 전략의 첫 번째 단계는 픽셀 종속성을 학습하는 것입니다.

338
00:40:31,760 --> 00:40:36,800
이 단계는 자연 이미지의 픽셀 분포를 이해하는 데 전념합니다.

339
00:40:36,800 --> 00:40:42,880
본질적으로 모델에 고유한 구조를 가르치는 텍스트 프롬프트의 간섭

340
00:40:42,880 --> 00:40:48,640
시각적 데이터 내에서 일관성이 발견됩니다. 이는 클래스 조건을 사용하여 수행됩니다.

341
00:40:48,640 --> 00:40:56,480
확산 변압기, DIT Excel 2, 사전 훈련된 28개의 변압기 블록 포함

342
00:40:56,480 --> 00:41:04,160
ImageNet 데이터 세트에서. 이 방법론을 통해 Pixar Alpha는

343
00:41:04,160 --> 00:41:12,960
시각적 정보. 이 모델은 300,000개의 학습 단계를 통해 100만 개의 이미지로 학습되었습니다.

344
00:41:13,200 --> 00:41:19,360
따라서 이 문서에서는 확산 변환기 아키텍처를 활용합니다.

345
00:41:20,560 --> 00:41:25,440
이는 잠재 확산의 전통적인 단위 백본에서 아키텍처 변경을 중심으로 합니다.

346
00:41:25,440 --> 00:41:31,600
모델을 변압기 기반 모델로 변환합니다. 확산 트랜스포머는 노이즈를 변환하여 먼저 작동합니다.

347
00:41:31,600 --> 00:41:37,680
선형 레이어 및 재형성을 거쳐 정규화 및 다중 처리를 통해 처리됩니다.

348
00:41:37,680 --> 00:41:46,320
DIT 블록. 이러한 블록은 다중 헤드 self-attention 및 포인트별 피드포워드로 구성됩니다.

349
00:41:46,320 --> 00:41:53,520
스케일링 및 정규화가 산재된 레이어. 확산과 같은 조건부 입력

350
00:41:53,520 --> 00:42:02,480
시간 단계와 클래스 라벨은 AMLP를 통해 변환기에 통합됩니다. 의 적응

351
00:42:02,560 --> 00:42:08,960
적응형 레이어 표준 레이어가 장착된 VIT 블록 또는 비전 변환기 블록

352
00:42:09,600 --> 00:42:16,400
유연한 정규화를 보장합니다. 게다가 Pixar Alpha와 이것의 통합은

353
00:42:16,400 --> 00:42:23,840
아키텍처는 250-60 주파수 임베딩과 2계층 MLP를 활용합니다.

354
00:42:26,400 --> 00:42:32,080
그래서 2단계에서는 텍스트 이미지 정렬 학습을 강화하는 데 중점을 둡니다. 사전 훈련에서 전환

355
00:42:32,080 --> 00:42:40,640
클래스 기반 이미지 생성과 텍스트-이미지 생성 사이의 정확한 정렬이 필요합니다.

356
00:42:40,640 --> 00:42:47,840
자세한 텍스트 설명 및 생성된 이미지. 따라서 이 단계에서 중요한 문제가 발생합니다.

357
00:42:47,840 --> 00:42:53,280
유익한 콘텐츠가 부족한 데이터 세트의 텍스트 캡션에서. 정렬이 잘못됨

358
00:42:53,280 --> 00:42:57,840
해당 이미지를 사용하거나 학습이 가능한 긴 꼬리 분포를 제시합니다.

359
00:42:57,840 --> 00:43:03,760
어려운가요? 따라서 이러한 문제에 대응하기 위해 Pixar Alpha는 다음과 같은 솔루션을 구현합니다.

360
00:43:03,760 --> 00:43:09,360
대규모 비전 언어 모델인 Lava의 도움으로 매우 유익한 텍스트 캡션을 생성합니다.

361
00:43:09,920 --> 00:43:17,040
Pixar Alpha는 자동 라벨링 파이프라인에서 Lava를 활용하여 정보성을 향상시키는 것을 목표로 합니다.

362
00:43:17,040 --> 00:43:23,600
텍스트 데이터의 학습 효율성과 정렬 정확도를 향상시킵니다.

363
00:43:24,320 --> 00:43:31,920
다음은 Lyon 데이터세트의 원시 캡션과 Lava 정제 캡션의 몇 가지 예입니다.

364
00:43:32,640 --> 00:43:39,600
보시다시피 개선을 통해 텍스트 이미지 정렬 불량, 설명 부족,

365
00:43:39,600 --> 00:43:43,520
그리고 드물게 사용되는 어휘로 인해 이러한 긴 꼬리 특성이 발생합니다.

366
00:43:44,320 --> 00:43:54,400
자세한 데이터 세트 개선 프로세스는 다음과 같습니다. 따라서 Lava를 사용하여 데이터 세트는 다음을 수행합니다.

367
00:43:54,400 --> 00:44:02,880
유익한 설명을 장려하는 프롬프트가 포함된 자동 라벨링 프로세스. 그래서 특히

368
00:44:02,880 --> 00:44:10,000
프롬프트는 이 이미지와 스타일을 매우 자세하게 설명하도록 요청합니다. 그래서 SAM을 사용합니다.

369
00:44:10,000 --> 00:44:17,120
Lyon 대신 세그먼트 데이터 세트가 더 강력한 어휘로 이어지는 경향이 있기 때문에

370
00:44:17,120 --> 00:44:23,680
어휘 분석을 보여줍니다. 따라서 이 분석은 Lava가 SAM과 Lyon을 개선했음을 보여줍니다.

371
00:44:23,680 --> 00:44:31,840
데이터 세트는 총 명사의 수와 전체에 대한 유효한 고유 명사의 비율이 증가합니다.

372
00:44:31,840 --> 00:44:41,120
뚜렷한 명사. 따라서 유효한 고유명사는 더 자주 나타나는 명사를 의미합니다. 그리고 SAM Lava는 가능합니다

373
00:44:41,120 --> 00:44:50,560
Lyon Lava보다 성능이 좋기 때문에 사용됩니다. 이 단계에서는 1,000만 개의 SAM 캡션이 정제됩니다.

374
00:44:51,280 --> 00:45:00,720
따라서 Pixar Lava 교육의 3단계에서는 세련된 이미지를 사용하여 고해상도 이미지를 생성하는 데 중점을 둡니다.

375
00:45:00,720 --> 00:45:08,800
세트의 품질입니다. 따라서 이 단계에서는 우수한 품질의 데이터를 미세 조정하고

376
00:45:08,800 --> 00:45:17,040
특히 JourneyDB의 4백만 개의 이미지 데이터세트와 1천만 개의 내부 이미지 데이터세트를 결합합니다.

377
00:45:18,000 --> 00:45:26,720
이 단계의 교육은 이전 단계에서 확립된 기초 작업의 이점을 얻습니다.

378
00:45:26,720 --> 00:45:36,800
단계로 인해 수렴 속도가 빨라집니다. 그래서 표에는 세 가지 척도가 있음을 보여줍니다.

379
00:45:36,800 --> 00:45:46,000
이 단계의 훈련에 사용되는 해상도입니다. 그리고 해상도가 256x256에서 증가함에 따라

380
00:45:46,000 --> 00:45:54,720
1024x1024로 변경하면 이 단계에서 필요한 훈련 단계가 줄어들고 배치 크기도 줄어듭니다.

381
00:45:55,920 --> 00:46:01,760
이는 입력 데이터의 높은 품질로 인한 학습 프로세스의 최적화를 반영합니다.

382
00:46:01,760 --> 00:46:08,240
이전 훈련 단계에서 얻은 누적 지식. 여기에 몇 가지 디자인 고려 사항이 있습니다.

383
00:46:08,240 --> 00:46:16,640
이 논문에서 제시되었습니다. 그래서 DIT XL2를 기본 아키텍처로 사용하고 43억

384
00:46:16,640 --> 00:46:25,200
Flan T5는 조건부 특징 추출을 위한 텍스트 인코더이자 사전 훈련 및

385
00:46:25,200 --> 00:46:30,000
잠재 특징 추출을 위한 잠재 확산 모델의 동결 VAE 인코더.

386
00:46:30,960 --> 00:46:37,760
그리고 여기 DIT 블록 내에서는 다중 모자 교차 관심이 상호 작용을 촉진하는 데 사용됩니다.

387
00:46:37,760 --> 00:46:42,640
모델과 언어 모델에서 추출된 텍스트 임베딩 사이.

388
00:46:45,920 --> 00:46:51,120
그래서 Pixar와 Alpha의 저자는 원래의 적응형 레이어 표준 레이어를 수정했습니다.

389
00:46:51,120 --> 00:46:57,280
더 효율적으로 만드는 확산 변압기. 그들은 적응형 예측에서 선형 투영이 가능하다는 것을 발견했습니다.

390
00:46:58,240 --> 00:47:06,080
DIT의 레이어 표준 레이어 모듈은 상당한 비율을 차지하며 이는 전체의 27%입니다.

391
00:47:06,080 --> 00:47:12,560
매개변수. 그리고 이렇게 많은 수의 매개변수는 클래스 조건이 적합하지 않기 때문에 유용하지 않습니다.

392
00:47:12,560 --> 00:47:20,080
텍스트-이미지 생성 모델에 사용됩니다. 그래서 그들은 적응형 레이어 표준 단일을 제안했습니다.

393
00:47:21,040 --> 00:47:28,560
여기서 블록의 모든 항목은 시간 조건에 대해 동일한 매개변수를 공유합니다. 레이어별 글로벌 MLP

394
00:47:28,560 --> 00:47:36,560
레이어별 MLP 대신 임베딩이 사용됩니다. 그래서 이 새로운 구조에 적응하기 위해,

395
00:47:36,560 --> 00:47:42,320
교대 및 스케일을 적응적으로 조정하기 위해 레이어별 훈련 가능한 임베딩이 도입되었습니다.

396
00:47:43,280 --> 00:47:48,800
클래스 조건 없이 다른 블록에 있습니다. 따라서 이는 다음과의 호환성을 유지합니다.

397
00:47:49,760 --> 00:47:54,720
사전 훈련된 확산 변환기에서 동일한 스케일과 이동을 생성하도록 초기화될 때.

398
00:47:55,920 --> 00:48:03,120
다음은 FID 점수를 다음과 비교하는 MsCoCo 데이터 세트에 대한 몇 가지 실험 결과입니다.

399
00:48:03,120 --> 00:48:09,360
다른 최신 모델. 비록 가장 낮은 FID 점수를 달성하지는 못했지만,

400
00:48:10,400 --> 00:48:16,560
매개변수 수, 훈련 이미지 수, GPU 일수가 가장 낮습니다.

401
00:48:19,520 --> 00:48:26,080
정렬 평가를 위한 벤치마크인 T2ICOM 벤치의 결과는 다음과 같습니다.

402
00:48:26,080 --> 00:48:32,400
텍스트 조건과 생성된 이미지 사이. 그리고 이것은 Pixar Alpha가

403
00:48:33,680 --> 00:48:39,840
속성 바인딩과 개체 관계 분야에서 매우 경쟁력이 있으며,

404
00:48:40,480 --> 00:48:47,760
뿐만 아니라 복잡성. 그래서 저자들은 인간의 평가에 대한 실험도 진행했습니다.

405
00:48:47,760 --> 00:48:56,560
300개의 프롬프트에서 생성된 이미지를 기반으로 합니다. 이를 위해 50명의 참가자가 다음을 기반으로 모델 순위를 매겼습니다.

406
00:48:56,560 --> 00:49:04,720
지각 품질 및 텍스트 이미지 정렬. 보시다시피, 결과는 Pixar Alpha가

407
00:49:04,720 --> 00:49:10,480
품질과 정렬 측면에서 다른 모델보다 뛰어납니다.

408
00:49:10,480 --> 00:49:21,120
마지막 논문으로 넘어가기 전에 무엇이 좋은 세대 모델을 만드는지 생각해 보고 싶습니다.

409
00:49:22,640 --> 00:49:28,800
여기서 볼 수 있듯이 확산 모델은 빠른 샘플링을 수행할 수 없습니다.

410
00:49:30,240 --> 00:49:39,360
좋은 모델 범위와 다양성을 제공하고 고품질을 생산하지만

411
00:49:39,920 --> 00:49:48,240
이미지에서 치사율이 높은 경우, 이를 방지할 수 있는 최첨단 방법이 있는지 궁금합니다.

412
00:49:48,880 --> 00:49:56,800
샘플링 속도가 더 빠른 확산 모델이 있나요? 이것은 우리를 최종 논문으로 인도합니다.

413
00:49:56,800 --> 00:50:06,080
오늘 소개해드릴 제품은 SDXL Turbo 입니다. ADD라는 기술을 선보입니다.

414
00:50:06,800 --> 00:50:14,160
이는 텍스트를 이미지로 변환하는 새로운 증류 기술인 역직렬 확산 증류(Adverse Serial Diffusion Distillation)입니다.

415
00:50:14,160 --> 00:50:23,440
모델. 그리고 분류자 없는 안내가 가능한 SDXL Turbo의 몇 가지 특별한 속성이 있습니다.

416
00:50:23,440 --> 00:50:31,040
여기서는 사용되지 않았습니다. 그리고 이 논문의 주요 하이라이트는 단일 단계를 수행할 수 있다는 것입니다.

417
00:50:31,040 --> 00:50:38,400
이미지 생성 속도가 매우 빠릅니다. 실시간으로 처리할 수도 있습니다.

418
00:50:39,120 --> 00:50:46,320
원래 SDXL에서는 50단계 정도를 수행해야 하지만 SDXL Turbo에서는 기본적으로 한 단계로 수행합니다.

419
00:50:48,320 --> 00:50:55,600
그래서 직관적으로 실제로 확산 모델의 우수한 샘플 품질을 결합하고 있습니다.

420
00:50:56,320 --> 00:51:03,520
뿐만 아니라 GAN의 고유한 속도도 마찬가지입니다. 그럼 이 논문을 자세히 살펴보겠습니다.

421
00:51:05,360 --> 00:51:12,480
하지만 먼저, Adverse Serial Models, Generative Adverse Serial Networks, GAN을 다시 살펴보겠습니다.

422
00:51:13,440 --> 00:51:21,520
훈련 분포를 포착하는 샘플을 합성하도록 최적화하는 생성기가 있습니다.

423
00:51:21,520 --> 00:51:28,720
데이터. 그런 다음 실제 데이터와 가짜 데이터를 분류하기 위해 최적화하는 판별기도 있습니다.

424
00:51:29,840 --> 00:51:35,920
따라서 훈련 과정에서 생성기는 수많은 가짜 이미지를 생성합니다.

425
00:51:37,360 --> 00:51:43,520
훈련 세트, 실제 훈련 데이터, 가짜 데이터는 모두

426
00:51:43,520 --> 00:51:48,400
판별자는 어느 것이 가짜인지 진짜인지 구별해야 합니다.

427
00:51:48,480 --> 00:51:57,440
생성자와 판별자는 동시에 훈련됩니다.

428
00:51:57,440 --> 00:52:02,880
훈련 중 각 시대마다 더 나아져야 합니다.

429
00:52:02,880 --> 00:52:11,280
공정한 싸움을 통해 서로에게서 배울 수 있습니다. 목표는 한 지점에서 배우는 것입니다.

430
00:52:12,000 --> 00:52:19,440
해당 판별자는 생성자의 가짜 이미지가 진짜인지 가짜인지 구분할 수 없습니다.

431
00:52:19,440 --> 00:52:24,080
따라서 해당 기간에는 생성기가 성공적으로 연결되었음을 의미합니다.

432
00:52:24,720 --> 00:52:30,960
그 후, 판별자는 쓰레기통에 버려질 수 있으며 우리는 오직

433
00:52:30,960 --> 00:52:41,360
발전기. SDXL Turbo에는 실제로 학생-교사 모델이 있으므로

434
00:52:41,360 --> 00:52:48,800
학생이 상대적으로 짧은 시간 단계를 거치는 증류 과정

435
00:52:49,760 --> 00:52:54,480
교사 모델은 예약을 위해 긴 시간 단계를 사용합니다.

436
00:52:55,040 --> 00:53:01,680
훈련 과정에서 먼저 잠재된

437
00:53:03,840 --> 00:53:08,240
이는 기본적으로 다음을 추가하여 순방향 확산 프로세스에 의해 계산됩니다.

438
00:53:09,280 --> 00:53:18,640
점차적으로 소음이 발생하면 여기 XS가 생기고 발전기로 전달됩니다.

439
00:53:19,280 --> 00:53:27,440
학생 모델이에요. 우리는 이것을 훈련시켜서 이미지를 생성할 것입니다.

440
00:53:28,080 --> 00:53:36,880
그것이 우리가 여기서 원했던 것 같습니다. 그런 다음 판별기에 입력되어 그것이 맞는지 여부를 알려줍니다.

441
00:53:37,600 --> 00:53:45,520
가짜든 아니든. 이는 GAN 방법론과 매우 유사하지만 동시에

442
00:53:46,240 --> 00:53:58,320
합성된 출력에는 점진적으로 다시 노이즈가 추가되어 교사 모델에 공급되었습니다.

443
00:53:58,320 --> 00:54:04,480
이것은 다시 생성하려고 시도하는 동결 확산 모델입니다.

444
00:54:05,120 --> 00:54:11,200
하지만 이번에는 기울기를 계산하지 않고 증류 법칙을 계산합니다.

445
00:54:11,200 --> 00:54:17,840
생성기에서 생성된 이미지 사이에 어떤 종류의 거리 측정법이 있는지 확인합니다.

446
00:54:18,400 --> 00:54:21,760
또한 교사 확산 모델에서 생성된 이미지도 있습니다.

447
00:54:23,520 --> 00:54:29,840
학생 모델의 그라디언트에 페널티를 주어 학습할 수 있도록 했습니다.

448
00:54:30,480 --> 00:54:34,880
선생님, 제 말은 학생이 선생님 모델처럼 되는 방법을 배울 수 있다는 뜻입니다.

449
00:54:35,200 --> 00:54:43,120
이 경우 학생이 결국 교사보다 더 나은 성과를 낼 수 있을 것으로 예상되었습니다.

450
00:54:43,920 --> 00:54:48,800
또한 판별자가 말할 수 없도록 판별자를 속이는 동시에

451
00:54:49,520 --> 00:54:56,080
그것은 사실이거나 가짜입니다. 이는 학생이 성공적으로 훌륭한 신디사이저가 되었음을 의미합니다.

452
00:54:57,920 --> 00:55:04,320
그런 다음 생성 프로세스는 원패스 모델과 같아집니다. 원패스가 필요하지는 않지만

453
00:55:04,960 --> 00:55:10,800
매우 짧은 시간 간격으로 추론할 수 있으므로 간단히 노이즈를 맞출 수 있습니다.

454
00:55:11,440 --> 00:55:17,840
그런 다음 일종의 기술 조건을 사용하여 생성할 수 있습니다.

455
00:55:18,640 --> 00:55:23,360
실시간으로 계산할 수 있도록 매우 낮은 시간 간격으로 생성할 수 있습니다.

456
00:55:24,320 --> 00:55:36,320
그래서 자세하게는 발전기를 다시 유닛처럼 네트워크에서 사용했는데,

457
00:55:36,320 --> 00:55:45,440
생성기로서의 픽셀 공간 단위. 판별자는 다음을 갖춘 비전 변환기인 반면

458
00:55:45,440 --> 00:55:54,560
dyno 대물렌즈와 Teacher 모델, 논문에서는 SDXL과 SD 2.1을 Teacher 모델로 사용했습니다.

459
00:55:55,520 --> 00:56:04,240
훈련 목표를 위해 그들은 적대적인 전통적인 이득 손실을 사용합니다.

460
00:56:04,240 --> 00:56:10,960
그런 다음 GP 이득에서 나누어지는 기울기 페널티와 함께 ​​힌지 손실을 사용합니다.

461
00:56:10,960 --> 00:56:18,880
종이. 그리고 증류 손실의 경우 이는 단순히 CT가 가중 L2 손실인 것입니다.

462
00:56:19,760 --> 00:56:24,800
그들에 의해 정의된 시간 단계에 따른 알파 t입니다.

463
00:56:29,280 --> 00:56:36,800
요약하자면, 이 프레젠테이션에서는 다음을 포함하여 6개의 중추적인 텍스처 이미지 모델을 다룹니다.

464
00:56:37,440 --> 00:56:44,160
잠재성을 압축잠재와 같이 이용한 최초의 연구인 잠재분할모델

465
00:56:44,160 --> 00:56:51,040
그리고 교차주의도요. 그런 다음 계단식 분할 모델인 이미지 생성 종이가 있습니다.

466
00:56:51,040 --> 00:57:00,000
텍스트를 이미지 생성으로 수행합니다. 다음으로는 클립 임베딩을 활용하는 Thor E2가 있습니다.

467
00:57:00,000 --> 00:57:07,920
그리고 고품질 이미지를 합성하는 잠재성. 그럼 우리는 변압기 기반의 파티를 가집니다.

468
00:57:07,920 --> 00:57:14,000
자동 회귀 모델과 확산 변환기인 픽셀 알파가 있습니다.

469
00:57:14,960 --> 00:57:23,440
마지막으로 SDXL의 적대적 증류를 활용한 SDXL 터보가 있습니다.

470
00:57:24,320 --> 00:57:29,840
우리는 또한 t2i에 대한 몇 가지 일반적인 측정 항목을 다루었습니다. 즉, 다음을 포함한 텍스트-이미지 모델을 의미합니다.

471
00:57:30,720 --> 00:57:38,160
이미지 충실도를 측정하는 근막염 시작 거리와 측정할 클립 점수

472
00:57:38,160 --> 00:57:48,480
이미지 텍스트 정렬. 실제로 탐색할 더 많은 t2i 모델이 있으므로 여기에서 확인할 수 있습니다.

473
00:57:49,280 --> 00:57:58,960
우리가 방금 다루었던 6개 이상의 위도와 우리의 프레젠테이션을 듣는 사람들이

474
00:57:59,600 --> 00:58:03,840
이 분야에 대한 관심이 더 높아질 것입니다. 감사합니다.