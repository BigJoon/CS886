1
00:00:00,000 --> 00:00:04,240
제 이름은 입샤 바이란입니다. 나는 작가입니다. 나는 우리가 어떻게 할 수 있는지에 대해 이야기 할 것입니다

2
00:00:06,000 --> 00:00:11,440
알고리즘을 보다 효율적으로 수행합니다. 첫 작품을 시작하기 전에 꼭 하고 싶은 말은

3
00:00:11,440 --> 00:00:15,680
첫 번째 칩의 기반을 형성하는 이 두 가지 논문을 제안합니다. 나는 당신이 그런 사람이라는 것을 압니다.

4
00:00:16,560 --> 00:00:22,720
응. 따라서 이것은 첫 번째 논문의 기반, 즉 알고리즘 네트워크와 분기를 형성할 것입니다.

5
00:00:23,520 --> 00:00:32,800
이것이 알고리즘의 모습입니다. 그래서 우리는 모든 레이어를 통해 결론을 내립니다.

6
00:00:32,800 --> 00:00:40,720
그런 다음 자동 회귀적으로 문서를 운반합니다. 그래서 얕은 저자들은

7
00:00:40,720 --> 00:00:50,080
약한 네트워크를 실험하면서 그들은 각 칩이 어떻게 진화했는지 알아보기 위해 실험을 했습니다.

8
00:00:51,040 --> 00:00:57,440
과거. 그래서 그들은 각 층을 분류했고, 어느 시점 이후에 그것을 보았습니다.

9
00:00:57,440 --> 00:01:06,080
과거에는 모델이 진화하는 경향이 있습니다. 이제 이것은 무엇입니까? 점이 보이면 x축은 레이어를 나타내고,

10
00:01:06,800 --> 00:01:15,120
그리고 우리는 두 개의 메트릭 예측 엔트로피를 가지고 있는데, 이는 이것이 다음 중 하나일 수 있음을 보여줍니다.

11
00:01:15,120 --> 00:01:20,400
모델이 얼마나 자신감이 있는지 측정합니다. 그리고 또 하나는 알고리즘 라인입니다.

12
00:01:22,240 --> 00:01:25,040
모델이 얼마나 정확한지 보여줍니다. 그럼 우리는 그 후에 그것을 볼 수 있습니다

13
00:01:27,440 --> 00:01:38,000
네, 최대 10개 레이어까지는 모델의 신뢰도가 혼란스러워지지만 더 부정확해집니다.

14
00:01:38,000 --> 00:01:44,560
따라서 모델은 부정확한 예측에 대해 자신감을 갖고 있습니다. 이것이 그들이 생각하는 것에 대해 부르는 것입니다.

15
00:01:45,520 --> 00:01:54,880
모델. 그렇다면 이 시점에서 우리는 무엇을 종료했습니까? 따라서 조기 종료의 전반적인 메커니즘은 다음과 같습니다.

16
00:01:54,880 --> 00:02:03,280
과거에는 조기 종료 메커니즘이 무엇인지입니다. 응, 그래서 이렇게 보이는구나. 그래서 우리는

17
00:02:05,200 --> 00:02:12,240
중간 계층 중 하나의 출력을 취하고 우리가 했던 것처럼 자동 회귀 방식으로 진행합니다.

18
00:02:15,200 --> 00:02:22,960
이제 어떤 지표를 사용하여 종료 시점을 어떻게 결정합니까? 따라서 얕은 심층 네트워크는 예측을 사용합니다.

19
00:02:22,960 --> 00:02:31,680
종료할 확률과 분기 행위는 종료하기 위해 엔트로피 점수를 사용합니다. 따라서 예에서는

20
00:02:31,680 --> 00:02:39,600
임계값은 0.9입니다. 따라서 측정항목이 이 임계값을 초과할 때마다 해당 레이어에서 나갑니다.

21
00:02:39,600 --> 00:02:48,320
그래서 이것은 핀 9 방법입니다. 그리고 네, 물러나는 방법은 신뢰도 점수가 더 높다는 것입니다

22
00:02:48,320 --> 00:02:52,400
확률이 높다고 해서 반드시 작업자가 옳다는 것을 의미하는 것은 아닙니다.

23
00:02:54,480 --> 00:03:04,960
맞습니다. 다른 방법 중 하나는 인내심을 갖는 것입니다. 여기서는 하이퍼 매개변수를 설정합니다.

24
00:03:05,760 --> 00:03:14,000
예를 들어 P로 설정됩니다. 따라서 레이어의 모델 출력이 T에 대해 동일하게 유지되면 더하기

25
00:03:14,000 --> 00:03:20,000
하나의 연속 레이어가 있으면 T + 하나의 레이어에서 종료됩니다. 따라서 이 예에서 환자는 1명입니다.

26
00:03:20,560 --> 00:03:31,040
따라서 레이어의 출력이 두 번 연속 동일하게 유지되면 여기서 종료합니다.

27
00:03:31,360 --> 00:03:43,280
층. 예, 이 접근 방식은 Albert 모델을 기본 모델로 사용하고 뷰벤치에서 평가됩니다.

28
00:03:43,280 --> 00:03:51,520
모델. 그리고 약 1.5배의 속도 향상을 볼 수 있습니다. 그리고 여기서 주목해야 할 점은 모든 사람이

29
00:03:51,520 --> 00:03:57,360
Tasking Viewbench 모델, 개발 세트 및 테스트 세트의 정확도가 향상됩니다.

30
00:03:57,360 --> 00:04:06,320
기본 모델부터. 그렇다면 왜 그렇습니까? 그래서 저자는 이것이 가능한 한 다음과 같이 주장하고 증명합니다.

31
00:04:06,320 --> 00:04:13,040
조건이 유지되면 모델의 정확도가 높아질 것입니다. 이에 대해서는 자세히 다루지 않겠습니다.

32
00:04:13,040 --> 00:04:20,640
지금은 자세히 설명하고 있지만 시간이 허락하는 대로 다시 논의할 수 있습니다. 그리고 일부, 일부 환자들에게는

33
00:04:22,160 --> 00:04:26,160
특정 범위의 환자에 대해 볼 수 있는 다양한 작업에 대한 모델의 정확도가 향상됩니다.

34
00:04:26,160 --> 00:04:33,040
정확성 및 정확성과 같은 x 부분은 레이어이고 환자는입니다.

35
00:04:36,240 --> 00:04:45,040
예, 이제 저자는 모델에 따라 다르다고 주장하고 있습니다. 이 메커니즘은 또한

36
00:04:45,040 --> 00:04:52,480
개발 세트 공격에 따라 달라집니다. 따라서 이 텍스트 풀러 모델은 바로 그 모델입니다.

37
00:04:52,560 --> 00:05:01,280
침입하면 우리는 텍스트를 볼 수 있습니다. 따라서 일부 토큰, 일부 단어를 해당 토큰으로 변경한다는 것은 무엇을 의미합니까?

38
00:05:02,720 --> 00:05:08,480
이를 동의어로 대체하면 출력 모델이 변경됩니다. 그러니까 이게 한 종류야.

39
00:05:08,480 --> 00:05:21,920
토큰 조작이라고 불리는 공격입니다. 그래서 이것은, 예, 그래서 이 공격은 성공적으로 작동합니다

40
00:05:22,480 --> 00:05:28,800
새 모델, 새 기본 모델이며 정확도가 8.96에서 5.5로 감소합니다. 그래서 공격은

41
00:05:28,800 --> 00:05:39,360
꽤 잘 간다. 그리고 이러한 초기 종료 단계 메커니즘을 사용한 후에는 공격 후 정확도가 향상됩니다.

42
00:05:39,360 --> 00:05:44,560
5.5에서 9.3으로 증가합니다. 그래서 공격에 다소 의존합니다. 이제 왜 이런 일이 발생합니까?

43
00:05:44,560 --> 00:05:51,600
모델이 특정 시점에서 올바른 예측에 도달하는 것을 확인했기 때문입니다.

44
00:05:51,600 --> 00:06:00,720
파워 패스를 시도했지만 그 이후에는 출력을 잘못 분류했습니다. 따라서 파워 패스에서 일찍 나가면

45
00:06:03,840 --> 00:06:06,960
일반적으로 정확도가 높아집니다.

46
00:06:09,600 --> 00:06:16,960
예, 일부 입력 샘플에서는 작동합니다. 그래서 전반적으로

47
00:06:16,960 --> 00:06:28,880
관찰에 따르면 일부 입력 샘플의 경우 계산처럼 통과할 필요가 없습니다.

48
00:06:28,880 --> 00:06:35,120
대신 모든 레이어를 중간에서 종료할 수 있습니다. 응, 이게 처음으로 다른 거였어

49
00:06:35,120 --> 00:06:43,760
여기에 질문이 있습니다. 이전 페이지로 돌아갈 수 있나요? 그래서 당신이 여기서 의존하고 있는 공격은

50
00:06:43,840 --> 00:06:48,880
이 논문에서는 가공되지 않은 문장이나 진술 중 일부가

51
00:06:48,880 --> 00:06:56,080
내가 생각하는 예에 대한 더 많은 진술을 생성합니다. 그래서 그것은 제가 말하고자 하는 것 중 하나입니다.

52
00:06:58,080 --> 00:07:04,800
이 메커니즘이 의존한다고 말할 수 있는 장점 중 하나이므로 전반적인 목표는

53
00:07:05,440 --> 00:07:10,000
이것을 보여주지만 이는 메커니즘의 결과 중 하나입니다.

54
00:07:10,560 --> 00:07:18,000
공격. 알았어, 응. 추가 공격에 대한 방어는 다음과 같습니다.

55
00:07:18,000 --> 00:07:21,840
일반 LLM 모델을 위해 구축된 특정 유형의 공격. 누군가가 자신이 사용하고 있다는 것을 알고 있다면

56
00:07:21,840 --> 00:07:26,960
이러한 유형의 요새 회로 시스템은 설계상의 적대적 공격처럼 방어할 뿐입니다.

57
00:07:26,960 --> 00:07:34,720
이 자신감을 이용하세요. 동맹 공격을 방어하는 능력에 대한 평가입니다.

58
00:07:34,720 --> 00:07:38,160
왜냐하면 해당 모델에서 사용할 수 없는 공격을 방어하고 있기 때문입니다.

59
00:07:38,160 --> 00:07:43,280
따라서 이 토큰 조작, 즉 특정 종류의 공격에 따라 달라지는 한 가지는

60
00:07:43,280 --> 00:07:54,160
네, 이것을 수용하는 방법을 배우는 사람들을 훈련시킬 수도 있습니다.

61
00:07:54,160 --> 00:07:58,560
하지만 정확도에 5459포인트 정도만 포함된다는 것을 알면 다소 다릅니다.

62
00:07:58,560 --> 00:08:03,040
하지만 예, 빠르면 5시쯤에 학습할 수도 있습니다. 질문이 하나 더 있습니다.

63
00:08:03,760 --> 00:08:30,240
그래서 제 주요 질문은 만약 우리가 좀 더 일반적인 작업을 다루고 있다면, 이 확신이 어떤가요?

64
00:08:30,240 --> 00:08:33,440
그게 여전히 효과적이라고 생각하시나요, 아니면 이것이 정말 효과가 있다고 생각하시나요?

65
00:08:33,440 --> 00:08:42,240
우리가 샘플 상황을 다룰 때? 좋아요. 이 특정 메커니즘에 대해서는 잘 모르겠습니다.

66
00:08:42,240 --> 00:08:50,640
하지만 일종의 교정 세트를 사용하는 다른 기술도 있습니다. 그리고 그것은 좋아한다

67
00:08:50,640 --> 00:08:56,480
다음으로 소개할 내용은 교육입니다. 세트에서 보정되어 다음과 같은 정보를 제공합니다.

68
00:08:56,480 --> 00:09:04,400
일부 유한한 표본 통계적 동일성. 따라서 유한 샘플에서는 작동하지만 교정 세트는

69
00:09:04,400 --> 00:09:08,560
비용을 지불하고 교정 세트를 업데이트하고 개별 기관에서 작업할 수 있습니다.

70
00:09:11,040 --> 00:09:14,240
이에 대해 잘 모르겠지만 작동하는 다른 방법이 있으므로 이를 일반화하겠습니다.

71
00:09:16,800 --> 00:09:19,840
PABY와 쉘의 차이점은 무엇입니까?

72
00:09:20,160 --> 00:09:30,960
네, PABY는 환자를 사용하고 Shell은 분기 확률을 사용하는 것입니다.

73
00:09:31,680 --> 00:09:35,680
쉘의 분기 확률과 분기의 엔트로피.

74
00:09:36,560 --> 00:09:38,560
좋아요.

75
00:09:46,080 --> 00:09:52,720
네, 이 알고리즘 적응형 언어 모델도 초기 종료 기반 메커니즘입니다.

76
00:09:52,720 --> 00:09:57,920
그러나 이는 효율성을 높이는 주요 방법을 제공합니다.

77
00:09:59,280 --> 00:10:05,360
결과 예측의 품질에 대한 확신을 유지함으로써 가능합니다. 그래서 전반적으로

78
00:10:06,320 --> 00:10:15,760
출력 순서를 확인하고 품질을 확인합니다. 우리는 출력 시퀀스가 ​​높은지 확인합니다.

79
00:10:15,760 --> 00:10:22,400
컴퓨팅은 또한 다른 문제도 해결합니다. 그러면 긍정적으로 더 많은 통찰력을 얻거나

80
00:10:22,400 --> 00:10:30,480
사무실 테이블에서 더 많은 통찰력을 얻으세요. 그래서 조기 퇴출 기반에서 원하는 한 가지는

81
00:10:30,560 --> 00:10:38,240
메커니즘은 예, 주의 메커니즘에서 숨겨진 상태를 계산하려는 경우

82
00:10:38,240 --> 00:10:43,200
어떤 레이어의 레이어에는 항상 이전 레이어의 숨겨진 상태가 필요합니다.

83
00:10:43,200 --> 00:10:50,960
하지만 여기서 나가면 이러한 숨겨진 상태가 나타나지 않습니다. 그래서 우리는 무엇을

84
00:10:50,960 --> 00:10:57,360
해야 할 일은 숨겨진 상태를 레이어 아래로 전파하는 것입니다. 따라서 기본적으로 숨겨진 상태를 복사합니다.

85
00:10:58,320 --> 00:11:10,480
그래서 여기의 저자들은 그것이 정확성에 미치는 영향을 찾으려고 노력합니다. 그래서 그들은 이 일을 하면서

86
00:11:10,480 --> 00:11:17,840
해당 레이어의 출력이 있는 레이어에서 나가는 주문 콘테스트를 수행합니다.

87
00:11:17,840 --> 00:11:26,960
지상 증거와 일치합니다. 따라서 이 경우 이 레이어의 출력은 y1과 같습니다. 그래서 그들은 퇴장한다

88
00:11:26,960 --> 00:11:36,400
이 레이어에서 다른 모든 유형도 마찬가지입니다. 따라서 출력이 갖게 될 유일한 차이는

89
00:11:37,200 --> 00:11:43,360
이는 상태 복사 메커니즘 때문입니다. 그러니까 이건 그들이 펼친 주문 콘테스트였지

90
00:11:44,560 --> 00:11:52,720
모델의 정확도가 38.32에서 38.24로 약간 감소한 것으로 나타났습니다.

91
00:11:52,720 --> 00:12:01,840
감소하여 8개 레이어 중 평균 1.53개 레이어를 사용하였다. 따라서 이는 두 가지를 나타냅니다.

92
00:12:02,400 --> 00:12:09,360
상태 복사 메커니즘이 작동하고 컴퓨팅을 절약할 수 있는 잠재력이 크다는 점

93
00:12:09,600 --> 00:12:12,320
기본 폴리 종료 기반 메커니즘만 사용하는 경우.

94
00:12:19,760 --> 00:12:26,560
그런 다음 저자는 두 가지 종류의 교란도 수행했습니다. 첫 번째는 샘플링 기반입니다.

95
00:12:27,520 --> 00:12:33,920
그들은 한 시간 단계에서 샘플링했고, 10위 토큰을 샘플링했고, 그 외의 모든 시간에는 샘플링했습니다.

96
00:12:33,920 --> 00:12:39,360
단계, 그들은 샘플링했고, 사용 가능한 첫 번째 순위 토큰을 크게 샘플링했습니다. 그래서 샘플링을

97
00:12:39,360 --> 00:12:45,280
그들이 선택한 레이어를 기반으로 나는 대답했고 그들은 출력을 선택했습니다.

98
00:12:45,280 --> 00:12:53,600
첫 번째 레이어와 위와 마찬가지로 다른 모든 레이어의 경우 마지막 레이어에서 크게 샘플링되었습니다.

99
00:12:54,880 --> 00:13:01,920
그리고 그들은 샘플링 기반의 경우 10위 토큰을 선택해야 한다는 것을 알게 되었습니다.

100
00:13:02,880 --> 00:13:09,600
초기 추론 과정에서 시간 단계는 모델에 큰 영향을 미치며,

101
00:13:09,600 --> 00:13:19,120
정확도가 많이 떨어집니다. 그리고 그것이 나중에 시간 단계에서 작동한다면 정확도는

102
00:13:19,120 --> 00:13:26,080
그렇게 많이 떨어지지는 않네요. 그리고 레이어 기반의 경우 주황색 선이 한 가지 더 있습니다.

103
00:13:27,040 --> 00:13:33,760
추론 초기에 이 섭동을 수행하더라도 그런 경우는 삭제되지 않습니다.

104
00:13:33,760 --> 00:13:38,960
전반적으로 이것이 보여주는 것은 모델이 이전 단계에서 섭동을 받는 경우입니다.

105
00:13:38,960 --> 00:13:50,160
성능에 부정적인 영향을 미칩니다. 그래서 그들이 한 일은 서로 다른 기준점을 설정하기로 결정한 것입니다.

106
00:13:50,160 --> 00:13:55,680
다른 시간 단계에 대해. 따라서 이전 시간 단계에는 더 높은 임계값을 사용하고 더 낮은 임계값을 사용할 수 있습니다.

107
00:13:55,760 --> 00:14:06,960
나중 시간 단계를 위해. 또는 우리가 할 수 있는 것처럼 람다에 대한 붕괴 함수를 사용할 수 있습니다.

108
00:14:08,880 --> 00:14:12,640
온도에 따라 구성하십시오. 따라서 처음에는 더 높은 값을 갖게 됩니다.

109
00:14:13,440 --> 00:14:18,400
임계값과 나중에 우리는 시간 단계에서 다른 시간으로 임계값을 낮췄습니다.

110
00:14:18,800 --> 00:14:24,800
이제 이러한 임계값을 어떤 측정항목에 적용합니까? 그래서 그들은 세 가지를 실험했습니다.

111
00:14:26,320 --> 00:14:34,320
자신감 측정. 따라서 먼저 소프트웨어는 상위 두 소프트웨어 값의 차이를 결정합니다.

112
00:14:37,520 --> 00:14:42,720
그리고 그 차이에 임계값을 적용합니다. 또 하나는 밀도 상황이다. 그래서 그들은

113
00:14:43,680 --> 00:14:50,560
두 개의 구성 숨겨진 레이어와 조기 종료 간의 코사인 유사성을 계산합니다.

114
00:14:50,560 --> 00:14:56,320
분류자와 분류자로 붕괴됩니다. 조기 종료 가능성을 예측하는 분류기입니다.

115
00:14:59,120 --> 00:15:05,840
예, 임계값과 측정항목이 있습니다. 그래서 이 방법은 무엇입니까?

116
00:15:05,840 --> 00:15:11,600
또는 토큰 종료 결정. 즉, 우리는 글로벌 시퀀스가

117
00:15:11,680 --> 00:15:18,320
레벨 제한이 유지됩니다. 최종 출력물의 품질이 체커에 있도록

118
00:15:19,520 --> 00:15:26,560
임의적으로, 나중에 그것에 대해 이야기할 확률이 임의로 높습니다. 그래서 선택한 사용자입니다

119
00:15:28,720 --> 00:15:36,480
개연성. 네, 다음으로 구성된 제거 세트에는 무엇이 필요합니까?

120
00:15:37,280 --> 00:15:42,160
내가 말했던 제거 및 전역 제약 조건을 묻는 메시지가 표시됩니까? 최종 예측을 유지하자

121
00:15:42,160 --> 00:15:51,600
확인 중. 예, 전역 제약 조건이므로 이를 적응형 출력으로 표시하면

122
00:15:51,600 --> 00:15:58,720
표준 출력은 양극성이며 차이점이 있으면 차이점 기능이 있습니다.

123
00:15:59,520 --> 00:16:04,720
따라서 이 둘 사이의 차이점은 허용 오차보다 작아야 합니다.

124
00:16:05,680 --> 00:16:11,440
그리고 이것이 어떤 허용오차보다 작다는 신뢰도는 어떤 확률보다 커야 합니다.

125
00:16:11,440 --> 00:16:15,120
이 확률은 1 마이너스 엡실론이고 여기서 엡실론은 도함수입니다.

126
00:16:15,760 --> 00:16:27,520
그리고 그것은 사용자가 선택한 것입니다. 기본적으로 우리는 이 제약 조건을 사용하여 람다를 보정합니다.

127
00:16:28,080 --> 00:16:38,560
주어진 프롬프트에 단일 금색 답변이 있을 필요는 없습니다. 그것은 여러 개를 가질 수 있습니다

128
00:16:39,280 --> 00:16:45,760
금 답변. 그래서 우리가 하는 일은 위험을 경계하는 것입니다. 그래서 우리는 금 답변과 격렬하게 비교

129
00:16:46,400 --> 00:16:54,720
또는 골드 답변을 설정하고 위험에 따라 제한되어야 합니다.

130
00:16:58,000 --> 00:17:02,320
표준 출력은 허용 오차보다 작아야 합니다. 그러니까 이거 둘 다

131
00:17:03,840 --> 00:17:07,920
비유사성 함수와 이 함수는 다음을 기반으로 하는 표준 측정항목을 사용합니다.

132
00:17:08,320 --> 00:17:15,120
데이터 세트, 데이터 세트에 사용하는 방법입니다. 여기에 세 가지 데이터 세트가 있습니다.

133
00:17:15,120 --> 00:17:32,000
그들은 제안합니다. 이제 전역 제약 조건과 로컬 임계값이 생겼습니다. 방법은 무엇입니까?

134
00:17:32,000 --> 00:17:36,960
이 방법은 가능한 값, 가능한 람다, 가능한 그리드를 지정하기 때문입니다.

135
00:17:36,960 --> 00:17:42,800
허용 가능한 생성이 가능하며 일부를 사용하여 람다를 선택합니다.

136
00:17:42,800 --> 00:17:52,640
통계 도구. 간단히 말해서 우리는 이 하이퍼 매개변수 선택 문제를 여러

137
00:17:52,640 --> 00:17:58,480
이 테스트 문제를 각도로 파악하고 학습 후 테스트 프레임워크를 적용하여 이를 수행합니다.

138
00:17:58,800 --> 00:18:09,920
따라서 기본적으로 지정된 람다 값의 통계적으로 유효한 하위 집합을 식별합니다.

139
00:18:09,920 --> 00:18:16,800
그리고 그것은 우리에게 유효한 이것의 부분 집합을 제공합니다. 이제 이것이 통계적으로 유효하다는 것은 무엇을 의미합니까?

140
00:18:17,200 --> 00:18:28,240
이는 예를 들어 오류 반경이 0.05임을 의미합니다. 따라서 이는 최적의 위험을 의미합니다.

141
00:18:28,240 --> 00:18:36,000
최소 95%의 시간 허용 오차 범위 내에서 보장됩니다. 그래서 우리는 이것을 사용합니다

142
00:18:38,640 --> 00:18:43,840
하이퍼 매개변수 선택 대신 가설 테스트 문제. 여기 몇 가지가 있습니다

143
00:18:43,840 --> 00:18:52,960
통계적 보장. 이것은 세 가지 데이터 세트인 CNN, DN, 텍스트화 목록,

144
00:18:52,960 --> 00:19:02,000
WNT 프로모션 번역 및 뭔가. 기본적으로 여기에 보이는 세 줄은

145
00:19:02,000 --> 00:19:09,040
는 소프트맥스이고 주황색은 짝수 상태 포화 측정항목이며 이것들은 세 가지입니다.

146
00:19:09,040 --> 00:19:17,760
견고하고 X축은 평균 디코더 레이어 프로토콜입니다. 그러니 당신도 할 수 있는 것 같아요

147
00:19:17,760 --> 00:19:30,000
Softmax는 모든 특정 디코더에 대한 거의 모든 작업에서 더 높은 정확도를 가지고 있음을 확인하십시오.

148
00:19:30,000 --> 00:19:40,320
주어진 평균 디코더 레이어에 대해. 따라서 죄송합니다. 표에서도 볼 수 있습니다. 그래서

149
00:19:40,320 --> 00:19:46,000
이것은 세 가지 데이터 세트이므로 저자가 허용하는 수준이 다릅니다.

150
00:19:46,000 --> 00:19:52,480
실험을 했고 그들은 모든 실험에서 오류 반경 0.05를 취했습니다.

151
00:19:53,440 --> 00:20:03,840
네, 모든 실험에서 소프트맥스가 사용하는 평균 디코더 레이어 수는 다음과 같습니다.

152
00:20:04,960 --> 00:20:10,800
다른 두 가지에 비해 적고 대부분의 경우 속도가 향상됩니다.

153
00:20:11,760 --> 00:20:18,560
소프트맥스에 더 많이 있습니다. 이것은 경험적 결과이며 평균적으로 다음을 사용합니다.

154
00:20:18,560 --> 00:20:23,840
2에서 1.5까지 라운드하면 최대값이 표시됩니다.

155
00:20:26,880 --> 00:20:37,120
2.8, 3.5. 평균적으로 1.5~3.5배 사용하면 1.5~3.5배의 속도 향상을 보여줍니다.

156
00:20:38,000 --> 00:20:43,040
하지만 부동 소수점 연산을 수행하는 대신 더 많은 부동 소수점 연산이 필요합니다.

157
00:20:43,040 --> 00:20:49,760
소프트맥스에서 네, 모든 소프트맥스 문제에 대해 가장 큰 감소 수를 생성합니다.

158
00:20:49,760 --> 00:20:59,520
디코더 레이어의 일부가 늘어날 수 있기 때문입니다.

159
00:20:59,520 --> 00:21:16,240
그래서 우리가 종료 메커니즘을 진행한 이 두 가지 방법을 통해 파워 패스에서 우리는 평가합니다.

160
00:21:16,240 --> 00:21:21,840
특정 레이어에서 종료할지 여부. 이제 세 번째는

161
00:21:21,840 --> 00:21:31,280
세 번째는 약간 다르므로 여기서는 더 빠른 방법으로 모델에서 샘플링합니다.

162
00:21:32,320 --> 00:21:35,440
모든 내용이 변경되지 않도록

163
00:21:38,080 --> 00:21:45,840
따라서 함수 병렬을 계산하여 개요를 제공하므로 여기에 있습니다.

164
00:21:46,320 --> 00:21:52,000
훨씬 더 작고 효율적인 초안 모델 및 근사 모델

165
00:21:53,520 --> 00:22:00,800
그리고 우리는 그 모델을 사용하여 추측하거나 거의 추측하지 않으며 이러한 추측은 다음과 같습니다.

166
00:22:03,760 --> 00:22:11,120
동시에 대상 모델에 의해 확인되거나 확인됩니다. 예를 들자면 이렇게 가정해보자

167
00:22:11,120 --> 00:22:24,160
지금까지 모델에서 얻은 출력이 있고 우리가 선택한 것과 같은 것을 선택한다고 가정합니다.

168
00:22:24,160 --> 00:22:31,280
더 작은 모델에서 원하는 추측 수는 2개이므로 순차적으로 예측하겠습니다.

169
00:22:31,920 --> 00:22:37,760
더 작은 모델의 두 토큰을 예측할 때 9,700만 개의 매개변수가 있을 수 있습니다.

170
00:22:38,080 --> 00:22:46,720
모델이므로 이제 순차 예측 이후 이 시점에서 우리는 세 개의 입력을 갖게 됩니다.

171
00:22:49,520 --> 00:23:00,640
우리가 원하는 3개의 설정 입력은 첫 번째 퀵다운 박스이고 두 번째는

172
00:23:00,640 --> 00:23:07,600
퀵다운 박스 점프와 오버를 포함하는 세 번째 점프 그리고 이 세 가지 입력을 목표에 제공합니다.

173
00:23:07,600 --> 00:23:17,520
우리의 모델은 많은 수의 매개변수를 가지고 있으며 우리는 다음 토큰을 예측합니다.

174
00:23:17,520 --> 00:23:24,960
세 개의 입력이 모두 병렬로 연결되어 있으므로 질문에 대해서는 두 번째 입력에 대해 점프합니다.

175
00:23:25,280 --> 00:23:36,000
끝났습니다. 이제 이 출력을 대상 모델의 출력과 비교합니다.

176
00:23:36,000 --> 00:23:44,240
근사 모델인 어두운 모델의 출력이 정확하다면 우리는 손을 잡고

177
00:23:44,240 --> 00:23:51,360
이것은 추측적 디코딩 프로세스의 한 반복이며 최선의 경우입니다.

178
00:23:51,360 --> 00:24:06,080
사례 시나리오는 모든 토큰이 허용되므로 감마를 만든 경우 우리가 가져가는 경우입니다.

179
00:24:06,080 --> 00:24:10,560
그렇다면 우리는 매춘이 끝날 때 감마에 하나의 예측을 더한 결과를 얻게 될 것이라고 추측합니다.

180
00:24:10,800 --> 00:24:24,800
예 그리고 추측 중 하나가 틀렸기 때문에 목표 모델 대신 8개를 선택했다면 어떻게 될까요?

181
00:24:24,800 --> 00:24:33,360
점프하여 이 시점 이후에 트랩 모델이 만든 모든 예측을 폐기합니다.

182
00:24:34,320 --> 00:24:38,320
8개로 바꾸겠습니다.

183
00:24:41,760 --> 00:24:51,120
그렇죠, 이것이 제가 방금 보여드린 알고리즘이므로 여기서는 감마 추측을 해보겠습니다.

184
00:24:51,120 --> 00:24:59,600
순차적으로 우리는 근사 모델인 mq를 사용하여 다음을 만듭니다.

185
00:25:00,560 --> 00:25:11,440
어, 목표 모델인 mq를 사용할 때 이를 사용하여 토큰을 병렬로 계산합니다.

186
00:25:12,400 --> 00:25:20,080
그리고 이 특정 블록은 수정된 거부 샘플링이므로 다음을 수행하는 방법입니다.

187
00:25:20,640 --> 00:25:27,760
어 출력할 내용이 같은지 확인하고 이 거부를 사용합니다.

188
00:25:27,760 --> 00:25:37,520
개요가 제공되는 거부 샘플링은 복잡한 분포에서 데이터를 샘플링하는 방법입니다.

189
00:25:38,160 --> 00:25:49,360
분석적으로 가능한 것처럼 쉬운 방법을 사용하여 분석적으로 가능한 분포를 마무리합니다.

190
00:25:49,520 --> 00:25:59,120
이것은 분석적으로 실현 가능한 분포라고 말할 수 있으므로 주의해야 할 사항입니다.

191
00:25:59,120 --> 00:26:08,240
초안 모델 분포이고 이것이 어 목표 모델 분포이므로

192
00:26:09,280 --> 00:26:14,720
간단히 말해서 우리는 다음의 도움으로 분포와 복잡한 분포를 일치시키려고 합니다.

193
00:26:15,360 --> 00:26:16,880
이 근사 분포

194
00:26:20,080 --> 00:26:26,080
네, 그래서 그것이 하는 일은 시스템의 출력이 다음과 같이 보장된다는 것입니다.

195
00:26:26,800 --> 00:26:30,160
목표 모델의 분포와 동일한 분포를 갖습니다.

196
00:26:35,280 --> 00:26:41,520
네, 그래서 그들은 또한 두 번째 모델과 비슷합니다.

197
00:26:41,840 --> 00:26:52,160
전역 또는 전역 출력은 최종 시퀀스의 품질을 취하고 이를 전체라고 부릅니다.

198
00:26:52,160 --> 00:27:02,720
괜찮으니까 프로세스를 진행하세요. 예, 그렇습니다. 그냥 알아두시면 됩니다.

199
00:27:03,680 --> 00:27:11,840
완전히 여러 입력을 병렬로 계산하므로 추가가 필요합니다.

200
00:27:13,840 --> 00:27:15,360
기억이 그래서

201
00:27:17,440 --> 00:27:24,480
예, 따라서 이 접근 방식을 사용하려면 추가 계산 리소스를 사용할 수 있어야 하며

202
00:27:24,960 --> 00:27:34,720
우리는 이 방법을 사용하여 변수를 변경하지 않고도 추론을 가속화할 수 있습니다.

203
00:27:34,720 --> 00:27:42,480
아키텍처 어 훈련 절차 운영 모델을 변경하지 않고 변경하지 않고

204
00:27:42,480 --> 00:27:46,640
모델은 수정된 분포 샘플링 에피소드를 사용하여 좌절감을 모델링합니다.

205
00:27:46,960 --> 00:27:58,480
예, 그렇다면 감마로 표시하면 추측할 수 있는 토큰의 수가 있습니다.

206
00:27:59,280 --> 00:28:09,840
초안 모델이 목표 모델에 얼마나 잘 근접하는지에 대한 척도를 알파로 나타냅니다.

207
00:28:09,840 --> 00:28:17,920
그래서 그들이 얼마나 가까운지는 알파가 될 것이고, 만약 7개의 실제와 같은 특정 감마에 대해 말하자면

208
00:28:17,920 --> 00:28:27,120
어, 좋은지 측정해 보세요. 분포 출력 분포가 더 가깝습니다.

209
00:28:27,120 --> 00:28:34,640
서로 알파가 높기 때문에 더 많은 수의 토큰을 추측할 수 있습니다.

210
00:28:34,640 --> 00:28:44,000
쉽습니다. 7이고 7이고 알파가 거기 있으면 동전이고 그러면 우리는 할 수 있을 것입니다.

211
00:28:47,600 --> 00:28:50,640
6라운드가 토큰 자산의 90%인 것으로 추측해 보세요.

212
00:28:53,440 --> 00:28:58,480
즉, 알파는 허용되는 예상 토큰 수에 비례합니다.

213
00:28:59,280 --> 00:29:01,280
또한

214
00:29:05,120 --> 00:29:10,880
모델 알파 계열의 경우 근사 모델의 크기에 따라 달라집니다.

215
00:29:11,600 --> 00:29:22,160
따라서 초안 모델 분포가 대상 대상 모델에 매우 가깝다면 이는 다음을 의미합니다.

216
00:29:23,040 --> 00:29:29,840
초안 모델은 더 크고 다양한 표현을 실행할 수 있습니다.

217
00:29:31,760 --> 00:29:38,560
그래서 절충안이 있습니다. 그래서 nq를 만드는 것과 역유사한 것과 nq를 만드는 것 사이에는

218
00:29:40,240 --> 00:29:44,560
nq 모델을 저렴하게 만드는 것을 좋아하는 모델

219
00:29:44,960 --> 00:29:46,960
하다

220
00:29:50,400 --> 00:29:57,520
네, 그렇다면 모델의 총 실행 시간은 몇 시입니까? 마지막 행은

221
00:29:57,520 --> 00:30:05,280
어쩌면 베이스일지도 모르죠 그래서 노란색 부분은 nq 부분이고 남쪽은 어 보라색 보라색

222
00:30:05,360 --> 00:30:15,520
블록은 어 하나의 디코딩 시간 단계이고 어 빨간색 빨간색이므로 추측을 사용하면

223
00:30:15,520 --> 00:30:22,880
추론적 디코딩의 경우 여기서 빨간색 선은 대상 모델 인코더이고 파란색 스트립은

224
00:30:22,880 --> 00:30:32,560
우리는 기록된 어 근사 모델을 순차적으로 실행하므로 전반적으로

225
00:30:32,560 --> 00:30:35,840
볼 시간이 줄어들어 볼 시간이 줄어듭니다.

226
00:30:45,840 --> 00:30:53,760
노란색은 어 대상 모델입니다. mp는 대상 모델입니다. p는 대상 모델이고 q는

227
00:30:53,760 --> 00:31:00,160
초안 모델 아 선택기 그렇군요. 따라서 더 큰 모델은 인코딩과 인코딩에 더 많은 시간이 걸릴 것입니다.

228
00:31:00,160 --> 00:31:06,160
초안 모델과 비교하여 디코딩 uh

229
00:31:10,240 --> 00:31:20,160
아 그렇구나 그런데 수영장은 왜 여기야? 아 이게 수영장이야 맞아 맞아 그런데 그렇지

230
00:31:22,720 --> 00:31:26,720
그래서 그 조합은 여기에 여러 개의 파란색 줄이 있다는 것입니다. 아 그렇군요.

231
00:31:27,120 --> 00:31:31,280
커버가 클수록 가속력도 높아집니다.

232
00:31:32,400 --> 00:31:40,560
하지만 그것은 음 목표 모델과 초안 모델이 얼마나 잘 같은지에 달려 있습니다.

233
00:31:41,440 --> 00:31:47,040
그렇죠, 시간적으로 서로 가깝습니다. 그렇죠, 그렇군요. 모델이 낮고 그렇지 않습니다.

234
00:31:48,320 --> 00:31:54,400
닫히지 않는 것처럼 바짝 다가서면 그래 맞아 알았어 돌아가

235
00:31:54,880 --> 00:32:01,200
응, 그래서 그들은 감마 흐름을 개선하는 것을 좋아하지 않았어. 그래서 어떤 시점에서는 감마 흐름이 좋아졌어

236
00:32:02,480 --> 00:32:08,480
늘리면 그게 다야 그래 정확히 그래 그게 내 또 다른 요점이 될 테니 선택해야 해

237
00:32:09,360 --> 00:32:13,680
최적 감마 그래 알았어 알겠어 그래 그래

238
00:32:13,680 --> 00:32:27,840
응, 내가 말했듯이 아, 그럼 알파가 지정되어 초안 모델이

239
00:32:27,840 --> 00:32:37,120
대상 모델이 훨씬 더 가깝고 여기에 비용 계수가 표시되므로 소요된 시간의 비율입니다.

240
00:32:37,120 --> 00:32:46,160
초안 모델을 실행하는 데 걸린 시간과 대상 모델을 실행하는 데 걸린 시간의 비율로 실행합니다.

241
00:32:46,160 --> 00:32:52,640
그것은 비용 계수입니다. 따라서 더 높으면 우리는 잘못된 것을 좋아할 여유가 없습니다.

242
00:32:52,640 --> 00:33:00,640
잘못된 자리가 많아지면 시간이 더 걸리므로 c가 0.1인 경우

243
00:33:01,600 --> 00:33:09,440
uh 더 낮은 최적 감마가 있는 모델에 비해 uh

244
00:33:11,040 --> 00:33:18,480
실행하는 데 훨씬 더 작거나 훨씬 더 빠르므로 숫자가 더 높습니다.

245
00:33:20,640 --> 00:33:23,120
그래서 그들은 이것을 어 특별한 것으로 추론했습니다

246
00:33:23,120 --> 00:33:31,040
어 그들은 기계적인 방법을 사용하여 이 플롯과 이 플롯을 도출했습니다.

247
00:33:31,600 --> 00:33:41,920
그래, 우리가 그것을 안다면 계산할 수 있습니다. 어 수용률을 안다면

248
00:33:43,680 --> 00:33:47,600
어 초안 모델이면 이것을 계산할 수 있습니다

249
00:33:47,920 --> 00:33:50,400
좋아요

250
00:33:54,080 --> 00:34:00,480
네 화면이 그래서 e5 xxl을 대상 모델보다 10만배나 사용했군요

251
00:34:00,480 --> 00:34:07,280
실험의 근사치는 적어도 3명입니다. 예 88억 2억 5천만 그리고 과학자

252
00:34:07,280 --> 00:34:15,520
찾고 있는데 네 그들은 이 두 가지에 대해 영어에서 독일어로 번역하고 텍스트 요약을 사용했습니다.

253
00:34:15,760 --> 00:34:29,280
데이터 세트와 어 그리고 그들은 t5 모델이 이 감마의 균형과 같다는 것을 관찰했습니다

254
00:34:29,920 --> 00:34:38,720
그래서 그들이 얻는 추측 토큰과 알파는 약 3.4~3.1 정도 속도가 약 2~3배 빨라집니다.

255
00:34:39,440 --> 00:34:48,320
아 그게 뭐야 어 아 알파는 초안 모델이 얼마나 잘 작동하는지를 나타내는 척도입니다.

256
00:34:48,320 --> 00:34:50,560
대략적인 다이어그램

257
00:34:51,840 --> 00:34:56,240
죄송해요 질문이 있어요 음 최적의 감마를 얻은 플롯 하나를 아시나요?

258
00:34:56,240 --> 00:35:02,480
직관적으로 축에서 자체를 측정합니다. 추측하는 토큰이 많을수록 확률이 낮아진다고 생각합니다.

259
00:35:02,480 --> 00:35:06,560
당신의 외풍 측정은 마치 이것이 반대 방향으로 움직이는 것처럼 보일 것입니다

260
00:35:06,560 --> 00:35:10,960
우호적인 방향이 맞습니다. 따라서 모든 단일 토큰을 확인해야 하고 더 높은 점수를 얻은 경우

261
00:35:10,960 --> 00:35:15,840
장거리 복리 오류가 없기 때문에 추측할 확률

262
00:35:18,800 --> 00:35:24,160
그래서 내가 작은 모델과 큰 모델을 가지고 있고 감마는 내가 할 토큰의 수입니다.

263
00:35:24,160 --> 00:35:28,800
작은 모델을 예측한 다음 큰 모델과 비교하면 수용 정도가 측정됩니다.

264
00:35:28,800 --> 00:35:33,680
내가 예측한 토큰 수에 따라 더 길어질수록 감마가 클수록 더 많은 토큰이 필요합니다.

265
00:35:33,680 --> 00:35:38,800
그들은 정확하다고 생각합니다. 숫자가 올라가면 성공의 척도가 내려갈 것이라고 생각합니다.

266
00:35:38,800 --> 00:35:44,640
수집하지 않으며 감마가 아닌 최적의 감마를 나타내므로 이것이 최적입니다.

267
00:35:44,640 --> 00:35:52,720
감마 값은 맞지만 0.9로 측정하면 받아들일 가능성이 매우 높으며 토큰이 24개 있습니다.

268
00:35:52,720 --> 00:36:00,320
어 실행에 매우 효율적인 모델에 대한 토큰 수이므로 결과가 좋지 않습니다.

269
00:36:00,400 --> 00:36:06,000
모델에 대한 호출 수가 더 많지만 만약 그렇다면 모델이 uh

270
00:36:07,680 --> 00:36:13,840
실행하는 것이 효율적이지 않으면 토큰 수가 매우 적기 때문에 우리는 좋은 결과를 얻을 여유가 있습니다.

271
00:36:13,840 --> 00:36:22,240
런타임이 더 적은 각 모델에 대해 추측한 다음 이 큰 감마 값을 취합니다.

272
00:36:22,240 --> 00:36:28,160
실제 합격 가능성과는 관련이 없지만 제가 감당할 수 있는 금액이니까요.

273
00:36:28,160 --> 00:36:30,160
그게 바로 내 얘기야, 그래 확인해 볼게

274
00:36:35,600 --> 00:36:40,160
네, 그렇다면 이 모델의 한계는 추가 컴퓨팅이 필요하다는 것입니다.

275
00:36:43,280 --> 00:36:49,440
기억력이 덜해요 네 그럼 그건 아니죠 네 확인을 할 때 질문 하나 더 할게요

276
00:36:49,440 --> 00:36:53,280
우리가 더 작은 모델에서 예측한 것을 버린다는 것은 단 하나의 토큰이라는 것입니다.

277
00:36:53,280 --> 00:36:57,760
우리가 할 것이라고 예측된 일이 끝나면 다른 모든 것이 성공했다고 가정하거나 어떻게 해야 할까요?

278
00:36:58,400 --> 00:37:01,680
전체 시퀀스를 확인하려면 우리가 얼마나 확인하고 있는지 확인하세요.

279
00:37:01,680 --> 00:37:06,880
그러면 모델이 느리기 때문에 시간이 지남에 따라 지연 계산 버퍼처럼 쌓이게 됩니다.

280
00:37:06,880 --> 00:37:11,360
한 번에 하나의 토큰을 수행할 것이고 더 빠른 모델은 24개 정도 수행했을 것이므로 큰 모델은

281
00:37:11,360 --> 00:37:16,960
성공이 무엇인지 알아내기 위해 영원히 쫓겨났습니다. 아니 우리는 순차적으로 통과했습니다.

282
00:37:16,960 --> 00:37:23,360
예를 들어 10번의 추측을 하고 모든 추측을 순차적으로 예측한 다음 11개의 입력을

283
00:37:24,000 --> 00:37:31,520
모델을 병렬로 계산하고 일단 대상 모델이 모든 레벨을 완료하면

284
00:37:31,520 --> 00:37:40,160
11개의 입력이 모두 포함되면 11개의 출력이 올바르게 표시되고 확인합니다.

285
00:37:40,160 --> 00:37:46,640
순차적으로 어 어떤 토큰이 매개변수를 쓰는지 그래서 내 질문은 다음과 같습니다.

286
00:37:46,640 --> 00:37:50,480
더 작은 모델이 실제로 효율적으로 실행되는 것은 점점 더 발전하고 있습니다.

287
00:37:51,040 --> 00:37:56,880
모델이 확인을 시도할 때 실제로는 결코 도달하지 못할 지점에 도달하지 않습니까?

288
00:37:56,880 --> 00:38:02,160
더 작은 모델이 훨씬 앞서 있기 때문에 확인할 수 있습니다. 아니요 아니요 아니요 제한합니다. 제한합니다.

289
00:38:02,160 --> 00:38:09,280
우리는 그림을 어 감마로 제한하므로 한 번의 반복으로 토큰의 수가

290
00:38:09,280 --> 00:38:16,880
여기서 사용할 수 있는 것은 2개이고 그 반복에서는 그 이후의 토큰 2개만 추측하고 중지됩니다.

291
00:38:17,840 --> 00:38:24,960
그리고 그것은 모델 합성을 목표로 할 것이고 실제로 더 작은 모델은 기다려야 할 것입니다.

292
00:38:24,960 --> 00:38:29,680
참조는 있지만 대상 모델에만 묶여 있기 때문에 효율성 향상을 볼 수 없습니다.

293
00:38:29,680 --> 00:38:35,680
다시 속도를 높이세요 아니요 아니요 대상 모델은 그냥 그렇게 하기만 하면 됩니다. 그게 맞는지는 모르겠습니다.

294
00:38:38,160 --> 00:38:43,440
그래 그래 그래서 그래 대상 모델은 여전히 ​​모든 것을 검토해야 하지만 그건 마치

295
00:38:43,440 --> 00:38:48,960
영향력 있는 권리는 내 의견이 아니기 때문에 그게 사실이야 그래 그래 그래서 그렇게 생각하는 거야

296
00:38:51,040 --> 00:38:52,160
응 다른 버전도 있어

297
00:38:54,960 --> 00:39:00,080
나는 이것이 추론의 범위일 뿐이라는 것을 알고 있지만 어 당신이 정말로 찾고 있는지는 모르겠습니다.

298
00:39:00,080 --> 00:39:06,960
모델을 훈련시키는 것과 같은 훈련을 위해 이것을 수행하는 연구의 일부가 있습니다.

299
00:39:06,960 --> 00:39:11,840
제 생각엔 작은 모델이 서밋에 있는 것 같아요. 피드백을 말씀하시는 것처럼 크고 더 큰 서밋에 두는 거죠.

300
00:39:13,520 --> 00:39:18,080
확인을 통해 소형 모델이 학습할 수 있는 피드백을 가져올 수 있다는 것을 알고 있습니다.

301
00:39:18,080 --> 00:39:21,280
다만 비슷한 일을 하면 스스로 훈련을 하게 되는지 모르겠어요

302
00:39:22,480 --> 00:39:28,480
어 그거 흥미로운 점인데 훈련 부분은 잘 모르겠어요

303
00:39:30,240 --> 00:39:34,160
그럼 당신은 일종의 증류를 의미하는군요

304
00:39:34,400 --> 00:39:40,080
내 말은, 당신은 더 큰 모델을 근사화하기 위해 작은 모델을 사용하고 있다는 뜻이에요 uh

305
00:39:40,080 --> 00:39:45,920
당신이 그렇다고 하고 제가 모델로부터 작은 모델을 개선하기 위해 답장을 한다면

306
00:39:46,480 --> 00:39:49,680
네, 정확하게 피드백을 드리는 것이 더 많은 구현입니다.

307
00:39:53,360 --> 00:39:58,720
대략 당신이 알다시피 내가 처리할게 그래 그래 그럼 서류가 있어야 해

308
00:39:59,040 --> 00:40:06,480
그래서 전반적으로 처음 두 모델 pavy와 Calm은 조기 퇴출 전략을 채택했습니다.

309
00:40:08,000 --> 00:40:13,440
그 평온함 외에도 글로벌 일관성도 고려했습니다.

310
00:40:15,360 --> 00:40:22,960
드디어 어 마지막 접근 방식이 처음 두 모델과 호환됩니다

311
00:40:22,960 --> 00:40:31,200
다른 것도 마찬가지야 어 동적 속도 향상 어 그래 그것도 내 몫이야