1
00:00:00,000 --> 00:00:04,080
그래서 나는 다음 세 편의 논문을 위해 출발할 것입니다.

2
00:00:04,080 --> 00:00:07,600
그래서 오늘 우리가 논의하고 있는 내용은 본질적으로 다음과 같습니다.

3
00:00:07,600 --> 00:00:12,200
이를 둘러싼 추론과 기술의 속도를 어떻게 높일 수 있습니까?

4
00:00:12,200 --> 00:00:15,640
특정 모델에 실제로 고정된 것은 없으며

5
00:00:15,640 --> 00:00:18,320
그것은 심지어 일반적인 경향이 될 것입니다

6
00:00:18,320 --> 00:00:20,760
내가 검토할 서류.

7
00:00:20,760 --> 00:00:24,480
그래서 스타터로서,

8
00:00:24,480 --> 00:00:27,960
적어도 제가 다루고 있는 세 편의 논문에 대해서는

9
00:00:28,520 --> 00:00:31,480
그리고 적어도 지난 몇 주 동안 우리가 살펴본 내용은

10
00:00:31,480 --> 00:00:35,080
LLM 효율성을 향상시킵니다.

11
00:00:35,080 --> 00:00:40,480
우리는 주로 훈련 측면의 속도를 높이는 방법을 살펴보았습니다.

12
00:00:40,480 --> 00:00:44,880
실제로 이러한 모델을 어떻게 규모에 맞게 배포할 것인지도 아닙니다.

13
00:00:44,880 --> 00:00:48,200
지난 강의에서 우리는 무엇인가에 대한 몇 가지 측면을 다루었습니다.

14
00:00:48,200 --> 00:00:52,320
이러한 모델을 대규모로 배포할 수 있는 좋은 아키텍처입니다.

15
00:00:52,320 --> 00:00:57,200
하지만 이는 처음 세 가지 범주의 대부분에 속합니다.

16
00:00:57,200 --> 00:01:01,120
예를 들어, 오늘 우리가 살펴볼 것은 마지막 4개입니다.

17
00:01:01,120 --> 00:01:03,160
그래서 우리는 추측을 다룰 것입니다.

18
00:01:03,160 --> 00:01:07,800
따라서 우리는 주로 대기 시간을 줄이고 처리량을 향상시키는 데 중점을 두고 있습니다.

19
00:01:07,800 --> 00:01:10,360
그리고 그것은 모든 논문의 공통 주제입니다.

20
00:01:10,360 --> 00:01:12,440
그들 중 일부는 그 중 하나를 목표로 삼고 있습니다.

21
00:01:12,440 --> 00:01:14,320
그들 중 일부는 두 가지 모두를 개선하려고 노력하고,

22
00:01:14,320 --> 00:01:17,560
우리는 추측적 디코딩에 대해 다룰 것입니다.

23
00:01:17,560 --> 00:01:23,720
이를 위한 솔루션으로 BLLM이 있습니다.

24
00:01:24,720 --> 00:01:28,080
네, 첫 번째 논문에서는

25
00:01:28,080 --> 00:01:31,920
우리는 들어오는 LLM 요청을 효율적으로 처리하고 싶습니다.

26
00:01:31,920 --> 00:01:36,000
그래서 최근에는 LLM 지원이 늘어나고 있습니다.

27
00:01:36,000 --> 00:01:39,000
Cilsrani에 부조종사가 있다고 가정해 보겠습니다.

28
00:01:39,000 --> 00:01:42,320
코드를 반복할 때마다

29
00:01:42,320 --> 00:01:47,680
부조종사 서버로 가는 API 열이 있고 여러분에게 응답합니다.

30
00:01:47,680 --> 00:01:50,560
따라서 수백만 건의 요청이 들어오면

31
00:01:50,560 --> 00:01:53,520
어떻게 효율적으로 처리하고 활용합니까?

32
00:01:53,520 --> 00:01:56,040
우리가 가지고 있는 인프라는 무엇입니까?

33
00:01:56,040 --> 00:02:06,520
네, 그럼 일반적인 추론 절차를 살펴보겠습니다.

34
00:02:06,520 --> 00:02:09,800
우리는 이미 KQB에 대해 관심을 갖고 살펴보았는데,

35
00:02:09,800 --> 00:02:14,040
하지만 추론 단계에서 실제로 무슨 일이 일어나는지 살펴보겠습니다.

36
00:02:14,040 --> 00:02:19,440
입력 토큰이 들어오고 있다고 가정해 보겠습니다.

37
00:02:19,440 --> 00:02:21,960
이를 임베딩으로 변환하고,

38
00:02:21,960 --> 00:02:29,960
x라고 가정하고 여기에 우리가 배운 가중치 행렬을 곱합니다.

39
00:02:29,960 --> 00:02:35,320
Wk, Wq, Wv가 있습니다.

40
00:02:35,320 --> 00:02:39,480
핵심 쿼리와 값 가중치,

41
00:02:39,480 --> 00:02:46,520
그런 다음 Kv에 대해 K를 계산합니다.

42
00:02:46,520 --> 00:02:50,080
그리고 그것을 작은 q라고 합시다.

43
00:02:50,080 --> 00:02:57,000
그런 다음 K와 V가 이 가중치를 기반으로 계산됩니다.

44
00:02:57,000 --> 00:03:01,800
이것이 행렬이고 이것이 벡터라고 합시다.

45
00:03:01,800 --> 00:03:06,400
이것은 들어오는 입력 토큰을 삽입하는 것입니다.

46
00:03:06,400 --> 00:03:09,960
그래서 우리가 주로 관심을 갖는 것은 K와 V입니다.

47
00:03:09,960 --> 00:03:11,720
적어도 첫 번째 논문에서는요.

48
00:03:12,040 --> 00:03:20,280
그들이 내적을 통해 주의를 계산할 때,

49
00:03:20,280 --> 00:03:27,400
qA 전치를 이것의 루트 차원으로 나눈다고 가정해 보겠습니다.

50
00:03:27,400 --> 00:03:41,680
그런 다음 소프트맥스를 취하고 값 행렬을 곱합니다.

51
00:03:41,680 --> 00:03:47,440
따라서 이 계산을 수행하면

52
00:03:47,440 --> 00:03:49,400
순진한 방법으로,

53
00:03:49,400 --> 00:03:55,520
결국 QK 전치 행렬의 각 요소를 다시 계산하게 됩니다.

54
00:03:55,520 --> 00:04:00,160
따라서 모든 요소를 ​​다시 계산할 필요가 없도록 하려면

55
00:04:00,160 --> 00:04:04,040
시퀀스에 대한 새로운 쿼리 토큰이 들어옵니다.

56
00:04:04,040 --> 00:04:06,760
우리는 KvCache라는 것을 구현합니다.

57
00:04:06,760 --> 00:04:08,720
KvCache는 어떻게 작동하나요?

58
00:04:08,760 --> 00:04:15,560
그럼 내가 좋지 않다는 문장이 있다고 가정해 봅시다.

59
00:04:15,560 --> 00:04:20,640
들어오는 각 입력 토큰에 대해 이 작업을 수행해야 합니다.

60
00:04:20,640 --> 00:04:22,360
그렇게 하면서

61
00:04:22,360 --> 00:04:26,280
우리는 각 토큰에 대해 KvCache를 계속 구축하고 있습니다.

62
00:04:26,280 --> 00:04:32,080
그럼 이들 각각이 들어오는 토큰이라고 가정해 보겠습니다.

63
00:04:32,280 --> 00:04:38,320
K 행렬과 V 행렬을 생성합니다.

64
00:04:38,320 --> 00:04:40,360
우리는 이것을 메모리에 저장합니다.

65
00:04:40,360 --> 00:04:43,200
그리고 이 각 토큰은

66
00:04:43,200 --> 00:04:47,760
따라서 여기서는 K 행렬의 특정 열에 해당합니다.

67
00:04:47,760 --> 00:04:52,000
여기서는 V 행렬의 특정 행이 됩니다.

68
00:04:52,000 --> 00:04:55,200
시간이 지나면서 이렇게 되면서

69
00:04:55,200 --> 00:04:57,280
이 행렬의 크기는 점점 커지고 있습니다.

70
00:04:57,520 --> 00:05:03,440
그래서 우리는 대신에 이렇게 말할 수 있습니다

71
00:05:03,440 --> 00:05:08,520
이 전체 행렬을 다시 계산하거나 해당 계산을 다시 실행하고,

72
00:05:08,520 --> 00:05:12,120
우리가 할 수 있는 일은 이전의 모든 토큰에 대해

73
00:05:12,120 --> 00:05:14,320
Kcache를 가질 수 있고,

74
00:05:14,320 --> 00:05:18,200
지금까지 계산한 모든 열입니다.

75
00:05:18,200 --> 00:05:20,400
저는 그냥 캐시에 저장합니다.

76
00:05:20,400 --> 00:05:23,320
그런 다음 새로운 토큰이 들어오면

77
00:05:23,320 --> 00:05:25,800
그냥 캐시에 추가하고

78
00:05:26,000 --> 00:05:28,560
이것이 제가 하는 유일한 새로운 계산입니다.

79
00:05:28,560 --> 00:05:30,800
값 행렬에 대해서도 마찬가지입니다.

80
00:05:30,800 --> 00:05:32,680
지금까지 온 모든 것,

81
00:05:32,680 --> 00:05:35,520
Vcache에 저장하고,

82
00:05:35,520 --> 00:05:40,400
그런 다음 새로운 계산이 Vcache의 새로운 행으로 이동됩니다.

83
00:05:40,400 --> 00:05:42,360
보시다시피,

84
00:05:42,360 --> 00:05:44,800
시퀀스 길이가 크기 때문에

85
00:05:44,800 --> 00:05:48,560
K 및 V 행렬의 크기는 계속 커질 것입니다.

86
00:05:48,560 --> 00:05:59,560
이제 9개 구현에서는

87
00:05:59,560 --> 00:06:02,160
이를 위해서는 연속된 메모리 덩어리가 필요합니다.

88
00:06:02,160 --> 00:06:05,400
앞으로 일어날 주의 작업 때문입니다.

89
00:06:05,400 --> 00:06:08,840
첫 번째 논문에서는 이를 어떻게 분해할 수 있는지 살펴보겠습니다.

90
00:06:08,840 --> 00:06:12,480
메모리를 더 잘 관리할 수 있습니다.

91
00:06:12,480 --> 00:06:16,520
그렇다면 KvCache에는 정확히 무엇이 포함되어 있습니까?

92
00:06:16,640 --> 00:06:18,240
높은 수준의 개요입니다.

93
00:06:18,240 --> 00:06:20,680
이것이 나의 프롬프트라고 가정해 봅시다.

94
00:06:20,680 --> 00:06:23,600
그래서 우리는 그것을 가지고 있습니다.

95
00:06:23,600 --> 00:06:26,080
이것이 현재 스니펫입니다.

96
00:06:26,080 --> 00:06:29,120
모델이 응답을 생성하고 있습니다.

97
00:06:29,120 --> 00:06:31,280
현재 IAM에서 중지되었습니다.

98
00:06:31,280 --> 00:06:34,720
그래서 이 특별한 스냅샷에서

99
00:06:34,720 --> 00:06:38,120
4개의 토큰이 있습니다.

100
00:06:38,120 --> 00:06:43,720
하지만 이것이 2048 컨텍스트 길이의 모델이라고 가정해 보겠습니다.

101
00:06:44,680 --> 00:06:48,480
OpenAI API로 작업한 경우에도 가능합니다.

102
00:06:48,480 --> 00:06:49,720
또는 추론 호출,

103
00:06:49,720 --> 00:06:53,360
생성하려는 양에 대한 최대 토큰을 지정할 수 있습니다.

104
00:06:53,360 --> 00:06:56,440
따라서 가변 시퀀스 길이 문제가 있습니다.

105
00:06:56,440 --> 00:06:58,520
우리가 마음속에 간직해야 할 것,

106
00:06:58,520 --> 00:07:03,120
하지만 시퀀스 길이가 2048이라고 가정해 보겠습니다.

107
00:07:03,120 --> 00:07:09,680
따라서 본질적으로 저는 2048개 토큰을 위한 메모리를 예약할 것입니다.

108
00:07:09,840 --> 00:07:15,080
이는 예비 토큰으로 표시됩니다.

109
00:07:15,080 --> 00:07:19,880
그리고 모델은 조기 종료를 선택할 수도 있습니다.

110
00:07:19,880 --> 00:07:21,760
그러니까 나한테 부탁을 했다면,

111
00:07:21,760 --> 00:07:23,000
답변을 마쳤습니다.

112
00:07:23,000 --> 00:07:24,360
미국 토큰을 전달할 수 있고,

113
00:07:24,360 --> 00:07:27,720
그러면 내가 해야 할 일은 끝났습니다.

114
00:07:27,720 --> 00:07:30,920
따라서 이것은 미래에 일어날 일의 일부일 뿐입니다.

115
00:07:30,920 --> 00:07:34,040
이 특정 응답은 해당 미국 토큰에서 끝납니다.

116
00:07:35,800 --> 00:07:36,920
그러다가 마지막으로 갈수록,

117
00:07:36,920 --> 00:07:40,840
그 녹색은 들어온 또 다른 프롬프트입니다.

118
00:07:40,840 --> 00:07:43,400
모델이 다시 서비스를 제공하고 있습니다.

119
00:07:43,400 --> 00:07:45,080
그러니까 이것은 단지 기억의 한 조각일 뿐입니다.

120
00:07:45,080 --> 00:07:46,960
그리고 작가들은 어떤 사람들인지,

121
00:07:46,960 --> 00:07:49,040
저자가 해결하려고 하는 문제.

122
00:07:49,040 --> 00:07:52,040
보시다시피,

123
00:07:52,040 --> 00:07:53,920
두 가지 문제가 있습니다.

124
00:07:53,920 --> 00:07:55,560
저자는 이를 인터넷 단편화라고 부릅니다.

125
00:07:55,560 --> 00:07:57,160
그리고 외부 단편화.

126
00:07:57,160 --> 00:07:59,640
따라서 예비 토큰으로 인해

127
00:08:00,640 --> 00:08:04,040
우리는 할당된 메모리의 큰 덩어리를 가지고 있습니다

128
00:08:04,040 --> 00:08:05,680
요청이 들어왔으니까요.

129
00:08:05,720 --> 00:08:09,360
그리고 우리는 출력 길이를 모르기 때문에

130
00:08:09,360 --> 00:08:12,360
이 모델이 메모리를 어느 정도 소비할 것인지,

131
00:08:12,360 --> 00:08:16,400
다음 토큰 사이에도 약간의 패딩이 있습니다.

132
00:08:16,400 --> 00:08:17,520
메모리에 저장되는 중입니다.

133
00:08:17,520 --> 00:08:20,520
따라서 외부 단편화도 발생합니다.

134
00:08:20,520 --> 00:08:22,000
시퀀스 길이가 다르기 때문에

135
00:08:22,000 --> 00:08:24,040
내부 단편화는 당신이 모르기 때문에 발생합니다

136
00:08:24,040 --> 00:08:27,680
이것이 종료되는 출력 길이.

137
00:08:27,680 --> 00:08:30,800
그래서 저자가 발견한 것은

138
00:08:31,800 --> 00:08:35,600
이러한 순진한 메모리 할당을 기반으로

139
00:08:35,600 --> 00:08:38,120
약 60%는 사용되지 않은 상태로 남아 있습니다.

140
00:08:38,120 --> 00:08:41,400
이것이 바로 그들이 해결하려고 하는 문제입니다.

141
00:08:41,400 --> 00:08:44,520
캐시를 끄고 싶다면 코드를 현명하게 사용하세요

142
00:08:44,520 --> 00:08:45,720
캐시를 켜고,

143
00:08:45,720 --> 00:08:48,920
TensorFlow에서 사용 캐시를 사용할 수 있습니다.

144
00:08:48,920 --> 00:08:53,800
Colab에서 T4와 같은 장치를 사용하여 해당 작업을 수행하려고 하면

145
00:08:53,800 --> 00:08:55,960
5X 정도의 성능 차이가 있을 것입니다.

146
00:08:55,960 --> 00:08:57,640
따라서 KV 캐시가 필요합니다.

147
00:08:57,640 --> 00:08:59,400
하지만 효율적으로 수행해야 합니다.

148
00:08:59,400 --> 00:09:01,000
그리고 이 9가지 방법은 작동하지 않습니다.

149
00:09:04,080 --> 00:09:06,720
전체 메모리 블록을 제공하는 것뿐입니다.

150
00:09:06,720 --> 00:09:08,480
전체 시퀀스 길이에 대해.

151
00:09:08,480 --> 00:09:10,680
따라서 캐시를 비울 수 있습니다.

152
00:09:10,680 --> 00:09:12,960
예, 이 메모리를 사전 할당할 수는 없습니다.

153
00:09:12,960 --> 00:09:13,800
효율적으로 하기 위해서는,

154
00:09:13,800 --> 00:09:15,840
이 문제를 처리하는 더 좋은 방법이 있어야 합니다.

155
00:09:15,840 --> 00:09:18,720
그리고 이것이 저자가 이 논문에서 제안한 것입니다.

156
00:09:19,720 --> 00:09:22,280
다시 말하지만, 사물의 규모를 살펴보십시오.

157
00:09:22,280 --> 00:09:24,560
발표에 앞서,

158
00:09:24,560 --> 00:09:27,280
방금 Lama가 얼마나 많은 코드를 소비하는지 살펴보았습니다.

159
00:09:27,280 --> 00:09:28,800
본질적으로 우리는

160
00:09:30,240 --> 00:09:31,080
의 말을하자,

161
00:09:32,720 --> 00:09:34,760
그래서 우리는 두 개의 행렬을 가지고 있습니다.

162
00:09:34,760 --> 00:09:37,560
그래서 K와 V.

163
00:09:37,560 --> 00:09:38,880
그리고 우리는

164
00:09:40,360 --> 00:09:43,280
추론을 위해 우리는 보통 FT16을 사용합니다.

165
00:09:43,280 --> 00:09:45,080
그래서 우리는 그것이 2바이트라고 말할 수 있습니다.

166
00:09:46,600 --> 00:09:50,120
그런 다음 여러 레이어가 필요합니다.

167
00:09:50,120 --> 00:09:54,440
그러니 코드 라마, 가장 큰 600억을 본다면

168
00:09:54,440 --> 00:09:55,280
매개변수 모델,

169
00:09:56,200 --> 00:09:57,760
600억 개의 매개변수 모델을 보면

170
00:09:57,760 --> 00:09:59,280
코드 라마를 위해.

171
00:09:59,280 --> 00:10:03,120
그러면 약 120GB가 됩니다.

172
00:10:04,080 --> 00:10:05,800
단지 모델 요금 때문입니다.

173
00:10:05,800 --> 00:10:08,920
하지만 내가 보려고 하는 것은 얼마나 많은가이다.

174
00:10:08,920 --> 00:10:10,560
KV 캐시가 될 것입니다.

175
00:10:10,560 --> 00:10:14,400
따라서 600억 개의 매개변수 모델에 대한 레이어 수는

176
00:10:14,400 --> 00:10:15,240
80입니다.

177
00:10:16,200 --> 00:10:20,000
그리고 600억 개의 매개변수 모델의 임베딩 차원

178
00:10:20,000 --> 00:10:20,840
8192입니다.

179
00:10:21,760 --> 00:10:24,120
일부 소형 모델과 달리,

180
00:10:24,120 --> 00:10:26,120
510 등이 있습니다.

181
00:10:26,120 --> 00:10:30,200
그리고 네, 본질적으로 우리는 이것에 대한 다중 클라이언트를 가지고 있습니다.

182
00:10:30,200 --> 00:10:33,120
따라서 우리는 80개의 레이어에 2바이트를 2바이트로 구성했습니다.

183
00:10:33,120 --> 00:10:35,200
8192로.

184
00:10:35,200 --> 00:10:39,600
그리고 다시 한 번 방정식에 배치 크기가 포함됩니다.

185
00:10:39,600 --> 00:10:43,400
그러나 추론을 위해 배치 크기는 본질적으로 다음을 다룹니다.

186
00:10:43,400 --> 00:10:45,680
우리가 처리 중인 요청 수

187
00:10:45,680 --> 00:10:47,240
훈련 배치 크기보다는

188
00:10:47,240 --> 00:10:50,080
그래서 이것을 하나로 유지하더라도

189
00:10:50,280 --> 00:10:52,320
우리는 한 번에 하나씩만 서비스하고 있습니다.

190
00:10:52,320 --> 00:10:54,960
여기까지 완료하면 10GB가 나옵니다.

191
00:10:54,960 --> 00:10:58,840
KV 캐시는

192
00:10:58,840 --> 00:11:01,440
컨텍스트 길이를 완전히 활용한다고 가정

193
00:11:01,440 --> 00:11:03,400
10GB 정도 될 겁니다.

194
00:11:04,360 --> 00:11:07,600
그렇군요. 이건 부모 모델 같군요.

195
00:11:07,600 --> 00:11:08,800
우리가 배포한 것 맞죠?

196
00:11:08,800 --> 00:11:10,840
그래서 코드 라마 2는 최고 중 하나입니다.

197
00:11:10,840 --> 00:11:12,640
최선이라고는 말할 수 없지만 일부 작업의 경우

198
00:11:12,640 --> 00:11:14,240
좋은 오픈 소스 모델.

199
00:11:14,240 --> 00:11:18,680
배치당 10GB인가요, 아니면 10GB인가요?

200
00:11:18,680 --> 00:11:20,920
네, 어떤 의미에서는 요청에 따른 것입니다.

201
00:11:20,920 --> 00:11:24,320
재사용한 건가요, 아니면 조종하는 건가요?

202
00:11:25,600 --> 00:11:28,360
따라서 작업이 끝나면 분명히 해제할 수 있습니다.

203
00:11:28,360 --> 00:11:30,320
하지만 말하자면...

204
00:11:32,320 --> 00:11:34,440
응, 그게 네가 소비할 수 있는 최대치야

205
00:11:34,440 --> 00:11:35,280
요청을 위해.

206
00:11:35,280 --> 00:11:38,720
8,192의 비용은 얼마입니까?

207
00:11:38,720 --> 00:11:42,160
응, 이게 더 큰 문제가 되네

208
00:11:42,160 --> 00:11:44,080
새롭게 출시된 Gemini 모델을 고려한다면

209
00:11:44,080 --> 00:11:46,720
왜냐하면 그들은 천만 개의 컨텍스트 길이를 자랑하기 때문입니다.

210
00:11:47,720 --> 00:11:49,120
예, 다음과 같이 볼 수 있습니다.

211
00:11:49,120 --> 00:11:51,680
그렇게 많은 메모리를 연속적으로 할당할 수는 없습니다.

212
00:11:51,680 --> 00:11:54,840
필요한 만큼만 할당해야 합니다.

213
00:11:54,840 --> 00:11:55,680
이 시점에서

214
00:11:55,680 --> 00:11:58,720
여러 요청이 발생하는 대로 처리할 수 있도록 말이죠.

215
00:11:58,720 --> 00:12:00,440
이 메모리를 초과하면 분명히 충돌이 발생합니다.

216
00:12:00,440 --> 00:12:02,480
그리고 당신은 그것에 대해 아무것도 할 수 없습니다.

217
00:12:02,480 --> 00:12:06,680
그래서 저자가 우려하는 바에 따르면,

218
00:12:06,680 --> 00:12:09,200
그들은 130억 개의 매개변수 모델을 봅니다.

219
00:12:09,200 --> 00:12:12,520
사용 가능한 메모리의 약 30%를 소비합니다.

220
00:12:12,520 --> 00:12:13,640
KV 캐시의 경우.

221
00:12:14,880 --> 00:12:16,120
난 단지 요점을 말하려고 했을 뿐이야

222
00:12:16,120 --> 00:12:18,080
지금은 그게 훨씬 더 큰 문제라는 걸

223
00:12:18,080 --> 00:12:19,160
순진하게 하면.

224
00:12:20,520 --> 00:12:23,480
그래, 그러니까 우리는 연속적인 기억 공간을 가질 수 없어

225
00:12:23,480 --> 00:12:25,440
그렇게 큰 규모로 할당되고

226
00:12:25,440 --> 00:12:29,080
그리고 메모리 공유는 할 수 없는 일이에요

227
00:12:29,960 --> 00:12:31,200
순진한 구현에서.

228
00:12:31,200 --> 00:12:32,120
왜 그렇게 해야 합니까?

229
00:12:32,120 --> 00:12:34,680
이에 대해서는 이후 슬라이드에서 다루겠습니다.

230
00:12:34,680 --> 00:12:39,400
네, 이게 우리가 OS에서 공부하는 것 같군요.

231
00:12:39,400 --> 00:12:41,440
메모리 조각화 및 공유에 대해

232
00:12:41,440 --> 00:12:44,240
그것은 본질적으로 저자도 하는 일과 같습니다.

233
00:12:44,280 --> 00:12:47,360
그러면 논리적인 비유를 할 수 있습니다.

234
00:12:47,360 --> 00:12:51,000
따라서 들어오는 요청을 프로세스로 간주하고 있습니다.

235
00:12:51,000 --> 00:12:53,680
우리가 뭔가를 정의한다고 생각하고 계시나요?

236
00:12:53,680 --> 00:12:54,760
논리적 KV 블록으로

237
00:12:54,760 --> 00:12:56,760
이는 본질적으로 가상 메모리와 동일합니다.

238
00:12:56,760 --> 00:12:58,800
물리적 KV 블록도 정의합니다.

239
00:12:58,800 --> 00:13:01,080
이는 실제 물리적 메모리의 표현입니다.

240
00:13:01,080 --> 00:13:02,640
그리고 그 물리적 메모리.

241
00:13:02,640 --> 00:13:05,640
페이지 테이블과 유사하게 블록 테이블도 정의합니다.

242
00:13:07,080 --> 00:13:10,360
그렇다면 저자는 무엇을 제안하는가?

243
00:13:10,440 --> 00:13:15,440
전체 연속 블록을 할당하는 대신

244
00:13:15,680 --> 00:13:17,600
방금 크기를 계산했는데,

245
00:13:17,600 --> 00:13:21,400
대신에 더 작은 블록으로 나눕니다.

246
00:13:21,400 --> 00:13:24,200
이 특정 예에서는 크기가 4라고 가정하겠습니다.

247
00:13:24,200 --> 00:13:28,040
하지만 실험에 따르면 16이나 32를 사용하는 것이 더 좋습니다.

248
00:13:28,040 --> 00:13:29,320
나중에 살펴보겠습니다.

249
00:13:29,320 --> 00:13:34,320
그리고 크기가 4인 블록이 있다면

250
00:13:34,640 --> 00:13:36,360
그리고 우리는 일종의 추상화를 하고 있어요

251
00:13:36,360 --> 00:13:38,000
다음과 같은 일부 구현 수준 세부 정보

252
00:13:38,040 --> 00:13:41,120
우리는 이러한 KV 블록 각각에 대해 K와 V를 저장하고 있습니다.

253
00:13:41,120 --> 00:13:46,120
하지만 본질적으로 논리적 테이블에는

254
00:13:46,320 --> 00:13:49,200
모든 것이 연속되어 있지만 물리적 테이블에서는

255
00:13:49,200 --> 00:13:51,280
본질적으로 블록 테이블을 사용하여 매핑하고 있습니다.

256
00:13:51,280 --> 00:13:54,400
물리적 테이블 내의 연속되지 않은 청크로.

257
00:13:54,400 --> 00:13:59,400
그리고 예, 모델이 새 토큰을 생성하는 동안

258
00:14:01,640 --> 00:14:05,120
이 블록이 예약되어 있습니다.

259
00:14:05,120 --> 00:14:07,720
그러면 블록에 추가됩니다.

260
00:14:07,760 --> 00:14:09,600
사용 가능한 메모리가 있기 때문입니다.

261
00:14:09,600 --> 00:14:11,800
하지만 이제 새 토큰을 추가하고 싶습니다.

262
00:14:11,800 --> 00:14:14,720
두 블록에 더 이상 공간이 없어요

263
00:14:14,720 --> 00:14:15,640
내가 예약한 것.

264
00:14:15,640 --> 00:14:18,480
이제 다른 블록을 요청해야 합니다

265
00:14:18,480 --> 00:14:20,640
해당 블록이 여기에 할당됩니다.

266
00:14:20,640 --> 00:14:22,920
이 특정 요청을 완료하려면

267
00:14:22,920 --> 00:14:27,840
저는 그 4개의 블록 그룹에 2개의 블록을 더 사용합니다.

268
00:14:27,840 --> 00:14:32,840
따라서 블록 테이블을 사용하면 해당 매핑을 만들 수 있습니다.

269
00:14:33,320 --> 00:14:36,200
확장이 필요한지 계속 추적하세요.

270
00:14:38,440 --> 00:14:41,320
따라서 이를 여러 요청으로 확장하면

271
00:14:41,320 --> 00:14:43,040
당신도 같은 일을 할 수 있습니다.

272
00:14:43,040 --> 00:14:45,000
개별 블록 테이블이 있습니다.

273
00:14:45,000 --> 00:14:46,240
각 요청에 대해

274
00:14:46,240 --> 00:14:48,320
하지만 본질적으로 같은 일을 할 수 있습니다.

275
00:14:48,320 --> 00:14:51,040
하지만 여기서 주목해야 할 점은 이것이 특별하다는 것입니다.

276
00:14:51,040 --> 00:14:52,920
제가 선택한 예입니다.

277
00:14:52,920 --> 00:14:55,320
동일한 메시지가 표시됩니다.

278
00:14:55,320 --> 00:14:58,280
하지만 두 가지 다른 반응이 일어나고 있습니다.

279
00:14:58,280 --> 00:15:01,280
그래서 당신이 할 수 있는 일종의 디코딩은 무엇입니까?

280
00:15:01,280 --> 00:15:06,280
병렬 디코딩을 수행하거나 원하는 경우

281
00:15:07,280 --> 00:15:10,520
예를 들어 개방형 AI를 원하는 경우

282
00:15:10,520 --> 00:15:12,680
두 가지 응답 선택권을 제공하기 위해

283
00:15:12,680 --> 00:15:15,800
예를 들어 채팅 GPT API와의 인터페이스만 있는 경우

284
00:15:15,800 --> 00:15:18,080
GPT-4 또는 GPT-3.5를 사용하여

285
00:15:18,080 --> 00:15:19,760
때로는 다음과 같은 내용이 표시됩니다.

286
00:15:19,760 --> 00:15:21,640
안녕하세요, 제가 받은 응답은 두 가지입니다.

287
00:15:21,640 --> 00:15:22,840
어느 것이 더 좋다고 생각하시나요?

288
00:15:22,840 --> 00:15:24,680
그런 다음 이를 사용하여 모델을 개선합니다.

289
00:15:24,680 --> 00:15:27,800
그러나 그것은 적극적으로 사용되는 것입니다.

290
00:15:27,800 --> 00:15:29,200
그리고 그렇게 하고 싶다면,

291
00:15:29,200 --> 00:15:31,520
이건 좋은 방법은 아닌 것 같아

292
00:15:31,520 --> 00:15:32,920
왜냐면 넌 또 메모리를 낭비하고 있으니까

293
00:15:32,960 --> 00:15:36,800
이러한 요청의 공통 비트에 대해.

294
00:15:36,800 --> 00:15:41,800
네, 그 문제에 뛰어들기 전에

295
00:15:42,760 --> 00:15:44,280
내부 단편화가 있습니다.

296
00:15:44,280 --> 00:15:48,600
하지만 블록 크기에 의해 제한되기 때문에 제한됩니다.

297
00:15:48,600 --> 00:15:50,280
왜냐하면 당신은 그만큼만 예약하기 때문입니다.

298
00:15:50,280 --> 00:15:53,160
그리고 내부 조각화가 발생하면

299
00:15:53,160 --> 00:15:55,000
그건 네가 소비하지 않기 때문이야

300
00:15:55,000 --> 00:15:56,920
그 정도의 블록 크기.

301
00:15:56,920 --> 00:15:59,600
그리고 이 논리에 의한 외부 단편화는 없습니다.

302
00:15:59,600 --> 00:16:03,280
더 많은 메모리 할당이 필요하기 때문입니다.

303
00:16:03,280 --> 00:16:06,240
따라서 외부 단편화라는 개념이 없습니다.

304
00:16:07,440 --> 00:16:10,920
이제 KB 블록을 어떻게 공유합니까?

305
00:16:10,920 --> 00:16:13,960
그럼, 병렬 녹음을 한다고 가정해 봅시다.

306
00:16:13,960 --> 00:16:17,400
우리는 이 특정 문제에 대해 두 가지 응답을 원합니다.

307
00:16:17,400 --> 00:16:21,280
저자가 제안하는 것은 우리에게 뭔가가 있다는 것입니다.

308
00:16:21,280 --> 00:16:23,020
참조 횟수와 같습니다.

309
00:16:23,020 --> 00:16:25,960
따라서 이 두 시퀀스 모두에 대해

310
00:16:25,960 --> 00:16:29,600
참조 횟수가 2인 블록이 있습니다.

311
00:16:29,600 --> 00:16:32,280
지금은 이에 따라 두 가지 요청이 있다고 말씀드렸습니다.

312
00:16:32,280 --> 00:16:34,880
그리고 이 시점에서 나는 분기를 선택한다고 가정해 봅시다.

313
00:16:34,880 --> 00:16:38,240
나는 두 가지 다른 응답 트랙을 선택했습니다.

314
00:16:39,120 --> 00:16:42,680
내가 할 일은 참조 횟수를 하나로 줄이는 것입니다.

315
00:16:42,680 --> 00:16:44,120
그리고 수술이 있어요

316
00:16:44,120 --> 00:16:45,920
쓰기시 복사라고 불리는 것이 허용됩니다.

317
00:16:45,920 --> 00:16:48,400
본질적으로 블록을 복사합니다.

318
00:16:48,400 --> 00:16:49,480
당신이 함께 일하고있는 것입니다.

319
00:16:49,480 --> 00:16:52,840
그러다가 그것만 반복됩니다.

320
00:16:52,840 --> 00:16:55,160
그리고 오늘 우리는 배우고 있습니다

321
00:16:55,160 --> 00:16:56,960
여전히 참조 횟수가 2인 것.

322
00:16:56,960 --> 00:17:01,480
그런 다음 일반적인 흐름으로 계속 진행할 수 있습니다.

323
00:17:03,320 --> 00:17:06,400
저자가 보여주려고 하는 또 다른 사례

324
00:17:06,400 --> 00:17:09,480
그들이 지원할 수 있는 것은 빔 검색입니다.

325
00:17:09,480 --> 00:17:14,480
따라서 포크 추가 및 무료 작업을 통해 지원됩니다.

326
00:17:14,680 --> 00:17:18,240
이는 다시 구현의 일부입니다.

327
00:17:18,240 --> 00:17:21,240
그리고 빔 검색이란 정확히 무엇입니까?

328
00:17:21,240 --> 00:17:24,560
따라서 기계 번역과 같은 작업을 수행하는 경우

329
00:17:24,560 --> 00:17:28,320
시퀀스를 진행하면서 종종 다음과 같은 현상이 발생합니다.

330
00:17:28,320 --> 00:17:30,880
그리고 당신은

331
00:17:30,880 --> 00:17:33,480
내가 영어를 독일어로 계산한다고 가정해 봅시다.

332
00:17:33,480 --> 00:17:35,880
그리고 순서대로 진행하다 보면,

333
00:17:35,880 --> 00:17:37,920
모델은 다음과 같이 결정할 수 있습니다.

334
00:17:37,920 --> 00:17:39,600
아 이 특별한 해석은

335
00:17:39,600 --> 00:17:41,540
내가 생각한 것은 실제로 최선의 방법은 아닙니다.

336
00:17:41,540 --> 00:17:44,120
아마 다른 방식으로 생각했어야 했을 것 같아요.

337
00:17:44,120 --> 00:17:46,400
따라서 빔 검색을 사용하면 다음을 유지할 수 있습니다.

338
00:17:46,400 --> 00:17:48,400
특별한 생각의 광선.

339
00:17:48,400 --> 00:17:51,160
그리고 병렬과 비교했을 때 이것의 문제는

340
00:17:51,160 --> 00:17:53,000
빔을 삭제할 수 있다는 것입니다

341
00:17:53,000 --> 00:17:55,400
여기 바로 위에 새로운 광선이 있을 수 있습니다.

342
00:17:55,400 --> 00:17:58,000
그래서 빔 1이 삭제된 것처럼

343
00:17:58,000 --> 00:18:02,680
이제 새 빔 1은 이전 빔 0이 됩니다.

344
00:18:02,680 --> 00:18:04,440
그리고 그것의 가지.

345
00:18:04,440 --> 00:18:09,440
따라서 이는 이 메모리 공유 모델에서 다시 지원됩니다.

346
00:18:10,540 --> 00:18:15,540
그리고 네, 또 하나 필요한 것은,

347
00:18:17,440 --> 00:18:19,280
이유는 이유가 있었어

348
00:18:19,280 --> 00:18:20,840
왜 우리는 연속적인 기억을 가지고 있었는가?

349
00:18:20,840 --> 00:18:22,800
그리고 그것은 효율적인 지원을 좋아하는 것이었습니다.

350
00:18:22,800 --> 00:18:24,760
이런 주의 계산이죠, 그렇죠?

351
00:18:24,760 --> 00:18:28,480
그래서 저자는 페이지 디텐션도 도입했습니다.

352
00:18:28,480 --> 00:18:31,720
즉, 그들은 단지 커널을 구현합니다.

353
00:18:31,720 --> 00:18:34,760
이 경우에는 다른 블록을 처리해야 합니다.

354
00:18:34,760 --> 00:18:39,760
따라서 본질적으로 다시 작성하는 것입니다.

355
00:18:39,880 --> 00:18:41,400
원래 방정식의

356
00:18:41,400 --> 00:18:44,000
블록별로 계산합니다.

357
00:18:44,000 --> 00:18:47,520
값도 해당 특정 블록에 대한 것입니다.

358
00:18:47,520 --> 00:18:50,520
그런 다음 그것을 주의 점수에 다시 동화시킵니다.

359
00:18:50,520 --> 00:18:53,080
따라서 주의 계산이 일어나는 것을 볼 수 있습니다.

360
00:18:53,080 --> 00:18:55,520
각 블록마다 이동합니다.

361
00:18:57,240 --> 00:19:00,680
네, 그러면 모든 블록을 반복하면 됩니다.

362
00:19:00,680 --> 00:19:02,600
그리고 그것에 대한 관심 점수를 계산합니다.

363
00:19:04,200 --> 00:19:06,400
그럼, 저자들의 결과는 어땠나요?

364
00:19:06,400 --> 00:19:10,600
저자는 ORCA를 다시 구현합니다.

365
00:19:10,600 --> 00:19:11,960
따라서 ORCA를 완전히 사용할 수는 없습니다.

366
00:19:11,960 --> 00:19:14,680
그래서 그들은 ORCA를 다시 구현한 다음 비교합니다.

367
00:19:14,680 --> 00:19:17,240
ORCA를 사용한 모델 제공 기술

368
00:19:17,240 --> 00:19:22,240
또한 빠른 텐서도 있습니다.

369
00:19:23,440 --> 00:19:26,280
Iliuth-Reynman X 그래프와 비교됩니다.

370
00:19:26,280 --> 00:19:29,760
그러나 보시다시피 VLLM 구현은

371
00:19:29,760 --> 00:19:31,640
그게 지금까지 논의된 내용이야

372
00:19:32,600 --> 00:19:35,800
훨씬 더 높은 배치 크기를 지원합니다.

373
00:19:35,800 --> 00:19:38,320
예를 들어 그들은 30개 정도의 요청을 지원하고 있습니다.

374
00:19:38,320 --> 00:19:40,040
어떤 의미에서는 평행하게, 맞습니다.

375
00:19:40,040 --> 00:19:42,600
그렇죠.

376
00:19:42,680 --> 00:19:45,000
응, 그렇구나.

377
00:19:45,000 --> 00:19:47,440
글쎄요, 어떻게 구현하나요?

378
00:19:47,440 --> 00:19:49,560
Python 수준과 같나요, 아니면 켜져 있나요?

379
00:19:49,560 --> 00:19:51,040
아니요, CUDA 수준이어야 합니다.

380
00:19:51,040 --> 00:19:52,360
CUDA 수준이어야 합니다.

381
00:19:52,360 --> 00:19:54,560
지금 당장 그 VM을 망치고 있으니까요, 그렇죠?

382
00:19:54,560 --> 00:19:56,000
아냐, 아냐, 아냐, 알았어.

383
00:19:56,000 --> 00:19:59,640
따라서 YouTube에서 다음을 정의하는 방법이 있습니다.

384
00:19:59,640 --> 00:20:04,640
CUDA 수준의 블록 매핑과 같은 거죠, 그렇죠?

385
00:20:04,920 --> 00:20:05,760
응.

386
00:20:05,760 --> 00:20:08,080
아, 알겠습니다. 무슨 말인지 알겠습니다.

387
00:20:08,080 --> 00:20:10,320
그래서 그 슬라이드에 있는 사람들을 차단해야 했죠

388
00:20:10,400 --> 00:20:12,400
알다시피 확률이 높으니까

389
00:20:12,400 --> 00:20:14,240
이것과 우리에게 무슨 일이 일어나는지.

390
00:20:14,240 --> 00:20:16,840
그들도 비슷한 것을 사용하려고 하는지 아시나요?

391
00:20:16,840 --> 00:20:21,840
테이블과 같은 블록 테이블을 위해 TFB로?

392
00:20:22,840 --> 00:20:24,960
예를 들어, 그것에 대한 일종의 열정이 있습니까?

393
00:20:24,960 --> 00:20:27,240
더 많을 것이라고 생각하면?

394
00:20:27,240 --> 00:20:30,240
이 질문에 대한 대답은 좋지 않다고 생각합니다.

395
00:20:35,240 --> 00:20:40,040
네, 그럼 다음 논문으로 넘어가겠습니다.

396
00:20:40,080 --> 00:20:41,960
이제 우리는 끝났습니다.

397
00:20:41,960 --> 00:20:43,880
그래서 정말 좋은 질문이 생겼습니다.

398
00:20:43,880 --> 00:20:46,760
따라서 이 메커니즘은 Attention Night 메커니즘을 기반으로 구축되었습니다.

399
00:20:46,760 --> 00:20:49,720
아이디어는 후속 토큰에 추가할 수 있다는 것입니다.

400
00:20:49,720 --> 00:20:52,800
이전의 계산을 유지합니다.

401
00:20:52,800 --> 00:20:54,720
Transformer 레이어를 쌓으면,

402
00:20:54,720 --> 00:20:56,440
이런 주의 방해물처럼요, 그렇죠?

403
00:20:56,440 --> 00:21:00,280
다음 레이어의 입력은 이전 레이어의 출력입니다.

404
00:21:00,280 --> 00:21:02,560
그리고 우리는 Qs를 통해 합산합니다.

405
00:21:02,560 --> 00:21:05,000
즉, 이전 레이어의 X가

406
00:21:05,000 --> 00:21:07,040
반드시 같은 X는 아닙니다

407
00:21:07,040 --> 00:21:09,040
모델에 대해 더 깊이 들어갈수록.

408
00:21:09,040 --> 00:21:11,080
이미 지속적으로 할 수 있듯이.

409
00:21:11,080 --> 00:21:13,440
따라서 사전 조치를 시행하면 성능은 어떻게 되나요?

410
00:21:13,440 --> 00:21:16,000
그 블록도 똑같을 거에요?

411
00:21:17,880 --> 00:21:22,880
따라서 KV 캐시 자체는 우리가 수정하는 것이 아닙니다.

412
00:21:24,040 --> 00:21:26,680
마찬가지로 KV 캐시는 오래 전에 이미 존재했습니다.

413
00:21:26,680 --> 00:21:29,760
우리는 본질적으로 변화하고 있습니다.

414
00:21:29,760 --> 00:21:31,640
그냥 KV 캐시를 분해하는 것뿐입니다.

415
00:21:31,640 --> 00:21:35,720
작은 조각을 좋아하는 연속된 메모리 블록이죠, 그렇죠?

416
00:21:35,720 --> 00:21:37,560
오른쪽.

417
00:21:37,560 --> 00:21:39,520
네, 그럼 이 모든 것은 사실에 근거한 것입니다

418
00:21:39,520 --> 00:21:42,880
그것은 이진법이 아니라 인과관계입니다.

419
00:21:42,880 --> 00:21:45,920
그래서, 이 세대 때문에, 그렇죠?

420
00:21:45,920 --> 00:21:49,960
또한 이러한 기본 모델은 인코더가 아닙니다.

421
00:21:49,960 --> 00:21:50,800
그들은 디코더입니다.

422
00:21:50,800 --> 00:21:51,920
네, 디코더일 뿐입니다.

423
00:21:51,920 --> 00:21:54,400
그래, 그래서 당신은 이것을 할 수 있습니다.

424
00:21:55,840 --> 00:21:58,160
그럼 다음으로 넘어가겠습니다.

425
00:21:58,160 --> 00:21:59,880
그래서 우리는 이미 flash attention을 살펴보았습니다.

426
00:21:59,880 --> 00:22:03,600
그리고 내가 이 이야기를 다시 꺼내는 유일한 이유는

427
00:22:03,600 --> 00:22:07,040
다음 논문은 일부 결과에서 파생되기 때문입니다.

428
00:22:07,040 --> 00:22:09,960
순간적으로 관심을 갖고 가져오려고 합니다.

429
00:22:09,960 --> 00:22:13,200
훈련 측면에 어떤 플래시 관심이 있었는지

430
00:22:13,200 --> 00:22:14,880
추론 측면에 대해 설명합니다.

431
00:22:14,880 --> 00:22:19,600
그래서 플래시 어텐션이 가능했던 이유는

432
00:22:19,600 --> 00:22:23,080
출력을 향상시키는 것은 하나였습니다. 왜냐하면

433
00:22:24,440 --> 00:22:27,760
그들은 소프트맥스 계산을 분해할 수 있었습니다

434
00:22:27,760 --> 00:22:32,760
사용자 용어와 사용자가 가져온 키에

435
00:22:33,600 --> 00:22:35,960
우리는 실제로 계산에 얽매이지 않았다는 것입니다.

436
00:22:35,960 --> 00:22:37,640
우리는 기억에 묶여 있습니다.

437
00:22:39,000 --> 00:22:43,400
여기서 우리의 속도를 늦추는 것은 메모리 작업 때문입니다

438
00:22:43,400 --> 00:22:47,480
그리고 플래시 주목 보고서의 그래프를 다시 살펴보겠습니다.

439
00:22:47,480 --> 00:22:51,680
그들은 HVM에서 쓰는 대신에 그것을 보여주었습니다

440
00:22:51,680 --> 00:22:54,200
SRAM에 들어갔다가 계속해서 돌아옵니다.

441
00:22:54,200 --> 00:22:56,960
몇 가지 스크램블을 생성하면

442
00:22:56,960 --> 00:22:58,560
이 모든 작업을 함께 수행하면

443
00:22:58,560 --> 00:23:01,440
주의 계산 속도를 크게 높일 수 있습니다.

444
00:23:02,280 --> 00:23:07,280
그래서, 그리고 또한 행렬 곱셈은 그렇지 않았습니다.

445
00:23:07,360 --> 00:23:09,160
무엇이 가장 많은 시간을 소비했는지

446
00:23:09,160 --> 00:23:11,760
관련된 시대의 분석을 보면.

447
00:23:13,480 --> 00:23:16,160
그래서 우리가 보고 있는 것은 플래시 디코딩입니다.

448
00:23:16,160 --> 00:23:21,160
그래서 애니메이션은 기술적으로

449
00:23:21,520 --> 00:23:23,760
플래시 주의력과 작동 방식을 파괴하고,

450
00:23:25,160 --> 00:23:29,120
하지만 저자가 여기서 다루고자 하는 우려는

451
00:23:29,160 --> 00:23:32,800
만약 당신이 단지 단순한 추론을 하고 있다면

452
00:23:32,800 --> 00:23:36,400
배치 크기가 크지 않고

453
00:23:36,400 --> 00:23:39,440
GPU의 대부분은 사용되지 않은 채로 남아 있을 수 있습니다.

454
00:23:39,440 --> 00:23:43,240
플래시 어텐션이 구현된 방식은

455
00:23:43,240 --> 00:23:46,760
키와 B를 분해하면

456
00:23:46,760 --> 00:23:51,040
하지만 실제로는 여러 배치로 나누려고 하지 않습니다.

457
00:23:51,040 --> 00:23:55,560
그래서 저자가 제안한 수정안은

458
00:23:55,560 --> 00:23:59,600
단지 그것을 더 작은 덩어리로 나누고 다시 계산하는 것뿐입니다.

459
00:23:59,600 --> 00:24:03,880
그래서 순간적으로 주의를 기울이면 이 스칼라 값을 갖게 됩니다.

460
00:24:03,880 --> 00:24:07,120
우리는 일반적인 계산과 함께 계산하고 있었습니다.

461
00:24:07,120 --> 00:24:10,400
어떤 의미에서는 알파와 베타처럼

462
00:24:10,400 --> 00:24:14,640
최종 세트에서 이를 다시 결합할 수 있습니다.

463
00:24:14,640 --> 00:24:19,640
따라서 여기에서도 분할 전체에 걸쳐 동일한 일이 발생합니다.

464
00:24:19,880 --> 00:24:22,120
따라서 이 값을 계속 추적해야 합니다.

465
00:24:22,120 --> 00:24:23,960
각 분할에 대해 결합할 수 있도록

466
00:24:23,960 --> 00:24:24,800
마지막 페이지에.

467
00:24:29,040 --> 00:24:32,760
그래, 그럼 높은 수준에서는 그게 유일한 변화야

468
00:24:32,760 --> 00:24:35,400
플래시 디코딩 내에서 분할 구현

469
00:24:35,400 --> 00:24:38,760
모든 것을 더 작은 덩어리로 처리하도록

470
00:24:38,760 --> 00:24:41,960
이 추가 스칼라를 추적해야 합니다.

471
00:24:41,960 --> 00:24:43,760
이는 로그 합계 지수입니다.

472
00:24:43,760 --> 00:24:46,560
최종 값으로 다시 계산할 수 있습니다.

473
00:24:46,560 --> 00:24:48,520
당신이 얻었어야 했던 것.

474
00:24:48,520 --> 00:24:53,520
그리고 이렇게 함으로써 이 일을 하는 이유는

475
00:24:54,160 --> 00:24:55,720
컨텍스트 길이가 늘어나고 있습니다.

476
00:24:55,720 --> 00:24:57,680
따라서 단일 요청이 들어올 수 있습니다.

477
00:24:57,680 --> 00:24:59,320
확실히 Gemini 모델과 비슷하네요

478
00:24:59,320 --> 00:25:01,360
나는 대략 천만 개의 컨텍스트 길이로 언급했습니다.

479
00:25:01,360 --> 00:25:05,400
그래서 이렇게 엄청난 길이의 프롬프트가 들어오면

480
00:25:06,520 --> 00:25:09,720
순간적인 관심 자체는 잘 되지 않는다

481
00:25:09,720 --> 00:25:11,320
라고 말한 이유 때문에

482
00:25:12,200 --> 00:25:15,680
GPU를 최대한 효율적으로 활용하지 못하고 있습니다.

483
00:25:15,680 --> 00:25:18,000
그리고 그래프를 보면 알 수 있습니다.

484
00:25:18,000 --> 00:25:22,280
그래서 그들은 순진한 PyTorch 프리미티브를 시도합니다.

485
00:25:22,280 --> 00:25:24,640
따라서 이 문서는 PyTorch 자체에서 나온 것입니다.

486
00:25:24,640 --> 00:25:27,760
그래서 그들은 PyTorch 자체의 프리미티브를 시도합니다.

487
00:25:27,760 --> 00:25:30,880
그리고 그것은 실제로 플래시 주의보다 더 잘 작동합니다

488
00:25:30,880 --> 00:25:32,240
어떤 경우에는.

489
00:25:32,240 --> 00:25:35,200
그리고 플래시 디코딩을 사용하면 데이터를 보존할 수 있습니다.

490
00:25:35,200 --> 00:25:37,600
훨씬 더 높은 모델에서도 성능이 향상됩니다.

491
00:25:37,600 --> 00:25:41,160
이 계산을 분석하여 컨텍스트 길이를 계산합니다.

492
00:25:41,160 --> 00:25:45,040
그리고 상당한 양의 속도 향상은

493
00:25:45,040 --> 00:25:47,800
플래시의 관심이 그것을 퍼뜨리는 방식 때문에

494
00:25:47,800 --> 00:25:49,680
Fuse 커널 등을 사용합니다.

495
00:25:49,680 --> 00:25:52,520
그래서 그들은 50X 속도 향상과 같은 것을 활용하고 있습니다.

496
00:25:52,520 --> 00:25:55,600
이는 플래시 어텐션을 사용한 결과입니다.

497
00:25:55,600 --> 00:25:59,120
주의를 순진하게 구현하는 것과는 반대로,

498
00:25:59,120 --> 00:26:01,400
앞뒤로 읽고 있던 계산

499
00:26:01,400 --> 00:26:02,240
기억 속으로.

500
00:26:03,280 --> 00:26:05,440
예, 더 긴 컨텍스트 길이에는 여전히 더 좋습니다.

501
00:26:05,440 --> 00:26:08,000
이것이 여기서 주요 포인트입니다.

502
00:26:09,560 --> 00:26:10,400
응.

503
00:26:10,400 --> 00:26:15,560
그럼 쪼개진다는 말은 무슨 뜻인가요?

504
00:26:15,560 --> 00:26:16,400
KMP?

505
00:26:17,400 --> 00:26:18,240
그래서,

506
00:26:23,680 --> 00:26:27,000
순간적으로 주의를 기울이면 이런 일이 일어나는 것을 볼 수 있습니다.

507
00:26:27,000 --> 00:26:28,240
순차 작업으로요?

508
00:26:28,240 --> 00:26:33,240
전체 KMB 블록에 대해 훈련하기에 적합합니다.

509
00:26:34,880 --> 00:26:38,080
하지만 추론을 하고 있는데 그럴 만한 정보가 없다면

510
00:26:40,280 --> 00:26:42,360
배치 크기가 충분하지 않은 것처럼

511
00:26:42,360 --> 00:26:44,080
전체 GPU를 채우려면.

512
00:26:44,120 --> 00:26:46,480
따라서 GPU가 본질적으로 수행하게 될 작업은 무엇입니까?

513
00:26:46,480 --> 00:26:49,160
기본적으로는 이 작업을 순차적으로 수행하고 있습니다.

514
00:26:49,160 --> 00:26:54,160
더 작은 작업으로 나누는 거죠, 그렇죠?

515
00:26:54,200 --> 00:26:55,040
그래서,

516
00:26:56,560 --> 00:26:57,800
그럴지는 모르겠습니다.

517
00:26:57,800 --> 00:26:58,640
그래서,

518
00:26:58,640 --> 00:26:59,640
그것은 그것을 통과하는 여러 개의 for 루프로 나눕니다.

519
00:26:59,640 --> 00:27:12,600
따라서 단일 쿼리가 들어오더라도

520
00:27:12,640 --> 00:27:14,840
당신은 그것을 더 잘 처리할 수 있습니다.

521
00:27:14,840 --> 00:27:17,040
검색어가 10개도 없는 것처럼,

522
00:27:18,200 --> 00:27:21,320
훈련하는 동안 128 배치 크기처럼 쉽게 만들 수 있습니다.

523
00:27:21,320 --> 00:27:26,000
하지만 당신이 원한다면 대기 시간의 측면이 있습니다

524
00:27:26,000 --> 00:27:26,840
여기에 관련되어 있는 거죠, 그렇죠?

525
00:27:26,840 --> 00:27:29,760
쿼리가 20개 정도 일괄 처리될 때까지 기다리고 싶지는 않을 것입니다.

526
00:27:29,760 --> 00:27:32,120
첫 번째 사람에게 응답하기 위해 오는 것입니다.

527
00:27:32,120 --> 00:27:37,120
네, 그렇다면 이 경우에는 뭔가 조치를 취해야 할 것 같아요

528
00:27:37,120 --> 00:27:39,080
실제로는 그것을 조금 바꿔야 합니다. 그렇죠?

529
00:27:39,080 --> 00:27:43,880
왜냐하면 계산을 하기 전에,

530
00:27:43,880 --> 00:27:46,400
당신은 이미 당신의 첫 번째 것, 새로운 첫 번째 것을 가지고 있습니다.

531
00:27:46,400 --> 00:27:50,880
당신은 계산을 첫 번째로 세웠고,

532
00:27:50,880 --> 00:27:52,320
하지만 출력 데이터에는 있습니다.

533
00:27:52,320 --> 00:27:53,320
응.

534
00:27:53,320 --> 00:27:54,640
그런데 지금 당신은 모든 것을 다 하려고 하고 있어요.

535
00:27:54,640 --> 00:27:56,280
그리고 그것들을 하나로 합칩니다.

536
00:27:56,280 --> 00:27:57,120
응.

537
00:27:57,120 --> 00:27:59,480
그럼 여기에 몇 가지 축소 항목이 있나요?

538
00:27:59,480 --> 00:28:01,320
네, 최종 감소 세트가 있습니다.

539
00:28:01,320 --> 00:28:02,160
알겠어요.

540
00:28:05,880 --> 00:28:08,600
그리고 우리가 추적하고 있는 스칼라는 가중치와 같습니다.

541
00:28:08,600 --> 00:28:10,320
이러한 개별 구성 요소 각각의

542
00:28:10,320 --> 00:28:12,080
우리가 감소 세트를 할 수 있게 해줍니다.

543
00:28:13,080 --> 00:28:13,920
알겠어요.

544
00:28:13,920 --> 00:28:15,240
응.

545
00:28:15,240 --> 00:28:17,200
그래서 내가 이해하는 한,

546
00:28:17,200 --> 00:28:21,840
그래서 이것은 기본적으로 플래시 어텐션(Flash Attention)과 거의 같습니다.

547
00:28:21,840 --> 00:28:24,000
어떤 방식이나 장소와 마찬가지로요.

548
00:28:24,000 --> 00:28:26,400
예, 이것은 전체 논문이 아니며 단지 블로그 게시물일 뿐입니다.

549
00:28:26,400 --> 00:28:27,240
응.

550
00:28:27,240 --> 00:28:30,160
저는 일종의 처리를 위한 것입니다.

551
00:28:30,160 --> 00:28:31,680
오래 갈까요?

552
00:28:31,680 --> 00:28:35,440
응, 그래서 더 나은 성과를 내겠다는 의도야

553
00:28:35,440 --> 00:28:37,360
긴 컨텍스트 길이와 같은가요?

554
00:28:37,360 --> 00:28:38,280
긴 컨텍스트 길이.

555
00:28:38,280 --> 00:28:39,840
작고 나쁜 크기.

556
00:28:39,840 --> 00:28:40,840
추론을 위해.

557
00:28:40,840 --> 00:28:42,160
아, 그래요.

558
00:28:42,160 --> 00:28:45,120
하지만 생성한다면 더 나은 결과를 얻을 수 있을까요?

559
00:28:45,120 --> 00:28:46,440
그렇게 오래?

560
00:28:46,440 --> 00:28:47,280
그게 바로 그거야?

561
00:28:47,280 --> 00:28:48,120
응.

562
00:28:48,120 --> 00:28:52,000
프로세스와 긴 입력에만 특화되어 있습니까?

563
00:28:52,000 --> 00:28:53,600
아, 아뇨, 둘 다에 효과가 있을 겁니다.

564
00:28:53,600 --> 00:28:55,960
출력 모두에 대해 작동해야합니다.

565
00:28:55,960 --> 00:28:56,800
응.

566
00:28:56,800 --> 00:28:57,640
하지만 출력과 마찬가지로

567
00:28:57,640 --> 00:29:00,240
이 기간을 가지고 이만큼 길게 생성해야 합니다.

568
00:29:01,520 --> 00:29:03,600
토론을 위해 그들은 프롬프트 길이까지만 보관합니다.

569
00:29:03,600 --> 00:29:07,240
아, 그들은 긴 프롬프트라는 용어만 사용합니다.

570
00:29:07,240 --> 00:29:08,080
응.

571
00:29:08,880 --> 00:29:10,160
응답이 아닙니다.

572
00:29:10,160 --> 00:29:11,000
아, 그래요.

573
00:29:12,200 --> 00:29:14,320
응, 그냥 똑같은 일이 일어날 수 있을 것 같아

574
00:29:14,320 --> 00:29:16,600
대답을 하자면, 하지만 그래,

575
00:29:16,600 --> 00:29:18,520
점진적으로 구축하고 있기 때문입니다.

576
00:29:18,520 --> 00:29:20,080
아, 맞아, 맞아, 맞아.

577
00:29:20,080 --> 00:29:20,920
그래서 같은 것입니다.

578
00:29:20,920 --> 00:29:21,760
응.

579
00:29:21,760 --> 00:29:23,560
질문은 무엇입니까?

580
00:29:23,560 --> 00:29:27,400
그렇군요. 혹시 이것만 적용되는 것인지 궁금합니다.

581
00:29:27,400 --> 00:29:30,160
긴 메시지를 좋아하나요, 아니면 긴 응답을 좋아하나요?

582
00:29:31,160 --> 00:29:32,000
오른쪽.

583
00:29:32,000 --> 00:29:33,320
알겠어요.

584
00:29:33,320 --> 00:29:35,400
하지만 똑같은 것 같습니다.

585
00:29:35,400 --> 00:29:36,240
알겠어요.

586
00:29:39,080 --> 00:29:42,920
그럼 제가 살펴볼 마지막 논문은

587
00:29:42,920 --> 00:29:45,960
리샤브가 보고 있던 것의 연속이다

588
00:29:45,960 --> 00:29:48,520
추측적 디코딩과 관련하여.

589
00:29:48,520 --> 00:29:51,400
본질적으로 당신이 하려는 일은

590
00:29:51,400 --> 00:29:53,080
추측에 의한 디코딩은

591
00:29:54,720 --> 00:29:57,320
디코딩 단계를 거치는 대신

592
00:29:57,320 --> 00:29:59,360
하나의 토큰만 제공합니다.

593
00:29:59,360 --> 00:30:02,880
일부 디코딩 단계가 포함된 시나리오를 원합니다.

594
00:30:02,880 --> 00:30:06,480
단일 토큰보다 몇 가지 더 많은 토큰을 제공합니다.

595
00:30:06,480 --> 00:30:09,320
따라서 실제로 추론 속도를 높일 수 있습니다.

596
00:30:09,320 --> 00:30:14,320
그러니 만약 네가 쌓아올리듯이 되돌아보고 싶다면

597
00:30:16,760 --> 00:30:21,680
현재까지 Lama 토크나이저에는 32,000개의 토큰이 있습니다.

598
00:30:21,680 --> 00:30:25,440
다음 토큰이 무엇인지 무작위로 추측한다면,

599
00:30:25,440 --> 00:30:28,280
그것은 당신이 그것을 맞힐 확률이 32,000분의 1과 같습니다.

600
00:30:28,280 --> 00:30:32,920
하지만 언어에서는 다음과 같은 일을 할 수 있습니다.

601
00:30:32,920 --> 00:30:35,160
당신은 가지고 있지 않습니다. 모든 토큰의 확률이 동일하지는 않습니다.

602
00:30:35,160 --> 00:30:38,640
그 분포를 사용하여 무언가를 선택할 수 있습니다.

603
00:30:38,640 --> 00:30:41,280
아니면 또 다른 논문이 있다는 것입니다.

604
00:30:41,280 --> 00:30:43,680
그것을 보면 프롬프트를 사용하는 것입니다

605
00:30:43,680 --> 00:30:45,240
추론 속도를 높이려면.

606
00:30:45,240 --> 00:30:50,240
따라서 본질적으로 아이디어는 토큰이 있다는 것입니다.

607
00:30:50,880 --> 00:30:53,440
응답 내에서 반복되는 프롬프트에서.

608
00:30:53,440 --> 00:30:55,600
따라서 이를 기반으로 속도를 높일 수 있습니다.

609
00:30:56,600 --> 00:30:59,280
그것은 작가들이 우리를 본 것입니다.

610
00:31:00,280 --> 00:31:01,720
ngram을 조회 테이블로 사용

611
00:31:01,720 --> 00:31:04,200
이 접근 방식에서도 다시 설명하겠습니다.

612
00:31:04,240 --> 00:31:09,240
그리고 많은 측면의 추측적 디코딩이 사라졌습니다.

613
00:31:09,480 --> 00:31:11,880
더 작은 초안 모델이나 도우미 모델을 사용합니다.

614
00:31:11,880 --> 00:31:14,920
하지만 널리 채택되지는 않았습니다.

615
00:31:14,920 --> 00:31:17,080
다시 제기된 문제 때문이다.

616
00:31:17,080 --> 00:31:22,080
그리고 서로 다른 두 모델을 관리해야 하는 복잡성

617
00:31:23,480 --> 00:31:25,480
훈련할 뿐만 아니라.

618
00:31:25,480 --> 00:31:27,280
그래서 나온 종이가 있었어요

619
00:31:27,280 --> 00:31:30,240
추측 디코딩 후 온라인 추측 디코딩이라고 함

620
00:31:30,240 --> 00:31:33,480
본질적으로 도우미 모델을 개선하려고 시도합니다.

621
00:31:33,480 --> 00:31:36,000
시간이 지남에 따라 더 나은 성능을 발휘합니다.

622
00:31:36,000 --> 00:31:40,640
그리고 메두사(Medusa)도 있습니다.

623
00:31:40,640 --> 00:31:45,160
프로세스 수행을 위한 다중 어텐션 헤드

624
00:31:45,160 --> 00:31:48,000
쉽게 채택할 수 있는 방식으로 말이죠.

625
00:31:48,000 --> 00:31:50,080
Medusa는 좀 더 적용 가능한 접근 방식입니다.

626
00:31:50,080 --> 00:31:53,840
초안 모델을 사용한 추측 디코딩보다.

627
00:31:53,840 --> 00:31:58,840
따라서 이러한 배경을 바탕으로 미리보기 디코딩을 살펴볼 수 있습니다.

628
00:31:59,680 --> 00:32:03,920
이것은 그들이 웹사이트에서 제공한 그래픽일 뿐입니다.

629
00:32:03,920 --> 00:32:07,160
우리가 얼마나 빠르고 추론을 보고 있는지

630
00:32:07,160 --> 00:32:09,280
일반적인 디코딩 프로세스와 비교됩니다.

631
00:32:10,960 --> 00:32:15,240
그리고 우리가 어떻게 더 나은 추측을 할 수 있는가가 문제입니다

632
00:32:15,240 --> 00:32:16,560
우리가 대답하려고 하는 것 맞죠?

633
00:32:16,560 --> 00:32:21,560
네, 그래서 우리는 본질적으로 개선하려고 노력하고 있습니다.

634
00:32:21,880 --> 00:32:23,360
토큰 수락률.

635
00:32:23,360 --> 00:32:26,640
따라서 초안 모델을 내놓는 경우

636
00:32:26,680 --> 00:32:29,360
5개의 서로 다른 토큰, 그리고 그 토큰 모두

637
00:32:29,360 --> 00:32:30,720
그냥 본질적으로 폐기됩니다

638
00:32:30,720 --> 00:32:32,920
왜냐하면 메인 토큰이 이것이 내가 원하는 것이 아니라고 말하기 때문입니다.

639
00:32:32,920 --> 00:32:34,280
그래서 그것은 낭비되는 계산입니다.

640
00:32:34,280 --> 00:32:37,160
그래서 우리는 수용률을 향상시키고 싶습니다.

641
00:32:40,720 --> 00:32:44,640
그리고 그 기술 중 하나가 야코비 반복(Jacobi iteration)입니다.

642
00:32:45,680 --> 00:32:48,400
자코비 방법은 뭔가요

643
00:32:48,400 --> 00:32:51,440
우리가 고등학교 때 겪었을 일

644
00:32:51,440 --> 00:32:54,160
선형대수학에서 배운 것을 기억한다면요.

645
00:32:54,200 --> 00:32:57,880
그러나 본질적으로 저자가 제안한 것은

646
00:32:57,880 --> 00:33:01,760
그것이 당신이 해결하려는 핵심 문제라는 것입니다

647
00:33:01,760 --> 00:33:06,600
응답 시퀀스에서 M개의 토큰을 예측합니다.

648
00:33:06,600 --> 00:33:09,480
X가 있고 Y가 있는데 이것이 바로 여러분의 반응입니다.

649
00:33:09,480 --> 00:33:10,720
그리고 거기에 M 토큰이 있습니다

650
00:33:10,720 --> 00:33:14,040
그리고 당신은 그것을 회귀적으로 해독하려고 합니다.

651
00:33:14,040 --> 00:33:18,760
그리고 우리는 그것을 더 빨리 할 수 ​​있나요?

652
00:33:18,760 --> 00:33:23,240
한 번에 모든 토큰을 계산할 수 있나요?

653
00:33:23,240 --> 00:33:25,560
아니면 한 번에 특정 세트를 좋아하시나요?

654
00:33:26,600 --> 00:33:30,920
이를 위해 그들은 회귀적 디코딩 모델을 다시 작성했습니다.

655
00:33:30,920 --> 00:33:34,280
따라서 본질적으로 회귀 단계에서는

656
00:33:34,280 --> 00:33:36,880
당신은 토큰이 무엇인지 보려고합니다

657
00:33:36,880 --> 00:33:40,160
내가 출력해야 할 것은

658
00:33:40,160 --> 00:33:42,080
나한테 이런 특정한 입력이 있는 거죠, 그렇죠?

659
00:33:42,080 --> 00:33:45,920
그리고 우리가 다음 단계로 나아가면서

660
00:33:45,920 --> 00:33:47,400
답변을 작성하는 동안

661
00:33:47,400 --> 00:33:49,200
각 특정 토큰에 대해 그렇게 합니다.

662
00:33:49,200 --> 00:33:50,720
그래서 그들은 함수를 정의합니다.

663
00:33:51,720 --> 00:33:53,880
여기에 표시된 것은

664
00:33:53,880 --> 00:33:57,040
그리고 그들은 그것을 그 형태로 변환합니다.

665
00:33:58,880 --> 00:34:02,400
따라서 본질적으로 비선형 방정식 시스템이 있습니다.

666
00:34:02,400 --> 00:34:03,720
당신이 해결하려고하는 것입니다.

667
00:34:04,960 --> 00:34:07,680
선형대수학을 기억한다면 야코비 방법(Jacobi method)

668
00:34:07,680 --> 00:34:10,040
선형 방정식 시스템에 적용됩니다.

669
00:34:11,640 --> 00:34:14,680
그리고 그것은 실제로 설명의 문제입니다.

670
00:34:14,680 --> 00:34:18,360
이 논문의 내용은 일반적인 Jacobi 방법과 연결되어 있기 때문입니다.

671
00:34:18,360 --> 00:34:19,640
Wikipedia 페이지 링크처럼

672
00:34:19,640 --> 00:34:22,120
이것은 일반적인 Jacobi 방법을 사용하는 것입니다.

673
00:34:22,120 --> 00:34:23,840
고등학교 때 공부하는 게 이런 거야

674
00:34:23,840 --> 00:34:25,640
Jacobi 방법을 사용하는 방법,

675
00:34:25,640 --> 00:34:28,800
그러나 이는 선형 방정식에만 적용됩니다.

676
00:34:28,800 --> 00:34:32,200
그리고 본질적으로 재충전을 위해 무엇을 하게 되는지

677
00:34:32,200 --> 00:34:35,920
방정식 시스템을 다시 작성하면

678
00:34:35,920 --> 00:34:40,000
각 변수에 이 형식으로 표현하면

679
00:34:40,000 --> 00:34:43,120
각 토큰이 무엇인지 초기 추측합니다.

680
00:34:43,120 --> 00:34:46,560
그런 다음 Newton-Raphson과 비슷하게 반복적으로

681
00:34:46,560 --> 00:34:47,720
또는 그러한 반복적인 방법 중 하나,

682
00:34:47,720 --> 00:34:50,480
방정식 시스템을 반복적으로 풀려고 합니다.

683
00:34:50,480 --> 00:34:53,160
방법이 수렴되면

684
00:34:53,160 --> 00:34:56,160
필요한 모든 토큰이 있습니다.

685
00:34:56,160 --> 00:35:01,160
그러나 이것은 선형 시스템에 대한 것입니다.

686
00:35:01,480 --> 00:35:03,400
그리고 이 작업을 수행하기 위해 내가 찾을 수 있는 유일한 참조는

687
00:35:03,400 --> 00:35:06,800
비선형 방정식에 대해서는 이 특별한 책이 있었습니다.

688
00:35:06,800 --> 00:35:09,680
그리고 네, 그들은 본질적으로 이렇게 말합니다

689
00:35:09,680 --> 00:35:12,480
비선형 시스템에 대해 선형 방법을 사용할 수 있습니다.

690
00:35:12,480 --> 00:35:14,040
그리고 그것은 같은 방식으로 작동합니다.

691
00:35:14,040 --> 00:35:16,120
그리고 이 계산의 한 가지 속성은

692
00:35:16,160 --> 00:35:19,000
모든 단계에서 변수 중 하나를 해결한다는 것입니다.

693
00:35:20,080 --> 00:35:24,920
m 단계를 거치면 전체 방정식을 풀 수 있습니다.

694
00:35:24,920 --> 00:35:28,320
하지만 이는 본질적으로 회귀적으로 수행하는 것과 동일합니다.

695
00:35:28,320 --> 00:35:30,320
왜냐하면 만약 당신이 m걸음을 걸어야 한다면,

696
00:35:32,280 --> 00:35:35,000
순차적으로 하는 것과 동일합니다.

697
00:35:35,000 --> 00:35:37,840
하지만 이 경우에는 방정식 시스템을 풀고 있습니다.

698
00:35:37,840 --> 00:35:38,680
그것에 도달하기 위해.

699
00:35:38,680 --> 00:35:41,240
이것이 Jacobi 반복의 차이점입니다.

700
00:35:41,240 --> 00:35:43,880
그리고 이건 그 책에 나온 알고리즘이에요

701
00:35:43,880 --> 00:35:46,560
병렬화하는 방법에 대해

702
00:35:46,560 --> 00:35:48,160
비선형 야코비 반복.

703
00:35:49,680 --> 00:35:52,720
예, 시각적으로 확인하려면

704
00:35:52,720 --> 00:35:56,080
Jacobi 반복을 수행하면 이런 일이 발생합니다.

705
00:35:56,080 --> 00:36:00,960
프롬프트를 받은 다음 추측을 합니다.

706
00:36:00,960 --> 00:36:02,360
다른 토큰은 무엇입니까?

707
00:36:02,360 --> 00:36:04,960
이것은 Jacobi 방법의 초기 추측과 같습니다.

708
00:36:04,960 --> 00:36:08,200
그리고 당신은 무작위로 그것을 채우고 있습니다

709
00:36:08,200 --> 00:36:11,920
다음 토큰이 무엇이어야 한다고 생각하시나요?

710
00:36:11,920 --> 00:36:15,000
그리고 여기서 다시 문제는

711
00:36:15,000 --> 00:36:17,720
당신은 수술이

712
00:36:17,720 --> 00:36:20,840
메모리 작업에 시간이 걸리는 것처럼

713
00:36:20,840 --> 00:36:24,280
그래서 GPU를 최대한 최적화하고 싶습니다.

714
00:36:24,280 --> 00:36:27,520
따라서 이 특별한 경우에는 계산이 문제가 되지 않습니다.

715
00:36:27,520 --> 00:36:30,280
우리는 특히 모델을 보고 있습니다.

716
00:36:30,280 --> 00:36:33,000
대기 시간을 개선하고 싶은 경우

717
00:36:33,000 --> 00:36:38,000
그리고 계산량이 늘어나서

718
00:36:39,160 --> 00:36:41,600
실제 모델에 들어가면 이런 일이 발생합니다.

719
00:36:41,600 --> 00:36:44,320
하지만 그건 여기서 봐야 할 내용이 아닙니다

720
00:36:44,320 --> 00:36:47,640
왜냐하면 이 특별한 경우에는 대기 시간이 문제이기 때문입니다.

721
00:36:47,640 --> 00:36:52,640
이제 이 경우 초안 모델이 다시는 없습니다.

722
00:36:52,640 --> 00:36:55,320
이를 예측하는 것은 단지 주요 모델일 뿐입니다.

723
00:36:55,320 --> 00:37:00,320
그래서 당신의 메인 모델은 다음 토큰이

724
00:37:00,600 --> 00:37:05,600
Alan Turing에게 다음 토큰은 다음과 같습니다.

725
00:37:07,000 --> 00:37:10,160
그런 다음 작업을 병렬화했습니다.

726
00:37:10,160 --> 00:37:12,760
당신이 추측한 몇 개의 무작위 토큰을 가짐으로써

727
00:37:12,760 --> 00:37:15,200
그리고 모델에게 물어보시죠

728
00:37:15,200 --> 00:37:17,320
이것들도 병렬로 계산합니다.

729
00:37:17,320 --> 00:37:21,320
모델은 이렇게 말합니다. 만약 이 사람이 누구라면,

730
00:37:21,320 --> 00:37:22,880
그러면 다음 토큰은 yes여야 합니다.

731
00:37:22,880 --> 00:37:26,120
그렇다면 A여야 합니다.

732
00:37:26,120 --> 00:37:29,800
당신이 추측한 만큼의 토큰에 대해.

733
00:37:29,800 --> 00:37:32,120
이것이 나중에 설정할 매개변수입니다.

734
00:37:32,120 --> 00:37:34,640
앞으로 얼마나 많은 숫자를 추측하고 싶은지,

735
00:37:34,640 --> 00:37:38,120
하지만 모델은 다음 단계가 무엇인지 계산합니다.

736
00:37:38,160 --> 00:37:42,720
계속 놔두면

737
00:37:43,920 --> 00:37:47,920
우리는 새로 생성된 토큰을 다음과 같이 사용합니다.

738
00:37:47,920 --> 00:37:50,520
무작위 선택으로 업데이트한 것처럼

739
00:37:50,520 --> 00:37:55,520
이제 우리는 한 단계 앞으로 나아갔습니다.

740
00:37:56,400 --> 00:37:57,240
이전 단계와 마찬가지로,

741
00:37:57,240 --> 00:37:58,680
우리는 승인된 토큰을 하나만 얻었습니다

742
00:37:58,680 --> 00:38:00,560
나머지는 낭비된 계산이었습니다.

743
00:38:00,560 --> 00:38:03,040
그러니 그렇게 할 가치가 없었어요.

744
00:38:03,040 --> 00:38:08,040
하지만 무작위로 선택한 초기 추측을 업데이트했다는 점만 제외하면

745
00:38:08,720 --> 00:38:10,520
계산과 유사합니다.

746
00:38:10,520 --> 00:38:11,880
우리는 하나의 변수를 해결했습니다.

747
00:38:11,880 --> 00:38:14,040
하지만 나머지 변수는 여전히 무작위입니다.

748
00:38:14,040 --> 00:38:16,160
아직 제어할 수 없습니다.

749
00:38:16,160 --> 00:38:19,600
이제 모델은 이러한 모든 변수를 살펴봐야 합니다.

750
00:38:19,600 --> 00:38:23,840
다음 번 오픈이 무엇인지 계산해 보세요.

751
00:38:23,840 --> 00:38:27,360
이것이 이 지점의 특정 변수라면.

752
00:38:27,360 --> 00:38:31,400
다시 한 번 동일한 단계를 수행하고 계속합니다.

753
00:38:31,400 --> 00:38:36,240
그리고 그런 경우에는

754
00:38:39,080 --> 00:38:40,880
네, 이 단계도 실패할 것 같아요.

755
00:38:40,880 --> 00:38:42,680
따라서 이 특별한 경우에는

756
00:38:43,760 --> 00:38:46,000
본질적으로 다시 작성하고 모든 단계를 수행하는 것입니다.

757
00:38:46,000 --> 00:38:48,480
단 하나의 새 토큰만 허용됩니다.

758
00:38:48,480 --> 00:38:53,480
하지만 다음 토큰이

759
00:38:54,680 --> 00:38:59,200
메인 모델이 예측하는 것과 동일합니다.

760
00:38:59,200 --> 00:39:01,440
추측성 디코딩을 통해 얻은 결과입니다.

761
00:39:01,440 --> 00:39:04,680
여러 개의 토큰을 허용할 수 있습니다.

762
00:39:04,680 --> 00:39:06,840
이는 다시 우리가 세운 목표였습니다.

763
00:39:06,840 --> 00:39:09,040
몇 걸음만 걷는 것 같은

764
00:39:09,040 --> 00:39:11,120
두 개 이상의 토큰이 허용되는 경우.

765
00:39:11,120 --> 00:39:13,560
나머지는 낭비되는 계산일 뿐입니다.

766
00:39:13,560 --> 00:39:15,040
그래서 우리는 그곳에서 다시 평가를 받았습니다.

767
00:39:15,040 --> 00:39:17,920
하지만 마지막 단계에서 우리는 두 개의 토큰을 수락했습니다.

768
00:39:17,920 --> 00:39:20,920
그래서 이것은 순진하게 Jacobi 반복을 수행하는 것입니다.

769
00:39:23,200 --> 00:39:26,280
저자가 제안하는 것은 좀 더 복잡합니다.

770
00:39:26,280 --> 00:39:29,680
따라서 낭비되는 모든 계산 대신

771
00:39:29,680 --> 00:39:32,000
궤적을 저장했다면 어떨까요?

772
00:39:32,000 --> 00:39:34,360
그 모델은 어떤 것을 찍으려고 했죠?

773
00:39:34,360 --> 00:39:36,080
마찬가지로 n-gram을 저장합니다.

774
00:39:36,120 --> 00:39:37,920
기본 모델을 사용하여 계산하는 경우

775
00:39:37,920 --> 00:39:39,920
다음 토큰은 무엇이어야 하는지,

776
00:39:39,920 --> 00:39:41,920
이 경우에는 이렇게 보입니다.

777
00:39:41,920 --> 00:39:44,800
2그램이라 회복과 매우 비슷해요

778
00:39:44,800 --> 00:39:48,120
우리는 더 높은 목적을 다룰 것입니다.

779
00:39:48,120 --> 00:39:50,680
하지만 시각화를 위해서만

780
00:39:50,680 --> 00:39:52,440
2그램이 생성되고 있습니다

781
00:39:52,440 --> 00:39:56,840
그리고 당신이 할 수 있는 일은 모든 것을 버리기 전에,

782
00:39:56,840 --> 00:40:00,120
당신은 당신이 저장한 2그램을 볼 수 있습니다

783
00:40:00,120 --> 00:40:04,320
그런 다음 그것을 다음 토큰이 무엇인지 조사하는 데 사용하십시오.

784
00:40:04,320 --> 00:40:07,800
이를 사용하여 계속 진행하세요.

785
00:40:07,800 --> 00:40:08,880
반복에서.

786
00:40:08,880 --> 00:40:11,960
상충되는 내용이 있으면 어떻게 되나요?

787
00:40:11,960 --> 00:40:13,080
다음 슬라이드에 있나요?

788
00:40:13,080 --> 00:40:15,880
하지만 적어도 시각화를 위해서는

789
00:40:15,880 --> 00:40:19,080
당신은 케이스가 하나뿐인 케이스가 있습니다

790
00:40:19,080 --> 00:40:20,480
그 것과 일치하는 것.

791
00:40:20,480 --> 00:40:22,760
아, 기본적으로 이것은 캐시와 같습니다. 그렇죠?

792
00:40:22,760 --> 00:40:24,440
예, 캐시가 구현되었습니다.

793
00:40:24,440 --> 00:40:28,240
대상 모델의 기반이 무엇인지 대략적으로 알 수 있습니다.

794
00:40:28,240 --> 00:40:29,600
응, 초안 모델이 없으니까

795
00:40:29,600 --> 00:40:32,320
타겟 모델의 분포가 모두 일어나는 거죠, 그렇죠?

796
00:40:33,320 --> 00:40:36,920
하지만 실패하는 경우도 있어요

797
00:40:36,920 --> 00:40:40,600
이 2그램은 두 곳에서 발생할 수 있습니다.

798
00:40:40,600 --> 00:40:42,840
네, 두 곳에서 그런 일이 발생하면

799
00:40:42,840 --> 00:40:44,640
확인 단계를 거쳐야 합니다

800
00:40:44,640 --> 00:40:47,560
실제 구현은 이보다 훨씬 더 복잡합니다.

801
00:40:47,560 --> 00:40:48,560
이건 2그램이니까

802
00:40:48,560 --> 00:40:50,800
이는 추측적 디코딩과 매우 유사합니다.

803
00:40:50,800 --> 00:40:53,240
하지만 실제 사례는 n-gram입니다.

804
00:40:54,480 --> 00:40:56,360
따라서 이 특별한 경우에는

805
00:40:56,360 --> 00:40:58,440
n이 4로 설정되어 있고,

806
00:40:58,440 --> 00:41:00,520
그래서 결국 4그램이 되는 거죠

807
00:41:01,440 --> 00:41:02,440
예.

808
00:41:02,440 --> 00:41:04,280
그러니 숫자는 의미가 없고,

809
00:41:04,280 --> 00:41:05,120
제발 그걸 보지 마세요.

810
00:41:05,120 --> 00:41:06,640
숫자는 본질적으로 말해줍니다

811
00:41:06,640 --> 00:41:08,440
이 특정 토큰의 위치는 무엇입니까

812
00:41:08,440 --> 00:41:10,320
초기 토큰과 관련하여

813
00:41:10,320 --> 00:41:11,880
예측 과정에서.

814
00:41:11,880 --> 00:41:16,880
따라서 이 특별한 경우에 유효한 n-그램은 다음과 같습니다.

815
00:41:20,760 --> 00:41:22,880
하나, 둘, 셋이 있다고 가정해 보겠습니다.

816
00:41:22,880 --> 00:41:24,400
그리고 이 단계에서 무슨 일이 일어나든 말이죠.

817
00:41:24,400 --> 00:41:27,320
이제 우리는 Jacob 반복의 마지막 단계에 있습니다.

818
00:41:27,320 --> 00:41:29,120
확인 단계가 발생하는 곳입니다.

819
00:41:29,160 --> 00:41:30,560
그래서 네 번째 토큰이

820
00:41:30,560 --> 00:41:32,240
실제로는 다이어그램에 없지만

821
00:41:32,240 --> 00:41:35,800
하지만 본질적으로 n-gram이 생성되고 있습니다.

822
00:41:35,800 --> 00:41:39,920
그리고 다음 토큰도 예측해야 합니다

823
00:41:39,920 --> 00:41:41,320
일어난 일을 바탕으로.

824
00:41:42,440 --> 00:41:44,200
그리고 이 전체 문제에는 두 가지 매개변수가 있습니다.

825
00:41:44,200 --> 00:41:47,840
우리가 얼마나 멀리 보고 있는지에 대한 창 크기가 있습니다.

826
00:41:47,840 --> 00:41:49,920
그래서 얼마나 많은 새로운 토큰이 결정되는지를 결정합니다.

827
00:41:49,920 --> 00:41:51,280
당신은 각 단계에서 생성하고 있습니다.

828
00:41:51,280 --> 00:41:52,440
따라서 이 경우에는 5개가 있습니다.

829
00:41:52,440 --> 00:41:54,360
그래서 창 크기가 5인 이유입니다.

830
00:41:54,360 --> 00:41:56,400
그리고 n-gram 크기가 제어하고 있습니다.

831
00:41:56,400 --> 00:41:58,200
당신이 얼마나 많은 역사를 유지하고 있는지.

832
00:41:58,200 --> 00:41:59,960
그래서 당신은 4그램을 만들고 있습니다.

833
00:41:59,960 --> 00:42:02,600
이것이 바로 n-gram 크기가 4인 이유입니다.

834
00:42:02,600 --> 00:42:07,600
이제 시나리오가 있다면

835
00:42:08,200 --> 00:42:12,600
선택할 수 있는 올바른 n-그램이 여러 개 있는 경우

836
00:42:12,600 --> 00:42:14,160
그런 다음 확인을 해야 합니다.

837
00:42:14,160 --> 00:42:15,680
따라서 이 특별한 경우에는

838
00:42:15,680 --> 00:42:18,240
g라는 추가 매개변수가 있습니다.

839
00:42:18,240 --> 00:42:21,360
확인하기로 선택한 개수입니다.

840
00:42:21,360 --> 00:42:25,400
모든 실험에서 저자는 이를 w로 설정하도록 선택했습니다.

841
00:42:25,400 --> 00:42:29,440
창 크기는 사물의 수와 같습니다

842
00:42:29,440 --> 00:42:30,520
확인을 선택합니다.

843
00:42:30,520 --> 00:42:33,920
그리고 이것이 보여주는 것은 주의 마스크입니다.

844
00:42:33,920 --> 00:42:36,520
본질적으로 말하는 유일한 것은 다음과 같습니다.

845
00:42:36,520 --> 00:42:37,480
인과적 관심과 동일

846
00:42:37,480 --> 00:42:39,800
당신은 그 앞에 있는 토큰만 보고 있을 뿐입니다.

847
00:42:39,800 --> 00:42:43,720
따라서 토큰 1은 자신과 0만 볼 수 있습니다.

848
00:42:43,720 --> 00:42:47,640
그리고 모든 단계에서 동일한 작업이 수행됩니다.

849
00:42:47,640 --> 00:42:52,640
따라서 검증 지점에서는 아무것도 볼 수 없습니다.

850
00:42:53,720 --> 00:42:54,640
미리보기 지점에서,

851
00:42:54,640 --> 00:42:56,360
그래서 모든 것이 가려져 있습니다.

852
00:42:56,360 --> 00:42:59,040
그리고 다시, 당신은 그것을 갖게 됩니다.

853
00:42:59,040 --> 00:43:01,080
이전 토큰에만 주의

854
00:43:01,080 --> 00:43:02,560
그리고 이제 막 이 일을 시작하고 있어요.

855
00:43:02,560 --> 00:43:05,200
확인 중입니다. 이 단계에서는 0만 있습니다.

856
00:43:05,200 --> 00:43:07,120
그 밖의 모든 일은 다음을 통해 일어나고 있습니다.

857
00:43:08,080 --> 00:43:10,160
마치 반복적으로.

858
00:43:10,160 --> 00:43:13,440
그래서 이것은 검증 지점을 위한 것입니다.

859
00:43:13,440 --> 00:43:17,840
이것이 바로 지금 일어나고 있는 예측입니다.

860
00:43:17,840 --> 00:43:21,560
그리고 그들은 그것을 하나의 병렬 단계로 결합합니다.

861
00:43:21,560 --> 00:43:22,560
효율적으로 하려고.

862
00:43:24,640 --> 00:43:28,520
그리고 네, 이건 단지 비교일 뿐입니다

863
00:43:28,520 --> 00:43:30,840
주의 마스크가 어떻게 생겼는지

864
00:43:30,840 --> 00:43:32,440
메두사와 비교해 보면요.

865
00:43:32,440 --> 00:43:35,520
메두사에는 머리가 여러 개 있습니다

866
00:43:35,520 --> 00:43:37,640
그리고 첫 번째 머리가 그렇게 말한다면

867
00:43:37,640 --> 00:43:39,560
다음 토큰은 게이트 파 i가 될 수 있습니다

868
00:43:39,560 --> 00:43:42,840
두 가지만 선택하는 경우와 같이

869
00:43:42,840 --> 00:43:44,880
다음 머리는 다시 계속 될 것입니다

870
00:43:44,880 --> 00:43:47,680
토큰 세 개를 말하세요.

871
00:43:47,680 --> 00:43:50,920
i 경우에는 다시 3개의 토큰이 생성됩니다.

872
00:43:50,920 --> 00:43:53,360
그리고 그것이 바로 주의 마스크의 모습입니다.

873
00:43:54,360 --> 00:43:56,960
이 주의 마스크가 보이는 것과 대조됩니다.

874
00:43:59,080 --> 00:44:00,520
응, 난 대부분 깨어있지만 깨어 있었어.

875
00:44:00,520 --> 00:44:03,280
그래서 스케일링 법칙은 그들이 논의하는 것입니다.

876
00:44:03,280 --> 00:44:05,520
이 논문의 결과를 바탕으로.

877
00:44:05,520 --> 00:44:10,520
그래서 그들이 찾은 것은 N이 충분히 크다면

878
00:44:10,960 --> 00:44:13,800
빌더 크기를 기하급수적으로 늘리면

879
00:44:13,800 --> 00:44:17,680
단계 압축에서 선형 추세를 얻을 수 있습니다.

880
00:44:17,680 --> 00:44:20,960
따라서 디코딩 단계 수를 선형적으로 줄일 수 있습니다.

881
00:44:20,960 --> 00:44:21,960
이는 필수입니다.

882
00:44:23,360 --> 00:44:24,880
그래서 그들이 찾아낸 결과가 바로 이것이다.

883
00:44:24,880 --> 00:44:26,480
그들의 실험으로.

884
00:44:26,480 --> 00:44:28,520
그리고 이것이 어떤 영향을 미치나요?

885
00:44:29,600 --> 00:44:32,680
그래서 라마챗 모델에 적용했어요

886
00:44:32,680 --> 00:44:35,880
1.5배의 속도 향상을 관찰했습니다.

887
00:44:35,880 --> 00:44:39,280
인간 평가에 대한 대기 시간 감소

888
00:44:39,280 --> 00:44:41,680
수학 문제도 그렇고.

889
00:44:41,680 --> 00:44:43,600
네, GSM 8K입니다.

890
00:44:43,600 --> 00:44:47,400
그렇군요. 그러면 대기 시간이 줄어듭니다.

891
00:44:47,400 --> 00:44:49,400
이것이 이 모든 일의 주요 목표였습니다.

892
00:44:49,400 --> 00:44:53,560
다시 계산을 낭비하므로 여기서는 문제가 되지 않습니다.

893
00:44:53,560 --> 00:44:56,760
당신은 대부분의 GPS를 효율적으로 활용하려고 노력하고 있습니다.

894
00:44:58,680 --> 00:45:02,040
그래서 저는 LLM 추론을 봅니다.

895
00:45:02,040 --> 00:45:03,720
메모리 제약으로 인해 병목 현상이 발생했습니다.

896
00:45:03,720 --> 00:45:06,040
그리고 우리는 건물을 짓는 방법을 보았습니다.

897
00:45:06,040 --> 00:45:07,800
보다 효율적인 KV 캐시.

898
00:45:09,960 --> 00:45:12,040
저자는 또한 커널처럼 구현해야 했습니다.

899
00:45:12,040 --> 00:45:14,800
블록 단위 주의 또는 페이지 유지와 같은 경우

900
00:45:14,800 --> 00:45:15,640
그들이 부르는대로.

901
00:45:16,560 --> 00:45:18,720
다음 논문에서는 플래시 디코딩을 살펴보았습니다.

902
00:45:18,720 --> 00:45:20,840
향상된 기능을 가져오려고 했던 것입니다.

903
00:45:20,840 --> 00:45:24,200
플래시 어텐션 V1과 V2를 추론 단계로 전환합니다.

904
00:45:24,200 --> 00:45:26,160
그리고 우리가 마지막으로 본 논문은

905
00:45:26,160 --> 00:45:28,040
미리보기 디코딩 중이었어

906
00:45:28,040 --> 00:45:30,920
추측성 디코딩을 개선하려고 노력하고 있는 것입니다.

907
00:45:30,920 --> 00:45:32,720
하지만 다른 접근 방식을 취하세요

908
00:45:32,720 --> 00:45:34,240
초안 모델을 사용하는 것보다

909
00:45:34,240 --> 00:45:37,720
네, 대기 시간이 중요한 애플리케이션의 경우

910
00:45:37,720 --> 00:45:40,720
낭비되는 계산이 많이 발생하기 때문입니다.

911
00:45:40,720 --> 00:45:41,560
감사합니다.

912
00:45:41,560 --> 00:45:42,400
감사합니다.

913
00:45:42,400 --> 00:45:43,240
감사합니다.