1
00:00:00,000 --> 00:00:07,920
안녕하세요 여러분. 제 이름은 Shengyu Chen이고 이쪽은 Eric Huang입니다. 오늘 우리는

2
00:00:07,920 --> 00:00:13,840
다중 모델 모델 사전 훈련을 제시합니다.

3
00:00:13,840 --> 00:00:17,280
그럼 오늘 다룰 내용에 대해 간단히 살펴보겠습니다.

4
00:00:17,280 --> 00:00:22,240
먼저 소개와 배경, 특히 제가 이야기하는 부분에 대해 이야기하겠습니다.

5
00:00:22,240 --> 00:00:26,760
다중 모델 표현 및 다중 모델 작업에 대해 설명합니다.

6
00:00:26,760 --> 00:00:32,280
그런 다음 7가지 다중 모델 사전 학습에 대해 이야기하겠습니다.

7
00:00:32,280 --> 00:00:38,840
논문을 주요 기여도별로 분류합니다.

8
00:00:38,840 --> 00:00:44,640
그런 다음 마지막에는 검토 및 요약 섹션을 갖게 됩니다.

9
00:00:44,640 --> 00:00:49,760
그리고 다루어야 할 내용이 많기 때문에 각 보고서 끝 부분에 질문을 저장해 두십시오.

10
00:00:49,760 --> 00:00:51,640
오늘.

11
00:00:51,640 --> 00:00:54,080
그럼 첫 번째 소개와 배경을 살펴보겠습니다.

12
00:00:54,080 --> 00:01:02,760
다중 모델 표현의 대부분인 다중 양식은 이질적이고

13
00:01:02,760 --> 00:01:08,200
상호 연결된 데이터이며 이질적이라는 것은 작업 이미지, 오디오와 같을 수 있음을 의미합니다.

14
00:01:08,200 --> 00:01:11,320
그리고 데이터의 형식이 다릅니다.

15
00:01:11,320 --> 00:01:18,920
그리고 우리의 목표는 다중 모델 데이터를 공동 표현으로 표현하고 요약하는 것입니다.

16
00:01:18,920 --> 00:01:24,760
보완적인 정보가 공동 표현에 보존되도록

17
00:01:24,760 --> 00:01:28,480
다중 양식의 중복 부분은 필터링됩니다.

18
00:01:28,480 --> 00:01:36,160
그래서 여기에 세 가지 다른 양식의 시각화가 있습니다. 즉, 작업 이미지와

19
00:01:36,160 --> 00:01:40,640
오디오는 공동 공간에서 표현될 수 있습니다.

20
00:01:40,640 --> 00:01:44,440
그리고 오늘 프레젠테이션에서는 비전 언어 사전 훈련에만 중점을 둘 것입니다.

21
00:01:44,440 --> 00:01:50,880
따라서 우리는 예를 들어 오디오 데이터에 대해서는 다루지 않을 것입니다.

22
00:01:50,880 --> 00:01:57,760
다중 모델 모델에 대한 몇 가지 작업도 있습니다.

23
00:01:57,760 --> 00:02:01,040
따라서 두 가지 주요 작업이 있습니다.

24
00:02:01,040 --> 00:02:04,640
하나는 이해이고, 다른 하나는 생성 작업이다.

25
00:02:04,640 --> 00:02:08,840
따라서 이해를 위해 이미지 텍스트 검색을 사용할 수 있습니다.

26
00:02:08,840 --> 00:02:13,760
따라서 이것은 기본적으로 주어진 이미지 텍스트 쌍이 정렬되었는지 여부를 예측합니다.

27
00:02:13,760 --> 00:02:19,240
그리고 자연어 시각적 추론과 LBR이 있습니다.

28
00:02:19,240 --> 00:02:23,840
따라서 이미지와 텍스트 쌍이 주어지면 텍스트 문이 다음과 같은지 여부를 결정해야 합니다.

29
00:02:23,840 --> 00:02:25,920
이미지에 대해서는 사실입니다.

30
00:02:25,920 --> 00:02:28,280
여기에 예가 있습니다.

31
00:02:28,280 --> 00:02:35,680
우리는 두 세트의 사진을 가지고 있으며 설명에는 총 두 대의 열차가 여행하고 있다고 나와 있습니다.

32
00:02:35,680 --> 00:02:36,680
같은 방향.

33
00:02:36,680 --> 00:02:41,680
그러니까 얘기하는 부분은 맞고 하나는 틀렸어요.

34
00:02:41,680 --> 00:02:53,520
그리고 이해의 또 다른 작업은 간단히 말해서 VQA에 대답하는 시각적 질문입니다.

35
00:02:53,520 --> 00:02:58,160
따라서 이미지와 질문이 주어지면 모델은 다음 중 정답을 선택해야 합니다.

36
00:02:58,160 --> 00:03:00,320
다중 선택 목록.

37
00:03:00,320 --> 00:03:06,800
예를 들어, 이 사진에서 다음과 같은 질문을 할 수 있습니다. 테이블 위에 있는 트레이는 검은색인가요?

38
00:03:06,800 --> 00:03:09,480
아니면 밝은 갈색?

39
00:03:09,480 --> 00:03:14,280
우리는 모델이 연한 갈색으로 대답할 것으로 예상합니다.

40
00:03:14,280 --> 00:03:19,720
그리고 생성 작업에서는 주로 이미지 캡션을 작성합니다.

41
00:03:19,720 --> 00:03:24,160
따라서 우리는 모델이 콘텐츠에 대한 자연어 설명을 생성하기를 원합니다.

42
00:03:24,160 --> 00:03:25,720
이미지.

43
00:03:25,720 --> 00:03:27,680
여기에 예가 있습니다.

44
00:03:27,680 --> 00:03:34,080
예를 들어, 노란색 넥타이를 매는 남자가 걱정하는 표정을 짓는 것처럼 모델을 생성하려고 합니다.

45
00:03:34,080 --> 00:03:43,040
또한 VQA를 개방형 VQA로 공식화하여 생성 작업이 될 수도 있습니다.

46
00:03:43,040 --> 00:03:51,760
그리고 2020년 이전의 BLP 방법에 대한 몇 가지 역사입니다.

47
00:03:51,760 --> 00:03:59,480
예를 들어, 시각적 영역 특징과 단어 임베딩을 단순히 연결하는 시각적 새

48
00:03:59,480 --> 00:04:07,120
쌍 태그를 변환기 모델에 대한 입력으로 사용한 다음 self-attention을 사용하여 암시적으로

49
00:04:07,120 --> 00:04:11,880
작업, 태그 및 이미지 영역의 요소를 정렬합니다.

50
00:04:11,880 --> 00:04:12,880
여기에 예가 있습니다.

51
00:04:12,880 --> 00:04:22,480
여기에서와 같이 시각적 특징을 추적하는 몇 가지 객체 감지기가 있습니다.

52
00:04:22,480 --> 00:04:32,480
그런 다음 캡션이 있습니다. 즉, 필드를 알 수 있도록 함께 태그를 지정할 수 있습니다.

53
00:04:32,480 --> 00:04:36,080
플랫폼과 블록을 통해 그들에게 다가갑니다.

54
00:04:36,080 --> 00:04:44,840
그런 다음 다양한 VQA 기능을 출력으로 추가하여 모델을 학습할 수 있습니다.

55
00:04:44,840 --> 00:04:50,520
그런 다음 self-attention을 통해 예를 들어 사이의 히트 맵을 배울 수 있기를 바랍니다.

56
00:04:50,520 --> 00:05:03,280
시각적 특징과 트랙 특징 사이의 시각적 특징.

57
00:05:03,280 --> 00:05:07,000
그래서 오늘은 우리 논문에 대해 이야기를 시작하겠습니다.

58
00:05:07,000 --> 00:05:13,160
첫 번째 그룹은 시각적 표현 개선에 관한 것입니다.

59
00:05:13,160 --> 00:05:16,600
우리가 이야기할 첫 번째 논문은 OSCAR입니다.

60
00:05:17,000 --> 00:05:21,920
의미론은 시각적 언어 작업을 위한 사전 훈련을 조정했습니다.

61
00:05:21,920 --> 00:05:26,920
이 논문의 주요 기여는 텍스트와 이미지 간의 의미론적 정렬입니다.

62
00:05:26,920 --> 00:05:31,520
객체 태그를 사용합니다.

63
00:05:31,520 --> 00:05:39,240
앞서 볼 수 있듯이 시각적 새에 대한 과제는 하나, 모호함, 이미지 영역입니다.

64
00:05:39,240 --> 00:05:46,280
서로 다른 위치에서 겹치면 추출된 시각적 임베딩이 모호해집니다.

65
00:05:47,160 --> 00:05:51,000
두 번째 과제는 명시적인 정렬이 부족하다는 것입니다.

66
00:05:51,000 --> 00:06:00,200
따라서 이미지 텍스트 쌍의 개체 영역 간에는 명시적으로 레이블이 지정된 정렬이 없습니다.

67
00:06:00,200 --> 00:06:05,000
따라서 OSCAR의 동기는 두드러진 개체를 사용하는 것입니다.

68
00:06:05,000 --> 00:06:08,920
따라서 물체 감지기로 정확하게 감지할 수 있습니다.

69
00:06:08,920 --> 00:06:12,520
그리고 그들은 짝을 이루는 텍스트에서 자주 언급됩니다.

70
00:06:12,600 --> 00:06:16,520
이미지 간의 의미적 정렬을 학습하기 위한 기준점으로 사용할 수 있습니다.

71
00:06:16,520 --> 00:06:18,920
지역 특징과 단어 임베딩.

72
00:06:18,920 --> 00:06:23,080
여기 소파에 앉아 있는 개가 있는 예가 있습니다.

73
00:06:23,080 --> 00:06:29,240
여기 개와 소파는 이미지에서 감지될 수 있기 때문에 눈에 띄는 개체입니다.

74
00:06:29,240 --> 00:06:33,720
그리고 그것들은 본문에도 포함되어 있습니다.

75
00:06:33,720 --> 00:06:37,400
그래서 앵커 포인트를 추출하는 방법입니다.

76
00:06:37,480 --> 00:06:45,640
따라서 OSCAR 작성자는 더 빠른 RCNN을 사용하여 먼저 시각적 임베딩을 추출합니다.

77
00:06:46,680 --> 00:06:51,240
각 영역의 시각적 의미를 v 프라임 및 z로 추출합니다.

78
00:06:52,200 --> 00:06:56,120
v prime은 영역 기능이고 z는 영역 위치입니다.

79
00:06:57,960 --> 00:07:02,600
위치 감지 영역 특징 벡터를 형성하기 위해 이들을 서로 연결합니다.

80
00:07:03,160 --> 00:07:10,280
그리고 그들은 v 프라임과 z를 v로 전송하기 위해 훈련 가능한 선형 투영을 사용합니다.

81
00:07:11,880 --> 00:07:14,840
이는 숨겨진 차원 h의 벡터입니다.

82
00:07:16,200 --> 00:07:21,240
여기 이 그래프에서 독자가 인쇄 소스를 특징으로 하는 것을 볼 수 있습니다.

83
00:07:25,480 --> 00:07:27,960
그런 다음 단어 임베딩을 추출합니다.

84
00:07:28,520 --> 00:07:33,480
먼저 더 빠른 RCNN을 사용하여 태그 임베딩을 추출합니다.

85
00:07:35,720 --> 00:07:43,000
따라서 시각적 객체를 추출한 후에는 텍스트도 얻을 수 있고 태그를 피드에 제공할 수도 있습니다.

86
00:07:44,040 --> 00:07:50,520
역시 h차원인 단어 토큰을 대기열에 추가하는 새 토크나이저입니다.

87
00:07:51,480 --> 00:08:00,280
여기에서는 지역 임베딩과 비교하여 각 섹터의 태그 탐색자 데이터를 볼 수 있습니다.

88
00:08:01,640 --> 00:08:07,080
그런 다음 새 토크나이저를 사용하여 텍스트나 캡션을 추출합니다.

89
00:08:08,440 --> 00:08:17,640
텍스트 임베딩 w로, 삽입된 텍스트를 역시 h차원 벡터인 단어 토큰 w로 만듭니다.

90
00:08:17,640 --> 00:08:23,400
그리고 그들이 하는 일은 단순히 그것들을 모두 연결하는 것입니다.

91
00:08:25,480 --> 00:08:31,000
여기에 캡션 토큰이 있고 다른 태그의 토큰이 있습니다.

92
00:08:31,560 --> 00:08:34,760
우리는 이미지로부터 임베딩을 갖고 있습니다.

93
00:08:36,360 --> 00:08:43,960
그런 다음 두 가지를 합친 후에는 두 가지 다른 관점에서 다양한 임베딩을 볼 수 있습니다.

94
00:08:44,760 --> 00:08:47,960
하나는 여기에 표시된 양식 보기입니다.

95
00:08:48,600 --> 00:08:58,040
따라서 양식 보기에서는 텍스트 양식의 새 토큰인 텍스트 양식을 볼 수 있습니다.

96
00:08:58,680 --> 00:09:06,120
그런 다음 지역 특징과 관련 개체 텍스트를 이미지 양식으로 결합할 수 있습니다.

97
00:09:06,120 --> 00:09:11,400
그리고 우리의 목표는 텍스트와 이미지의 표현을 구별하는 것입니다.

98
00:09:12,200 --> 00:09:15,560
우리는 사전적 관점에서도 그것들을 볼 수 있습니다.

99
00:09:15,560 --> 00:09:22,360
따라서 사전 보기에는 단어 토큰을 넣는 언어적 의미 공간이 있습니다.

100
00:09:22,360 --> 00:09:27,240
그리고 객체 텍스트를 함께 넣은 다음 이미지 영역 기능을 별도로 배치합니다.

101
00:09:27,240 --> 00:09:32,280
그리고 우리의 목표는 텍스트와 이미지 사이의 의미 공간을 구별하는 것입니다.

102
00:09:33,000 --> 00:09:42,440
따라서 양식 보기에 대한 손실을 설계하기 위해 w와 같은 텍스트 양식이 있다는 것을 기억합니다.

103
00:09:42,440 --> 00:09:47,640
그런 다음 객체 태그와 영역 기능을 h 소수로 연결합니다.

104
00:09:47,640 --> 00:09:55,400
그런 다음 오스카는 태그의 50%가 포함된 이미지 세트를 포함하도록 h 프라임을 오염시킵니다.

105
00:09:55,400 --> 00:09:57,560
다른 태그로 대체됩니다.

106
00:09:57,720 --> 00:10:02,840
그리고 이미지 텍스트 양식 쌍이 ​​다음과 같은지 여부를 예측하기 위해 이진 분류기 f를 훈련시킵니다.

107
00:10:02,840 --> 00:10:06,040
원본 이미지나 오염된 이미지가 포함되어 있습니다.

108
00:10:06,040 --> 00:10:14,040
그런 다음 이진 분류기가 얼마나 좋은지 간단히 측정하는 대조 손실을 가질 수 있습니다.

109
00:10:14,040 --> 00:10:19,880
그리고 우리의 목표는 텍스트가 쌍을 이루는 이미지와 유사한 단어 임베딩 공간을 해결하는 것입니다.

110
00:10:19,880 --> 00:10:22,440
이는 오염된 이미지와 유사합니다.

111
00:10:22,600 --> 00:10:25,160
이것이 양식적 관점의 손실입니다.

112
00:10:26,760 --> 00:10:29,720
그리고 사전 보기에 대한 손실을 설계합니다.

113
00:10:31,000 --> 00:10:37,320
따라서 기억해야 할 점은 토큰 임베딩이라는 단어와 객체 태그 임베딩을 h로 연결한 것입니다.

114
00:10:37,320 --> 00:10:40,520
그런 다음 지역 특성을 별도로 처리합니다.

115
00:10:41,160 --> 00:10:50,120
그리고 이것을 위해 그들은 h에 있는 토큰의 15%를 h 프라임으로 대체합니다.

116
00:10:50,200 --> 00:10:56,120
h의 토큰 중 Bert가 수행한 것과 매우 유사한 마스크 토큰으로 대체됩니다.

117
00:10:57,240 --> 00:11:03,640
Bert와 같은 마스크 언어 모델과 유사하게 마스크 텍스트 토큰 h를 예측하려고 합니다.

118
00:11:04,200 --> 00:11:08,760
주변 텍스트 토큰 및 기타 모든 이미지 기능을 기반으로 v.

119
00:11:10,200 --> 00:11:12,280
그리고 마스크 토큰 손실이 발생합니다.

120
00:11:13,160 --> 00:11:18,280
따라서 우리의 목표는 지역 컨텍스트에 임베딩이라는 단어를 기반으로 하는 것입니다.

121
00:11:20,360 --> 00:11:22,360
예.

122
00:11:30,200 --> 00:11:32,200
응.

123
00:11:33,880 --> 00:11:35,880
응.

124
00:11:41,320 --> 00:11:43,320
하나

125
00:11:45,320 --> 00:11:47,320
그게 요점입니다. 음

126
00:11:50,680 --> 00:12:00,360
제 생각에는 그들이 주로 마스크, 어, 단어, 어, 이미지를 좋아했던 것 같아요.

127
00:12:04,120 --> 00:12:08,760
아니, 내 말은, 그러니까, 이 부분처럼, 어, 선물용으로

128
00:12:08,760 --> 00:12:12,440
그들은, 어, 이미지 영역과 객체 태그를 제공합니다.

129
00:12:12,440 --> 00:12:16,120
그리고 그들은 기본적으로 여러분이 이것을 사용하기를 원합니다.

130
00:12:16,120 --> 00:12:18,120
당신은 무엇을 예측하고 싶은지, 어,

131
00:12:20,360 --> 00:12:22,360
각 이미지마다.

132
00:12:22,360 --> 00:12:24,360
사용하는 각 이미지.

133
00:12:25,240 --> 00:12:27,880
각 이미지는 그 사이에 있는 언어를 만듭니다.

134
00:12:29,160 --> 00:12:30,600
캘리포니아에서 본 적이 있어요.

135
00:12:30,600 --> 00:12:32,600
응.

136
00:12:32,600 --> 00:12:36,120
음, 15%나 시간 단위로요.

137
00:12:37,320 --> 00:12:39,320
h는 w와 q입니다.

138
00:12:39,800 --> 00:12:41,800
응.

139
00:12:48,600 --> 00:12:51,080
네, 잘 모르겠습니다. 아마도 다른 것과는 다를 것 같습니다.

140
00:12:51,800 --> 00:12:53,560
하나,

141
00:12:53,560 --> 00:12:55,080
작은 조각인 것 같아요.

142
00:12:55,080 --> 00:12:57,080
그래서 개체 중 하나만 좋아합니다.

143
00:12:58,840 --> 00:12:59,800
응.

144
00:12:59,800 --> 00:13:00,120
응.

145
00:13:00,120 --> 00:13:05,800
그렇다면, 어, 둘 다 떨어뜨리면 뭔가가 필요할 것 같은데요, 어,

146
00:13:05,880 --> 00:13:08,680
자세히, 어, 시계 작업을 돕기 위해요.

147
00:13:08,680 --> 00:13:10,680
같은 색상처럼 유지됩니다.

148
00:13:10,680 --> 00:13:13,160
개체 텍스트와 이미지 내용을 보려면

149
00:13:13,880 --> 00:13:16,840
예를 들어, 어떤 반응이 나올지 봅시다.

150
00:13:16,840 --> 00:13:17,640
제 생각에는 그렇습니다.

151
00:13:17,640 --> 00:13:18,520
응.

152
00:13:18,520 --> 00:13:19,960
응.

153
00:13:19,960 --> 00:13:22,040
그럼 그게 그 부인을 돕는 또 다른 방법일 수도 있겠네요.

154
00:13:29,080 --> 00:13:35,080
그리고 나서, 어, 우리는 두 법칙을 합치고, 어, 그러면 우리는 전체 사전 훈련 법칙을 갖게 됩니다.

155
00:13:35,880 --> 00:13:41,240
어, 훈련 가능한 매개변수는 지역을 매핑할 수 있는 선형 투영 측정항목입니다.

156
00:13:41,240 --> 00:13:45,640
어, 단어 임베딩과 동일한 차원의 기능입니다.

157
00:13:45,640 --> 00:13:47,160
그리고 그들은 또한 새를 훈련시킵니다.

158
00:13:47,960 --> 00:13:54,840
음, 그리고 그들이 사용하는 데이터 세트는, 어, 650만 개의 이미지 텍스트 쌍입니다.

159
00:13:54,840 --> 00:13:56,200
4백만 개의 고유한 이미지.

160
00:13:58,440 --> 00:14:00,840
그리고 여기에 정량적 결과가 있습니다.

161
00:14:01,560 --> 00:14:07,160
어, 어, 그들은 오스카상을 이전 탄산음료 모델과 비교한다는 점에 주목하세요.

162
00:14:07,880 --> 00:14:11,640
어, 오스카상은 650만 개의 데이터 세트를 조회합니다.

163
00:14:12,200 --> 00:14:14,520
어, 다른 부분에서는 91 이상을 사용합니다.

164
00:14:14,520 --> 00:14:23,160
음, 그러면 오스카 얼굴이 더 적은 데이터 세트를 사용하여 이미 가장 큰 탄산음료보다 성능이 뛰어나다는 것을 알 수 있습니다.

165
00:14:24,680 --> 00:14:30,760
음, 이것은, 음, 오스카의 매개변수 효율성을 강조하는 것입니다. 부분적으로는

166
00:14:31,560 --> 00:14:36,920
객체 태그를 사용하면 입력 지점이 있습니다. 어, 이는 이러한 학습을 ​​사용할 수 있습니다.

167
00:14:36,920 --> 00:14:37,800
정신적 기후.

168
00:14:43,080 --> 00:14:43,560
죄송합니다.

169
00:14:51,080 --> 00:14:57,880
그래서, 어, 이미지 텍스트 검색과 같이 그들은 코코아, 어, 코코아, 음, 이미지를 사용하는 것 같습니다.

170
00:14:57,880 --> 00:15:11,160
캡션, 음, 그것도, 음, 예, 예.

171
00:15:18,760 --> 00:15:20,440
우리의 30K와 두 개의 QA처럼 말이죠.

172
00:15:20,440 --> 00:15:20,680
응.

173
00:15:20,680 --> 00:15:26,600
그것들은 어, 다른 다음 모델에 사용될 매우 일반적인 데이터 세트와 같습니다.

174
00:15:28,840 --> 00:15:34,040
음, 그리고, 어, 저자는 목적 텍스트의 효과를 더 분석합니다.

175
00:15:34,680 --> 00:15:40,760
어, 세 가지 다른 작업에서 볼 수 있듯이 예측 태그를 사용한 훈련은

176
00:15:41,400 --> 00:15:46,360
기본적으로 지역 특성을 고려하여 지역이 무엇인지 예측하고 싶고

177
00:15:46,360 --> 00:15:52,280
어, 해당 태그는, 어, 그들은, 예측된 태그를 사용하면, 어, 더 적은 시간이 걸립니다

178
00:15:52,280 --> 00:15:57,640
태그를 사용하지 않고 최종 성능을 달성하는 데 훈련 시간의 절반이 소요됩니다.

179
00:15:58,760 --> 00:16:05,400
이는 객체 태그를 활용하는 효율성을 보여줍니다.

180
00:16:05,400 --> 00:16:11,000
Ground Truth 태그를 사용하면 학습 시간을 50% 더 줄일 수 있습니다.

181
00:16:12,200 --> 00:16:18,440
예측된 태그를 사용하는 것과 비교하여 최종 성능을 달성하는 것입니다.

182
00:16:22,120 --> 00:16:24,520
그리고, 음, 질적인 결과가 나옵니다.

183
00:16:24,920 --> 00:16:32,440
음, 그래서 저자는 텍스트와 이미지 임베딩을 2차원으로 시각화합니다.

184
00:16:32,440 --> 00:16:33,160
심상.

185
00:16:34,520 --> 00:16:39,640
그런 다음 그들은 태그를 사용하지 않고도 여기에서 볼 수 있는 것을 발견했습니다. 예를 들어,

186
00:16:40,920 --> 00:16:46,120
개인별 이미지 임베딩과 개인별 텍스트 임베딩은 꽤 거리가 멀습니다.

187
00:16:47,000 --> 00:16:50,360
대조적으로, 오스카에서는 그들은 매우 가깝습니다.

188
00:16:51,320 --> 00:17:00,760
그리고 마찬가지입니다, 어, 그게 하나의, 어, 관찰입니다, 어, 두 가지 양식 사이의 동일한 대상입니다

189
00:17:00,760 --> 00:17:02,120
인용된 것 맞죠?

190
00:17:02,120 --> 00:17:08,520
그리고 인터클래스의 경우, 어, 관련 의미론 클래스는 4분의 1이지만 이건

191
00:17:08,520 --> 00:17:15,480
라고 불리는데, 그 의미는 예를 들어 오른쪽에 있는 이 지역과 같습니다. 어, 이것은

192
00:17:15,480 --> 00:17:17,080
모든 요소에 대한 임베딩.

193
00:17:17,800 --> 00:17:22,840
그리고 예를 들어 운송, 프린터 및

194
00:17:22,840 --> 00:17:23,480
곧.

195
00:17:23,480 --> 00:17:29,400
흥미로운 관찰 중 하나는 무엇보다 먼저 다음과 같습니다.

196
00:17:29,400 --> 00:17:32,680
이 두 가지 유형을 모두 추가할 수 있습니다.

197
00:17:33,320 --> 00:17:35,000
그래서 좀 재미있는 것 같아요.

198
00:17:37,480 --> 00:17:43,880
그리고 Oscar에 대한 제한 사항은 음, 매우 강력한 물체 감지기가 필요하다는 것입니다.

199
00:17:43,880 --> 00:17:45,880
매우 복잡한 장면을 처리하기 위해.

200
00:17:47,080 --> 00:17:51,800
그리고 텍스트에서 두드러진 개체가 누락되면 잘 작동하지 않습니다.

201
00:17:51,800 --> 00:17:58,040
예를 들어, 이 사진에는 부츠 한 묶음이 있지만 캡션에는 몇 가지 좋은 부츠가 있습니다.

202
00:17:58,040 --> 00:18:00,040
사물을 통제하는 것부터 시작해야 하는 이유.

203
00:18:04,360 --> 00:18:09,880
안녕하세요 여러분, 어, 이제부터 Vimbo라는 논문을 소개하겠습니다.

204
00:18:09,880 --> 00:18:13,560
시각적 언어 모델의 시각적 표현을 다시 살펴봅니다.

205
00:18:13,560 --> 00:18:20,120
그리고 본 논문은 단순한 융합이 아닌 더 나은 시각적 표현을 추출하는 데 중점을 두고 있습니다.

206
00:18:21,000 --> 00:18:22,600
다중 모드 정보.

207
00:18:25,880 --> 00:18:28,920
Vimbo의 배경과 동기에 대한 정보는 다음과 같습니다.

208
00:18:28,920 --> 00:18:34,920
그 전에 VLP는 일반적으로 두 단계로 구성된다는 점을 명시해야 합니다.

209
00:18:34,920 --> 00:18:39,480
첫 번째는 이미지를 인코딩하도록 연동된 객체 감지 모델입니다.

210
00:18:39,480 --> 00:18:42,040
이미지의 시각적 객체를 표현으로 표현합니다.

211
00:18:42,840 --> 00:18:48,760
그런 다음 텍스트와 시각적 요소를 혼합하도록 훈련된 교차 모델 융합 모델이 있습니다.

212
00:18:48,760 --> 00:18:49,560
표현.

213
00:18:50,600 --> 00:18:55,880
최근 몇 년 동안 VLP를 표현하는 시각적 언어의 성공이 많은 연구에서 나타났습니다.

214
00:18:55,880 --> 00:18:57,720
우리의 작업을 통해 시각적 언어로.

215
00:18:57,720 --> 00:19:00,920
예를 들어, 비아, 어, 뷰 버드와 오스카.

216
00:19:01,480 --> 00:19:07,160
따라서 우리는 모델과 교차 모델에 대한 객체 감지 아키텍처를 가질 수 있습니다.

217
00:19:07,160 --> 00:19:08,360
퓨전 모델.

218
00:19:08,360 --> 00:19:14,440
그러나 비전 언어 융합 모델에서는 OD 모델 개선이 항상 그대로 유지됩니다.

219
00:19:15,160 --> 00:19:18,840
그리고 우리는 VLP 작업에서 시각적 기능이 매우 중요하다는 것을 알고 있습니다.

220
00:19:19,480 --> 00:19:24,760
따라서 실제로 우리는 다음과 같은 대규모 객체 속성 감지 모델을 활용할 수 있습니다.

221
00:19:24,760 --> 00:19:31,560
Resnext 152c4는 VLP 작업의 성능을 향상시킵니다.

222
00:19:31,560 --> 00:19:35,560
또한 시각적 게놈 VG 데이터 세트와 같은 더 큰 데이터 세트를 사용할 수도 있습니다.

223
00:19:38,760 --> 00:19:45,960
이 부분에서는 Vimbo가 시각적 언어의 비전을 어떻게 향상시키는지에 대해 이야기하겠습니다.

224
00:19:46,600 --> 00:19:51,400
학습된 언어 모델의 큰 성공에 영감을 받아

225
00:19:51,400 --> 00:19:55,720
주류 작품은 실제로 이런 방식으로 디자인되었습니다.

226
00:19:55,720 --> 00:20:00,920
첫째, 훨씬 더 다양하고 풍부하며 대규모의 훈련 데이터 세트를 개발할 수 있습니다.

227
00:20:00,920 --> 00:20:04,680
그런 다음 객체 감지 알고리즘에 대한 새로운 통찰력을 얻습니다.

228
00:20:05,480 --> 00:20:09,320
그런 다음 더 큰 모델을 훈련하기 위해 더 강력한 GPU를 활용합니다.

229
00:20:09,960 --> 00:20:16,440
그러나 본 논문에서는 객체 중심의 시각적 표현을 개선하는 데 중점을 둡니다.

230
00:20:16,440 --> 00:20:19,320
VLP 작업의 비전 측면에서.

231
00:20:20,040 --> 00:20:23,320
특히 그들은 새로운 객체 감지 모델을 개발했습니다.

232
00:20:23,320 --> 00:20:27,880
기존 OD 모델보다 이미지가 더 나은 시각적 기능을 생성할 수 있습니다.

233
00:20:28,440 --> 00:20:32,360
시각적 객체와 속성 카테고리를 풍부하게 함으로써,

234
00:20:32,840 --> 00:20:36,760
모델 크기를 확대하고 훨씬 더 큰 OD 데이터 세트에 대한 교육을 수행합니다.

235
00:20:40,840 --> 00:20:47,160
이제 우리는 그들이 어떻게 작동하는지에 대한 몇 가지 작은 실험과 예를 볼 것입니다.

236
00:20:47,160 --> 00:20:48,840
시각적 언어로 시력을 향상시킵니다.

237
00:20:49,960 --> 00:20:54,920
그 전에 우리는 실제로 그들이 대규모 개체 속성을 가지고 있다고 말하고 싶습니다.

238
00:20:54,920 --> 00:21:01,320
Resnext 152c4를 기반으로 한 감지 모델을 여기서는 c4라고 부르겠습니다.

239
00:21:01,320 --> 00:21:06,280
그리고 새로운 모델은 VL 작업에 더 적합하게 설계되었으며 더 크고 더 많은 교육을 받았습니다.

240
00:21:06,280 --> 00:21:10,520
여러 공공 객체 감지 데이터 세트를 결합하여 더 많은 양의 데이터를 생성합니다.

241
00:21:11,080 --> 00:21:16,600
따라서 왼쪽 그림의 일반적인 OD 모델의 일반적인 개체 클래스와 비교하면,

242
00:21:16,600 --> 00:21:21,000
오른쪽 그림의 모델에서 풍부하고 다양한 지역의 특징을 볼 수 있습니다.

243
00:21:21,000 --> 00:21:23,000
비전 언어 작업에 중요합니다.

244
00:21:23,640 --> 00:21:30,200
예를 들어, 두 모델 모두에서 감지된 개념의 경우 소년은 해당 모델의 속성을 자주 사용합니다.

245
00:21:31,560 --> 00:21:36,520
맨발의 맨발의 젊은, 서있는 모습,

246
00:21:36,520 --> 00:21:41,720
서빙, 미소, 작은 놀이, 금발처럼 보이는 등 모든 속성 정보가 있습니다.

247
00:21:46,280 --> 00:21:52,920
따라서 VL 작업에 대한 OD 모델을 개선하기 위해 4개의 공용 개체 감지 데이터 세트를 활용합니다.

248
00:21:53,640 --> 00:21:57,720
대부분의 데이터 세트에는 속성 주석이 없기 때문입니다.

249
00:21:58,680 --> 00:22:02,680
그들은 OD 모델을 구축하기 위해 사전 훈련 및 미세 조정 전략을 채택합니다.

250
00:22:03,640 --> 00:22:08,680
첫째, 그들은 다음의 조합으로 구성된 대규모 코퍼스에서 OD 모델을 훈련시킵니다.

251
00:22:08,680 --> 00:22:12,360
샘플링, 밸런싱, 병합을 통해 4개의 공개 데이터 세트를 생성합니다.

252
00:22:13,080 --> 00:22:19,880
그런 다음 VG에서 OD 모델을 미세 조정하여 속성 정보를 주입하여 속성 분기를 추가합니다.

253
00:22:19,880 --> 00:22:25,720
사전 훈련된 OD 모델에 적용하여 객체와 속성을 모두 감지할 수 있습니다.

254
00:22:28,520 --> 00:22:34,120
이제 우리는 이해를 강화하기 위해 VL 모델을 다시 살펴보겠습니다.

255
00:22:34,760 --> 00:22:38,520
그리고 딥러닝 기반 VL 모델은 일반적으로 두 개의 모듈로 구성됩니다.

256
00:22:39,240 --> 00:22:44,840
이미지 이해 모듈 비전 및 교차 모달 이해 모듈 VL.

257
00:22:45,960 --> 00:22:47,960
여기서 우리는 몇 가지 정의를 내릴 수 있습니다.

258
00:22:47,960 --> 00:22:50,760
Vision은 이미지 이해 모듈입니다.

259
00:22:50,760 --> 00:22:52,840
VL은 교차 모달 이해 모듈입니다.

260
00:22:52,840 --> 00:22:55,400
그리고 IMG는 실제로 비전을 의미합니다.

261
00:22:55,400 --> 00:23:00,520
그리고 Q는 객체 태그와 마찬가지로 이미지의 의미론적 표현입니다.

262
00:23:01,080 --> 00:23:05,880
그리고 V는 시각적 표현과 마찬가지로 이미지의 분포 표현입니다.

263
00:23:05,880 --> 00:23:10,360
그리고 W는 VQA의 질문과 마찬가지로 언어 또는 텍스트입니다.

264
00:23:11,000 --> 00:23:16,600
그리고 Y는 VQA 작업에서 예측되는 답변과 마찬가지로 출력 또는 텍스트일 수 있습니다.

265
00:23:17,560 --> 00:23:24,840
따라서 VL 모델의 규칙은 먼저 비전과 언어 모델링 VL을 통합하는 것과 같습니다.

266
00:23:24,840 --> 00:23:25,800
변압기와 함께.

267
00:23:26,440 --> 00:23:31,080
그런 다음 대규모 텍스트 이미지 말뭉치로 통합 VL을 묘사합니다.

268
00:23:35,080 --> 00:23:38,360
이제 사전 훈련에 사용하는 데이터 세트에 대해 간략하게 이야기하겠습니다.

269
00:23:39,320 --> 00:23:44,680
그들은 세 가지 유형의 기존 비전 및 VL 데이터 세트를 기반으로 사전 훈련 코퍼스를 구축합니다.

270
00:23:45,480 --> 00:23:47,480
첫째, 이미지 캡션 데이터 세트입니다.

271
00:23:47,480 --> 00:23:53,480
두 번째는 GQA를 포함한 시각적 QA 데이터세트이고, 세 번째는 이미지 태깅 데이터세트입니다.

272
00:23:53,480 --> 00:23:59,560
OSCAR에는 방정식과 같이 사전 훈련 손실과 함께 두 개의 항이 있습니다.

273
00:23:59,560 --> 00:24:02,120
논문에서 가장 중요한 부분이다.

274
00:24:02,120 --> 00:24:10,360
여기서 lossMTL은 이미지 애니메이션의 텍스트 양식에 정의된 마스크된 토큰 손실입니다.

275
00:24:10,360 --> 00:24:12,360
OSCAR를 밀접하게 따르고 있습니다.

276
00:24:12,840 --> 00:24:17,480
그리고 lossCl3은 효과적으로 새로운 3방향 대비 손실입니다.

277
00:24:17,480 --> 00:24:22,200
VQA 및 텍스트 이미지 매칭에 사용되는 교육 목표를 최적화합니다.

278
00:24:25,080 --> 00:24:28,920
여기서는 손실 함수의 구체적인 내용에 대해 이야기하겠습니다.

279
00:24:29,560 --> 00:24:33,560
lossMTL의 경우 이미지 캡션의 단어 토큰은 W입니다.

280
00:24:33,560 --> 00:24:38,360
그리고 목적어 Q의 단어 토큰은 동일한 언어 의미 공간을 공유합니다.

281
00:24:39,000 --> 00:24:42,840
그리고 lossMTL은 W와 Q 토큰 모두에 적용됩니다.

282
00:24:43,640 --> 00:24:49,960
따라서 우리는 이산 토큰 시퀀스를 H가 W 및 Q와 동일하다고 정의합니다.

283
00:24:51,160 --> 00:24:53,800
그런 다음 사전 훈련을 위해 lossMTL을 적용합니다.

284
00:24:54,440 --> 00:25:00,040
각 반복에서 H의 각 입력 토큰을 확률로 무작위로 마스킹할 수 있습니다.

285
00:25:00,040 --> 00:25:04,520
마스크1 HI를 특수 토큰 마스크로 교체합니다.

286
00:25:05,480 --> 00:25:10,840
이 훈련의 목표는 주변 토큰을 기반으로 마스크된 토큰을 예측하는 것입니다.

287
00:25:10,840 --> 00:25:14,040
부정적인 우도를 최소화하여 이미지 특징을 파악하고,

288
00:25:14,920 --> 00:25:17,560
실제로 음의 로그 우도를 최소화함으로써 말이죠.

289
00:25:18,440 --> 00:25:20,520
따라서 BERT와 완전히 동일한 프로세스입니다.

290
00:25:23,240 --> 00:25:29,640
이제 두 가지 유형의 훈련 샘플을 고려하는 lossCl3에 대해 이야기하겠습니다.

291
00:25:29,640 --> 00:25:34,600
X. 여기에 VQA 예를 들어보겠습니다. 예를 들어 질문, 답변,

292
00:25:34,600 --> 00:25:37,240
이미지는 VQA 데이터세트의 삼중항을 특징으로 합니다.

293
00:25:38,280 --> 00:25:43,080
대비 손실을 계산하려면 부정적인 예를 구성해야 합니다.

294
00:25:43,720 --> 00:25:49,720
그리고 훈련 샘플과 오염된 샘플에 대해 음수 또는 일치하지 않는 삼중항을 구성할 수 있습니다.

295
00:25:49,720 --> 00:25:56,440
답변. 이제 질문과 답변 이미지 삼중항에 오염된 이미지가 포함되어 있는지 여부를 분류해 보겠습니다.

296
00:25:56,440 --> 00:25:59,000
답변은 VQA의 답변 선택 작업입니다.

297
00:26:00,760 --> 00:26:06,520
BERT에서 CLS 토큰의 인코딩은 삼중항 W의 표현으로 볼 수 있으므로,

298
00:26:06,520 --> 00:26:13,400
Q, V, 그 위에 완전히 연결된 레이어를 3방향 분류기 F로 적용할 수 있습니다.

299
00:26:14,040 --> 00:26:17,960
삼중항이 일치하는지 또는 오염된 Q를 포함하는지 예측합니다.

300
00:26:19,000 --> 00:26:24,600
이제 코퍼스의 모든 Q에서 샘플링하여 훈련 세트를 쉽게 구축할 수 있습니다.

301
00:26:25,240 --> 00:26:30,520
이제 훈련 모델은 주어진 삼중항에 대한 참 답이 포함되어 있는지 예측하려고 합니다.

302
00:26:30,520 --> 00:26:37,800
질문. 여기서는 훈련 모델과 아키텍처, 매개변수에 대해 이야기할 수 있습니다.

303
00:26:38,360 --> 00:26:44,440
BERT로 초기화된 모델을 묘사하고 이미지 영역 기능이 다음과 같은지 확인합니다.

304
00:26:44,440 --> 00:26:50,440
BERT에서 동일한 입력 임베딩 크기를 사용하려면 선형 투영을 사용하여 영역 기능을 변환해야 합니다.

305
00:26:50,440 --> 00:26:59,080
행렬 W를 통해. 따라서 훈련 가능한 매개변수는 BERT, theta BERT 및 W 매개변수를 포함한 theta입니다.

306
00:27:02,920 --> 00:27:08,120
지금까지 우리는 이미 텍스트 임베딩과 이미지 임베딩을 통합하는 방법에 대해 이야기했습니다.

307
00:27:08,120 --> 00:27:13,800
통합된 임베딩을 갖기 위한 다중 모드 사전 훈련 모델. 이제 우리는 방법에 대해 이야기하고 있습니다.

308
00:27:13,800 --> 00:27:19,960
통합 임베딩을 사용하여 다운스트림 작업을 완료합니다. 저는 두 가지 유형의 다운스트림 작업을 수행하고 있습니다.

309
00:27:20,520 --> 00:27:26,040
예를 들어 생성 작업부터 시작합니다. 예를 들어 이미지 캡션이 있습니다.

310
00:27:27,240 --> 00:27:33,480
그리고 캡션 작업은 이미지에 대한 자연어 캡션을 생성하는 것입니다. 첫째로,

311
00:27:33,480 --> 00:27:39,240
시퀀스 대 시퀀스 목표를 사용하여 모델을 미세 조정합니다. 따라서 각 훈련 샘플은 변환됩니다.

312
00:27:39,240 --> 00:27:45,080
캡션, 이미지 영역 기능 집합, 개체 텍스트 집합으로 구성된 삼중항으로 구성됩니다.

313
00:27:45,960 --> 00:27:52,040
그런 다음 캡션이 있는 토큰을 무작위로 마스크 처리하고 나머지 토큰의 인코딩을 사용합니다.

314
00:27:52,040 --> 00:27:57,160
단방향 생성 프로세스에서 마스크의 토큰을 예측하는 컨텍스트입니다.

315
00:27:58,440 --> 00:28:05,400
따라서 추론 중에 먼저 이미지 영역, 객체 태그 및 특수 토큰 CLS를 인코딩합니다.

316
00:28:05,400 --> 00:28:12,920
BERT에서 입력으로. 그런 다음 모델은 마스크 토큰 메커니즘을 사용하여 생성을 시작합니다.

317
00:28:12,920 --> 00:28:22,120
BERT와 완전히 같습니다. 여기서는 VQA와 GQA에 대해 이야기하겠습니다.

318
00:28:22,120 --> 00:28:29,000
작업 이해. VQA와 GQA는 VL 모델을 평가하기 위해 널리 사용되는 두 가지 이해 작업입니다.

319
00:28:29,000 --> 00:28:34,120
그리고 작업을 수행하려면 모델이 이미지를 기반으로 자연어 질문에 대답해야 합니다.

320
00:28:35,080 --> 00:28:39,000
따라서 각 질문에 대해 모델은 공유 답변 세트에서 답변을 선택합니다.

321
00:28:39,720 --> 00:28:45,560
VQA 작업을 미세 조정하겠습니다. 연결을 포함하는 하나의 입력 시퀀스를 구성합니다.

322
00:28:45,560 --> 00:28:56,520
주어진 질문과 개체 텍스트 및 지역 특징, 그리고 CLS 토큰 출력

323
00:28:56,520 --> 00:29:02,680
모델의 데이터는 예측을 위해 작업별 선형 분류기에 공급됩니다.

324
00:29:03,240 --> 00:29:09,240
구체적으로 우리는 VQA를 다중 라벨 분류 문제로 취급하여 소프트 타겟을 할당합니다.

325
00:29:09,240 --> 00:29:14,120
인간의 답변 응답과의 관련성을 기준으로 각 답변에 점수를 매긴 다음

326
00:29:14,120 --> 00:29:19,240
예측 점수를 사용하여 계산된 교차 엔트로피 손실을 최소화하여 모델을 미세 조정합니다.

327
00:29:19,240 --> 00:29:27,000
그리고 소프트 타겟 점수. 추론 단계에서는 간단하게 Soft Max 기능을 사용할 수 있습니다.

328
00:29:27,000 --> 00:29:27,800
예측을 위해.

329
00:29:32,920 --> 00:29:37,720
이 문서는 실제로 보다 정교한 객체를 사용하여 성능을 향상시키는 방법을 알려줍니다.

330
00:29:37,720 --> 00:29:43,800
VL 작업을 위한 감지 모델 및 미세 조정. 그러나 대규모 언어 모델이 등장하면서

331
00:29:43,800 --> 00:29:49,480
CV와 NLP를 포함한 전체 AI 커뮤니티는 점점 더 많은 관심을 기울이고 있습니다.

332
00:29:49,480 --> 00:29:56,200
제로 샷만을 사용하여 다운스트림 작업을 완료할 가능성. 그래서 다음에서,

333
00:29:56,200 --> 00:30:01,480
제로 샷의 문을 여는 가장 중요한 이정표 중 하나를 소개하겠습니다.

334
00:30:01,480 --> 00:30:12,200
다중 모드 모델, CLIP. 이제부터 대조에 대해 이야기해보겠습니다.

335
00:30:12,200 --> 00:30:17,000
우리가 방금 제시한 두 논문과는 좀 다른 훈련별 언어 이미지

336
00:30:17,000 --> 00:30:25,080
이야기했다. 예를 들어 이제 CV의 주요 표현 학습 패러다임을 다시 검토해 보겠습니다.

337
00:30:26,200 --> 00:30:30,760
첫째, 이미지와 텍스트를 활용하는 등 이미지와 텍스트 사이의 연결을 구축해야 합니다.

338
00:30:30,760 --> 00:30:36,600
오스카처럼 서로 표현을 학습하기 위한 텍스트 쌍 데이터 세트입니다. 그러면 우리는

339
00:30:36,600 --> 00:30:41,240
주간 지도 사전 훈련을 통해 더 많은 데이터를 얻을 수 있지만, 수업 수에 관계없이

340
00:30:41,240 --> 00:30:47,080
데이터 세트에는 최종 분류자가 실제로 정적이고 제한적입니다.

341
00:30:47,080 --> 00:30:53,000
분류를 위해 특정 클래스 수로 고정해야 하며 이로 인해 확실히 제한됩니다.

342
00:30:53,000 --> 00:31:01,240
보기에서와 마찬가지로 모델의 제로 샷 기능. 따라서 이 두 가지 방법을 결합하면 다음과 같은 결과를 얻을 수 있습니다.

343
00:31:01,240 --> 00:31:08,280
데이터를 확장하기 위해 인터넷에서 많은 수의 텍스트와 이미지 쌍을 얻는 것을 고려하십시오.

344
00:31:08,280 --> 00:31:13,480
그리고 나서 우리는 정적 변수를 사용하지 않고 분류를 위한 표현을 갖는 방법을 알아냈습니다.

345
00:31:13,480 --> 00:31:20,920
분류자. 그리고 이미지에 대한 원시 텍스트에서 직접 학습하는 것은 좋은 생각입니다.

346
00:31:21,480 --> 00:31:24,040
훨씬 더 광범위한 감독 소스.

347
00:31:28,360 --> 00:31:33,960
이제부터 유명한 논문인 CLIP, Learning Transferable Visual Models를 살펴보겠습니다.

348
00:31:33,960 --> 00:31:39,160
Natural Language Supervision의 논문이며, 본 논문은 자기지도 학습을 소개하는 데 중점을 두고 있습니다.

349
00:31:39,160 --> 00:31:47,640
NLP 개입에 널리 사용되는 신호. 이제 배경과 동기를 살펴보겠습니다.

350
00:31:47,640 --> 00:31:54,360
CLIP으로. 최근의 많은 연구에 따르면 NLP에서 사전 훈련 모델의 성공은 다음과 같습니다.

351
00:31:54,360 --> 00:32:01,080
GPT 제품군과 마찬가지로 제로 샷 CV 작업이 개선된 것을 이미 확인했습니다.

352
00:32:01,080 --> 00:32:07,640
네트워크 성능 및 보다 표적화된 약한 감독. 하지만 아까도 말씀드렸듯이,

353
00:32:07,640 --> 00:32:13,560
실제로 SOTA CV 시스템에는 여전히 몇 가지 문제가 있습니다. 예를 들어 미리 결정된 개체의 고정 세트가 있습니다.

354
00:32:13,800 --> 00:32:22,600
카테고리 또는 낮은 일반성과 유용성. 그래서 CLIP은 더 큰 모델을 디자인하려고 합니다.

355
00:32:22,600 --> 00:32:27,640
빅 데이터 세트와 더 큰 모델 크기를 사용하여 이러한 격차를 해소합니다.

356
00:32:31,560 --> 00:32:36,120
이 논문은 어떤 캡션을 예측하는 간단한 사전 훈련 작업을 보여줍니다.

357
00:32:36,120 --> 00:32:42,280
어떤 이미지가 이미 SOTA 이미지 표현을 학습하는 효율적이고 확장 가능한 방법인지,

358
00:32:42,280 --> 00:32:48,600
사전 훈련 후 Natural Language를 사용하여 학습된 시각적 개념을 참조하고,

359
00:32:48,600 --> 00:32:55,480
모델을 다운스트림 작업으로 제로 샷 전송할 수 있습니다. 따라서 CLIP은 다양한 유형을 수행할 수 있습니다.

360
00:32:55,480 --> 00:33:01,880
세밀한 객체 분류가 가능하며 모델은 종종 완전히 감독되는 모델과 경쟁적입니다.

361
00:33:01,880 --> 00:33:08,040
이 그림에 표시된 것처럼 데이터 세트별 교육이 필요 없이 기준선을 따릅니다.

362
00:33:08,440 --> 00:33:19,240
여기에서는 CLIP의 방법에 대해 이야기하겠습니다. 그리고 그 전에 우리는 아직 정리해야 할 것이 있습니다

363
00:33:20,200 --> 00:33:26,440
문제. 문제는 우리가 자연어 감독 신호를 사용하여 훈련하고 싶다는 것입니다.

364
00:33:26,440 --> 00:33:33,240
더 나은 시각적 모델. 그리고 인터넷에서 대량의 텍스트, 이미지 쌍 데이터를 얻을 수 있다면

365
00:33:33,240 --> 00:33:39,080
더 이상 데이터에 라벨을 붙일 필요 없이 이 데이터를 사용하여 더 큰 모델을 훈련할 수 있습니다.

366
00:33:39,080 --> 00:33:47,560
데이터에 시끄러운 경우에도 마찬가지입니다. 그래서 CLIP에서는 먼저 4억 개의 새로운 데이터 세트를 구축했습니다.

367
00:33:47,560 --> 00:33:52,840
인터넷에서 공개적으로 이용 가능한 다양한 소스로부터 수집된 이미지와 텍스트 쌍,

368
00:33:53,720 --> 00:33:58,840
그런 다음 사전 훈련 최적화 목표를 설정해야 합니다. 그런 다음 그들은

369
00:33:58,840 --> 00:34:07,560
전체 시스템을 실행하고 사전 훈련 작업을 시작합니다. 여기서는 사전 훈련 작업의 세부 사항을 논의합니다.

370
00:34:07,560 --> 00:34:13,560
이 논문의 가장 중요한 부분인 CLIP입니다. 실제로 많은 업무가 공동으로 교육을 받았습니다.

371
00:34:13,560 --> 00:34:20,360
이미지 CNN과 텍스트 변환기를 처음부터 다시 사용하여 이미지 캡션을 예측합니다. 그들은 노력한다

372
00:34:20,360 --> 00:34:28,760
각 이미지에 수반되는 텍스트의 정확한 단어를 예측합니다. 그리고 이것은 매우 어려운 작업입니다

373
00:34:28,920 --> 00:34:35,080
실제로는 다양한 설명, 의견 및 관련 텍스트가 함께 발생하기 때문입니다.

374
00:34:35,080 --> 00:34:42,360
이미지들. 이러한 종류의 이미지 생성 모델은 고품질 이미지를 학습할 수 있지만

375
00:34:42,360 --> 00:34:49,960
표현을 위해서는 여전히 대조보다 훨씬 더 많은 계산이 필요합니다.

376
00:34:49,960 --> 00:34:58,360
동일한 성능을 가진 모델입니다. 따라서 이러한 발견에 주목하여 예측을 위한 시스템 교육을 탐색합니다.

377
00:34:58,440 --> 00:35:05,000
텍스트의 정확한 단어 대신 전체적으로 어떤 텍스트가 어떤 이미지와 쌍을 이루는지.

378
00:35:06,040 --> 00:35:12,200
그리고 실험에서는 0의 비율로 효율성이 4배 더 향상되는 것을 관찰했습니다.

379
00:35:12,200 --> 00:35:20,680
ImageNet으로 전송됩니다. 이제 사전 훈련에 대해 자세히 이야기하겠습니다.

380
00:35:21,800 --> 00:35:28,200
n 쌍의 배치가 주어지면 CLIP은 n x n 가능한 쌍 중 어느 것이 가능한지 예측하도록 훈련됩니다.

381
00:35:28,600 --> 00:35:35,160
일괄적으로 실제로 발생했습니다. 이를 위해 CLIP은 공동으로 다중 모드 임베딩 공간을 학습합니다.

382
00:35:35,160 --> 00:35:40,280
이미지의 코사인 유사성을 최대화하기 위해 이미지 인코더와 텍스트 인코더를 훈련합니다.

383
00:35:40,280 --> 00:35:47,320
코사인 유사성을 최소화하면서 배치에 있는 n개의 실수 쌍에 대한 텍스트 임베딩

384
00:35:47,320 --> 00:35:55,640
n 제곱에서 n 잘못된 쌍을 뺀 임베딩의 수입니다. 그런 다음 최적화를 시도합니다.

385
00:35:55,640 --> 00:36:04,200
대칭 손실, 그들은 이러한 유사성에 대한 대칭 교차 엔트로피 손실을 최적화하려고 합니다.

386
00:36:04,200 --> 00:36:13,480
점수. 구체적으로 텍스트 인코더로 Transformer를 선택하고 ResNet 또는 VIT를 선택할 수 있습니다.

387
00:36:13,480 --> 00:36:23,800
이미지 인코더로. 예를 들어 1단계에서는 각 항목의 특징 표현을 추출합니다.

388
00:36:23,800 --> 00:36:31,400
두 인코더의 양식. 그리고 2~3단계에서 공동 다중 모드 임베딩을 얻습니다.

389
00:36:31,400 --> 00:36:39,240
선형변환과 정규화를 통해 동일한 차원을 구현합니다. 4단계에서는 다음을 수행합니다.

390
00:36:39,240 --> 00:36:45,560
쌍별 코사인 유사성, 쌍별 코사인 유사성을 조정하여 코사인을 얻습니다.

391
00:36:45,560 --> 00:36:52,280
유사성 매트릭스. 마지막으로 5단계에서는 대칭 대비 손실을 계산합니다.

392
00:36:52,360 --> 00:36:54,040
교차 엔트로피 손실 함수를 통해

393
00:36:58,600 --> 00:37:03,640
우리는 이 의사 코드가 다소 혼란스럽다는 것을 알고 있으므로 여기에 코드를 작성하여 무엇을 설명할지 설명하겠습니다.

394
00:37:03,640 --> 00:37:12,440
우리는 Torch를 사용합니다. 교차 엔트로피 손실을 계산하기 위해 두 가지 함수를 설계할 수 있습니다.

395
00:37:12,440 --> 00:37:17,640
손실 및 CLIP 손실. 그리고 대비 손실에서는 Torch 기능을 활용할 수 있습니다.

396
00:37:18,440 --> 00:37:23,160
엔트로피 전반에 걸쳐 결합 기능을 수행하고 해당 롯지를 설정할 수 있습니다.

397
00:37:24,600 --> 00:37:31,720
그리고 먼저 이미지 인코딩에 대한 손실 i를 계산한 다음 손실 t를 계산합니다.

398
00:37:31,720 --> 00:37:39,560
텍스트 임베딩을 위해 손실 i와 손실 t의 평균을 계산합니다.

399
00:37:40,360 --> 00:37:41,880
그러면 우리는 최종적인 손실을 입을 수 있습니다.

400
00:37:42,280 --> 00:37:52,040
좋아요, 사전 훈련 후에 추론을 수행하는 방법에 대한 문제가 있습니다. 실제로,

401
00:37:52,040 --> 00:37:58,600
컴퓨터 비전에서 제로샷 학습은 일반적으로 보이지 않는 것을 일반화하는 연구를 의미합니다.

402
00:37:58,600 --> 00:38:03,720
이미지 분류의 객체 카테고리. 그래서 CLIP의 대조 사전 학습 방법은

403
00:38:03,720 --> 00:38:10,040
실제로 제로샷 분류를 매우 쉽게 만듭니다. 예를 들어 예측할 이미지가 있습니다.

404
00:38:10,840 --> 00:38:16,840
먼저 이미지 임베딩을 얻은 다음 레이블 텍스트에서 데이터 세트 분류자를 만듭니다.

405
00:38:16,840 --> 00:38:23,480
모든 레이블은 모두 텍스트 인코더에 의해 인코딩되어 해당 텍스트 임베딩을 얻습니다.

406
00:38:23,480 --> 00:38:28,520
그런 다음 코사인 유사성을 각각 계산합니다. 그런 다음 예측 라벨

407
00:38:28,520 --> 00:38:35,240
유사성이 가장 높은 것은 최상위 예측 라벨입니다. 그래서 이런 식으로 분류세를

408
00:38:36,040 --> 00:38:40,040
더 이상 고정된 데이터 세트나 고정된 수의 레이블로 제한되지 않습니다.

409
00:38:42,440 --> 00:38:47,640
그리고 실제로 사전 훈련에서 텍스트 데이터는 일반적으로 이미지를 설명하는 전체 문장입니다.

410
00:38:47,640 --> 00:38:54,680
어떤 식으로든 배포 격차를 해소하기 위해 사진과 같은 프롬프트 템플릿을 사용할 수 있습니다.

411
00:38:54,680 --> 00:39:01,160
상표. 텍스트를 지정하는 데 도움이 되는 좋은 기본값은 이미지의 내용에 관한 것이므로,

412
00:39:01,960 --> 00:39:06,920
이는 레이블 텍스트만 사용하는 기준에 비해 성능이 향상되는 경우가 많습니다.

413
00:39:06,920 --> 00:39:13,240
다른 더 큰 언어 모델에 대한 신속한 엔지니어링 토론과 유사합니다.

414
00:39:13,240 --> 00:39:19,480
각 텍스트에 대한 프롬프트 텍스트를 사용자 정의하면 제로샷 성능이 크게 향상될 수 있습니다.

415
00:39:21,080 --> 00:39:27,480
그리고 신속한 엔지니어링은 많은 세분화된 이미지 분류 데이터 세트에서 카테고리를 지정하는 데 도움이 됩니다.

416
00:39:28,280 --> 00:39:35,960
예를 들어, 동물에 관한 데이터 세트에서는 라벨 사진과 같은 프롬프트를 사용할 수 있습니다.

417
00:39:35,960 --> 00:39:41,880
프롬프트에 따라 애완동물의 일종입니다. 따라서 이는 맥락을 매우 잘 제공하는 데 도움이 될 수 있습니다.

418
00:39:45,800 --> 00:39:49,480
여기서는 Clip의 제로샷 분류 결과를 볼 수 있습니다.

419
00:39:50,120 --> 00:39:55,800
Clip은 데이터를 전혀 사용하지 않고 제로샷을 하기 때문에 Clip의 성능은 매우 강력합니다.

420
00:39:55,800 --> 00:40:04,040
선형 프로브 ResNet 50은 다운스트림 데이터를 사용하여 인간 배설물 분류기를 찾았습니다.

421
00:40:08,360 --> 00:40:14,200
마지막으로 Clip의 제로샷 성능은 일반적으로 Clip보다 우수하지만 결론을 내릴 수 있습니다.

422
00:40:14,200 --> 00:40:21,800
감독된 기준 ResNet 50은 실제로 많은 다운스트림의 SOTA 방법보다 약합니다.

423
00:40:21,800 --> 00:40:28,680
작업, 특히 세분화된 분류 및 추상 작업. 그래서 전이학습은

424
00:40:28,680 --> 00:40:36,840
Clip은 최근 몇 년 동안 매우 많이 연구되었습니다. 둘째, Clip 제로샷 분류기는

425
00:40:36,840 --> 00:40:43,960
다양한 작업을 할 수 있지만 제한된 범위 내에서 비교하고 추론하는 것이 Clip의 본질입니다.

426
00:40:43,960 --> 00:40:50,120
클래스이며 이미지 캡션만큼 완전히 유연한 새로운 개념을 생성할 수 없습니다.

427
00:40:50,680 --> 00:40:58,440
Clip도 Oscar와 마찬가지로 생성 모델이 아니기 때문입니다. 이러한 문제에도 불구하고,

428
00:40:59,160 --> 00:41:03,400
Clip은 최근 몇 년간 일련의 훌륭한 작품의 기반이 되었다고 말할 수 있습니다.

429
00:41:04,040 --> 00:41:10,520
제로샷 작업을 위한 매우 좋은 위치를 제공하고 텍스트 이미지 표현을 통합하기 때문입니다.

430
00:41:11,320 --> 00:41:17,400
조만간 우리는 대규모 애플리케이션에 적용할 수 있는 일련의 새로운 클립 기반 모델을 보게 될 것입니다.

431
00:41:17,480 --> 00:41:18,680
VL 작업 수.

432
00:41:24,200 --> 00:41:29,480
이번 파트에서는 ​​유명한 논문 Align Scaling of Visual and Visual을 소개하겠습니다.

433
00:41:29,480 --> 00:41:37,000
시끄러운 텍스트 감독을 통한 언어 표현 학습. 그리고 그 논문은 규모에 초점을 맞추고 있어요

434
00:41:37,000 --> 00:41:42,120
노이즈를 보완하고 SOTA 표현으로 이어지는 코퍼스입니다.

435
00:41:47,400 --> 00:41:50,920
여기서는 Align을 설립하게 된 배경과 동기에 대해 이야기해보겠습니다.

436
00:41:51,560 --> 00:41:57,480
시각적 언어의 경우 Clip과 같은 널리 사용되는 데이터 세트에는 모두 사소한 데이터 수집이 포함됩니다.

437
00:41:57,480 --> 00:42:03,800
그리고 청소 과정. 그리고 비용이 많이 드는 이 큐레이션 프로세스는 데이터 세트의 크기를 제한하므로

438
00:42:03,800 --> 00:42:11,000
훈련된 모델의 확장을 방해합니다. 그리고 이 논문에서는 잡음이 많은 데이터 세트를 활용합니다.

439
00:42:11,080 --> 00:42:20,440
값비싼 필터링이나 후처리 단계 없이 10억 명 이상의 이미지 납세자가 획득했습니다.

440
00:42:20,440 --> 00:42:26,520
따라서 단일 듀얼 인코더 아키텍처는 시각적 표현과 언어 표현을 정렬하는 방법을 학습합니다.

441
00:42:26,520 --> 00:42:32,200
Clip과 동일하게 대비 손실을 사용하여 이미지와 납세자를 계산합니다. 그리고 그들은

442
00:42:32,200 --> 00:42:37,960
코퍼스의 규모는 노이즈를 보완할 수 있으며 그러한 경우에도 SOTA 표현으로 이어질 수 있습니다.

443
00:42:37,960 --> 00:42:45,400
간단한 학습 계획. 정렬된 시각적 및 언어 표현을 통해 제로샷 이미지가 가능합니다.

444
00:42:45,400 --> 00:42:52,360
분류. 또한 표현을 통해 복잡한 텍스트와 텍스트를 사용한 교차 양식 검색도 가능합니다.

445
00:42:52,360 --> 00:43:02,360
플러스 이미지 쿼리. 여기서는 노이즈가 있는 이미지 텍스트 데이터세트에 대해 이야기하겠습니다.

446
00:43:02,360 --> 00:43:09,720
실제로 그들의 작업의 초점은 시각적 및 언어 표현 학습을 확장하는 것입니다.

447
00:43:10,280 --> 00:43:16,760
따라서 이를 위해 기존 데이터보다 더 큰 데이터 세트를 사용합니다.

448
00:43:17,400 --> 00:43:23,560
특히, 우리는 개념 인식 데이터 세트를 구성하는 방법론을 따릅니다.

449
00:43:23,560 --> 00:43:29,640
원시 영어 이미지 텍스트 데이터 버전을 가져옵니다. 그리고 여기서는 스케일링을 목적으로

450
00:43:30,600 --> 00:43:36,840
그들은 원본 작업의 대부분의 청소 단계를 포기함으로써 규모와 품질을 교환합니다.

451
00:43:37,720 --> 00:43:44,680
대신 최소한의 주파수 기반 필터링만 적용합니다. 그리고 분명히 이것은

452
00:43:44,680 --> 00:43:54,920
전체 노이즈 샘플, 두 번째 샘플. 그리고 다른 샘플에는 약간의 시끄러운 정보가 있습니다.

453
00:43:54,920 --> 00:44:00,440
이 이미지 위의 이 캡션이나 이 이미지 위의 이 캡션과 같이 텍스트에 표시됩니다.

454
00:44:02,680 --> 00:44:08,040
여기서는 그들의 작업이 Clip과 매우 유사하다는 점을 다시 한 번 강조하고 싶습니다.

455
00:44:08,920 --> 00:44:13,800
유사한 방식으로 자연어 감독을 통한 시각적 표현 학습을 제안합니다.

456
00:44:13,800 --> 00:44:19,400
대조 학습 설정. 게다가, 다른 비전과 언어 인코더를 사용하여

457
00:44:20,040 --> 00:44:26,600
아키텍처에서 주요 차이점은 훈련 데이터에 있습니다. 예를 들어, 선은 자연을 따릅니다.

458
00:44:26,600 --> 00:44:33,480
원시 데이터에서 이미지 텍스트 쌍을 배포하는 반면 Clip은 다음을 통해 데이터세트를 수집합니다.

459
00:44:33,480 --> 00:44:40,200
영어 Wikipedia에서 고주파 시각적 개념을 구성합니다. 그리고 이 작품은 보여줍니다.

460
00:44:40,200 --> 00:44:46,280
강력한 시각적 및 시각 언어 표현은 그렇지 않은 데이터세트를 사용하여 학습할 수 있습니다.

461
00:44:46,360 --> 00:44:53,400
치료하려면 전문적인 지식이 필요합니다. 이것이 라인과 클립의 차이점입니다.

462
00:44:56,200 --> 00:44:58,440
여기서는 선의 방법에 대해 이야기하겠습니다.

463
00:45:00,600 --> 00:45:06,600
시끄러운 이미지 아트 텍스트 데이터로부터 학습하기 위해 시각적 표현과 언어 표현이 실제로 결합됩니다.

464
00:45:07,160 --> 00:45:11,080
그리고 표현은 비전 전용 또는 비전 언어 작업에 사용될 수 있습니다.

465
00:45:11,960 --> 00:45:17,400
옮기다. 미세 조정 없이 라인만으로 제로샷 시각적 분류 가능

466
00:45:17,400 --> 00:45:22,840
이미지에서 텍스트로 검색, 텍스트에서 이미지로 검색을 포함한 교차 모드 검색,

467
00:45:22,840 --> 00:45:30,920
공동 이미지와 텍스트 큐리로 검색할 수도 있습니다. 지금 여기에서 텍스트 인코더를 볼 수 있습니다.

468
00:45:30,920 --> 00:45:37,560
그리고 이미지 인코더. 그리고 우리는 노이즈가 있는 이미지 텍스트 데이터를 인코더에 공급하기만 하면 됩니다.

469
00:45:37,560 --> 00:45:43,480
그리고 우리는 대조 학습 법칙을 사용하여 네트워크를 최적화합니다. 그래서 이것이 묘사입니다

470
00:45:43,480 --> 00:45:53,160
부분. 좀 쉽습니다. 구체적으로 듀얼 인코더 아키텍처를 사용하여 라인을 묘사합니다.

471
00:45:53,160 --> 00:45:59,080
앞서 언급했듯이 모델은 한 쌍의 이미지 인코더와 텍스트 인코더로 구성됩니다.

472
00:45:59,080 --> 00:46:05,640
상단의 코사인 유사성 조합 기능. 그리고 그들은 비전 넷을 이미지 인코더로 사용합니다.

473
00:46:05,640 --> 00:46:14,440
텍스트 임베딩 인코더로 CLS 토큰 임베딩을 사용하는 BERT. 완전히 연결된 레이어

474
00:46:14,440 --> 00:46:20,680
이미지 전력의 차원과 일치하도록 BERT 인코더 위에 선형 활성화가 추가됩니다.

475
00:46:21,800 --> 00:46:27,240
이미지와 텍스트 인코더는 모두 처음부터 학습됩니다. 이미지 및 텍스트 인코더는 다음과 같습니다.

476
00:46:27,240 --> 00:46:34,520
정규화된 소프트맥스 법칙을 통해 최적화되었습니다. 따라서 치료 훈련에서 일치하는 이미지 텍스트 쌍은 다음과 같습니다.

477
00:46:34,520 --> 00:46:41,800
긍정적인. 그리고 훈련 배치에서 음성으로 형성될 수 있는 다른 모든 임의의 이미지 텍스트 쌍입니다.

478
00:46:45,880 --> 00:46:49,000
여기서는 선에 대한 손실함수에 대해 이야기해보겠습니다.

479
00:46:51,080 --> 00:46:56,600
이제 최적화 목표를 살펴보겠습니다. 그들은 단지 두 가지 손실의 합을 최소화하려고 노력할 뿐입니다.

480
00:46:56,600 --> 00:47:02,360
하나는 이미지-텍스트 분류용이고 다른 하나는 텍스트-이미지 분류용입니다.

481
00:47:02,360 --> 00:47:10,360
그리고 여기에서 xi와 yj는 i번째 쌍에 있는 이미지의 정규화된 임베딩입니다.

482
00:47:10,360 --> 00:47:17,320
j번째 쌍의 텍스트입니다. 대문자 N은 배치 크기이고 시그마 항은 하이퍼입니다.

483
00:47:17,320 --> 00:47:26,280
로짓의 크기를 조정하는 온도에 대한 매개변수입니다. 이는 CLIP에서 수행하는 작업과 매우 유사합니다.

484
00:47:26,440 --> 00:47:35,800
CLIP과 같은 대조학습 방법이 적용되어 있기 때문에 자연스럽게 평가를 하게 됩니다.

485
00:47:35,800 --> 00:47:42,680
이미지의 라인 모델을 텍스트로, 텍스트를 이미지로 모델 검색 미세 조정이 있거나 없는 텍스트 검색

486
00:47:43,480 --> 00:47:48,840
이 점. 또한 시각적 분류 작업에 라인 전송을 적용합니다.

487
00:47:49,400 --> 00:47:53,800
그리고 실험의 초점은 제로샷의 성능이다.

488
00:47:56,600 --> 00:48:03,560
이제 우리는 그들의 실험 결과를 볼 수 있습니다. 표 1은 이전 연구와 비교하여,

489
00:48:03,560 --> 00:48:09,320
라인은 벤치마크의 모든 지표에서 SOTA 결과를 달성합니다. 그리고 제로샷 설정에서는

490
00:48:09,320 --> 00:48:15,640
라인은 이전 SOTA에 비해 이미지 검색 작업이 7% 이상 향상되었습니다.

491
00:48:15,640 --> 00:48:21,480
클립. 미세 조정을 통해 라인은 기존의 모든 방법보다 훨씬 뛰어난 성능을 발휘합니다.

492
00:48:22,440 --> 00:48:27,000
그리고 클래스 이름의 텍스트를 텍스트 인코더에 직접 공급하면,

493
00:48:27,000 --> 00:48:33,320
라인은 이미지 텍스트 검색을 통해 이미지를 후보 클래스로 분류할 수 있습니다.

494
00:48:34,920 --> 00:48:40,920
그리고 표 2는 ImageNet의 CLIP과 라인을 비교합니다. CLIP과 유사하게 선이 표시됩니다.

495
00:48:40,920 --> 00:48:46,760
다양한 이미지 분포를 가진 분류 작업에 대한 뛰어난 로봇성. 그래서 만들기 위해서는

496
00:48:46,840 --> 00:48:54,280
공정한 캠핑 이유는 CLIP과 동일한 조립 방법을 사용하기 때문입니다. 우리가 얘기했던 것처럼

497
00:48:54,840 --> 00:49:00,680
클립에서. 각 클래스 이름은 CLIP에서 정의한 ROM 템플릿 세트로 확장됩니다.

498
00:49:00,680 --> 00:49:03,080
사진 수준 라벨과 같은 것입니다.

499
00:49:07,400 --> 00:49:13,080
절제 연구도 있습니다. 실제로 이를 위해서는 대규모 확장 훈련 세트가 필수적입니다.

500
00:49:13,160 --> 00:49:19,080
모델을 확장하고 더 나은 성능을 달성합니다. 완전히하려면 더 큰 모델이 필요합니다.

501
00:49:19,080 --> 00:49:26,680
더 큰 데이터 세트를 활용하세요. 그리고 백본이 커지면 모델 품질도 크게 향상됩니다. 게다가,

502
00:49:26,680 --> 00:49:34,040
예상대로 이미지 인코더 기능 확장 또는 이미지 인코더 용량 확장

503
00:49:34,680 --> 00:49:42,040
비전 작업에 더 중요합니다. 이미지 텍스트 검색 작업에서 이미지 및 텍스트 인코더는

504
00:49:42,600 --> 00:49:46,440
이 표에 표시된 것처럼 용량도 똑같이 중요합니다.

505
00:49:51,560 --> 00:49:57,000
이 부분에서는 라인 모델에서 학습된 임베딩 분석에 대해 이야기하겠습니다. 나

506
00:49:57,000 --> 00:50:05,160
개인적으로 이 논문에서 가장 중요한 실험은 바로 이 부분이라고 생각합니다. 이 부분은 대략

507
00:50:05,160 --> 00:50:11,080
간단한 이미지 검색 시스템을 구축하여 학습된 임베딩을 분석하여

508
00:50:11,080 --> 00:50:17,640
라인에 의해 훈련된 임베딩의 동작. 그리고 그림은 라인이 검색할 수 있음을 보여줍니다.

509
00:50:17,640 --> 00:50:24,920
정확한 이미지, 장면에 대한 자세한 설명, 세부적인 또는 인스턴스 수준 개념까지.

510
00:50:25,720 --> 00:50:32,920
그리고 이러한 예는 정렬 모델이 실제로 이미지와 텍스트를 정렬할 수 있음을 보여줍니다.

511
00:50:32,920 --> 00:50:38,600
의미론이 유사하며 선이 고귀한 복잡한 개념으로 일반화될 수 있습니다.

512
00:50:38,600 --> 00:50:46,360
그리고 이것은 더 중요한 실험이라고 생각합니다. 이전에,

513
00:50:46,360 --> 00:50:52,040
word-to-vax는 단어 벡터 사이의 선형 관계가 다음의 결과로 나타난다는 것을 보여줍니다.

514
00:50:52,040 --> 00:50:58,200
인접한 단어, 인센티브 및 단락을 예측하도록 훈련합니다. 그리고 그들은 그것을 보여줍니다

515
00:50:58,200 --> 00:51:04,840
이미지와 텍스트 임베딩 간의 선형 관계도 선으로 나타납니다. 그리고 우리는 공연한다

516
00:51:04,840 --> 00:51:11,880
이미지와 텍스트를 결합한 쿼리를 사용하여 이미지 검색. 구체적으로, 쿼리 이미지와 텍스트가 주어지면

517
00:51:11,880 --> 00:51:16,840
문자열을 사용하여 정렬된 임베딩을 함께 추가하고 이를 사용하여 관련 이미지를 검색합니다.

518
00:51:17,480 --> 00:51:21,560
그리고 그림은 다양한 이미지와 텍스트 쿼리의 결과를 보여줍니다.

519
00:51:22,680 --> 00:51:30,360
이러한 예는 위대함을 보여줄 뿐만 아니라

520
00:51:31,320 --> 00:51:36,920
비전과 언어 영역 전반에 걸쳐 정렬된 임베딩의 구성성을 보여줍니다.

521
00:51:36,920 --> 00:51:42,840
그렇지 않으면 매우 어려울 수 있는 다중 모드 쿼리를 사용한 새로운 검색 패러다임의 실현 가능성

522
00:51:42,840 --> 00:51:49,400
텍스트 쿼리나 이미지 쿼리만 사용하는 것은 어렵습니다. 예를 들어, 검은색 신발 한 켤레를 바꿀 수 있습니다.

523
00:51:49,400 --> 00:51:56,200
똑같이 생긴 레드 컬러의 신발 속으로. 마지막으로 마지막 세 행을 보여주듯이

524
00:51:56,200 --> 00:52:03,000
그림에서 빼기를 수행하여 장면에서 객체 속성을 제거하는 것도 가능합니다.

525
00:52:03,560 --> 00:52:11,960
임베딩 공간에서. 지금까지 우리는 텍스트와 텍스트를 얼마나 잘 통합할 수 있는지 발견했을 것입니다.

526
00:52:11,960 --> 00:52:18,200
이미지의 경우 대조 학습을 통해 텍스트와 이미지 표현을 통합할 수 있습니다.

527
00:52:18,760 --> 00:52:24,760
안정적인 확산이 조건부 생성을 위한 텍스트 및 이미지 인코더로 클립을 사용하는 이유.

528
00:52:24,760 --> 00:52:34,120
그러나 클립 및 오스카와 유사하지만 Align Zero Shot이 매우 좋은 성능을 달성하지만,

529
00:52:34,120 --> 00:52:40,520
완전히 유연하기 때문에 여전히 새로운 것을 생성하거나 새로운 개념을 생성할 수 없습니다.

530
00:52:40,520 --> 00:52:48,360
생성된 모델이 아니기 때문에 이미지 캡션을 수행합니다. 그리고 다음에서는

531
00:52:49,080 --> 00:52:52,680
우리는 디코더를 사용하여 모델로 무엇을 할 수 있는지 살펴보겠습니다.

532
00:52:55,640 --> 00:53:07,960
괜찮은. 우리가 다룬 내용을 빠르게 요약해 보세요. 그래서 오스카와 미들에는 단일 인코더와 같습니다.

533
00:53:07,960 --> 00:53:14,600
기반 모델, 클립 및 정렬은 듀얼 인코더 기반 모델과 같습니다. 이제 우리는

534
00:53:15,560 --> 00:53:23,400
변환기의 디코더 블록을 사용하여 생성적 언어 이미지 훈련을 도입합니다.

535
00:53:25,160 --> 00:53:31,880
그래서 제가 다룰 첫 번째 논문은 훈련당 언어 이미지를 부트스트래핑하는 blip입니다.

536
00:53:31,880 --> 00:53:37,560
통합 비전 언어 이해 및 생성. 주요 기여는 개선이다.

537
00:53:38,360 --> 00:53:42,040
대조 훈련을 부트스트래핑하여 텍스트 품질을 향상하고 통합

538
00:53:43,000 --> 00:53:52,120
훈련당 비전 언어. 다시 한 번 말씀드리지만, 배경 블립은 클립을 개선하기를 원합니다.

539
00:53:52,120 --> 00:53:57,800
그리고 두 가지 관점에서 본 선. 제가 말했듯이, 모델 관점에서 보면,

540
00:53:58,440 --> 00:54:05,480
온라인으로 클립하면 인코더 기반 모델을 채택합니다. 문제는 그게 쉽지 않다는 거다.

541
00:54:05,480 --> 00:54:12,200
이미지 캡션과 같은 텍스트 생성 작업으로 직접 전송됩니다. 그리고 인코더

542
00:54:12,200 --> 00:54:17,240
디코더 기반 모델은 이미지 텍스트 검색 작업에 성공적으로 채택되지 않았습니다.

543
00:54:17,880 --> 00:54:24,520
두 번째 관점은 데이터 관점이다. 고품질 인간의 수

544
00:54:24,520 --> 00:54:32,360
예를 들어 MS Coco와 같은 장식된 이미지와 텍스트 쌍은 대규모 다중 모델에 충분하지 않습니다.

545
00:54:32,360 --> 00:54:40,440
모델 훈련. 자르기 및 정렬은 매우 크지만 시끄러운 웹 텍스트에 표시됩니다. 이것은 단지

546
00:54:41,400 --> 00:54:49,480
최적이 아닌 결과를 얻습니다. 그리고 여기 이미지에 대한 농담이 있습니다.

547
00:54:49,480 --> 00:54:56,280
나무에 나뭇잎. 축하합니다. 이제 지점장님이 되셨습니다. 아시다시피 그렇지 않아요

548
00:54:56,280 --> 00:55:09,400
다중 모델 대형 단위를 훈련하기 위한 좋은 훈련 데이터입니다. 그럼 그 블립은 개선을 원해요

549
00:55:09,480 --> 00:55:16,920
자막 품질. 텍스트 품질 문제를 해결하기 위한 자연스러운 접근 방식은

550
00:55:16,920 --> 00:55:23,800
좋은 이미지 텍스트 쌍과 나쁜 이미지 텍스트 쌍을 구별할 수 있는 판별기입니다. 그리고 발전기는

551
00:55:23,800 --> 00:55:33,720
시끄러운 캡션을 대체하기 위해 더 나은 품질의 캡션을 합성합니다. 그런 다음 클립 및 정렬과 동일하게

552
00:55:33,720 --> 00:55:39,320
우리는 비전과 언어 표현을 정렬하기 위해 단일 모델 인코더를 배우고 싶습니다.

553
00:55:40,440 --> 00:55:48,280
그래서 여기에 아이디어를 보여주는 또 다른 예가 있습니다. 매우 시끄러운 텍스트가 포함된 이미지가 있습니다.

554
00:55:48,280 --> 00:55:55,880
선셋파크에 있는 블루스카이 베이커리. 우리는 필터를 사용하여 이것이 올바른 캡션이 아님을 알리고 싶습니다.

555
00:55:56,760 --> 00:56:00,360
그리고 우리는 더 나은 캡션을 생성하기 위해 캡션 작성자를 사용하고 싶습니다.

556
00:56:00,920 --> 00:56:07,560
그 위에 크림 프로스팅과 초콜릿 스프링클을 얹은 초콜릿 케이크. 그리고 우리는 미래를 희망합니다

557
00:56:07,560 --> 00:56:17,560
이것이 더 나은 캡션이라고 말할 수 있습니다. 매우 간단한 아이디어입니다. 예전과 너무 비슷해서 배우고 싶어

558
00:56:18,280 --> 00:56:26,840
여기서부터 단일 모델 인코더를 시작하겠습니다. 따라서 다중 모델 정렬 작업은 다음을 장려하고 싶습니다.

559
00:56:26,840 --> 00:56:32,120
일치하는 이미지 텍스트 쌍은 네거티브와 대조적으로 유사한 표현을 갖습니다.

560
00:56:32,120 --> 00:56:42,280
한 쌍. 클립과 동일합니다. 그리고 클립과 동일한 ITC 손실 함수를 사용합니다. 이 회사는,

561
00:56:42,280 --> 00:56:51,640
두 개의 infoNCE 손실의 합입니다. NCE는 이미지 감지를 위한 노이즈 대비 분할(Noise Contrast divestimation)을 나타냅니다.

562
00:56:52,360 --> 00:56:58,440
그리고 텍스트 이미지. 그리고 infoNCE에서 사용하는 유사성 함수는 코사인 유사성입니다.

563
00:57:00,360 --> 00:57:11,800
그리고 모델 구조는 단지 이미지 인코더와 텍스트 인코더입니다. 이는 다음과 같습니다.

564
00:57:13,000 --> 00:57:20,920
이전에 다뤄졌습니다. 그런 다음 판별자나 필터의 경우 바이너리를 훈련하고 싶습니다.

565
00:57:20,920 --> 00:57:26,600
다중 모델이 주어지면 이미지 텍스트 쌍이 일치하는지 여부를 예측하는 분류 작업

566
00:57:26,600 --> 00:57:34,200
특징. 여기에 동일한 이미지 인코더가 있습니다. 우리는 모든 기능의 이미지를 가지고 있습니다.

567
00:57:34,920 --> 00:57:42,760
그런 다음 텍스트 인코더의 이미지가 있습니다. 처음에 인코딩 특수 토큰을 추가합니다.

568
00:57:42,760 --> 00:57:49,800
캡션의. 방향 치환을 통해 셀을 통해 전달합니다. 그리고 우리는

569
00:57:49,800 --> 00:57:57,560
이미지의 신호를 교차 주의로 보냅니다. 그리고 마지막에는 이진 분류가 있습니다.

570
00:57:57,560 --> 00:58:06,920
손실. 그리고 그들은 그것을 ITM 손실이라고 부릅니다. 이는 이미지 텍스트 일치 손실을 나타냅니다. 그리고 저자는 다음을 채택합니다.

571
00:58:06,920 --> 00:58:13,960
하드 네거티브 샘플링 전략이라고 불리는 것입니다. 이것은 매우 간단한 아이디어입니다. 우리는 부정적인 것을 사용하고 싶습니다

572
00:58:13,960 --> 00:58:20,440
이전에 배운 ITC 손실보다 더 높은 대비 유사성과 쌍을 이룹니다.

573
00:58:22,280 --> 00:58:30,280
따라서 우리는 ITM 손실을 훈련하기 위해 이미지 캡션 쌍과 더 유사한 것을 사용하고 싶습니다.

574
00:58:30,280 --> 00:58:35,960
이것은 더 어렵습니다. 매우 다른 이미지와 캡션을 사용하면 그다지 좋지 않습니다.

575
00:58:35,960 --> 00:58:46,760
우리 모델이 유익한 학습을 ​​학습하는 것은 어렵습니다. 네, 그게 ITM 손실입니다.

576
00:58:46,760 --> 00:58:58,440
필터. 그리고 생성자나 캡션 작성자에게는 일반적인 작업입니다. 이것은 새로운 것입니다. 과제는

577
00:58:58,440 --> 00:59:05,160
이미지가 주어지면 극도로 퇴행적인 방식으로 텍스트 설명을 생성합니다. 그리고 모델에서는

578
00:59:05,160 --> 00:59:14,600
아키텍처에서는 이것이 텍스트 디코더의 이미지라고 부르는 것입니다. 그래서 그들은 양방향을 교환합니다

579
00:59:14,600 --> 00:59:21,400
마스크 자가 입장을 요청하는 자가 입장. 나머지는 동일합니다. 그러면 출력은 다음과 같습니다.

580
00:59:21,400 --> 00:59:32,280
GPD와 같은 언어 모델 손실. 그리고 세 번의 손실을 합치면 이제 실수가 발생합니다.

581
00:59:32,280 --> 00:59:40,920
건축학. 따라서 비전 인코더의 경우 VIT를 사용합니다. 텍스트 인코더는 BERT를 사용합니다. 그런 다음

582
00:59:40,920 --> 00:59:48,360
교차 승인을 추가하기 위해 BERT 변환기 블록을 수정합니다. 그러면 이미지가 나타납니다.

583
00:59:48,360 --> 00:59:53,800
텍스트 인코더에서. 그런 다음 통화를 양방향 자체 승인으로 변경합니다.

584
00:59:54,760 --> 00:59:56,440
디코더를 얻었습니다.

585
01:00:02,120 --> 01:00:11,720
네, ITC는 이미지세 대비손실입니다. CLP도 마찬가지다. 그리고 ITM은 이미지세입니다.

586
01:00:11,720 --> 01:00:17,240
일치 손실. 이것이 양방향 분류입니다. 기본적으로 이미지와

587
01:00:17,240 --> 01:00:25,160
캡션, 그들이 쌍인지 아닌지 알고 싶습니다. 그리고 그들이 이것을 훈련시키는 데 사용하는 데이터는 다음과 같습니다.

588
01:00:25,160 --> 01:00:32,200
실제로 한 쌍의 이미지와 텍스트를 얻는 것은 영구적이지 않습니다. 그들은 ITC 손실을 사용하여 새로운 것을 만듭니다.

589
01:00:32,200 --> 01:00:40,520
형성 샘플링(formative sampling)은 텍스트가 주어졌을 때 시끄러운 캡션을 얻고자 함을 의미합니다.

590
01:00:40,520 --> 01:00:48,440
쌍이지만 매우 유사하고 대칭적입니다. 그렇다면 이것은 의미가 있을 것입니다. 그렇지 않으면 배우지 못할 것이다

591
01:00:52,440 --> 01:00:58,440
응. 그리고 사랑스러운 순간의 손실은 일반적인 손실입니다.

592
01:01:03,160 --> 01:01:09,800
이제 필터와 캡션 작성자가 있는 글이 생겼습니다. 데이터 세트를 부트스트랩할 수 있습니다.

593
01:01:10,840 --> 01:01:17,960
따라서 우리는 시끄러운 웹 규모 데이터 세트를 사용하여 인코더와 디코더를 묘사하는 것부터 시작합니다.

594
01:01:19,240 --> 01:01:25,480
그래서 이것은 매우 클 수 있습니다. 웹의 이미지와 웹의 텍스트가 있습니다.

595
01:01:25,480 --> 01:01:34,280
필터 캡션 작성자와 UV 모델 인코더를 설명하셨습니다. 그런 다음 깨끗한 것을 사용하십시오.

596
01:01:35,000 --> 01:01:41,640
데이터세트. 예를 들어 Ms. Coco는 사람이 주석을 달았습니다. 작지만 좋은 품질을 사용합니다.

597
01:01:41,640 --> 01:01:49,480
필터와 캡션 작성자를 미세 조정하기 위한 데이터 세트입니다. 그런 다음 미세 조정 모델을 사용하여

598
01:01:49,480 --> 01:01:59,320
웹 데이터세트에 대한 체계적 캡션입니다. 이제 우리는 캡션 작성자에게만 이미지를 제공하고

599
01:02:00,280 --> 01:02:07,480
합성된 텍스트. 그리고 합성된 텍스트와 웹 기반 텍스트를 평가하기 위해,

600
01:02:09,080 --> 01:02:15,960
우리는 둘 다 미래를 통해 전달합니다. 그리고 우리는 좋은 품질의 웹과 텍스트 쌍을 가지고 있습니다.

601
01:02:17,560 --> 01:02:24,120
그리고 이제 그들은 대략 1억 2천 9백만 개의 고품질 이미지 텍스트 쌍을 사용합니다.

602
01:02:24,680 --> 01:02:32,600
더 크고 깨끗합니다. 그들은 이것을 필터 캡션 작성자의 새로운 모델을 묘사하는 데 사용합니다.

603
01:02:32,600 --> 01:02:40,040
그리고 새로운 모델 인코더. 그리고 저자는 우리가 새로운 모델을 묘사하지 않으면

604
01:02:40,040 --> 01:02:47,480
단순히 세 가지 모델을 계속해서 훈련한다면 도움이 되지 않습니다. 이 관찰

605
01:02:47,480 --> 01:02:53,080
학생 모델이 할 수 없는 지식 증류의 일반적인 관행에 동의합니다.

606
01:02:53,080 --> 01:03:08,760
교사 모델에서 초기화됩니다. 예. 나는 그 아이디어가 확실히 GAN에서 빌린 것이라고 생각한다.

607
01:03:09,480 --> 01:03:19,880
그러나 나는 그들이 함께 훈련을 받았다고 생각하지 않습니다. 그래서 그들은 이것을 필터와 캡션 작성자를 묘사하는 데 사용합니다.

608
01:03:20,920 --> 01:03:29,320
뭐, 아키텍처를 보면 같이 훈련을 하긴 하지만 반대는 안 하더군요.

609
01:03:29,400 --> 01:03:34,600
서로. 그래서 그들은 서로를 수정하지 않습니다. 아니요.

610
01:03:38,120 --> 01:03:44,520
하지만 내 생각엔 그 아이디어가 GAN과 매우 유사하다고 생각한다. 죄송합니다. 이전 슬라이드로 돌아갑니다.

611
01:03:45,640 --> 01:03:53,800
대조 손실 RTC. 여기에 무엇이 있는지 다시 설명해야 할까요? 우리 할 일이라도 있어?

612
01:03:54,520 --> 01:04:03,960
응. 그러니까 기본적으로는 CLIP과 똑같습니다. 이미지 배치가 있습니다.

613
01:04:03,960 --> 01:04:11,480
배치의 모든 이미지에 대한 임베딩. 그리고 모든 캡션에 대한 텍스트 임베딩이 있습니다.

614
01:04:11,480 --> 01:04:17,880
귀하의 배치에. 그리고 기본적으로 대각선을 가장 높은 점수가 되도록 정렬하려고 합니다.

615
01:04:17,880 --> 01:04:23,000
대각선이 올바른 쌍을 나타내기 때문입니다. 그리고 바깥쪽 대각선은

616
01:04:25,000 --> 01:04:31,560
같은 쌍이 아닙니다. 그들은 오염된 쌍과 같습니다. 당신은 그것을 감독자에게 보내는 약한 신호로 사용하고 싶습니다.

617
01:04:31,560 --> 01:04:40,280
동일한 쌍을 학습하는 모델입니다. 유사한 의미는 동일한 쌍이어야 합니다. 다른

618
01:04:40,280 --> 01:04:52,680
의미론은 서로 다른 쌍이어야 합니다. 응. 다시 CLIP으로 돌아가면

619
01:04:55,880 --> 01:05:05,720
여기처럼. 따라서 각각의 I는 배치에 포함된 이미지입니다. 각 T는 배치에 포함된 텍스트입니다.

620
01:05:06,440 --> 01:05:10,360
우리는 대각선을 강조하고 싶기 때문에 소프트맥스를 사용하면

621
01:05:11,320 --> 01:05:17,960
수직 또는 수평으로 대각선이 가장 큰 값입니다. 왜냐하면 두 값은 동일하기 때문입니다.

622
01:05:17,960 --> 01:05:25,560
패키지. 예. 판별자가 가는 한 질문이 있었습니다. 그렇죠? 그래서 우리가 사용하고 있기 때문에

623
01:05:25,560 --> 01:05:29,640
이 이진 수정자는 무엇이 좋고 무엇이 나쁜가요? 라벨을 다시 지정해야 합니다.

624
01:05:29,640 --> 01:05:33,960
데이터에 대해서는 CLIP과 alignment에는 그런 것이 없었습니다. 그렇죠? 이런 상황에서 우리가 가야 할 일은

625
01:05:33,960 --> 01:05:40,040
미리 말해 보세요. 이게 좋은 캡션인가요, 아니면 나쁜 캡션인가요? 응. 데이터 샘플 수

626
01:05:40,040 --> 01:05:43,720
그런 다음 해당 판별자를 훈련해야 합니까? 통과하는 것은 의미가 없기 때문에

627
01:05:43,720 --> 01:05:49,960
설명하는 모든 단일 이미지에 라벨을 붙입니다. 응. 그래서 그들은 훈련을 위해 시끄러운 데이터 세트를 사용합니다.

628
01:05:49,960 --> 01:05:58,040
바이너리 헤더. 그런 다음 MS-Coco를 사용하여 미세 조정합니다. 그래서 그들의 일에서 나는 그들이

629
01:05:59,000 --> 01:06:05,000
데이터 세트에 수동으로 레이블을 지정합니다. 그들은 단지 인간 애니메이션 데이터 세트인 MS-Coco를 사용합니다.

630
01:06:06,520 --> 01:06:12,360
시끄러운 데이터 세트로 사전 훈련합니다. 응, 정말 크구나. 그들은 업샘플링을 위해 미세 조정을 사용합니다.

631
01:06:12,360 --> 01:06:18,680
시끄러운 것보다 데이터에 더 잘 맞도록 깨끗한 ​​데이터를 사용합니다. 응. 그런 다음 캡션 작성자를 사용합니다.

632
01:06:18,680 --> 01:06:26,440
대규모 이미지에 대한 합성 텍스트를 생성합니다. 좋아요. 그래서 수동으로 라벨을 붙일 필요가 없습니다.

633
01:06:26,440 --> 01:06:32,040
그게 뭐가 좋은데? 보여주시면 됩니다. 아, 그러면 그걸 보여주죠. 그냥 다음에 가세요. 응. 응.

634
01:06:32,040 --> 01:06:40,200
MS-Coco 데이터 세트를 통해 신호를 도입합니다. 오른쪽. 그리고 판별자, 코코아 하나는

635
01:06:40,200 --> 01:06:47,080
양질. 응. 악보에 기록된 것 같아요. 그리고 품질이 좋지 않은데 어떻게 하나요?

636
01:06:47,080 --> 01:07:04,360
차별하는 것 같나요? 내 생각에 그들은 좋은 것만 사용하는 것 같아요.

637
01:07:04,360 --> 01:07:11,320
미세 조정합니다. 나는 그들이 품질이 좋지 않은 것을 사용하여 미세 조정한다고 언급하지 않았다고 생각합니다.

638
01:07:11,320 --> 01:07:17,960
캡션 작성자가 있습니다. 그들은 대조 손실의 하드 네거티브를 사용하고 있습니까?

639
01:07:19,160 --> 01:07:26,600
무료 교육에 대한 부정적인 예로? 응. 사전 훈련 부분입니다. 하지만 그건 아니다.

640
01:07:27,160 --> 01:07:36,520
글쎄요. MS-코코처럼 하나하나가 퀄리티가 좋기 때문에 세세하게 조정하는 부분도 있어요

641
01:07:36,520 --> 01:07:41,880
쌍. 하지만 텍스트를 다른 이미지 텍스트로 바꾸면 부정적인 샘플이 됩니다.

642
01:07:41,880 --> 01:08:02,600
좋아요. 그리고 여기에서는 캡션 작성자와 필터의 성능에 대한 간단한 시각화를 제공합니다.

643
01:08:03,320 --> 01:08:11,080
따라서 이것은 웹의 이미지와 같이 웹 크기로 조정된 것과 같습니다. 처음 두 개는 텍스트가 잘못되었습니다.

644
01:08:11,640 --> 01:08:17,800
그러면 캡션 작성자는 꽤 괜찮은 텍스트를 생성할 수 있습니다.

645
01:08:19,000 --> 01:08:26,680
웹에서 온 텍스트. 그리고 세 번째로는 웹 텍스트가 실제로 더 좋습니다. 그것은 다음과 같은 것을 알고 있습니다.

646
01:08:26,680 --> 01:08:33,960
80년대의 쿠스코지만 합성 텍스트는 뉴스가 많은 대규모 집단일 뿐이죠.

647
01:08:33,960 --> 01:08:41,000
그다지 유익하지 않습니다. 캡션 작성자의 관점에서 본 것입니다. 하지만 필터는

648
01:08:42,360 --> 01:08:50,280
웹의 텍스트가 더 낫다는 것을 확인하십시오. 따라서 이 세 수치는 다음의 효과를 보여줍니다.

649
01:08:51,080 --> 01:08:59,320
캡션 작성자 및 필터. 그런 다음 다운스트림 작업에 GLEP를 채택하려면 다음 단계를 따르세요.

650
01:08:59,320 --> 01:09:07,000
심상. 예를 들어 VQA의 경우 이미지 인코더를 통해 이미지를 공급합니다. 우리는 이미지를 가지고 있습니다

651
01:09:07,000 --> 01:09:17,080
임베딩. 그런 다음 이를 인코딩 토큰과 함께 텍스트 인코더에 제공합니다. 그런 다음 출력을 공급합니다.

652
01:09:17,080 --> 01:09:27,320
디코더에 연결하면 개방형 VQA를 생성할 수 있습니다. 그리고 NLVR의 경우

653
01:09:27,960 --> 01:09:35,400
NLVR용 삽입은 한 쌍의 이미지입니다. 둘 다 이미지 인코더를 통과합니다. 그리고 우리는

654
01:09:35,400 --> 01:09:43,080
텍스트. 그런 다음 텍스트가 통과하고 크로스 인코더를 통해 이미지 인코더에 연결됩니다.

655
01:09:43,800 --> 01:09:50,600
그리고 마지막에는 이진 분류 문제를 풀어야 합니다. 나는 통과하지 않을 것이다

656
01:09:50,600 --> 01:09:59,160
자세히. 그리고 정량적 결과. 그래서 여기 이 표에는

657
01:09:59,160 --> 01:10:05,080
캡션 작성자만 사용하고 이 세 행에서 강조 표시된 필터만 사용합니다. 그래서 이건

658
01:10:05,080 --> 01:10:12,600
필터일 뿐입니다. 이 사람은 캡션 작성자일 뿐입니다. 그리고 우리의 관찰은 캡션 작성자가 생성한 것입니다.

659
01:10:13,240 --> 01:10:20,120
모델이 활용할 수 있는 더 많은 새로운 정보가 포함된 더욱 다양한 캡션입니다.

660
01:10:20,120 --> 01:10:27,560
따라서 캡션 작성자만 사용하면 실제로 캡션 작성자만 사용하는 것보다 약간 더 나은 성능을 얻을 수 있습니다.

661
01:10:27,560 --> 01:10:37,080
필터만. 그런 다음 이 두 행에 대해 캡션 작성자와 필터를 모두 사용하고 있습니다. 하나는 함께

662
01:10:37,160 --> 01:10:46,360
기본 크기. 다른 하나는 바지선 크기입니다. 캡션과 필터의 크기가 확대되는 것을 볼 수 있습니다.

663
01:10:46,360 --> 01:10:53,000
베이스부터 대형까지. 그런 다음 비전 인코더에 대한 VIT의 크기는 동일하게 유지됩니다.

664
01:10:54,200 --> 01:11:02,360
생성 작업 성능만 향상됩니다. 이것이 캡션이 적용되고 캡션이 붙은 것입니다.

665
01:11:02,360 --> 01:11:09,720
촬영 중. 이미지 크기 검색은 실제로 그다지 좋은 향상을 얻지 못합니다.

666
01:11:11,720 --> 01:11:18,680
비전 백본을 확장하면 검색 작업이 더욱 향상됩니다. 그래서 당신은

667
01:11:18,680 --> 01:11:26,600
기본적으로 VIT를 대규모로 확장해야 합니다. 이제 큰 캡션 필터를 결합하여 개선할 수 있습니다.

668
01:11:26,600 --> 01:11:41,960
너의 이야기. 그런 다음 BLIP를 Align 또는 LBAP와 비교합니다. 가장 작은 BLIP, 이것이다.

669
01:11:43,080 --> 01:11:52,360
1% 미만의 데이터를 사용하더라도 모두 정렬을 수행할 수 있습니다. BLIP은 129를 사용하기 때문에

670
01:11:53,240 --> 01:12:03,480
Align은 1400만개를 사용하는데, 이는 18억개입니다. 그리고 가장 작은 BLIP도 모두 LBAP를 수행합니다.

671
01:12:04,200 --> 01:12:12,760
BLIP과 동일한 1,400만 개의 이미지를 사용합니다. 하지만 LBAP는 인코더 기반 전용 설계를 채택하고 있어,

672
01:12:13,720 --> 01:12:24,760
부트스트래핑 데이터 세트가 없습니다. 그리고 다음 논문은 죄송합니다. BLIP에 관해 질문 있으신가요?

673
01:12:28,440 --> 01:12:35,320
지금까지는 너무 좋았어, 알았어. 다음 논문은 COCA에 관한 것입니다. 대조 캡셔너는 Image Text Foundation입니다.

674
01:12:35,320 --> 01:12:40,440
모델. COCA는 대조 훈련과 생성 훈련을 결합합니다.

675
01:12:42,280 --> 01:12:52,200
따라서 각 이미지 텍스트 쌍에 대해 BLIP을 기억하세요. BLIP의 단점은 사전 훈련이 필요하다는 것입니다.

676
01:12:52,200 --> 01:12:58,200
지역 변환기를 통과하는 한 번의 순방향 패스, 그런 다음 세 번의 순방향 패스가 필요합니다.

677
01:12:58,200 --> 01:13:07,560
매우 효율적이지 않은 텍스트 변환기를 통해. 그래서 COCA는

678
01:13:08,840 --> 01:13:12,120
훈련 효율성을 향상시키기 위해 BLIP의 미니멀한 디자인을 가지고 있습니다.

679
01:13:15,480 --> 01:13:24,600
그래서 그들이 한 일은 텍스트 인코더를 새로운 모델 텍스트 인코더로 교체한 것입니다.

680
01:13:25,560 --> 01:13:30,680
그리고 그들은 양방향 self-attention을 대량 self-attention으로 바꾸었습니다.

681
01:13:31,560 --> 01:13:41,880
그리고 텍스트 입력의 경우 마지막에 분류 토큰을 추가했습니다. 그래서 그들은

682
01:13:41,880 --> 01:13:50,120
텍스트 삽입으로 분류 토큰을 생성하고 이를 사용하여 ITC를 계산합니다.

683
01:13:50,120 --> 01:14:03,000
이전과 같은 손실. 동기는 텍스트 인코더가 너무 많기 때문입니다.

684
01:14:04,440 --> 01:14:13,960
인코더 2개, 인코더 1개. 효율적이지 않습니다. 그렇다면 그들의 생각은 우리가 단지

685
01:14:13,960 --> 01:14:19,400
인코더? 인코더를 전혀 사용하지 않으면 어떻게 되나요? 하지만 문제는 인코더만 사용하는 경우,

686
01:14:19,400 --> 01:14:28,200
텍스트를 어떻게 삽입하나요? 따라서 끝에 분류 토큰을 추가합니다.

687
01:14:28,840 --> 01:14:37,880
이는 훈련 가능한 토큰이며 분류 토큰의 출력이 좋은 결과를 얻을 수 있기를 바랍니다.

688
01:14:37,880 --> 01:14:48,760
전체 텍스트의 표현 임베딩. 디코더를 말하는 건가요? 이것을 보기 위해서가 아닙니다.

689
01:14:50,040 --> 01:14:57,880
따라서 전체 텍스트 디코더와 마찬가지로 분류 토큰을 포함하여 훈련 가능한 인코더도 있습니다.

690
01:14:58,680 --> 01:15:08,840
네, 이것이 COCA 아키텍처입니다. 그 사람들 신문에는 이런 내용이 없어요.

691
01:15:08,840 --> 01:15:18,280
하지만 저는 이것을 제가 이해한 바에 따라 그립니다. 네, 그들은 디코더만 사용합니다. 왼쪽 것

692
01:15:18,280 --> 01:15:29,720
단봉 텍스트 디코더입니다. 앞서 언급한 것처럼 이를 사용하여 분류를 추가합니다.

693
01:15:29,720 --> 01:15:39,400
마지막에 토큰. 따라서 출력 시퀀스의 마지막 토큰인 출력 시퀀스는

694
01:15:39,400 --> 01:15:46,520
CIS 토큰에 해당합니다. 그리고 이를 사용하여 이미징으로 ITC 손실을 계산합니다.

695
01:15:46,520 --> 01:15:54,440
인코더. 이미징 인코더를 사용하면 이 부분은 잊어버리세요. 그리고 나머지는

696
01:15:54,440 --> 01:16:02,520
디코더의 전반부 출력은 디코더의 후반부로 전송됩니다. 그리고 여기

697
01:16:02,520 --> 01:16:08,920
그들은 교차 관심을 가지고 있습니다. 신호는 이미지에서 나오며 LM 손실이 발생합니다.

698
01:16:09,080 --> 01:16:20,600
그렇습니다. 그들은 매우 큰 디코더를 하나 가지고 있습니다. 그들은 그것들을 반으로 잘랐습니다. 첫 번째 부분은 단일 모드이며,

699
01:16:20,600 --> 01:16:29,400
두 번째 부분은 다중 모드입니다. 따라서 이 설계를 사용하면 모든 이미지 공격 쌍에 대해

700
01:16:30,200 --> 01:16:36,680
이미지는 이미징 인코더를 한 번 통과하고, 공격은 디코더를 한 번 통과합니다.

701
01:16:38,680 --> 01:16:41,800
기본적으로는 기본적으로 디코더, 디코더라고 생각합니다.

702
01:16:43,320 --> 01:16:48,520
네, 디코더에 할당했어요.

703
01:16:54,520 --> 01:16:56,120
디코더입니다. 디코더입니다.

704
01:16:57,080 --> 01:17:01,080
디코더입니다. 인과적, 인과적 관심이기 때문입니다.

705
01:17:01,080 --> 01:17:03,480
아, 원인이군요. 주목을 받는 이유입니다.

706
01:17:03,480 --> 01:17:08,600
응, 응, 응. 나는 이 그림이 마음에 들지 않았다. 이 글을 읽어보니 그런 인물은 없습니다.

707
01:17:08,600 --> 01:17:13,400
무슨 일이 일어나고 있는지 이해하는 데 정말 오랜 시간이 걸렸습니다. 나는 그것을 이해하지 못했습니다.

708
01:17:13,400 --> 01:17:15,400
그들의 수치는 매우 높은 수준입니다.

709
01:17:15,400 --> 01:17:21,160
네, 우리는 매우 높은 수준에 있습니다. 하지만 그들은 하나의 디코더를 반으로 나누었습니다.

710
01:17:22,120 --> 01:17:28,600
하나는 전반부와 마찬가지로 텍스트를 인코딩하는 것입니다. 하지만 텍스트를 인코딩하는 디코더입니다.

711
01:17:30,600 --> 01:17:37,080
그런 다음 디코더의 다른 부분은 이미지 인코더로부터 입력을 받습니다.

712
01:17:37,080 --> 01:17:43,080
다시는 시간의 무게를 두지 않도록 말이죠. 이미 ITC에서 계산되어 있고,

713
01:17:43,080 --> 01:17:47,080
그런 다음 모든 것을 투자하고 또 다른 손실을 입습니다.

714
01:17:47,160 --> 01:17:50,680
그래서 이것은 매우 똑똑한 디자인입니다. 매우 효율적입니다.

715
01:17:55,480 --> 01:17:58,120
그것은 텍스트 디코더 부분에 관한 것입니다.

716
01:17:59,160 --> 01:18:05,960
이제 나는 그들이 주의를 끄는 사람이라고 부르는 것에 대해 이야기하겠습니다. 나는 이것이 흥미롭다고 생각한다.

717
01:18:07,160 --> 01:18:10,600
그래서 그들은 그것을 작업별 주의력 끌어당김 도구라고 부릅니다.

718
01:18:11,320 --> 01:18:19,000
따라서 이 사람은 학습 가능한 쿼리가 포함된 단일 다중 모자 관심 레이어입니다.

719
01:18:20,600 --> 01:18:27,160
이미지 인코더 출력을 키와 값으로 사용합니다. 따라서 학습 가능한 쿼리는 Q입니다.

720
01:18:27,880 --> 01:18:30,360
이미지 인코더의 출력은 K와 D입니다.

721
01:18:30,600 --> 01:18:45,400
그렇다면 왜 이것이 필요한가요? ITC 손실을 계산하려면 다음과 비교할 1 by D 벡터가 필요합니다.

722
01:18:45,400 --> 01:18:52,280
이것은 1 by D 벡터입니다. 그래서 그들은 조사를 1로 설정했습니다. 그래서 주의를 집중시킨 후에,

723
01:18:52,840 --> 01:18:58,440
이 출력은 D에 의해 1입니다. 그리고 이 출력을 사용하여 ITC를 계산할 수 있습니다.

724
01:18:59,000 --> 01:19:09,320
하지만 이 디코더의 경우 교차 어텐션에는 더 긴 시퀀스가 ​​필요합니다. 그래서 그들의 논문에서는

725
01:19:09,320 --> 01:19:15,400
그들은 LN 손실에 대해 문의를 256으로 설정했습니다. 기본적으로 이것은 어댑터입니다.

726
01:19:16,440 --> 01:19:22,680
삽입된 동일한 이미지에 대해 서로 다른 길이의 시퀀스를 갖습니다.

727
01:19:23,080 --> 01:19:29,480
그리고 그들이 이 시퀀스를 더 길게 원하는 이유는,

728
01:19:30,840 --> 01:19:38,040
길이가 길수록 지역 수준의 기능이 더 많아집니다. 미세한 그래프에 가깝습니다

729
01:19:38,920 --> 01:19:45,000
1xD와 비교하면 이미지 기능을 전체적으로 표현한 것입니다.

730
01:19:45,000 --> 01:19:53,720
그리고 주의를 끄는 사람의 이점은 그것이 자연스럽다는 것입니다.

731
01:19:54,280 --> 01:20:01,080
다운스트림 작업을 위한 어댑터. 예를 들어 비디오 분류 작업의 경우 단일 쿼리 토큰

732
01:20:01,080 --> 01:20:07,800
다른 프레임과 같은 모든 토큰의 출력에 가중치를 부여하는 방법을 학습합니다.

733
01:20:07,800 --> 01:20:22,280
여기에는 6개의 프레임이 있으며, 각 프레임은 다양한 패치처럼 가져올 수 있습니다.

734
01:20:22,280 --> 01:20:28,600
그리고 당신은 그것들을 서로 연결하고 주의를 끄는 도구를 사용하여 그것들을 끌어당깁니다.

735
01:20:29,480 --> 01:20:37,560
단일 1 x D 벡터. 그리고 그들은 또한 주의를 끄는 도구를 사용하는 것과 같은 것을 발견합니다.

736
01:20:37,560 --> 01:20:46,360
선형 프로빙은 정확하게 측정하는 데 어려움을 겪기 때문에 동결된 특징 평가를 향상시킵니다.

737
01:20:46,360 --> 01:20:54,600
학습된 표현. 그러나 새로운 풀러 학습을 사용하여 기능을 집계합니다.

738
01:20:54,600 --> 01:21:00,520
모델이 고정 인코더로서 더 강력한 성능을 얻을 수 있도록 해줍니다.

739
01:21:01,480 --> 01:21:08,840
그것은 또한 이익이 될 수 있으며, 동일한 정지 상태를 공유하는 다중 작업 문제에도 이익이 될 수 있습니다.

740
01:21:08,840 --> 01:21:13,720
이미지 인코더가 있지만 특정 작업에는 다른 작업이 있습니다.

741
01:21:16,920 --> 01:21:23,560
따라서 COCA의 사전 훈련, 손실 함수는 ITC와 LM의 조합입니다. 그리고 그들은

742
01:21:24,200 --> 01:21:30,520
이 읽기 오류 매개변수는 여기에 있습니다. 경험적으로는 더 큰 손실 가중치를 찾는 것이 더 좋습니다.

743
01:21:31,720 --> 01:21:43,320
그래서 그들은 LM 대비 LM ITC를 2:1로 설정했습니다. 그들의 설명은 ITC 손실이 해석될 수 있다는 것입니다.

744
01:21:43,320 --> 01:21:50,840
어휘가 단지 집합일 때 이미지에 적용되는 생성적 접근 방식의 특별한 경우

745
01:21:50,920 --> 01:22:03,160
당신의 모든 캡션 중. 디코더 레이어 수의 두 부분은 동일합니다.

746
01:22:03,160 --> 01:22:11,320
그들은 48억 개에 달하는 매우 거대한 데이터 세트를 보유하고 있습니다. 그리고 COCA에 대한 평가는,

747
01:22:11,320 --> 01:22:21,160
이는 낮은 인코더 모델 및 디코더 디코더 모델보다 성능이 뛰어나며 전문화되었습니다.

748
01:22:21,160 --> 01:22:30,280
12가지 벤치마크의 SOHA 모델. 제 생각에는 데이터 세트가 실제로

749
01:22:30,280 --> 01:22:36,680
정말 크다. 이번 시간에는 훈련 규모 확장에 대해 이야기하겠습니다.

750
01:22:37,640 --> 01:22:45,400
그리고 이 부분에는 언어 이미지 사전 훈련을 위한 SIGLIP Sigma loss라는 논문이 하나만 있습니다.

751
01:22:45,400 --> 01:22:49,960
이 문서에서는 시그마 손실을 사용하여 훈련을 확장하는 데 중점을 두고 있습니다.

752
01:22:52,440 --> 01:22:58,760
이제 SIGLIP의 탄생 배경과 동기를 살펴보자. 우리는 많은 작업에서

753
01:22:58,760 --> 01:23:04,600
대조 사전 훈련 자체가 약한 감독 작업이기 때문에 대조 사전 훈련

754
01:23:04,680 --> 01:23:10,440
CLIPPET에서와 마찬가지로 이미지와 텍스트의 표현 공간을 정렬할 수 있습니다.

755
01:23:10,440 --> 01:23:19,000
맞추다. 그리고 대조 사전 훈련은 항상 배치 수준 소프트맥스 기반 대조 손실을 사용합니다.

756
01:23:19,960 --> 01:23:26,040
모든 이미지와 모든 텍스트에 대해 쌍별 유사성 점수를 계산합니다.

757
01:23:27,480 --> 01:23:34,520
소프트맥스 기반 대비 손실은 어느 정도 수치적으로 불안정하며, 안정화는

758
01:23:34,760 --> 01:23:37,960
프로세스에는 항상 전체 배치에 대한 추가 경로가 필요합니다.

759
01:23:40,200 --> 01:23:47,720
따라서 본 논문에서는 언어 이미지 사전 훈련을 위한 단일 쌍별 시그마 손실인 SIGLIP을 제안합니다.

760
01:23:48,440 --> 01:23:54,040
소프트맥스 정규화를 사용한 대조 학습과 달리 시그마 손실은 단독으로 작동합니다.

761
01:23:54,600 --> 01:24:01,080
이미지 텍스트 쌍에 대한 것이며 정규화를 위해 쌍별 유사성에 대한 전역 보기가 필요하지 않습니다.

762
01:24:02,040 --> 01:24:08,040
시그마 손실을 통해 동시에 배치 크기를 추가로 확장하는 동시에 수행할 수도 있습니다.

763
01:24:08,040 --> 01:24:18,200
더 작은 배치 크기에서 더 좋습니다. 이제 먼저 널리 사용되는 소프트맥스 기반 대비 손실을 검토합니다.

764
01:24:19,080 --> 01:24:24,120
이미지 텍스트 쌍의 미니 배치와 대조 학습 목표가 주어지면

765
01:24:25,480 --> 01:24:30,440
일치하는 쌍의 임베딩을 서로 정렬하고 일치하지 않는 임베딩을 푸시합니다.

766
01:24:30,440 --> 01:24:38,760
짝을 이룬다. 따라서 실제 사용을 위해 항상 각 이미지와 관련 텍스트에 대해

767
01:24:38,760 --> 01:24:43,880
다른 이미지를 가진 것은 그 반대의 경우에도 이미지와 관련이 없습니다. 그리고 우리는 보여줄 것입니다

768
01:24:43,880 --> 01:24:51,560
이 가정은 다소 시끄럽고 불완전합니다. 특히, 소프트맥스 손실을 사용하여

769
01:24:51,560 --> 01:24:58,520
여기서 이 목표를 공식화하면 이미지 모델 F와 텍스트 모델 G가 최소화되도록 훈련됩니다.

770
01:24:58,600 --> 01:25:05,640
이 방정식에서와 같이 다음 목표를 달성합니다. 여기서 X와 Y는 텍스트에 대한 정규화된 임베딩이고

771
01:25:05,640 --> 01:25:13,960
각각 이미지. 그리고 우리는 소프트맥스 손실의 비대칭성으로 인해 정규화가

772
01:25:13,960 --> 01:25:21,800
이미지와 텍스트에 걸쳐 독립적으로 두 번 수행됩니다. 그것이 바로 의 분모이다.

773
01:25:21,800 --> 01:25:28,920
소프트맥스 함수. 그리고 이 모델에는 가중치 하이퍼파라미터인 스칼라 T도 있습니다.

774
01:25:33,160 --> 01:25:38,840
대조 훈련에 사용되는 전역 배치 데이터의 정규화는 과도한 결과를 초래합니다.

775
01:25:38,840 --> 01:25:45,160
통신 오버헤드 및 수동 사용. 그리고 대조 훈련은 일반적으로 데이터를 활용합니다.

776
01:25:45,800 --> 01:25:51,640
병렬성과 데이터가 여러 장치에 분할될 때 손실을 계산하려면 항상 다음이 필요합니다.

777
01:25:51,640 --> 01:25:57,800
값비싼 모든 캐들러로 모든 임베딩을 수집합니다. 그리고 무엇보다 중요한 것은 구체화이다.

778
01:25:57,800 --> 01:26:06,600
쌍별 유사성의 메모리 집약적 배치 크기 수준 행렬. 그래서 소프트맥스 대신

779
01:26:06,600 --> 01:26:12,200
기본 대비 손실을 계산할 필요가 없는 더 간단한 대안을 제안합니다.

780
01:26:12,200 --> 01:26:21,320
전역 정규화 요인. 여기서는 세그먼트 손실을 조사하겠습니다.

781
01:26:21,320 --> 01:26:28,840
자세히. 예를 들어, 시그모이드 손실은 특히 메모리 효율적인 방식에 적합합니다.

782
01:26:28,840 --> 01:26:35,000
이러한 문제를 모두 완화하는 빠르고 수치적으로 안정적인 구현입니다. 그리고 시그모이드

783
01:26:35,000 --> 01:26:41,000
기본 손실은 모든 이미지 텍스트 쌍을 독립적으로 처리하여 학습을 효과적으로 전환합니다.

784
01:26:41,000 --> 01:26:46,520
모든 쌍 조합의 데이터 세트에 대한 표준 이진 분류 문제

785
01:26:47,240 --> 01:26:51,960
일치하는 쌍에는 양수 레이블을, 다른 모든 쌍에는 음수 레이블을 사용합니다.

786
01:26:52,680 --> 01:27:01,560
그리고 이것은 여기에 다음과 같이 정의되어 있습니다. 여기서 Z는 실제로 주어진 이미지와 텍스트 입력에 대한 레이블입니다.

787
01:27:01,560 --> 01:27:07,320
그녀는 쌍을 이루는 경우 하나를 호출하고 그렇지 않으면 음수와 동일합니다.

788
01:27:11,480 --> 01:27:15,400
여기서는 손실 함수가 어떻게 변하는지 보여주기 위해 그림을 그립니다.

789
01:27:15,400 --> 01:27:21,640
단일 쌍의 예를 들어 보겠습니다. 양수 쌍인 경우 손실 값이 작아집니다.

790
01:27:21,640 --> 01:27:28,040
쌍의 코사인 유사성은 1이 됩니다. 그리고 음수 쌍인 경우 손실 값은

791
01:27:28,040 --> 01:27:34,760
쌍의 코사인 유사성이 0이 될수록 작아집니다. 그러면 모델이 최적화될 것입니다.

792
01:27:34,760 --> 01:27:40,360
임베딩을 푸시하는 동안 일치하는 쌍의 임베딩이 서로 정렬되도록 장려합니다.

793
01:27:40,360 --> 01:27:47,880
타의 추종을 불허하는 쌍이 떨어져 있습니다. 이제 효율적인 손실 구현에 대해 이야기하겠습니다.

794
01:27:47,880 --> 01:27:54,760
이 문서에서는 세 개의 장치와 한 개의 장치만으로 이 예를 통해 설명할 수 있습니다.

795
01:27:54,760 --> 01:28:03,640
글로벌 배치 크기는 12입니다. 실제로는 없음 또는 수집 및 어느 시점에서든 밝은

796
01:28:03,720 --> 01:28:11,000
메모리에는 4 x 4 크기의 노란색 사각형이 구현됩니다. 이제 무대가 보이네

797
01:28:11,000 --> 01:28:17,000
첫 번째, 단계 a에서는 처음에 각 장치가 4개의 이미지와 4개의 텍스트 표현을 보유합니다.

798
01:28:17,880 --> 01:28:22,920
그리고 각 장치는 전체 손실을 계산하기 위해 다른 장치의 표현을 확인해야 합니다.

799
01:28:22,920 --> 01:28:30,760
마지막으로. 그리고 우리는 예를 들어 I1의 경우 장치 내에서 다음을 수행해야 한다는 것을 알아야 합니다.

800
01:28:30,760 --> 01:28:39,480
마지막으로 I1과 T1 및 T12 사이의 손실을 계산합니다. 그리고 B단계로 갈 수 있습니다. B단계에서는

801
01:28:40,120 --> 01:28:47,320
그들은 각각 자신의 표현에 대한 손실 구성 요소를 계산합니다. 여기에는 긍정적인 내용이 포함됩니다.

802
01:28:47,320 --> 01:28:55,480
이는 각 장치가 로컬에서 손실을 계산한다는 의미입니다. 예를 들어, 이 그림에서는

803
01:28:56,200 --> 01:29:06,680
이제 I1, I4 및 T1에서 T4 사이의 손실을 계산할 수 있습니다. 이제 C단계로 넘어갈 수 있습니다.

804
01:29:08,280 --> 01:29:16,360
실제로 C 단계에서는 텍스트가 여러 장치에 걸쳐 스윕될 수 있습니다. 따라서 장치 1에는 이제 I1부터 4까지가 있습니다.

805
01:29:17,640 --> 01:29:23,640
그리고 T5 ~ 8 등. 그리고 새로운 손실은 이전 손실과 함께 계산되고 누적됩니다.

806
01:29:23,960 --> 01:29:31,960
예를 들어 이제 C 단계에서 이 그림을 살펴보겠습니다. 왜냐하면 장치 1에는 이제 T5 ~ T8이 있기 때문입니다.

807
01:29:31,960 --> 01:29:41,640
I1~4와 T5~8 사이의 손실을 계산할 수 있습니다. 이제 손실 함수만 남았습니다.

808
01:29:41,640 --> 01:29:51,800
I1에서 4와 T9에서 12 사이입니다. 따라서 D단계에서는 모든 이미지와 텍스트 쌍이 나타날 때까지 반복됩니다.

809
01:29:52,520 --> 01:30:02,040
상호작용을 했습니다. 예를 들어, 장치 1에는 I1에서 4, T1에서 12의 손실이 있습니다. 그리고 우리는 다음을 수행할 수 있습니다.

810
01:30:02,040 --> 01:30:08,840
최종 교차 장치 합계는 모든 것을 하나로 모읍니다. 예를 들어, D 단계에서는 마지막으로 다음을 계산할 수 있습니다.

811
01:30:08,840 --> 01:30:18,600
I1에서 4까지와 T9에서 12 사이의 손실. 이제 모든 것이 지워졌습니다. 그리고 이제 우리는 마침내

812
01:30:19,240 --> 01:30:27,560
이 모든 합계를 손실 함수에 대한 단일 스칼라로 변환하기 위해 최종 교차 장치 합계를 수행합니다.

813
01:30:32,200 --> 01:30:39,480
이제 실험을 살펴보겠습니다. 이 논문에서는 조명을 기반으로 모델 시그릿을 훈련합니다.

814
01:30:39,480 --> 01:30:45,880
시그미 손실이 있는 모델. 그리고 시그미 손실은 모두 이 소프트맥스 손실을 다음과 같이 크게 수행합니다.

815
01:30:45,880 --> 01:30:53,800
배치 크기가 작으며 더 큰 배치 크기에서도 유사한 성능을 발휘합니다. 특히 그들은 성공적으로

816
01:30:53,800 --> 01:30:59,480
배치 크기가 최대 100만 개인 시그릿 모델을 훈련했는데, 이는 시그미 손실이 다음과 같다는 것을 보여줍니다.

817
01:30:59,480 --> 01:31:06,920
메모리 절약의 이점. 그리고 실제로 TVU V4 칩 4개만으로 시그릿을 훈련합니다.

818
01:31:06,920 --> 01:31:15,080
이틀 만에 좋은 이미지 넷 제로 샷 정확도를 달성하는 모델입니다. 그리고 그들은 또한 모델을 훈련시킵니다.

819
01:31:15,080 --> 01:31:23,160
시그미 손실이 있는 클립 모델을 기반으로 한 시그릿. 그리고 시그미 손실과 소프트맥스 손실은 모두

820
01:31:23,160 --> 01:31:29,160
합리적인 배치 크기, 시그미 손실의 정점이 더 일찍 나타나고 약간 더 나은 성능을 발휘합니다.

821
01:31:29,160 --> 01:31:36,680
소프트맥스 손실의 정점. 매우 큰 배치 크기는 두 가지 손실 모두에 해를 끼치며, 이를 통해 확인할 수 있습니다.

822
01:31:36,680 --> 01:31:46,920
수치. 이제 라벨 노이즈 로비누스(Labinus) 실험에 대해 이야기하겠습니다. 실제로,

823
01:31:46,920 --> 01:31:53,960
이전 연구에서는 분류를 위해 시그미 손실을 사용할 때 라벨 노이즈에 대한 개선된 로비누스가 입증되었습니다.

824
01:31:53,960 --> 01:32:01,240
모델. 그리고 이 속성은 여기에서 시끄러운 것으로 유명한 상황에서 특히 유용할 것입니다.

825
01:32:01,240 --> 01:32:08,520
널리 사용되는 대규모 이미지 텍스트 데이터세트의 성격. 여기서 우리는 시그미 훈련을 볼 수 있습니다

826
01:32:08,520 --> 01:32:17,160
로비누스를 데이터 노이즈로 증가시키고 제목은 적용된 손상 유형과 x축을 보여줍니다.

827
01:32:17,160 --> 01:32:24,360
적용될 확률을 보여줍니다. 부패 심각도가 높아짐에 따라 모델은

828
01:32:24,360 --> 01:32:30,600
대규모 데이터 세트에 대해 시그미 손실로 훈련되어 해당 데이터 세트에 비해 매우 좋은 성능을 달성했습니다.

829
01:32:30,600 --> 01:32:40,840
소프트맥스 기준. 마지막으로, 우리는 이 논문에서 siglip에 대해 몇 가지 결론을 내릴 수 있습니다.

830
01:32:41,480 --> 01:32:48,280
시그미 손실을 사용하는 사례를 묘사하는 두 가지 언어 이미지에 대한 연구를 수행했습니다.

831
01:32:48,280 --> 01:32:53,960
siglit과 siglip의 결과는 sigmy loss가 sigmy loss보다 성능이 더 우수하다는 것을 보여줍니다.

832
01:32:53,960 --> 01:33:00,440
특히 소규모 교육 배치 크기의 경우 소프트맥스 기준선입니다. 그리고 이 손실 함수는 또한

833
01:33:00,440 --> 01:33:08,440
추가 리소스 없이 더 큰 훈련된 배치 크기를 허용하는 메모리 효율성이 더 높습니다.

834
01:33:08,440 --> 01:33:17,160
컴퓨터 리소스나 타이머 리소스와 같습니다. 드디어 오늘 보여드리는 마지막 논문입니다

835
01:33:17,160 --> 01:33:25,000
그리고 오늘 논문 발표 부분도 잘 마무리했습니다. 이번 논문발표부분에서는

836
01:33:25,000 --> 01:33:31,400
먼저 VL 작업의 성능을 향상하여 VL 작업의 성능을 향상시키는 방법을 소개합니다.

837
01:33:31,400 --> 01:33:37,320
물체 감지. 그런 다음 더 큰 모델을 설계하고 더 큰 모델을 적용하는 방법에 대해 이야기했습니다.

838
01:33:37,320 --> 01:33:45,640
대규모로 VL 작업의 성능을 향상시키기 위한 데이터 양. 나중에 더 소개하자면

839
01:33:45,640 --> 01:33:52,280
세대 관점에서 더 많은 VL 작업에 적응하기 위한 디코더 아키텍처를 갖춘 복잡한 모델입니다.

840
01:33:53,240 --> 01:33:57,960
그리고 마지막으로 Softmax를 sigmy 함수로 대체하는 방법을 소개합니다.

841
01:33:57,960 --> 01:34:03,480
빠른 훈련과 대규모 배치 크기 가능, 앞으로 직면할 수 있는 작업에 대비하기만 하면 됩니다.

842
01:34:04,120 --> 01:34:11,240
더 큰 데이터 볼륨과 더 많은 모델 매개변수. 이제 오늘의 전체 요약을 살펴보겠습니다.

843
01:34:11,240 --> 01:34:22,040
프레젠테이션. 오늘 우리가 다룬 내용을 요약하자면, 네 가지 손실을 다루었습니다.

844
01:34:22,760 --> 01:34:30,200
첫 번째는 다양한 변형을 갖는 대조 손실입니다. 가장 일반적인 것은

845
01:34:30,200 --> 01:34:39,400
이미지 텍스트 대비 손실, 줄여서 ITC입니다. 이미지를 텍스트로, 텍스트를 이미지 정보로 합친 것입니다.

846
01:34:39,400 --> 01:34:48,360
샘플링된 배치의 다른 텍스트와 쌍을 이루는 텍스트를 대조하기 위한 CE 손실. ITC 손실을 사용하는 모델

847
01:34:48,360 --> 01:34:58,120
클립과 글이 포함되어 있습니다. 그리고 시그모이드 손실(sigmoid loss)도 보았습니다. 이는 시그립(sig lip)에 사용됩니다.

848
01:34:59,080 --> 01:35:05,320
모든 쌍 조합의 이진 분류로 사용됩니다. 그리고 우리도 봤어요

849
01:35:06,440 --> 01:35:14,040
텍스트 이미지와 태그 삼중항에 원본 태그가 포함되어 있는지 예측하는 이진 분류

850
01:35:14,040 --> 01:35:23,560
또는 오염된 태그. 예를 들어 oscar에서 사용됩니다. 하지만 제 생각에는 이것이 그 이상이라고 생각합니다.

851
01:35:23,560 --> 01:35:31,720
오스카는 이를 손실의 대비라고 부르는데, 내 생각에는 이것이 손실의 이미지 텍스트 일치와 더 유사하다고 생각합니다.

852
01:35:33,640 --> 01:35:40,920
따라서 이미지 텍스트 일치 손실은 이미지 텍스트 쌍이 일치하는지 여부를 예측하는 이진 분류입니다.

853
01:35:40,920 --> 01:35:49,240
일치하거나 일치합니다. 이것은 블립에서 사용됩니다. 그리고 우리는 언어 모델링 손실도 보았습니다.

854
01:35:50,120 --> 01:35:58,920
이는 채팅 GPT에서 사용된 것과 동일한 손실입니다. 따라서 이것은 텍스트를 생성하는 생성 작업입니다.

855
01:35:58,920 --> 01:36:06,120
이미지가 주어지면 자동 회귀 방식으로 설명합니다. 다시 말하지만, blip은 이 손실을 사용합니다.

856
01:36:07,080 --> 01:36:11,400
그리고 마스크 언어 모델링 손실도 다루었습니다.

857
01:36:12,840 --> 01:36:19,160
이 손실은 주변 텍스트 토큰과 이미지 특징을 기반으로 텍스트 토큰을 예측합니다.

858
01:36:20,120 --> 01:36:22,840
예를 들어, oscar와 minville은 모두 이 손실을 사용합니다.

859
01:36:25,400 --> 01:36:34,120
그런 다음 아키텍처 관점에서 모델을 살펴보겠습니다. 첫 번째 가족

860
01:36:34,120 --> 01:36:41,800
우리는 유니 인코더 기반의 다중 모델을 가지고 있습니다. 첫 번째는 오스카입니다.

861
01:36:42,680 --> 01:36:50,440
Oscar는 BERT에 일련의 태그, 태그 및 이미지 영역 삽입 비용을 지불합니다. 그런 다음

862
01:36:50,440 --> 01:37:00,760
객체 태그를 사용하여 태그와 이미지 간의 의미 정렬. ITC 손실과 ML을 사용합니다.

863
01:37:01,720 --> 01:37:11,160
손실. 민빌도 다루었습니다. 더욱 강력한 객체 감지 모델로 오스카를 개선합니다.

864
01:37:11,160 --> 01:37:19,480
또한 3방향 대비 손실을 도입한 다음 오스카와 동일한 MLM 손실을 사용합니다.

865
01:37:20,840 --> 01:37:27,000
그래서 하단의 표는 두 사람의 평가를 보여줍니다.

866
01:37:27,880 --> 01:37:38,600
다양한 작업에 대한 uni 인코더 기반 모델. 파란색 벤치마크는 이해도입니다.

867
01:37:38,600 --> 01:37:45,720
작업이고 주황색 벤치마크는 생성 작업입니다. 그리고 우리는 민빌이 밖을 볼 수 있습니다

868
01:37:45,720 --> 01:37:55,800
대부분의 작업에서 오스카를 수행합니다. 더 강력한 객체 감지 기능을 사용하기 때문에 놀랄 것도 없습니다.

869
01:37:55,800 --> 01:37:59,960
모델과 두 개는 더 많은 이미지 텍스트 쌍을 사용합니다.

870
01:38:04,200 --> 01:38:11,880
그리고 다음에는 듀얼 인코더 기반 모델도 다루었습니다. 여기에는 클립, 정렬 및 시그립이 포함됩니다.

871
01:38:13,560 --> 01:38:21,400
그래서 Clip은 자유 형식 텍스트를 인코딩하기 위해 학습 가능한 텍스트 인코더를 도입했습니다. ITC 손실을 사용합니다.

872
01:38:22,280 --> 01:38:29,480
그런 다음 클립을 기준으로 정렬하면 수량을 얻기 위해 품질이 희생됩니다. 그래서 규모를 키웠습니다.

873
01:38:30,360 --> 01:38:38,360
총량은 18억 개로 늘어났습니다. 학습 정렬을 위해 데이터 세트를 다국어 설정으로 확장합니다.

874
01:38:39,800 --> 01:38:49,640
다국어. 그런 다음 siglip은 소프트맥스 기반 ITC 손실을 시그모이드 기반 손실로 변경했습니다.

875
01:38:50,600 --> 01:38:58,600
이러한 손실의 장점은 메모리 효율성이 더 높고 속도가 더 빠르며 성능이 향상된다는 것입니다.

876
01:38:58,600 --> 01:39:05,560
수치적으로 안정적인 구현. 그래서 아래 표는 이 세 가지 모델에 대한 평가를 보여줍니다.

877
01:39:06,840 --> 01:39:18,440
보시다시피 siglip out은 다른 두 가지를 수행하며 훨씬 더 큰 데이터 세트를 사용합니다.

878
01:39:18,440 --> 01:39:27,880
400억이면 미친짓이네요. 그런 다음 인코더 디코더 기반 제품군도 다루었습니다.

879
01:39:29,160 --> 01:39:37,240
여기에는 블립과 코카가 포함됩니다. 그래서 blip은 자연어 생성 기능을 추가했습니다.

880
01:39:37,240 --> 01:39:50,360
이전 모델. ITCLM 및 ITM 손실을 사용합니다. 따라서 Blip의 주요 동기는 품질입니다.

881
01:39:50,360 --> 01:40:00,920
또한 중요합니다. 텍스트를 부트스트래핑하여 텍스트 품질을 개선해야 합니다. 그리고 코카의 경우에는

882
01:40:00,920 --> 01:40:07,240
미니멀한 디자인의 블립. 변환 블록을 통한 정방향 패스 수를 줄입니다.

883
01:40:07,960 --> 01:40:16,920
그리고 코카는 손실을 ITC와 LM 손실로 줄입니다. 그리고 코카는 훨씬 더 큰 규모로 훈련됩니다.

884
01:40:16,920 --> 01:40:25,160
블립보다 코퍼스. 그래서 아래 표에서는 이 두 모델을 비교합니다. 보시다시피 코카는 많이 사용됩니다

885
01:40:25,160 --> 01:40:36,280
블립보다 더 큰 말뭉치. 하지만 내 말은 6가지 작업 중 4가지 작업이 모두 수행된다는 의미입니다.

886
01:40:39,640 --> 01:40:48,120
그래, 그게 다야. 들어주신 모든 분들께 감사드립니다.