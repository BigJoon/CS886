1
00:00:00,000 --> 00:00:07,600
안녕하세요 여러분, 제 이름은 Albert입니다. 효율적인 변압기에 대한 강의를 진행하겠습니다.

2
00:00:07,600 --> 00:00:11,840
CS86용.

3
00:00:11,840 --> 00:00:13,760
오늘은 무엇을 다룰까요?

4
00:00:13,760 --> 00:00:19,360
트랜스포머의 어텐션 메커니즘을 간략하게 검토한 다음 설명하겠습니다.

5
00:00:19,360 --> 00:00:24,680
기존 변압기의 시간 및 공간 복잡성과 이것이 문제가 되는 이유.

6
00:00:24,680 --> 00:00:31,560
나는 희소 변압기를 포함하여 과거에 제안된 몇 가지 솔루션을 마침내 제시했습니다.

7
00:00:31,560 --> 00:00:39,000
린 포머, 선형화된 장력, 롱 포머, 공명 특징 장력 등이 있습니다.

8
00:00:39,000 --> 00:00:44,520
물론 이것이 전부는 아니지만, 시간과 제약으로 인해 이것만 하겠습니다.

9
00:00:44,520 --> 00:00:45,520
오늘은 취재해.

10
00:00:45,520 --> 00:00:50,640
마지막으로 모든 효율적인 겟업의 성능을 비교하는 겟업 프로젝트를 제시하겠습니다.

11
00:00:50,640 --> 00:00:53,320
변압기.

12
00:00:53,320 --> 00:00:57,040
그럼 주의 메커니즘을 간단히 살펴보겠습니다.

13
00:00:57,040 --> 00:01:04,600
왼쪽에는 일련의 값을 정의하고 시퀀스 길이로 시작하며 Lh는 숫자입니다.

14
00:01:04,600 --> 00:01:12,440
쌍(보통 8개), 임베딩 크기가 이미 512인 일부 모델이 있습니다.

15
00:01:12,440 --> 00:01:18,840
디모델링을 위해 호출된 k를 h로 나눈 값은 64입니다.

16
00:01:18,840 --> 00:01:27,920
따라서 입력 시퀀스는 차원 n 배 디모델인 x에 포함됩니다.

17
00:01:27,920 --> 00:01:34,040
입력은 8개의 개별 헤드를 통해 전달되며, 각 헤드에는 x가 곱할 약점이 포함되어 있습니다.

18
00:01:34,040 --> 00:01:44,120
에 의해, 치수는 변형 시간 붕괴이며, 우리가 얻을 수 있도록 얻습니다.

19
00:01:44,120 --> 00:01:48,400
정확한 키와 값.

20
00:01:48,400 --> 00:01:54,560
그런 다음 q x k의 소프트맥스를 전치된 값으로 나누어 어텐션을 계산합니다.

21
00:01:54,560 --> 00:02:01,920
붕괴 시간의 제곱근, 종료 시간을 iv로, 마지막으로 각 결과

22
00:02:01,920 --> 00:02:09,240
헤드에 최종 너비 행렬을 곱하여 출력을 얻습니다.

23
00:02:09,240 --> 00:02:15,760
그러면 복잡성이 어디서 오는지 살펴보고 주의 기능을 기억해 봅시다.

24
00:02:15,760 --> 00:02:23,800
x, 어쩌고 저쩌고, 저쩌고를 부드럽게 하기 위해 주의 q, k, v가 호출됩니다. 여기서 차원은

25
00:02:23,800 --> 00:02:30,680
q의 차원을 k의 차원이라고 하며, 이는 n번 붕괴라고 부르므로 q의 계산은 다음과 같습니다.

26
00:02:30,680 --> 00:02:40,360
따라서 k를 곱하면 n제곱 곱하기 붕괴가 발생합니다.

27
00:02:40,360 --> 00:02:53,640
공간복잡도에 관해서는 이 부분인 Attention Matrix를 저장하면

28
00:02:53,640 --> 00:03:03,360
각 헤드에 대해 모든 n 제곱 공간 복잡도가 발생하므로 전체 공간 복잡도는

29
00:03:03,360 --> 00:03:11,280
모두 n 제곱 h가 될 것입니다. 따라서 매우 긴 수열이 있으면 훨씬 더 길어집니다.

30
00:03:11,280 --> 00:03:16,960
붕괴 및 하위 h보다 훨씬 긴 경우 시간 및 공간 비용은 2차가 됩니다.

31
00:03:16,960 --> 00:03:22,360
시퀀스 길이가 매우 길면 매우 끔찍할 것입니다.

32
00:03:22,360 --> 00:03:27,160
변압기의 확장성에 나쁜 영향을 미치고 실제로 그 확장성을 증가시킵니다.

33
00:03:27,160 --> 00:03:32,960
자원 소비, 이는 또한 다음과 같은 모든 종류의 상황에 대한 채택에 영향을 미칠 것입니다.

34
00:03:32,960 --> 00:03:39,960
이미지 완성 및 오디오 인식 등.

35
00:03:39,960 --> 00:03:45,520
따라서 이러한 상황에 대응하기 위해 많은 사람들이 해결책을 찾았습니다. 여기에는 여러 가지가 나열되어 있습니다.

36
00:03:45,520 --> 00:03:48,400
그들과 그들의 향상된 비용.

37
00:03:48,400 --> 00:03:58,160
첫 번째는 2019년 4월 Childed L이 제안한 스마트 트랜스포머로

38
00:03:58,160 --> 00:04:06,480
n 곱하기 공간 비용의 n 곱하기 p제곱근, 여기서 p는 n과 독립된 하이퍼파라미터입니다.

39
00:04:06,480 --> 00:04:11,680
그들의 작업은 pth도 가능하다는 예를 제시했지만 더 높은 차원이 있다고 주장했습니다.

40
00:04:11,680 --> 00:04:13,000
가능하기도 했습니다.

41
00:04:13,920 --> 00:04:20,120
다음으로 2020년 6월 Wang Edel이 제안한 린 전자가 있으며 이제 긴장감을 높였습니다.

42
00:04:20,120 --> 00:04:31,240
2020년 8월 Kassarou Polos Edel 작성, 2020년 12월 Delta G에서 오래전 제작, 무작위

43
00:04:31,240 --> 00:04:41,280
2021년 3월 Peng Edel의 장편 RFA입니다.

44
00:04:41,280 --> 00:04:49,360
네 가지 솔루션 모두 다양한 방식으로 선형적인 시간 및 공간 비용을 달성했으며 일부는

45
00:04:49,360 --> 00:04:53,960
나중에 다루게 될 일정한 공간 비용도 있습니다.

46
00:04:53,960 --> 00:05:04,800
그럼 먼저 스마트 트랜스포머에 대해 하나씩 이야기해보겠습니다.

47
00:05:04,800 --> 00:05:10,840
스마트 변압기는 다음 이미지로 설명할 수 있습니다. 이는 학습된 주의 사항입니다.

48
00:05:10,840 --> 00:05:21,960
완전한 주의를 기울여 C4-10 재설정 열차의 1228 레이어 네트워크에서 패턴을 나누고,

49
00:05:21,960 --> 00:05:29,880
그러한 영역은 주어진 픽셀을 생성하는 동안 머리의 주의 기반을 강조합니다.

50
00:05:29,880 --> 00:05:36,880
검은색은 마스크된 픽셀과 해당 영역과 같은 극도로 회귀적인 작업을 나타냅니다.

51
00:05:37,840 --> 00:05:42,720
여기서 레이어가 다양한 전문 기술을 학습할 수 있다는 점이 흥미롭습니다.

52
00:05:42,720 --> 00:05:51,040
희소 패턴, 예를 들어 우리가 발견한 초기 레이어의 이미지 세트 A에서

53
00:05:51,040 --> 00:06:01,280
예를 들어 이 픽셀은 대상 픽셀을 둘러싸고 있는 검은색 영역입니다.

54
00:06:01,280 --> 00:06:09,520
컨볼루션과 레이어 1920의 이미지 세트 B에서 행과 열에 주의가 있음을 알 수 있습니다.

55
00:06:09,520 --> 00:06:19,280
이런 것들. C와 같은 추가 계층에서는 데이터 종속 액세스 패턴을 확인할 수 있습니다.

56
00:06:20,560 --> 00:06:25,840
예를 들어 여기의 흰색 부분이 새를 둘러싸고 있습니다.

57
00:06:26,640 --> 00:06:39,360
레이어 64에서 128까지의 최종 세트에서 흰색은 여기에서 볼 수 있듯이 몇 가지 특정 위치에만 나타납니다.

58
00:06:41,760 --> 00:06:52,480
가끔은 그 사람이 여기에 있는지 확실하지 않아서 완전히 주의를 기울일 필요는 없을 수도 있습니다.

59
00:06:52,480 --> 00:06:59,520
주의를 기울이는 것만으로도 충분합니다. 여기서는 보다 수학적 방법으로 동기를 설명합니다.

60
00:07:00,480 --> 00:07:06,000
어텐션 함수인 소프트맥스 함수를 떠올려 보세요. 여기서 보고 있는 것은 단지

61
00:07:06,000 --> 00:07:12,320
토큰 형식의 토큰 또는 출력 토큰이 토큰에만 참석할 수 있는 기능

62
00:07:12,320 --> 00:07:19,120
그 전에. 이는 기본적으로 모든 입력이 포함된 변환기의 디코더 부분입니다.

63
00:07:19,120 --> 00:07:29,600
KSI에서 볼 수 있듯이 대상 출력 위치가 마스크된 후에 J는 도달할 수만 있습니다.

64
00:07:30,560 --> 00:07:41,520
SI에 지정된 인덱스입니다. 따라서 전체 주의 메커니즘에서 SI는 J와 같습니다.

65
00:07:41,520 --> 00:07:47,920
여기서 J는 I보다 작거나 같습니다. 이는 예를 들어 여기서 출력 토큰이 참석할 수만 있음을 의미합니다.

66
00:07:47,920 --> 00:07:58,320
이전의 모든 입력 벡터에 적용됩니다. 그러나 우리가 이전에 본 바에 따르면 이것이 아닐 수도 있습니다.

67
00:07:58,320 --> 00:08:05,760
필요하므로 주의를 집중하기 위해 주의를 분리하고 효율적인 방법을 찾는 것이 아이디어입니다.

68
00:08:05,760 --> 00:08:15,360
A의 크기가 N의 p제곱근에 비례하도록 이 SI의 하위 집합입니다.

69
00:08:19,840 --> 00:08:27,440
여기에서 저자는 p가 2일 때 두 가지 해법을 제안합니다. 왼쪽에는 전체 주의가 있습니다.

70
00:08:27,760 --> 00:08:38,400
중간에 하이퍼파라미터 L을 선택하고 이 경우 4를 선택하며 출력 토큰은

71
00:08:38,400 --> 00:08:45,680
그들 앞에 있는 세 개의 토큰에 주의하세요. 예를 들어 이전에 세 가지에 참석한 사람이 있습니다.

72
00:08:45,680 --> 00:08:59,440
그들과 네 번째, 그들 앞에는 A가 있습니다. 이런 이유, 이런 이유

73
00:08:59,440 --> 00:09:08,560
연한 파란색은 이 토큰이 이 토큰에 연결되어 있지 않기 때문에 완전한 연결을 유지하는 것입니다.

74
00:09:09,520 --> 00:09:17,600
아니면 이것. 하지만 이 토큰 이후 하늘색을 통해 이 토큰들은 다음과 연결됩니다.

75
00:09:19,120 --> 00:09:24,320
그 앞의 세 개, 이 토큰이 연결되어 있으면

76
00:09:27,920 --> 00:09:29,120
우리는 완전한 연결성을 갖고 있습니다.

77
00:09:29,280 --> 00:09:37,520
그래서 이것은 그 모양 때문에 스트라이드 스파스 변환기라고 불립니다.

78
00:09:40,880 --> 00:09:46,160
그래서 오른쪽에는 고정 희소 변환기(Fixed Sparse Transformer)라는 또 다른 솔루션이 제안되었다고 생각합니다.

79
00:09:46,960 --> 00:09:50,960
이 예에서는 다섯 번째, 아홉 번째, 열세 번째 토큰입니다.

80
00:09:50,960 --> 00:09:59,600
또한 17번째 토큰은 이전 3개의 토큰에 참석합니다.

81
00:10:01,920 --> 00:10:06,400
네 번째, 여덟 번째 및 이전 두 개에 참석하는 상위 토큰까지

82
00:10:08,320 --> 00:10:16,480
그리고 꼭대기도. 응, 그리고 두 개 등등. 보시다시피 진한 파란색 영역으로 표시됩니다.

83
00:10:17,440 --> 00:10:25,760
여기. 이제 완전한 연결을 유지하려면 4개의 토큰 세트가 마지막 토큰에 참석해야 합니다.

84
00:10:25,760 --> 00:10:33,920
이전 세트에서. 예를 들어 이것은 이것에 참석해야 하고 이것도 또한 해야 합니다.

85
00:10:33,920 --> 00:10:43,920
연한 파란색 영역으로 표시된 완전한 연결을 유지하려면 이 작업에 주의하십시오. 그래서 이것들은

86
00:10:43,920 --> 00:10:50,560
이는 P가 2일 때의 두 가지 예일 뿐입니다. 에 따라 높은 치수도 가능합니다.

87
00:10:50,560 --> 00:10:58,800
저자와 지금 여기의 계산은 이 경우 n의 제곱근 이상으로 축소됩니다.

88
00:10:59,440 --> 00:11:04,480
왜냐하면 이 영역의 크기는 n의 제곱근에 비례하기 때문입니다.

89
00:11:06,160 --> 00:11:12,080
그렇다면 공간 용량은 어떨까? 하지만 단순히 모든 주의 매트릭스를 기억하는 대신

90
00:11:12,720 --> 00:11:18,720
여기에서는 하위 네트워크의 주의에 대한 재계산을 수행하고 대신

91
00:11:18,720 --> 00:11:26,160
여기와 여기의 계산 결과만 회색 영역으로 표시됩니다.

92
00:11:28,160 --> 00:11:31,360
그래서 이것을 그래디언트 체크포인트 및 재계산이라고 합니다.

93
00:11:34,000 --> 00:11:41,600
따라서 공간 용량도 n에 선형인 모델보다 n배 감소합니다.

94
00:11:42,800 --> 00:11:44,960
n이 정말 커지면 좋습니다.

95
00:11:47,840 --> 00:11:52,640
그럼 이제 당시의 최첨단 모델과 비교한 성능을 살펴보겠습니다.

96
00:11:53,760 --> 00:12:00,400
따라서 10개의 새로운 세트를 훈련하고 저장하면 텍스트 데이터 세트와 순 평균을 얻습니다.

97
00:12:00,400 --> 00:12:07,360
60 x 4 x 64의 새로운 세트와 클래식 음악과 같은 5초 12kHz 클래식 음악입니다.

98
00:12:07,920 --> 00:12:15,040
따라서 여기서 제공하는 비트는 바이트 또는 픽셀을 생성하는 데 필요한 비트 수를 의미합니다.

99
00:12:16,160 --> 00:12:19,120
가장 낮은 숫자는 공연의 가치입니다.

100
00:12:20,720 --> 00:12:22,720
여기 희소 변환기를 볼 수 있습니다.

101
00:12:24,800 --> 00:12:29,360
단일 테스트에서 다른 모델보다 성능이 뛰어났습니다.

102
00:12:29,760 --> 00:12:41,760
여기에서 볼 수 있듯이 오른쪽에 표시된 것처럼 속도도 증가합니다.

103
00:12:46,240 --> 00:12:53,840
그리고 오른쪽 하단에는 최대 12,160개의 토큰까지 긴 연락처가 있는 경우에도 볼 수 있습니다.

104
00:12:54,560 --> 00:12:57,760
성능은 여전히 ​​좋다.

105
00:13:03,200 --> 00:13:05,760
다음으로 Neoformer에 대해 이야기하겠습니다.

106
00:13:09,680 --> 00:13:14,720
네오포머의 아이디어는 다음과 같습니다. 의도 함수를 기억한다면,

107
00:13:15,360 --> 00:13:21,280
Neoformer의 기본 아이디어는 기본적으로 키와 가치를 더 낮은 차원에 투영하는 것입니다.

108
00:13:21,280 --> 00:13:29,120
행렬 차원의 키 시간과 k가 n보다 훨씬 작은 dk를 곱합니다.

109
00:13:31,520 --> 00:13:38,000
이렇게 하면 계산 비용이 n 곱하기 k 모두로 감소됩니다.

110
00:13:39,200 --> 00:13:45,440
덧셈 행렬의 차원이 공간 복잡도의 n배 k가 되자마자

111
00:13:46,160 --> 00:13:50,960
또한 n배 k배 h로 감소됩니다.

112
00:13:52,800 --> 00:13:56,560
이제 그러한 예측이 존재하는지에 대한 질문이 생깁니다.

113
00:14:00,160 --> 00:14:03,680
여기에서 저자는 그들이 그렇다는 것을 보여주기 위해 정리를 제안했습니다.

114
00:14:05,040 --> 00:14:11,040
정리는 기본적으로 k의 특정 값에 대해 그러한 투영 E와 F가 존재한다는 것을 보여주었습니다.

115
00:14:11,920 --> 00:14:24,080
여기서 k는 n에 의존하지만 qi k의 순위는 다음과 같이 구성됩니다.

116
00:14:26,160 --> 00:14:32,960
dk를 제곱근으로 나눈 값은 qi의 차원이 다음과 같기 때문에 최대 dk입니다.

117
00:14:32,960 --> 00:14:35,600
n 곱하기 dk와 동일한 ki의 차원.

118
00:14:35,840 --> 00:14:41,360
따라서 이 정리는 그 사실과 결합되어 다음을 제공합니다.

119
00:14:43,680 --> 00:14:50,720
k는 dk의 로그를 엡실론 제곱 - 엡실론 세제곱으로 나눈 9배와 같습니다.

120
00:14:56,480 --> 00:15:03,440
이는 n과 무관하며 ei는 델타 곱하기 r과 같습니다. fi는 다음과 같습니다.

121
00:15:04,160 --> 00:15:14,320
e를 음의 델타 곱하기 r로 거듭제곱합니다. 여기서 r은 n x k 행렬입니다.

122
00:15:14,320 --> 00:15:24,560
평균이 0인 정규 분포의 id 항목과 독립된 항목

123
00:15:24,560 --> 00:15:30,880
k에 대한 1의 분산과 2에 대한 1의 n승과 같은 델타입니다.

124
00:15:31,840 --> 00:15:39,840
이 정리는 여전히 유효합니다. 따라서 이 정리의 증명은 여러 부등식 정리를 결합한 것입니다.

125
00:15:39,840 --> 00:15:45,440
등등 그리고 시간 제약으로 인해 증명에 대해 너무 자세히 다루지는 않겠습니다.

126
00:15:45,440 --> 00:15:54,000
우리가 변환할 수 있도록 구체적인 k 델타 ei와 fi를 찾을 수 있다는 것을 알아두세요

127
00:15:54,880 --> 00:16:02,720
이 엡실론 최대 함수는 ki와 vi가 더 낮은 차원으로 예측되는 왼쪽에 있습니다.

128
00:16:03,760 --> 00:16:10,960
이제 우리는 제가 평가하기 위해 수행한 정리의 몇 가지 예를 살펴보겠습니다.

129
00:16:10,960 --> 00:16:19,280
수식을 지원합니다. 여기서 tpl 값은 검증 복잡성을 의미하며 그보다 낮은 것은

130
00:16:19,280 --> 00:16:23,600
더 나은 성능을 예측할 때 모델의 불확실성이 낮아집니다.

131
00:16:25,520 --> 00:16:32,320
먼저, 고정된 시퀀스 길이와 다양한 k로 성능을 비교해 보겠습니다.

132
00:16:33,040 --> 00:16:39,920
여기 저기에서 린 전자의 검증 복잡성이 매우 가깝게 병합된 것을 볼 수 있습니다.

133
00:16:41,280 --> 00:16:48,080
표준 변압기에 연결하고 k가 클수록 성능이 더 좋아지는 것은 아닙니다.

134
00:16:48,080 --> 00:16:54,960
여기서 아주 약간 볼 수 있듯이 조금 확대하면 조금 더 좋아진 것을 알 수 있습니다.

135
00:16:59,680 --> 00:17:11,440
네, 다음은 enf를 시각화하는 세 가지 다른 방법을 비교해 보겠습니다. 먼저 다른 방법입니다.

136
00:17:11,920 --> 00:17:16,800
각 머리에 동일한 e와 f를 사용하는 경우

137
00:17:20,960 --> 00:17:27,600
하지만 레이어와 두 번째 사이에 변화가 있다는 것이 핵심 값입니다. 우리는 여전히 두 가지 모두에 대해 동일한 e를 사용합니다.

138
00:17:27,600 --> 00:17:34,560
레이어마다 달라지는 각 헤드의 키와 값과 세 번째 헤드는 단일 헤드를 사용하는 레이어 방식입니다.

139
00:17:34,560 --> 00:17:42,000
e 모든 레이어와 모든 헤드에 대해 이 세 가지 공유 방법의 성능은 그리 높지 않았습니다.

140
00:17:42,640 --> 00:17:49,840
하지만 레이어 상승 체인은 여기서 볼 수 있듯이 조금 더 안정적이었습니다.

141
00:17:52,240 --> 00:18:01,360
고정된 k와 변화하는 n의 경우 n이 클수록 복잡성은 감소하지만 결국에는

142
00:18:02,240 --> 00:18:04,480
비슷한 값으로 나옵니다.

143
00:18:08,080 --> 00:18:14,400
Robert A 기반 모델을 사용한 다른 모델과 비교하여 몇 가지 성능을 살펴보겠습니다.

144
00:18:14,400 --> 00:18:20,880
번트와 동일한 코퍼스에서 훈련된 기본 모델로서 4가지 다른 벤치마크에서 테스트되었습니다.

145
00:18:21,440 --> 00:18:30,720
벤치마크 자연어 이해 작업은 n이 다양하고 각 n에 대해 k가 다양하며 동일합니다.

146
00:18:30,720 --> 00:18:36,080
전략을 통해 여기와 여기에서 성과를 확인할 수 있습니다.

147
00:18:38,560 --> 00:18:45,680
기준선과 상당히 잘 비교되었으며 여기를 제외하고는 전반적으로 평균입니다.

148
00:18:46,560 --> 00:18:55,520
정보는 기본 라인보다 훨씬 더 나은 성능을 발휘했지만 그다지 좋지는 않지만 그렇습니다.

149
00:18:56,160 --> 00:18:59,360
약간의 개선이 있는 것 같습니다

150
00:19:03,600 --> 00:19:04,480
그리고 이것 때문에

151
00:19:07,600 --> 00:19:13,360
시간과 메모리 효율성 향상을 살펴보았으므로 왼쪽에서 왼쪽으로

152
00:19:13,360 --> 00:19:20,640
절약된 시간 오른쪽은 저장된 메모리입니다. 여기 기준선은 기본 변환기입니다.

153
00:19:21,280 --> 00:19:27,040
여기서 볼 수 있듯이 기본 변압기에 비해 크게 개선된 것을 볼 수 있습니다.

154
00:19:27,040 --> 00:19:31,520
시간과 공간이 늘어나면

155
00:19:34,480 --> 00:19:42,080
따라서 시퀀스가 ​​길어질수록 개선이 더 뚜렷해지고 프로젝트 차원이 높아집니다.

156
00:19:42,080 --> 00:19:51,280
k는 덜 분명한 개선입니다. 왜냐하면 차원이 커짐에 따라 더 많은 시간이 걸리기 때문입니다.

157
00:19:52,160 --> 00:19:59,920
n 곱하기 k와 같은 시간 의존성과 마찬가지로 k가 성장함에 따라 속도가 느려질 것입니다.

158
00:20:03,600 --> 00:20:06,240
다음으로 선형화에 대해 이야기하겠습니다.

159
00:20:07,200 --> 00:20:12,000
작성자: kathropoulos flow, 2020년 8월

160
00:20:14,960 --> 00:20:18,000
여기서 우리는 소프트맥스 함수의 정의를 떠올릴 수 있습니다

161
00:20:19,600 --> 00:20:27,520
그리고 주어진 시퀀스에서 x는 x의 소프트맥스의 i 위치인 xn까지 x1과 같습니다.

162
00:20:28,480 --> 00:20:35,920
e는 xi의 거듭제곱을 모든 xi의 지수의 합으로 나눈 값과 같습니다.

163
00:20:37,120 --> 00:20:44,640
그래서 내가 행렬의 i 행을 나타내도록 하면 y는 주의 메커니즘의 출력을 나타냅니다.

164
00:20:45,680 --> 00:20:49,120
i의 y 행은 다음과 같이 계산할 수 있습니다.

165
00:20:51,120 --> 00:20:56,240
y는 qi 곱하기 k의 소프트맥스를 dk 곱하기 v의 제곱근으로 나눈 것과 같습니다.

166
00:20:57,600 --> 00:21:01,040
Softmax 함수로 말하면 다음 부분이 나옵니다

167
00:21:01,920 --> 00:21:10,640
그리고 마지막으로 qi가 이와 같은 벡터라면 상단 부분은 다음과 같이 볼 수 있다고 추측합니다.

168
00:21:12,240 --> 00:21:19,680
그리고 k 아 죄송합니다. 이것은 여기서 전치여야 하고 k는 이렇습니다

169
00:21:20,640 --> 00:21:24,400
그러면 우리는

170
00:21:28,000 --> 00:21:34,000
이것은 q1입니다. 이것은 q2입니다. 첫 번째 행

171
00:21:34,960 --> 00:21:47,920
주의 매트릭스는 q1.k1 및 q1.k2가 되며 이는 v에 의해 한 번 수행됩니다.

172
00:21:50,800 --> 00:21:56,000
이것은 v1입니다. 이것은 v2입니다. 그러면 v1이 됩니다.

173
00:21:56,960 --> 00:21:59,760
타임스

174
00:22:02,080 --> 00:22:02,400
첫 번째

175
00:22:07,520 --> 00:22:16,000
v1 곱하기 q1.k1 더하기 v2 곱하기 q1.k2가 됩니다.

176
00:22:17,840 --> 00:22:23,920
전체 행에 대해 좋습니다. 그래서 이것이 우리가 온 곳입니다

177
00:22:23,920 --> 00:22:32,400
여기서 q와 k의 합은 q의 지수 곱하기 k를 dk의 제곱근으로 나눈 값과 같습니다.

178
00:22:42,000 --> 00:22:48,160
여기서 주의 함수 정의에 따른 유일한 제약은

179
00:22:48,240 --> 00:22:56,080
동일한 함수는 음수가 아니어야 하므로 여기서는 커널을 지정할 필요가 없습니다.

180
00:22:57,920 --> 00:23:02,000
따라서 특징 표시나 특징 표현만 지정하면 됩니다.

181
00:23:02,960 --> 00:23:10,560
따라서 커널 k의 특징 표현 phi는 x와 y의 k가 다음과 같도록 하는 함수입니다.

182
00:23:10,560 --> 00:23:13,360
는 x의 파이 곱하기 y의 파이와 같습니다

183
00:23:15,920 --> 00:23:20,080
이를 이전 방정식에 연결하면 다음이 제공됩니다.

184
00:23:22,720 --> 00:23:31,840
합계가 i에 영향을 주지 않기 때문에 여기에서 전치된 qi의 phi를 꺼낼 수 있습니다.

185
00:23:32,800 --> 00:23:44,000
이 합계는 매회마다 한 번씩 계산되도록 처리할 수 있으면 계산할 수 있습니다.

186
00:23:44,000 --> 00:23:51,600
나는 지금 한 번만 재사용했고 당신은 매 i마다 재사용했습니다. 순서를 바꿔야 합니다.

187
00:23:52,240 --> 00:23:59,520
네, 합계를 계산하는 비용은 선형적으로 n에 따라 달라지며 이를 계산합니다.

188
00:24:02,320 --> 00:24:12,080
상단 합계는 상수를 생성하고 하단의 하단 합계는 도트 프로젝트이므로 n의 비용 o

189
00:24:12,080 --> 00:24:23,040
그리고 y sub i에는 총 d개의 mod 행이 있으므로 결국 적당한 계산 비용은 n에 선형입니다.

190
00:24:26,880 --> 00:24:34,800
공간 비용도 n에 선형적으로 감소합니다. 왜냐하면 우리가 공간을 절약할 필요가 없기 때문입니다.

191
00:24:34,800 --> 00:24:43,520
부동 제한은 더 이상 친구가 저장해야 합니다.

192
00:24:46,320 --> 00:24:53,360
그리고 이 논문에서 phi x의 선택은 지수 선형 단위 활성화 함수였습니다.

193
00:24:53,360 --> 00:24:58,960
그래디언트를 0 및 xx 음수로 설정하지 않으려면 1을 더하세요.

194
00:25:04,400 --> 00:25:11,680
자동 회귀 특성에 사용되는 기술인 인과 마스킹에 대해 알아보겠습니다.

195
00:25:11,680 --> 00:25:17,600
앞서 언급한 세대는 기본적으로 대상 토큰 이후의 토큰을 마스크하여

196
00:25:17,600 --> 00:25:24,640
해당 토큰에 표시되기 전의 토큰 이 경우 방정식은 다음과 같습니다.

197
00:25:25,520 --> 00:25:28,000
우리는 단순히 n을 i로 대체합니다.

198
00:25:30,640 --> 00:25:39,200
상위 합을 si로, 하위 합을 zi로 대체하여 si를 다음과 같이 계산할 수 있습니다.

199
00:25:39,200 --> 00:25:48,640
si 빼기 1에 ki 곱하기 vi 전치의 phi ki phi를 더하면 나는 다음이 될 수 있습니다.

200
00:25:48,640 --> 00:25:56,480
합의 계산이 n에 선형이기 때문에 ki의 phi를 추가하여 zi에서 1을 뺀 값으로 계산됩니다.

201
00:25:56,480 --> 00:26:01,360
그리고 그것들을 n에 상수로 더하면 여전히 선형 시간 복잡도를 갖게 됩니다.

202
00:26:04,160 --> 00:26:06,000
하지만 공간 복잡도는 어떻습니까?

203
00:26:06,320 --> 00:26:14,880
여기서 역전파 동안 qk와 v에 대한 기울기는 다음과 같습니다.

204
00:26:15,600 --> 00:26:24,480
는 제곱의 o의 공간 복잡도를 제공하는 부동 행렬로부터 계산되어야 합니다.

205
00:26:24,480 --> 00:26:31,920
표준 변압기이지만 여기서는 시간 비용이 선형이 되도록 기울기도 도출합니다.

206
00:26:32,560 --> 00:26:35,040
공간 비용에 대한 공간 비용은 일정합니다.

207
00:26:36,960 --> 00:26:44,080
이전 방정식의 분자와 손실 함수의 기울기가 주어지면

208
00:26:48,160 --> 00:26:55,840
모호한 y bar of l 저자는 의 phi에 대한 기울기를 도출할 수 있었습니다.

209
00:26:56,640 --> 00:27:00,800
q phi phi q k와 v는 다음과 같다

210
00:27:03,200 --> 00:27:07,840
우리는 이러한 그래디언트가 모두 n 시간의 선형을 필요로 한다는 것을 알 수 있습니다

211
00:27:10,720 --> 00:27:15,600
여기 여기와 여기 표시된 것처럼 이러한 계산은

212
00:27:15,920 --> 00:27:25,600
여기에 합이 거기 있고 그만큼 많기 때문에 n에 선형인 것과 같습니다.

213
00:27:27,120 --> 00:27:31,520
이러한 작업은 n에 종속되지 않습니다.

214
00:27:33,280 --> 00:27:40,480
최종 값은 n에 의존하지 않기 때문에 이를 해결하려면 일정한 시간이 필요합니다.

215
00:27:41,360 --> 00:27:50,720
총 시간과 완료 비용은 다음과 결합되면 전반적으로 모두 선형이 됩니다.

216
00:27:51,440 --> 00:27:57,520
이전 단계에서는 그라디언트 도출에 관한 논의를 생략하겠습니다.

217
00:27:58,320 --> 00:28:04,480
시간과 공간은 제한되어 있지만 관심이 있으시면 신문에 가서 읽어보실 수 있습니다.

218
00:28:05,360 --> 00:28:09,360
그래서

219
00:28:12,000 --> 00:28:19,120
위에 모인 경우 변압기를 다음과 같이 공식화합니다.

220
00:28:19,120 --> 00:28:25,760
순환 신경망과 내가 인용하는 것은 더 나은 이해를 향한 첫 번째 단계입니다.

221
00:28:26,400 --> 00:28:32,240
변환기와 대중적인 순환 네트워크 및 프로세스 간의 관계

222
00:28:32,960 --> 00:28:40,320
rnn이 의미하는 이유와 같이 작성자가 지정하지 않은 정보를 저장하고 검색하는 데 사용됩니다.

223
00:28:40,320 --> 00:28:46,320
선형 시공간 복잡도 다른 방식으로 변환기를 공식화하려는 목표

224
00:28:46,320 --> 00:28:53,840
n은 변환기와 rnn이 서로 밀접하게 관련될 수 있다는 아이디어를 강조하기 위한 것입니다.

225
00:28:54,320 --> 00:29:06,640
여기 s와 z는 숨겨진 상태입니다. xi는 쉬운 i의 출력이고 yi는 i 테마입니다.

226
00:29:07,200 --> 00:29:10,880
알았어 미안해 si는 출력이고 yi는 i의 출력이다

227
00:29:13,040 --> 00:29:18,400
이제 몇 가지 실험 결과를 살펴보겠습니다. 빠르게 살펴보고

228
00:29:19,040 --> 00:29:27,120
검은색 선은 해당 모델을 모두 나타내고 파란색 선은 리포머 모델의 선입니다.

229
00:29:27,120 --> 00:29:34,160
이는 또 다른 효율적인 변압기 모델이었으며 여기 빨간색 선은 변형입니다.

230
00:29:35,120 --> 00:29:38,480
선형 주의 모델이 두 가지 모두에서 나머지 작업을 수행하는 것을 볼 수 있습니다.

231
00:29:39,280 --> 00:29:43,120
시퀀스가 증가함에 따라 시간 및 공간 요구 사항

232
00:29:43,440 --> 00:29:51,600
다음으로 교차 엔트로피 손실의 수렴을 살펴보겠습니다.

233
00:29:52,960 --> 00:29:56,720
단계적 성장에서 선형 추가 모델 손실이 발생하는 것을 볼 수 있습니다.

234
00:29:57,520 --> 00:30:04,960
개질기는 표준 변압기에 도달하지 못한 반면 표준 변압기의 변압기에 수렴되었습니다.

235
00:30:05,280 --> 00:30:15,600
따라서 이미지 생성을 비교하면 선형 주의 모델이 크게 향상되었음을 알 수 있습니다.

236
00:30:19,280 --> 00:30:26,640
성능이 최고는 아니지만 여기에 표시된 속도 측면에서 다른 모델을 확인합니다.

237
00:30:27,200 --> 00:30:31,680
차원당 기준이 가장 낮지 않으며 여전히 기준선에 가깝습니다.

238
00:30:35,920 --> 00:30:45,520
다음은 이미지 생성 손실에 대한 몇 가지 예시입니다.

239
00:30:46,320 --> 00:30:50,560
그리고 우리는 그 중 일부를 관찰하기 위해 잠시 멈추었지만 너무 많은 세부 사항을 탐구하지는 않을 것입니다.

240
00:30:53,520 --> 00:30:57,360
마지막으로 음성 인식 작업에서는 선형 변환기를 사용합니다.

241
00:30:58,160 --> 00:31:02,640
모델은 시대당 소요 시간 측면에서 여전히 나머지 모델보다 성능이 뛰어납니다.

242
00:31:02,640 --> 00:31:10,160
또한 두 가지 기본 모델보다 오류율이 낮지만 성능이 우수하지는 않습니다.

243
00:31:10,160 --> 00:31:13,600
표준 변압기에는 자체 최대 기능이 있습니다.

244
00:31:16,720 --> 00:31:23,200
다음으로 2020년 12월 벨트 g의 법률 전직에 대해 이야기하겠습니다.

245
00:31:25,920 --> 00:31:32,160
여기서의 아이디어는 희소 변환기의 아이디어와 매우 유사하므로 다루지 않겠습니다.

246
00:31:32,160 --> 00:31:39,280
너무 깊이 파고들어 기본적으로 n과 관계없이 고정된 창 크기를 정의합니다.

247
00:31:39,280 --> 00:31:44,720
총 계산 비용과 공간 비용이 선형이 되도록 n에 의존하는 대신

248
00:31:44,720 --> 00:31:56,960
n 여기에서는 그들의 접근 방식을 보여 주며 그들은 슬라이딩 윈도우 주의의 세 가지 형태를 제안합니다.

249
00:31:57,920 --> 00:32:03,600
첫 번째는 단순히 슬라이딩 윈도우 어텐션(Sliding Window Attention)을 호출하고 제공하는 것처럼 제공됩니다.

250
00:32:04,240 --> 00:32:10,560
고정된 창 크기 w와 같은 하이퍼 매개변수에서 각 토큰은 절반에 참석합니다.

251
00:32:10,560 --> 00:32:19,760
여기에서와 같이 양쪽에 w 토큰이 있습니다. 두 번째 토큰은 확장된 슬라이딩 창이라고 합니다.

252
00:32:19,760 --> 00:32:29,200
이는 여기에서 볼 수 있듯이 크기 확장 d의 간격이 있는 슬라이딩 창이며,

253
00:32:29,200 --> 00:32:37,200
반면에 다른 모든 토큰에 참여하고 관심을 받는 특정 토큰을 지정합니다.

254
00:32:37,200 --> 00:32:45,360
예를 들어 글로벌 관심 분류를 위한 다른 모든 토큰은 cls 토큰에 사용됩니다.

255
00:32:45,680 --> 00:32:52,080
글로벌 관심을 지정하는 것은 세금에 따라 다르지만 추가하는 쉬운 방법입니다.

256
00:32:52,640 --> 00:33:00,560
모델의 주의에 대한 귀납적 편향이 있으며 기존 작업별 것보다 훨씬 간단합니다.

257
00:33:00,560 --> 00:33:06,400
복잡한 아키텍처를 사용하여 유사한 정보를 결합하는 접근 방식

258
00:33:06,400 --> 00:33:14,160
더 작은 입력 덩어리와 마찬가지로 슬라이딩 창과 전역 다이어트를 결합한 것입니다.

259
00:33:14,160 --> 00:33:21,520
예를 들어 여기와 여기의 토큰과 여기에 있는 토큰에 주의를 기울이세요.

260
00:33:24,000 --> 00:33:29,920
전역 슬라이딩 윈도우가 n에 독립적이기 때문에

261
00:33:33,440 --> 00:33:39,200
전역 플러스 슬라이딩 윈도우 접근 방식에 대해 죄송합니다. 그들은 qk와 v의 두 세트를 사용합니다.

262
00:33:40,080 --> 00:33:50,480
희소 변환기와 유사하지만 모두 동일하게 초기화됩니다.

263
00:33:51,280 --> 00:33:56,240
n에 비해 작은 글로벌 토큰이 많이 있으므로 전체 계산은

264
00:33:56,240 --> 00:34:01,280
비용은 여전히 ​​n에 독립적이므로 n에 선형입니다.

265
00:34:09,200 --> 00:34:20,720
이제 여기서 몇 가지 실험을 해보겠습니다. dbc는 dpc가 다음과 같은 개념입니다.

266
00:34:21,520 --> 00:34:29,440
텍스트 기반 작업 측면에서 픽셀당 기본이 길다는 것을 알 수 있습니다.

267
00:34:30,400 --> 00:34:39,200
tx8 및 nvk 데이터 세트의 작은 vpc 모델 및 대형 모델의 경우 최고의 dpc에 가깝습니다.

268
00:34:40,560 --> 00:34:51,040
nvk이므로 예, 여기에서 오른쪽을 볼 수 있습니다. 이는 모델 내에서 다른 항목에 대한 비교를 보여줍니다.

269
00:34:51,040 --> 00:34:55,840
슬라이딩 윈도우 및 팽창 모드

270
00:35:01,760 --> 00:35:04,800
오른쪽 하단에서는 여러 가지 구성을 테스트합니다.

271
00:35:11,280 --> 00:35:14,960
여기에서는 q와 a에 대한 다른 정확도 결과를 보여줍니다.

272
00:35:15,520 --> 00:35:22,560
바르바타 및 현재 순위표와 비교한 분류 참조용

273
00:35:24,720 --> 00:35:27,760
2020년 5월 현재 기준

274
00:35:30,320 --> 00:35:38,880
여기에서 로봇을 기반으로 수행하는 방법을 기반으로 한 오프머를 볼 수 있으며 점수는 꽤 좋습니다.

275
00:35:38,880 --> 00:35:49,760
그리고 마지막 결승전 어 그냥 더 많은 결과를 원한다면 일시 중지하고 읽어도 되지만

276
00:35:50,560 --> 00:35:58,960
시간이 부족해서 계속 진행해야 할 것 같아서 마지막으로 장편 텐션을 실행하게 되었습니다.

277
00:35:58,960 --> 00:36:11,360
2021년 3월에 punk l이 제안한 제안이며 여기의 아이디어는 선형화에서 영감을 받았습니다.

278
00:36:11,360 --> 00:36:20,880
먼저 wi를 독립적으로 두는 경우 사용되는 예비 정리를 소개합니다.

279
00:36:20,880 --> 00:36:30,480
평균이 0이고 시그마 제곱에 항등을 곱한 정규 분포에서 샘플링된 벡터

280
00:36:30,480 --> 00:36:40,640
즉, wi의 각 항목을 다음에서 샘플링하도록 하는 것과 같습니다.

281
00:36:41,600 --> 00:36:45,600
정규분포 우리와 하자

282
00:36:48,400 --> 00:36:58,960
phi i는 5번 반전에서 r 작은 r 작은 d에서 r 2배 큰 d로의 비선형 변환입니다.

283
00:37:00,560 --> 00:37:09,280
여기서 벡터 x의 파이는 부호의 전치에 큰 d를 곱한 1의 제곱근과 같습니다.

284
00:37:09,280 --> 00:37:20,080
w 하나의 점 x wd의 부호까지 계속 도트 x 그리고 w의 코사인 하나의 점 x w의 코사인까지

285
00:37:20,080 --> 00:37:32,640
dx 전치 그런 다음 우리는 phi i 시간 phi x 곱하기 y의 예상 값을 취합니다.

286
00:37:36,160 --> 00:37:45,600
실례합니다. wi는 x - y 제곱의 길이를 2로 나눈 지수입니다.

287
00:37:45,600 --> 00:37:53,600
곱하기 시그마 제곱이 부정되고 여기서 우리는 큰 d가 하이퍼 매개변수임을 알 수 있습니다.

288
00:37:54,320 --> 00:38:03,920
시퀀스 길이와 무관하므로 이 정리가 어떻게 우리에게 도움이 됩니까? 이제 다음을 수행할 수 있습니다.

289
00:38:03,920 --> 00:38:12,720
연산 q의 지수는 전치 곱하기 ki를 dk의 제곱근으로 나눈 값입니다.

290
00:38:12,720 --> 00:38:20,800
공식은 그냥 내부를 제곱해서 분리하면 됩니다

291
00:38:26,080 --> 00:38:34,960
여기서 두 번째 구성 요소는 두 번째 구성 요소를 교체할 수 있다는 것을 알 수 있습니다.

292
00:38:35,760 --> 00:38:41,120
지금처럼 우리는 이전의 정리를 사용하여 그 종류의 표현으로 대체할 수 있습니다.

293
00:38:41,760 --> 00:38:49,920
이 최종 형태를 얻으려면 이제 이 작업이 선형화에서 영감을 받은 이유를 알 수 있습니다.

294
00:38:51,840 --> 00:38:54,960
우리는 이것을 선형 주의로부터 주의 함수라고 부릅니다.

295
00:38:56,880 --> 00:39:06,960
그래서 바로 여기에서 동일한 함수를 여기에 있는 함수로 바꾸고 무엇을 정의하면 됩니다.

296
00:39:07,680 --> 00:39:12,960
우리는 gikv의 rfa로 얻은 것을 정의합니다.

297
00:39:16,240 --> 00:39:26,560
그래서 여기의 모든 시간은 여기의 모든 시간이 다음과 같은 행렬을 생성한 출력을 나타냅니다.

298
00:39:26,560 --> 00:39:34,320
항목은 모두 첫 번째 벡터의 n 요소와 두 번째 벡터의 n 요소의 곱입니다.

299
00:39:34,880 --> 00:39:42,560
예를 들어 두 개의 벡터가 있는 경우 교차곱이 생성되어 이 행렬이 생성됩니다.

300
00:39:46,480 --> 00:39:47,680
등등 등등

301
00:39:51,520 --> 00:39:59,120
여기서는 rfa의 시간 및 공간 복잡성에 대해 논의하겠습니다. 여기서는 다른 방법을 사용합니다.

302
00:39:59,120 --> 00:40:06,160
편지이지만 가우스라고 불리는 이 경우에는 기본적으로 동일한 아이디어입니다.

303
00:40:06,960 --> 00:40:18,560
크로노 5 qt 또는 5 qi에는 시간 2d 2 곱하기 dk 2 곱하기 c 곱하기 dk 및 2d의 공간이 필요합니다.

304
00:40:18,560 --> 00:40:31,360
그리고 이 곱의 교차곱은 시간 2 곱하기 n 곱하기 d 곱하기 dk 와 2 곱하기 d 의 공간이 필요합니다.

305
00:40:32,160 --> 00:40:34,720
타임즈 dk 그리고

306
00:40:37,200 --> 00:40:47,120
여기서만 합산하는 데는 2n 곱하기 dk d 곱하기 dk의 시간과 2d의 공간이 모두 필요합니다.

307
00:40:48,800 --> 00:40:57,680
여기와 여기에서 5개의 qt 내적을 갖는 반대는 시간이 걸립니다. 2d k 따라서 시간은

308
00:40:58,240 --> 00:41:08,960
풀타임은 n에 선형이고 메모리 총 메모리는 4d + 2d입니다.

309
00:41:09,840 --> 00:41:16,000
dk는 일정하지만 d에 크게 의존합니다.

310
00:41:16,080 --> 00:41:23,840
가상 실험에서는 d가 작은 d의 2배보다 클 때

311
00:41:24,480 --> 00:41:34,320
이것에 의존하는 것은 미미하므로 선형화된 주의와 마찬가지로 이것은 다시 우리에게

312
00:41:34,320 --> 00:41:40,960
인과관계에 대한 관심이 있는 경우 논쟁에 대해 지금 여기 있는 모든 사람에게 이것이 설명되지 않는다는 점을 상기시켜야 합니다.

313
00:41:40,960 --> 00:41:48,080
복잡성이 말하는 시간이 선형인 이유는 이것이 단순히 관계를 강화하는 것입니다.

314
00:41:48,080 --> 00:41:58,320
변환기와 rnn 사이에 있으므로 이 rnn으로 인해 rfa와 같은 속성을 상태 저장으로 만들 수 있습니다.

315
00:41:59,280 --> 00:42:05,520
그러면 우리는 주에서 마지막으로 통과할 수 있습니다

316
00:42:08,480 --> 00:42:16,960
훈련과 평가절하 동안 st와 zt를 다음 미니 배치로 이동합니다. 나중에 살펴보겠습니다.

317
00:42:16,960 --> 00:42:26,400
이는 문제를 상당히 개선하므로 형식과 같은 rnn이 우리에게 제공하는 또 다른 이점은 다음과 같습니다.

318
00:42:26,400 --> 00:42:41,040
gt 메커니즘을 사용하여 이전에 편향을 학습하려면 여기서 wg와 bg는 학습 매개변수이고 gt는

319
00:42:41,040 --> 00:42:48,720
0과 1 사이에서 숨겨진 상태에 학습된 분산인 gt를 곱하여

320
00:42:48,720 --> 00:42:53,840
당신이 가장 최근의 상황에 있었다면 역사는 기하급수적으로 쇠퇴합니다

321
00:42:58,240 --> 00:43:06,960
그래서 그것들은 가우스 커널 외에 아크 코사인 커널도 제안했습니다.

322
00:43:07,680 --> 00:43:12,880
그들은 선형 통합 활성화 함수를 수정하기 위해 부서진 변형 커널 제품군 중에서 선택했습니다.

323
00:43:12,880 --> 00:43:24,400
여기서 다시 실행 기능은 단순히 0과 x의 최대값입니다. 이는 결과를 얻지 못하며

324
00:43:25,440 --> 00:43:34,160
가우스 커널 외에 self max에 대한 대안 보완

325
00:43:34,480 --> 00:43:40,800
그럼 몇 가지 실험 결과를 살펴보겠습니다.

326
00:43:42,640 --> 00:43:44,960
여기에 사용된 기준선은 선형화된 주의입니다.

327
00:43:46,560 --> 00:43:54,800
여기에서 제안된 분산과 비교하면 다음과 같은 점을 알 수 있습니다.

328
00:43:54,800 --> 00:44:04,720
성향은 여기서 볼 수 있듯이 gt 변형이 기준선과 gt 변형보다 훨씬 더 나은 성능을 발휘합니다.

329
00:44:07,680 --> 00:44:10,480
네, 인식의 도시는 훨씬 더 나은 성능을 발휘합니다

330
00:44:14,240 --> 00:44:20,000
기준선이 표준 변환기인 기계 번역의 경우 우리는

331
00:44:20,000 --> 00:44:25,920
rfa의 속도가 선형화된 주의만큼 매끄럽다는 것을 알게 될 것입니다.

332
00:44:27,120 --> 00:44:32,480
성능이 기준선에 가깝고 선형보다 나은 동안 여기에 표시됩니다.

333
00:44:33,840 --> 00:44:37,920
예를 들어 여기에 표시된 대로 주의를 기울이세요.

334
00:44:42,000 --> 00:44:45,840
다음은 다양한 텍스트 데이터 세트에 대한 다른 모델과의 비교입니다.

335
00:44:46,720 --> 00:44:53,120
테스트 모델 중 정확도가 매우 뛰어난 것을 확인할 수 있습니다.

336
00:44:53,920 --> 00:44:59,680
그렇지는 않았지만 속도와 메모리 소비 측면에서 앞서지 않았습니다.

337
00:45:00,800 --> 00:45:06,720
하지만 여전히 거기에 도달했습니다. 예를 들어 여기

338
00:45:06,720 --> 00:45:13,840
마지막으로 소프트맥스 함수와의 비교를 살펴보겠습니다.

339
00:45:15,520 --> 00:45:20,720
디코딩 속도와 메모리 소비 측면에서 두 가지 rfa 차이 중

340
00:45:21,840 --> 00:45:24,480
x 스케일은 로그 2 기반이라는 점에 유의하세요.

341
00:45:27,040 --> 00:45:33,200
질문 성장으로 소프트맥스의 속도는 분명히 2차 붕괴를 보여줍니다.

342
00:45:34,080 --> 00:45:40,000
두 가지 분산 모두에서 rfa의 속도는 크게 다르지 않았지만 약간 떨어졌습니다.

343
00:45:42,480 --> 00:45:47,040
아 미안 조금 올랐다가 다시 조금 떨어졌어

344
00:45:48,720 --> 00:45:58,320
메모리 소비 측면에서 Softmax는 올바르게 증가하지만 rfa는 거의 일정하게 유지됩니다.

345
00:45:59,200 --> 00:46:07,280
이것이 제가 오늘 사용할 제안된 솔루션의 전부이고 마지막으로 제시할 내용은 다음과 같습니다.

346
00:46:07,920 --> 00:46:10,640
모든 효율적인 변환기를 비교하는 github 페이지

347
00:46:12,080 --> 00:46:18,960
이 페이지의 Wen Hu Chen 교수에게 사진을 찍었습니다.

348
00:46:21,040 --> 00:46:26,720
이 웹사이트에는 2020년 11월 8일 현재 모든 효율적인 변압기 목록이 나와 있습니다.

349
00:46:26,800 --> 00:46:30,240
이 차트에 표시된 성능 비교

350
00:46:31,680 --> 00:46:34,720
이 링크로 이동하면 이에 대한 자세한 내용을 확인할 수 있습니다.

351
00:46:36,960 --> 00:46:39,840
그게 다야 시간 내주셔서 감사합니다