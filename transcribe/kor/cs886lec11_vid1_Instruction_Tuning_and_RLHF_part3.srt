1
00:00:00,000 --> 00:00:05,280
지금까지 우리는 Arleger 프레임워크가 어떻게 작동하는지 살펴보았습니다.

2
00:00:05,280 --> 00:00:09,040
이제 몇 가지 단점에 대해 알아 보겠습니다.

3
00:00:09,040 --> 00:00:16,120
첫째, 추가 보상 모델 자체를 교육하면 프로세스가 더 복잡해집니다.

4
00:00:16,120 --> 00:00:22,600
이 모델링 단계에서는 추가 하이퍼파라미터를 신중하게 조정해야 합니다.

5
00:00:22,600 --> 00:00:31,120
또한 Arleger 프레임워크가 기본 설정을 조정하는 데 도움이 되더라도

6
00:00:31,120 --> 00:00:36,560
여전히 때로는 덜 효율적이고 안정적인 방법입니다.

7
00:00:36,560 --> 00:00:41,920
전반적으로 이것이 Arleger 프레임워크가 작동하는 방식입니다.

8
00:00:41,920 --> 00:00:50,280
우리는 주어진 선호 데이터를 가지고 있으며, 이를 보상 모델에 맞추고 나중에 이 보상에 맞추는 데 사용합니다.

9
00:00:50,280 --> 00:00:57,120
모델은 언어 모델 정책을 조정하는 데 사용됩니다.

10
00:00:57,120 --> 00:01:02,840
그런데 보상 모델 없이 정렬을 수행할 수 있나요?

11
00:01:02,840 --> 00:01:12,480
글쎄요, 이에 답하기 위해 DPO 또는 DPO 논문 작성자는 직접적으로

12
00:01:12,480 --> 00:01:20,720
기본 설정 데이터와 정렬하기 위해 기본 설정 데이터를 사용합니다.

13
00:01:20,720 --> 00:01:27,760
간단히 말해서 DPO는 언어 모델을 훈련하기 위한 Arleger가 없는 간단한 알고리즘입니다.

14
00:01:27,760 --> 00:01:28,760
환경 설정.

15
00:01:28,760 --> 00:01:38,440
따라서 DPO 프로세스부터 시작하려면 먼저 DPO 프로세스가 실제로 어떻게 진행되었는지 살펴보겠습니다.

16
00:01:38,440 --> 00:01:45,000
이것이 우리가 Arleger에 사용한 최적화 문제입니다.

17
00:01:45,000 --> 00:01:50,800
모든 응답에 대한 보상이 높을수록 모델에 더 나은 것으로 간주됩니다.

18
00:01:50,800 --> 00:01:52,680
우리는 그럴 생각이다.

19
00:01:52,680 --> 00:01:57,640
또한 이것이 우리가 최적화하려는 정책입니다.

20
00:01:57,640 --> 00:02:08,120
이것이 참조 정책이며 여기서 우리는 편차를 제한하려고 합니다.

21
00:02:08,560 --> 00:02:12,480
참조 정책에서 학습하는 정책입니다.

22
00:02:12,480 --> 00:02:18,680
따라서 기본적으로 우리는 정책이 참조에서 너무 많이 벗어나는 것을 원하지 않습니다.

23
00:02:18,680 --> 00:02:26,520
그렇지 않으면 우리에게 비논리적인 반응을 보이는 경향이 있습니다.

24
00:02:26,520 --> 00:02:33,880
여기서 우리는 이러한 유사성을 제한하기 위해 케일 발산(Kale divergence)을 사용해 왔습니다.

25
00:02:34,840 --> 00:02:47,680
이제 저자는 여기에 표시된 최적의 솔루션을 생각해 냈습니다.

26
00:02:47,680 --> 00:02:57,560
참조 정책, 보상 기능 및 분할 기능 측면에서 최적의 솔루션입니다.

27
00:02:57,560 --> 00:03:04,280
수학적, 일반적인 수학 대수학을 사용하여 보상 함수를 다시 작성할 수도 있습니다.

28
00:03:04,280 --> 00:03:14,280
최적의 정책, 참조 정책, 파티션 기능 측면에서 죄송합니다.

29
00:03:14,280 --> 00:03:21,200
이를 바탕으로 이제 보상 기능이 있으면 다음과 같이 작성할 수도 있다는 것을 알았습니다.

30
00:03:21,200 --> 00:03:29,840
참조 정책과 최적 측면에서 신체 모델을 이용한 인간 선호 확률

31
00:03:29,840 --> 00:03:32,920
우리가 여기에 있는 정책입니다.

32
00:03:32,920 --> 00:03:37,920
바로 여기에 최적의 정책이 있고 이것이 참조 정책입니다.

33
00:03:37,920 --> 00:03:46,720
따라서 이제 우리는 최적의 정책과 측면에서 인간 선호 확률을 갖게 되었습니다.

34
00:03:46,720 --> 00:03:53,640
참조 정책을 사용하여 손실 함수를 다시 작성할 수도 있습니다.

35
00:03:53,640 --> 00:04:01,120
글쎄요, 여기서 볼 수 있듯이 이는 이미 RLHF에서 얻은 것과 매우 유사합니다.

36
00:04:01,120 --> 00:04:07,800
승자의 반응에는 긍정적인 반응이 있고 부정적인 반응도 있는 것을 볼 수 있습니다.

37
00:04:07,800 --> 00:04:10,160
패자 대응을 위한 작전.

38
00:04:10,160 --> 00:04:15,360
이전에 했던 것과 비슷한 것, 여기서는 승자 응답으로 무언가를 운영하고 있습니다.

39
00:04:15,360 --> 00:04:18,440
패자 응답을 부정합니다.

40
00:04:18,440 --> 00:04:24,760
자, 여기서 볼 수 있듯이, 여기서는 우리가 노력하고 있습니다. 먼저 우리는 확률을 최대화하려고 노력하고 있습니다.

41
00:04:24,760 --> 00:04:31,840
승자 응답의 확률을 최소화하고 패자 응답의 확률을 최소화하려고 노력합니다.

42
00:04:31,840 --> 00:04:42,800
그리고 또한 논문에 따르면 저자들은 이 특별한 부분을 이렇게 불렀습니다.

43
00:04:42,800 --> 00:04:46,200
암묵적인 보상으로서의 가치.

44
00:04:46,200 --> 00:04:57,320
따라서 이것은 또한 실제로 최적화가 어떻게 이루어지는지에 대한 아이디어를 제공한다고 말할 수 있습니다.

45
00:04:57,320 --> 00:04:59,000
일하고 있습니다.

46
00:04:59,000 --> 00:05:06,760
참조 정책으로부터의 이탈을 제한하려고 노력하고 있으며 또한 최대화하려고 노력하고 있습니다.

47
00:05:06,760 --> 00:05:11,120
승자 반응의 확률을 최대화하고 승자 반응의 확률을 최소화합니다.

48
00:05:11,120 --> 00:05:14,040
패자 반응.

49
00:05:14,040 --> 00:05:25,800
이제 결과 측면에서 저자는 여러 가지에 대해 평가를 수행했습니다.

50
00:05:25,800 --> 00:05:32,800
테스트, 여러 데이터 세트가 나오고 원래 PPO와 비교했습니다.

51
00:05:32,800 --> 00:05:36,200
새로운 방법 인 DPO와 비교합니다.

52
00:05:36,200 --> 00:05:39,720
이것이 작업 1, 즉 제어 감정 생성입니다.

53
00:05:39,720 --> 00:05:47,880
그래서 여러분도 아시다시피 이 작업의 목표는 영화 리뷰를 제공하는 것입니다.

54
00:05:47,880 --> 00:05:51,880
감정반응.

55
00:05:51,880 --> 00:06:02,760
여기에서 볼 수 있습니다. DPO의 경우 노란색이 DPO입니다. 엄격히 지배하고 있습니다.

56
00:06:02,760 --> 00:06:10,840
KL 발산 비율에 대한 보상인 PPO와 비교할 때.

57
00:06:10,840 --> 00:06:18,480
따라서 우리가 볼 수 있듯이 최적 정책과 참조 정책 간의 KL 차이가 발생하더라도

58
00:06:18,480 --> 00:06:27,640
낮다는 것은 충분히 가깝지만 DPO가 우리에게 좋은 높은 보상을 제공한다는 것을 의미합니다.

59
00:06:27,640 --> 00:06:35,400
왜냐하면 PPO의 경우 정책을 시행할 때 좋은 보상을 받지 못했기 때문입니다.

60
00:06:35,400 --> 00:06:39,720
서로 매우 유사합니다.

61
00:06:39,720 --> 00:06:48,520
또한 Ground Truth 모델을 사용하는 PPOGT는 Ground Truth입니다.

62
00:06:48,520 --> 00:06:54,080
특정 근거를 사용할 때 감정 분석을 위한 진실 보상 모델

63
00:06:54,080 --> 00:07:03,320
사실 그렇다고 해도 PPO로는 좋은 보상을 얻을 수 없습니다.

64
00:07:03,320 --> 00:07:14,160
따라서 이는 평가 및 안정성 측면에서 DPO가 얼마나 강력한지를 보여줍니다.

65
00:07:14,160 --> 00:07:17,440
다음은 요약입니다.

66
00:07:17,440 --> 00:07:27,120
요약을 위해 Reddit 게시물이 주어지면 주요 포인트를 추출하고 싶습니다.

67
00:07:27,120 --> 00:07:32,560
샘플의 주요 요점.

68
00:07:32,560 --> 00:07:40,960
음, 이 실험은 다양하고, 다양하고, 다양한 샘플링 온도에서 수행됩니다.

69
00:07:40,960 --> 00:07:49,840
보시다시피 노란색 DPO는 모든 샘플링 온도에서 거의 안정적입니다.

70
00:07:49,840 --> 00:07:59,320
PPO의 경우 매우 불안정하고 효율성이 감소합니다.

71
00:07:59,320 --> 00:08:06,640
우리가 알고 있듯이 샘플링 온도를 높이면 더 높은 확률적 결과를 얻을 수 있습니다.

72
00:08:06,640 --> 00:08:11,720
더 불안정한 모델을 제공하는 경향이 있습니다.

73
00:08:11,720 --> 00:08:19,320
하지만 그럼에도 불구하고 DPO는 PPO에 비해 여전히 좋은 성능을 보이고 있습니다.

74
00:08:19,320 --> 00:08:26,240
그리고 128개 베스트와 비교해 보면 비교불가한 결과를 보여주고 있습니다.

75
00:08:26,240 --> 00:08:34,200
따라서 논문에 따르면 128개 중 가장 좋은 것은 128개의 응답을 추출하려고 할 때입니다.

76
00:08:34,640 --> 00:08:42,080
최고의 답변처럼 최대한 활용하세요.

77
00:08:42,080 --> 00:08:50,960
128의 최고점을 고려하면 꽤 까다로울 수 있고 계산적으로도 꽤 까다로울 수 있습니다.

78
00:08:50,960 --> 00:08:56,520
우리는 120개의 응답을 생성한 다음 알아내려고 노력하기 때문에 까다로운 방법입니다.

79
00:08:56,520 --> 00:08:58,880
정답은 무엇입니까?

80
00:08:58,880 --> 00:09:04,560
이 시나리오, 작업 3에서도 우리가 제공하는 것은 단일 턴 대화입니다.

81
00:09:04,560 --> 00:09:12,280
인간의 질의로 인해 LLM으로부터 적절한 응답을 받고 싶습니다.

82
00:09:12,280 --> 00:09:21,280
이 경우에도 DPO는 꽤 좋은 성능을 보이고 있으며 최고 128과 충분히 비교할 수 있습니다.

83
00:09:21,280 --> 00:09:24,160
계산량이 매우 많은 방법입니다.

84
00:09:24,160 --> 00:09:31,200
이는 DPO가 다양한 작업에서 매우 잘 작동하고 있음을 보여줍니다.

85
00:09:31,200 --> 00:09:39,200
People의 작성자는 훈련 과정에서 모델의 성능을 분석했습니다.

86
00:09:39,200 --> 00:09:40,200
단계.

87
00:09:40,200 --> 00:09:47,600
여기서 볼 수 있듯이 모델은 다양한 샘플링 온도에서도 상당히 안정적으로 유지됩니다.

88
00:09:47,600 --> 00:09:49,600
변형 단계의 경우.

89
00:09:49,600 --> 00:10:03,640
여기에서 볼 수 있듯이 전반적으로 다양한 샘플링 온도에 대해 거의 안정적입니다.

90
00:10:03,640 --> 00:10:09,240
두 샘플링 온도 모두에 대해 쉽게 빠르게 수렴됩니다.

91
00:10:09,240 --> 00:10:18,080
이는 DPO가 언어 정렬을 위한 훨씬 더 안정적이고 효율적인 방법임을 보여줍니다.

92
00:10:18,080 --> 00:10:22,360
모델.

93
00:10:22,360 --> 00:10:27,320
이것이 DPO 용지의 전부입니다.

94
00:10:27,320 --> 00:10:35,240
계속해서 우리가 이야기할 다음 논문은 파일입니다.

95
00:10:35,240 --> 00:10:46,320
이 문서의 주요 하이라이트는 모델이 다양한 AI 모델을 사용했다는 것입니다.

96
00:10:46,680 --> 00:10:53,440
선호도 데이터를 생성하고 큐레이팅을 위해 독특한 증류 방식을 사용합니다.

97
00:10:53,440 --> 00:10:59,680
미세 조정을 위한 좋은 데이터 세트

98
00:10:59,680 --> 00:11:05,200
이것이 Zafire 훈련의 모습입니다.

99
00:11:05,200 --> 00:11:09,480
우리는 이미 교육 훈련이 어떻게 진행되는지 살펴보았습니다.

100
00:11:09,480 --> 00:11:12,280
이것도 비슷한 형식입니다.

101
00:11:12,280 --> 00:11:17,440
우리는 훈련 단계를 세 부분으로 나누었습니다.

102
00:11:17,440 --> 00:11:23,520
먼저 미세 조정 단계가 있는데, 이는 아직 미세 조정 단계입니다.

103
00:11:23,520 --> 00:11:33,640
따라서 여기에서의 목표는

104
00:11:33,640 --> 00:11:36,360
교사 모델의 능력

105
00:11:36,360 --> 00:11:40,720
따라서 Zafire 교육에 사용된 교사 모델은 GPT-4입니다.

106
00:11:40,720 --> 00:11:48,640
여기에서 볼 수 있듯이 몇 가지 초기 시드 프롬프트가 있으며 목표는 이 데이터 세트를 선별하는 것입니다.

107
00:11:48,640 --> 00:11:50,600
교사 모델을 사용합니다.

108
00:11:50,600 --> 00:11:53,720
이제 우리는 그렇게 합니다.

109
00:11:53,720 --> 00:12:01,200
시드 프롬프트 중 하나를 가져와 이를 교사 모델에 전달한 다음 생성한다고 가정해 보겠습니다.

110
00:12:01,200 --> 00:12:03,000
응답.

111
00:12:03,000 --> 00:12:10,040
응답이 있으면 프롬프트와 응답을 모두 교사 모델에 전달합니다.

112
00:12:10,040 --> 00:12:16,040
그리고 명령을 개선하도록 지시했습니다.

113
00:12:16,040 --> 00:12:23,720
따라서 우리는 이 프로세스를 몇 단계 동안 계속한 다음 좋은 고품질 제품을 생성할 수 있습니다.

114
00:12:23,720 --> 00:12:27,400
응답 데이터 세트.

115
00:12:27,400 --> 00:12:32,840
이 데이터 세트는 모델을 미세 조정하는 데 추가로 사용됩니다.

116
00:12:32,840 --> 00:12:36,120
이것이 우리가 첫 번째 미세 조정 단계라고 부르는 것입니다.

117
00:12:36,120 --> 00:12:45,080
데이터 세트를 생성하고 실제로 데이터 세트를 큐레이팅한 다음 이를 언어 미세 조정에 사용합니다.

118
00:12:45,080 --> 00:12:47,880
모델.

119
00:12:47,880 --> 00:12:49,840
그래서 그것이 우리가 가진 것입니다.

120
00:12:49,840 --> 00:12:56,600
Ultrajet은 생성된 데이터 세트이며 이를 모델 미세 조정에 사용합니다.

121
00:12:56,600 --> 00:13:00,440
이것이 바로 우리가 증류 감독 미세 조정이라고 부르는 것입니다.

122
00:13:01,400 --> 00:13:10,120
이제 두 번째 단계인 반응 생성 및 공기 순위로 넘어갑니다.

123
00:13:10,120 --> 00:13:18,440
따라서 이 2단계에는 교사 모델을 다시 사용하는 것이 포함되며 동일한 교사 모델이 사용되었습니다.

124
00:13:18,440 --> 00:13:26,000
이 2단계를 통해 다른 모델을 사용하여 선호도 데이터를 생성합니다.

125
00:13:26,000 --> 00:13:27,880
그럼 어떻게 할까요?

126
00:13:27,880 --> 00:13:36,600
우리는 프롬프트를 사용하여 이를 네 가지 다른 언어 모델에 전달한 다음 생성하도록 요청합니다.

127
00:13:36,600 --> 00:13:37,600
응답.

128
00:13:37,600 --> 00:13:46,080
각각은 응답을 생성하고 교사 모델은 응답을 채점하는 데 사용됩니다.

129
00:13:46,080 --> 00:13:47,080
우리에겐 이런 것이 있습니다.

130
00:13:47,080 --> 00:13:56,720
S1은 모델이 프롬프트와 응답을 기반으로 평가하려는 첫 번째 점수입니다.

131
00:13:56,720 --> 00:13:58,960
그리고 그에 따라 점수를 매기세요.

132
00:13:58,960 --> 00:14:06,880
점수가 준비되면 프롬프트로 구성된 데이터 세트를 선별합니다.

133
00:14:06,880 --> 00:14:15,680
가장 높은 점수를 받은 승자 응답과 패자 응답이 사용되었습니다.

134
00:14:15,680 --> 00:14:24,120
이제 여기서 중요한 점은 모델을 선택하지 않은 패자 응답에 대한 것입니다.

135
00:14:24,120 --> 00:14:31,400
가장 낮은 점수로 나머지 응답 중 하나를 무작위로 선택합니다.

136
00:14:31,400 --> 00:14:34,160
세 명의 후보.

137
00:14:34,160 --> 00:14:46,840
따라서 데이터 세트에 다양성을 포함하기를 원했기 때문에 이것이 선택된 접근 방식이었습니다.

138
00:14:46,840 --> 00:14:51,560
이것이 바로 이 두 번째 단계입니다.

139
00:14:51,560 --> 00:15:02,560
프롬프트를 샘플링하여 다른 모델에 전달하고 이진화를 위해 GPT4를 사용합니다.

140
00:15:02,560 --> 00:15:11,640
생성된 이 데이터세트는 울트라피드백이라고도 합니다.

141
00:15:11,640 --> 00:15:17,080
이제 세 번째 단계인 DPU로 이동합니다.

142
00:15:17,080 --> 00:15:23,840
그래서 DPU 우리는 이미 DPU가 어떻게 작동하는지 살펴봤고 Zephyr는 DPU를 활용하는 모델입니다.

143
00:15:23,840 --> 00:15:31,800
언어 모델을 피드백 데이터와 정렬하는 방법.

144
00:15:31,800 --> 00:15:36,600
따라서 훈련 절차는 매우 간단합니다.

145
00:15:36,600 --> 00:15:43,160
우리는 승자 응답과 패자 응답의 확률을 계산합니다.

146
00:15:43,160 --> 00:15:49,120
첫 번째 단계에서 얻은 미세 조정 모델에 적용합니다.

147
00:15:49,120 --> 00:15:55,580
그런 다음 업데이트하려는 DPU 모델로부터 확률을 얻으려고 합니다.

148
00:15:55,580 --> 00:16:07,400
그런 다음 이 방정식을 사용하여 역전파합니다. 우리는 이미 이 방정식을 어떻게 얻었는지 보았습니다.

149
00:16:07,400 --> 00:16:12,240
승자 응답을 최소화하고 패자 응답을 최소화합니다.

150
00:16:12,240 --> 00:16:19,480
이는 우리가 이미 DPU에서 배운 것과 동일합니다.

151
00:16:19,480 --> 00:16:26,600
훈련 세부 사항 중 일부를 살펴보기 위해 미세 조정 실험이 수행되었습니다.

152
00:16:26,600 --> 00:16:28,200
서부 7B.

153
00:16:28,200 --> 00:16:34,880
따라서 이것은 70억 크기 범주 모델에 대한 최첨단 모델입니다.

154
00:16:34,880 --> 00:16:39,680
그래서 저자는 이 모델을 선택했습니다.

155
00:16:39,680 --> 00:16:48,080
또한 미세 조정 및 최적화를 위해 변압기 강화 학습 라이브러리를 사용했습니다.

156
00:16:48,080 --> 00:16:55,640
기억력과 훈련 속도를 향상시키기 위해 딥 스피드 03과 육체 주의 메커니즘을 사용했습니다.

157
00:16:55,640 --> 00:17:02,400
이제 평가 부분으로 넘어가서 여러 데이터 세트를 평가에 사용했습니다.

158
00:17:02,400 --> 00:17:03,400
목적.

159
00:17:03,400 --> 00:17:06,800
데이터 세트의 요지는 다음과 같습니다.

160
00:17:06,800 --> 00:17:17,240
하나, 먼저 다중 조정 응답 벤치마크가 있는 빈 벤치였으며, 160개의 질문이 포함되어 있습니다.

161
00:17:17,240 --> 00:17:21,680
8가지 지식 분야에 걸쳐

162
00:17:21,680 --> 00:17:28,320
주로 하나의 초기 질문이 있고 모델은 이에 응답해야 하며

163
00:17:28,320 --> 00:17:35,320
미리 정의된 두 번째 응답, 두 번째 질문은 후속 질문입니다.

164
00:17:36,320 --> 00:17:44,920
모델은 두 번째 응답을 생성한 다음 GPT-4를 사용하여 조합을 평가합니다.

165
00:17:44,920 --> 00:17:46,720
응답의.

166
00:17:46,720 --> 00:17:50,960
싱글톤 벤치마크인 Alpaca 이벤트가 하나 더 있습니다.

167
00:17:50,960 --> 00:17:57,400
다양한 주제에 걸쳐 805개의 질문이 포함되어 있으며 대부분 유용성에 중점을 두고 있습니다.

168
00:17:57,400 --> 00:18:01,760
여기에서도 GPT-4가 평가에 사용됩니다.

169
00:18:01,760 --> 00:18:07,440
하지만 여기서는 pairwise-winded 비교를 수행합니다.

170
00:18:07,440 --> 00:18:19,040
여기에서 두 지표 모두에 대해 볼 수 있듯이 Zephyr는 70억 크기 모델 카테고리보다 성능이 뛰어납니다.

171
00:18:19,040 --> 00:18:28,040
가장 높은 점수를 제공합니다. 예, 여기에서 볼 수 있습니다.

172
00:18:28,320 --> 00:18:38,960
또한 더 큰 크기의 일부 데이터 세트와 비교하면 일치하지 않더라도

173
00:18:38,960 --> 00:18:46,120
GPT-4와 비교해 보면 여전히 꽤 비슷한 결과를 보여줍니다.

174
00:18:46,120 --> 00:18:57,840
이렇게 작은 모델이더라도 이 모델이 얼마나 잘 훈련되었는지 알 수 있습니다.

175
00:18:57,840 --> 00:19:08,640
여기서 주의할 점은 알파카 이벤트 결과가 다소 오해의 소지가 있을 수 있다는 점입니다.

176
00:19:08,640 --> 00:19:17,440
데이터 세트에는 평가에 필요한 다양한 질문이 포함되어 있지 않습니다.

177
00:19:17,440 --> 00:19:23,080
따라서 이것이 빈 벤치마크 벤치를 사용하여 평가하기 위해 수행하는 작업입니다.

178
00:19:23,080 --> 00:19:30,880
다양한 영역에 걸쳐 질문이 있으며 이를 여러 영역으로 나눌 수도 있습니다.

179
00:19:30,880 --> 00:19:38,880
여기에서 볼 수 있듯이 Zephyr는 여러 카테고리에서 매우 훌륭하고 비교 가능한 성능을 발휘합니다.

180
00:19:38,880 --> 00:19:44,960
역할극, 글쓰기, 인문학, STEM, 심지어 추출까지 말이죠.

181
00:19:44,960 --> 00:19:51,720
하지만 코딩이나 수학에 관해서는 성능이 매우 나쁩니다.

182
00:19:51,720 --> 00:19:56,040
대부분의 모델보다 훨씬 나쁩니다.

183
00:19:56,040 --> 00:20:02,360
그러므로 우리는 어떤 영역이나 어떤 것을 사용하려고 하는지 염두에 두어야 합니다.

184
00:20:02,360 --> 00:20:08,440
모델을 선택한 다음 적절한 모델을 사용할 수 있습니다.

185
00:20:08,440 --> 00:20:17,400
따라서 한 가지 주목해야 할 점은 우리가 어떤 영역에서 작업하고 있는지, 어떤 영역을 염두에 두어야 한다는 것입니다.

186
00:20:17,400 --> 00:20:22,520
모델은 그 분야에서 탁월합니다.

187
00:20:22,520 --> 00:20:30,320
다음 평가를 위해 다양한 공개 LLM 리더보드도 사용되었습니다.

188
00:20:30,320 --> 00:20:41,840
일반적으로 이는 다중 클래스 분류 질문과 같은 학문적 유형의 데이터 세트입니다.

189
00:20:41,840 --> 00:20:48,080
이 데이터 세트를 통해서도 Zephyr가 700만 개를 능가한다는 것을 알 수 있습니다.

190
00:20:48,080 --> 00:20:55,000
크기 카테고리뿐만 아니라 더 큰 크기의 모델과도 좋은 경쟁을 벌이고 있습니다.

191
00:20:55,000 --> 00:21:08,960
좋은 사이즈의 모델들과 거의 비슷한 결과를 보여주고 있음을 알 수 있습니다.

192
00:21:08,960 --> 00:21:16,680
이제 Zephyr의 저자는 또 다른 평가 연구를 제공하여

193
00:21:16,680 --> 00:21:26,320
최적화가 얼마나 도움이 되었는지, Zephyr 훈련에 얼마나 많은 증류가 도움이 되었는지를 보여줍니다.

194
00:21:26,320 --> 00:21:33,360
그래서 그들은 이 네 단계, 네 단계 훈련 과정을 수행했습니다.

195
00:21:33,360 --> 00:21:44,800
먼저 Mr. Model을 직접 사용하고

196
00:21:44,840 --> 00:21:55,160
울트라피드백 데이터 세트와 우리가 볼 수 있듯이 증류 단계는 수행되지 않았으며 매우

197
00:21:55,160 --> 00:21:59,880
모델 성능에 영향을 미쳤습니다.

198
00:21:59,880 --> 00:22:05,920
다음으로 한 epoch 동안 ultrajet을 사용하여 미세 조정 단계를 수행했으며 좋은 결과를 볼 수 있습니다.

199
00:22:05,920 --> 00:22:12,840
데이터 세트가 얼마나 효과적인지, 얼마나 효과적인지 보여주는 결과가 증가합니다.

200
00:22:12,880 --> 00:22:21,360
모델이 답을 정확하게 예측하는 데 도움이 됩니다.

201
00:22:21,360 --> 00:22:34,400
다음으로 우리는 울트라젯을 사용하여 한 단계의 미세 조정을 수행하고 두 번째 단계의 울트라피드백을 수행했습니다.

202
00:22:34,400 --> 00:22:39,560
성능이 약간 저하되는 것을 볼 수 있습니다.

203
00:22:39,560 --> 00:22:53,560
따라서 이는 모델에 좋지 않으며 마지막으로 다음을 사용하여 한 단계의 미세 조정을 수행했습니다.

204
00:22:53,560 --> 00:23:04,800
울트라젯과 울트라피드백을 이용한 DPO 원스텝은 우리에게 최대의 결과를 보여줍니다.

205
00:23:04,800 --> 00:23:13,640
실제로 DPO와 증류는 모두 모델의 이해를 향상시키는 데 도움이 됩니다.

206
00:23:13,640 --> 00:23:18,440
그리고 선호도 데이터를 정렬합니다.

207
00:23:18,440 --> 00:23:23,320
예, Zephyr에게는 그게 전부입니다. 감사합니다.