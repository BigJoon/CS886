1
00:00:00,000 --> 00:00:06,560
안녕하세요 여러분, 저는 Sahel입니다. 오늘은 Hoseina Hivani와 함께 교육에 관해 이야기해보겠습니다.

2
00:00:06,560 --> 00:00:12,460
인간의 피드백을 통한 훈련 및 강화 학습.

3
00:00:12,460 --> 00:00:19,520
언어 모델은 웹에서 대부분 레이블이 지정되지 않은 데이터를 사용하여 대규모 말뭉치에 대해 훈련됩니다.

4
00:00:19,520 --> 00:00:25,440
훈련 과정을 자체 감독하기 위해 목표는 종종 다음 토큰 예측입니다.

5
00:00:25,440 --> 00:00:32,760
또는 다른 언어 목표, 출생 시 잡음 제거와 같은 언어 마스킹 목표,

6
00:00:32,760 --> 00:00:33,760
예를 들어.

7
00:00:33,760 --> 00:00:40,040
이 설정을 사용하면 출력 모델이 단어 임베딩을 잘 학습하며 심지어 그렇게 할 수도 있습니다.

8
00:00:40,040 --> 00:00:44,480
훈련 중에 노출된 일부 NLP 작업에는 적합합니다.

9
00:00:44,480 --> 00:00:49,920
그러나 훈련 중 목적 함수는 추론 시 목적 함수와 일치하지 않습니다.

10
00:00:49,920 --> 00:00:56,240
인간이 제공한 지시를 따르는 시간입니다.

11
00:00:56,240 --> 00:01:01,880
일반적인 완화 방법 중 하나는 원하는 작업에 맞게 모델을 미세 조정하는 것입니다.

12
00:01:01,880 --> 00:01:09,160
오른쪽 그림에서 볼 수 있듯이 이 방법의 한계는 작업별로 다르다는 것입니다.

13
00:01:09,160 --> 00:01:12,160
그리고 종종 많은 예가 필요합니다.

14
00:01:12,160 --> 00:01:18,840
다른 해결책은 상황 학습 능력에서 언어 모델에 의존하고

15
00:01:18,840 --> 00:01:23,360
추론 중에 원하는 작업에 대한 몇 가지 예를 제공합니다.

16
00:01:23,360 --> 00:01:31,960
그러나 퓨샷 프롬프팅에는 효율적인 학습을 위해 신속한 엔지니어링이 필요한 경우가 많습니다.

17
00:01:31,960 --> 00:01:36,240
예제를 추가하면 추론 비용이 많이 들 수 있습니다.

18
00:01:36,240 --> 00:01:41,960
게다가 컨텍스트 크기가 제한되어 있기 때문에 어려움도 있습니다.

19
00:01:41,960 --> 00:01:48,960
오늘은 성능 향상을 제안하는 세 가지 논문을 발표하겠습니다.

20
00:01:48,960 --> 00:01:51,160
언어 모델 제로샷.

21
00:01:51,160 --> 00:01:56,840
여기서 제로샷이란 언어 모델이 이전에 작업을 본 적이 없다는 것을 의미합니다.

22
00:01:56,840 --> 00:02:01,440
상황에 맞는 예가 아니며, 사전 훈련이나 미세 조정 중에도 마찬가지입니다.

23
00:02:01,440 --> 00:02:05,440
Google의 Flan이라는 첫 번째 논문부터 시작하겠습니다.

24
00:02:05,440 --> 00:02:11,480
Flan 저자는 교육 훈련이라는 용어를 언어 모델을 미세 조정하는 것으로 정의합니다.

25
00:02:11,480 --> 00:02:16,280
자연어 교육을 통해 설명되는 많은 작업에 대해 설명합니다.

26
00:02:16,280 --> 00:02:22,160
모델은 많은 지침을 접함으로써 이를 따르는 법을 배우게 된다는 아이디어입니다.

27
00:02:22,160 --> 00:02:26,840
지침을 제공하면 보이지 않는 작업으로 일반화할 수 있습니다.

28
00:02:26,840 --> 00:02:33,120
그리고 이 방법은 많은 NLP 작업이 자연어를 통해 설명 가능하기 때문에 잘 작동합니다.

29
00:02:33,120 --> 00:02:38,680
다음 프로그램을 요약하거나 다음 문장을 번역하는 등의 지시 사항

30
00:02:38,680 --> 00:02:44,400
A에서 B까지. 여기서 예를 살펴보겠습니다.

31
00:02:44,400 --> 00:02:49,040
앞서 언급했듯이 미세 조정에는 동시에 여러 작업에 대한 교육이 포함됩니다.

32
00:02:49,040 --> 00:02:55,560
여기에는 상식 추론, 번역 및 기타 작업이 있습니다.

33
00:02:55,560 --> 00:03:02,720
추론은 제로샷입니다. 즉, 훈련 중에 사용되지 않는 작업에 대한 추론입니다.

34
00:03:02,720 --> 00:03:10,480
여기에는 자연어 추론이 있지만 상황에 맞는 예는 제공되지 않습니다.

35
00:03:10,480 --> 00:03:17,680
훈련 및 추론 시간 동안 작업은 모두 자연스러운 지침으로 설명됩니다.

36
00:03:17,680 --> 00:03:19,840
언어.

37
00:03:19,840 --> 00:03:27,040
명령어 튜닝을 위한 데이터 세트를 생성하기 위해 Flan 작성자는 62개의 NLP 데이터 세트를 다음과 같이 분류합니다.

38
00:03:27,040 --> 00:03:29,400
클러스터 12개.

39
00:03:29,400 --> 00:03:33,840
각 클러스터는 작업 유형을 나타냅니다.

40
00:03:33,840 --> 00:03:40,880
클러스터 세분성을 사용하여 모델이 작업을 확인했는지 여부를 결정합니다.

41
00:03:40,880 --> 00:03:46,680
이는 데이터 세트 세분성을 사용하는 것과 대조됩니다.

42
00:03:46,680 --> 00:03:53,440
이는 데이터세트에 대한 Flan의 성능이 제로샷으로만 평가되는 것으로 간주됨을 의미합니다.

43
00:03:53,440 --> 00:03:58,920
훈련 또는 미세 조정 중에 클러스터의 데이터 세트가 사용되지 않은 경우.

44
00:03:58,920 --> 00:04:04,880
각 클러스터에 대해 저자는 나머지 11개 클러스터에서 모델을 훈련하고 평가했습니다.

45
00:04:04,880 --> 00:04:08,720
제로 샷 설정에서 유지 클러스터의 모델입니다.

46
00:04:08,720 --> 00:04:16,680
각 데이터 세트에 대해 저자는 10개의 고유한 템플릿을 구성하고 프롬프트를 더욱 다양화합니다.

47
00:04:16,680 --> 00:04:20,600
이 템플릿 중 최대 3개는 역방향 형식이었습니다.

48
00:04:20,600 --> 00:04:27,040
예를 들어, 모델에게 주어진 감정이나 질문에 대한 영화 리뷰를 작성하도록 요청합니다.

49
00:04:27,040 --> 00:04:31,680
제공된 답변에 대해.

50
00:04:31,680 --> 00:04:35,280
12개의 작업 중 일부는 생성 작업입니다.

51
00:04:35,280 --> 00:04:41,640
디코더 전용 모델이기 때문에 Flan과 마찬가지로 잘 작동합니다.

52
00:04:41,640 --> 00:04:46,480
반면에 Close QA와 같은 분류 작업인 작업도 있습니다.

53
00:04:46,480 --> 00:04:48,960
또는 감정 분석.

54
00:04:48,960 --> 00:05:00,000
저자는 모델이 선택할 수 있도록 옵션을 사용하여 모델에 대한 프롬프트를 수정했습니다.

55
00:05:00,000 --> 00:05:04,000
다음 생성 토큰으로 이러한 옵션 중 하나를 사용합니다.

56
00:05:04,000 --> 00:05:12,440
여기 그림에는 감정 분석 작업에 대한 옵션이 추가된 것을 볼 수 있습니다.

57
00:05:12,440 --> 00:05:19,360
저자는 사전 훈련된 언어 버전인 Lambda PT를 기본 모델로 사용합니다.

58
00:05:19,360 --> 00:05:23,440
대화 애플리케이션용 모델도 Google에서 제공합니다.

59
00:05:23,440 --> 00:05:31,120
Lambda가 미세 조정된 다른 버전의 Lambda 모델이 있지만 이러한 모델은

60
00:05:31,120 --> 00:05:36,280
인간의 피드백과 대화 품질을 사용하여 더욱 세부적으로 조정됩니다.

61
00:05:36,280 --> 00:05:42,400
그러나 Flan 작성자는 사전 훈련된 버전을 사용하기로 결정했습니다.

62
00:05:42,400 --> 00:05:48,040
감마(Gamma)는 디코더 전용 모델로 1,370억 개의 매개변수를 가지고 있습니다.

63
00:05:48,040 --> 00:05:54,440
웹 문서, 대화 데이터, 위키피디아 데이터를 총 2.8개로 사전 학습했습니다.

64
00:05:54,440 --> 00:05:55,440
조 토큰.

65
00:05:55,440 --> 00:06:00,080
이 훈련 데이터의 10%는 영어가 아닌 언어로 되어 있습니다.

66
00:06:00,080 --> 00:06:06,240
데이터 세트를 샘플링하기 위해 저자는 모든 데이터 세트 예제를 단일 풀에 혼합합니다.

67
00:06:06,240 --> 00:06:11,800
그런 다음 모든 데이터 세트를 잘 유지하기 위해 이 풀에서 무작위로 데이터를 가져옵니다.

68
00:06:11,800 --> 00:06:18,200
최종 훈련 데이터에 표현된 각 데이터 세트의 샘플링은 30k로 유지되었습니다.

69
00:06:18,200 --> 00:06:22,480
데이터 세트의 규모가 크든 작든 상관없습니다.

70
00:06:22,480 --> 00:06:32,080
기본 모델과 유사하게 입력에는 1,000개의 토큰이 있었고 출력에는 256개의 토큰이 있었습니다.

71
00:06:32,080 --> 00:06:38,920
더 짧은 입력 길이를 처리하는 동안 패킹을 사용하여 여러 개의 더 짧은 입력 길이를 결합합니다.

72
00:06:38,920 --> 00:06:43,240
훈련을 위해 예제를 단일 시퀀스로 만듭니다.

73
00:06:43,240 --> 00:06:50,760
그라디언트 단계 수와 배치 크기 및 입력 길이를 곱하면

74
00:06:50,760 --> 00:06:59,320
Flan은 약 2,570억 개의 입력 토큰에 대해 교육을 받은 것으로 보입니다.

75
00:06:59,320 --> 00:07:06,200
저자는 AdaFactor를 최적화 도구로 사용했는데, 이는 Adam과 유사합니다.

76
00:07:06,200 --> 00:07:11,400
적응 속도가 있지만 메모리 효율성이 더 높습니다.

77
00:07:11,400 --> 00:07:18,200
훈련에는 배치 크기가 8k인 총 30k 경사 단계에 대해 60시간이 걸렸습니다.

78
00:07:18,200 --> 00:07:23,760
이 슬라이드는 논문의 주요 결과를 보여줍니다.

79
00:07:23,760 --> 00:07:30,880
그래프는 4가지 데이터 세트에 대한 다양한 모델의 제로샷 성능을 보여줍니다.

80
00:07:30,880 --> 00:07:33,080
클러스터를 내놓았습니다.

81
00:07:33,080 --> 00:07:39,120
보시다시피 별표로 표시된 Flan은 항상 기본 모델보다 성능이 뛰어납니다.

82
00:07:39,120 --> 00:07:41,400
파란색 점으로 표시됩니다.

83
00:07:41,400 --> 00:07:47,120
많은 경우 Flan은 GPT-3 및 GLAM보다 성능이 뛰어납니다.

84
00:07:47,120 --> 00:07:52,920
GLAM은 MOE 구조를 갖춘 일반 언어 모델입니다.

85
00:07:52,920 --> 00:08:00,280
번역이나 자연어 추론과 같은 일부 작업의 경우 Flan은

86
00:08:00,280 --> 00:08:02,200
감독 모델도 마찬가지입니다.

87
00:08:02,200 --> 00:08:08,160
여기에서 지도 모델은 회색 수직선으로 표시되며 다음 중 하나입니다.

88
00:08:08,160 --> 00:08:12,360
BERT 또는 T5 기반.

89
00:08:12,360 --> 00:08:18,080
또 다른 실험에서 Flan은 자신을 GPT-3과 비교합니다.

90
00:08:18,080 --> 00:08:23,840
이 그래프에서 볼 수 있듯이 Flan은 제로 샷 설정과 소수 샷 설정 모두에서 GPT-3보다 성능이 뛰어납니다.

91
00:08:23,840 --> 00:08:26,200
일부 작업.

92
00:08:26,200 --> 00:08:32,200
하지만 저자는 이러한 작업이 성능 향상 정도에 따라 선택된다고 말합니다.

93
00:08:32,200 --> 00:08:35,120
명령 튜닝을 통해 얻은 것입니다.

94
00:08:35,120 --> 00:08:39,320
이것이 일종의 최선의 비교입니다.

95
00:08:39,320 --> 00:08:44,960
이번 프레젠테이션에서 제가 다루게 될 몇 가지 절제 연구들이 있습니다.

96
00:08:44,960 --> 00:08:48,520
첫 번째는 클러스터 수입니다.

97
00:08:48,520 --> 00:08:55,920
이 연구에서 저자는 훈련 중에 클러스터를 하나씩 추가합니다.

98
00:08:56,640 --> 00:09:02,120
가장 왼쪽에 있고 7이 가장 오른쪽에 있습니다.

99
00:09:02,120 --> 00:09:08,600
보시다시피, 학습 중에 사용한 클러스터 수를 추가하면,

100
00:09:08,600 --> 00:09:14,720
미세 조정된 모델의 성능은 일반적으로 유지되는 클러스터에 대해 향상됩니다.

101
00:09:14,720 --> 00:09:20,000
감각, 자연어 추론, 클로즈북 키웨이 등이 있습니다.

102
00:09:20,080 --> 00:09:26,160
여기서 주목해야 할 또 다른 점은 클러스터 번호 3 이후에만 성능이 향상된다는 것입니다.

103
00:09:26,160 --> 00:09:33,160
실제로 미세 조정된 모델의 성능이 기본 모델의 성능보다 더 좋습니다.

104
00:09:33,520 --> 00:09:38,080
여기에는 수평 점선으로 표시됩니다.

105
00:09:38,080 --> 00:09:43,960
이제 이러한 클러스터의 순열이나 순서에 대한 다른 실험이 없기 때문에

106
00:09:43,960 --> 00:09:49,960
추가되는 항목 중 이러한 성능 향상이 바로 이 특정 항목 때문인지 확실하지 않습니다.

107
00:09:50,040 --> 00:09:56,040
추가되는 비교 클러스터를 읽거나 클러스터 수 때문에.

108
00:09:56,040 --> 00:10:03,040
또 다른 절제 연구는 다양한 모델 크기에 대한 미세 조정의 영향에 관한 것입니다.

109
00:10:05,200 --> 00:10:12,200
저자는 미세 조정 후에 모델의 정확도가 실제로 더 나빠진다는 사실을 발견했습니다.

110
00:10:13,200 --> 00:10:20,200
여기서 모델이 충분히 크지 않으면 컷은 80억 정도 될 것 같습니다.

111
00:10:21,600 --> 00:10:24,360
그리고 680억.

112
00:10:24,360 --> 00:10:29,720
제가 이 논문에서 다룰 마지막 절제 연구는 지침의 역할입니다.

113
00:10:29,720 --> 00:10:36,720
앞서 언급했듯이 명령어 조정은 미세 조정이며 명령어로 설명되는 여러 작업입니다.

114
00:10:38,360 --> 00:10:39,880
인간의 언어로.

115
00:10:39,920 --> 00:10:45,800
이제 저자는 제공된 지침이 중요한지 확인하기 위해 이 실험을 수행합니다.

116
00:10:45,800 --> 00:10:46,880
아니면.

117
00:10:46,880 --> 00:10:53,200
그들은 지침이 없고 데이터 세트만 있는 모델의 다른 버전을 미세 조정합니다.

118
00:10:53,200 --> 00:11:00,200
또 다른 하나는 데이터 세트의 이름과 데이터 자체만 있고 지침은 없습니다.

119
00:11:01,640 --> 00:11:06,440
그런 다음 지침이 있든 없든 평가를 수행합니다.

120
00:11:06,520 --> 00:11:12,520
그들은 모델을 훈련할 때 가장 좋은 조합이 다음을 사용하여 미세 조정한다는 것을 알고 있습니다.

121
00:11:12,520 --> 00:11:19,520
인간이 지침을 제공한 다음 지침을 사용하여 평가하기도 합니다.

122
00:11:20,720 --> 00:11:27,720
오늘 제가 다룰 두 번째 논문은 Hugging Face의 T0입니다.

123
00:11:28,240 --> 00:11:31,520
여기서의 아이디어는 Flan과 매우 유사합니다.

124
00:11:31,520 --> 00:11:38,520
저자는 멀티태스킹 훈련과 제로샷 일반화를 수행합니다.

125
00:11:38,600 --> 00:11:44,640
Flan과 마찬가지로 T0 작성자도 약 60개의 데이터 세트를 12개의 클러스터로 그룹화합니다.

126
00:11:44,640 --> 00:11:51,640
그러나 두 클러스터링 사이에는 일부 중복이 있지만 둘 다 다릅니다.

127
00:11:51,640 --> 00:11:56,920
그들이 사용하는 데이터 세트와 이를 그룹화하는 방법에 관한 것입니다.

128
00:11:56,920 --> 00:12:02,400
특히 T0 작성자는 형식을 기반으로 데이터 세트를 그룹화하는 데 의존했습니다.

129
00:12:02,400 --> 00:12:07,440
작업을 수행하는 데 필요한 규모가 아닌 출력의 규모입니다.

130
00:12:07,440 --> 00:12:14,440
Flan과 달리 T0 작성자는 프롬프트 템플릿을 생성하기 위해 클라우드 소싱을 사용합니다.

131
00:12:17,200 --> 00:12:24,200
또한 영어가 아닌 언어를 사용하는 프롬프트와 같은 비영어 프롬프트도 제외했습니다.

132
00:12:24,880 --> 00:12:27,840
또는 그에 따른 수학을 가지고 있는 사람들.

133
00:12:27,840 --> 00:12:34,840
마지막으로 잠재적으로 유해한 콘텐츠가 포함된 프롬프트를 제거하는 작업을 수행했습니다.

134
00:12:35,480 --> 00:12:40,980
이 프롬프트 수집 후에 그들은 당시에 사용되었던 프롬프트의 공개 풀을 출시했습니다.

135
00:12:40,980 --> 00:12:47,980
글을 쓰면서 2000개 이상의 프롬프트가 포함된 177개의 데이터 세트를 다루었습니다.

136
00:12:49,120 --> 00:12:53,920
저자는 T5의 언어 모델 채택 버전을 기본 모델로 사용합니다.

137
00:12:53,920 --> 00:13:00,920
이는 T5가 Bertostal 노이즈 제거 목표에 대해 훈련을 받았고 저자가 주장하기 때문입니다.

138
00:13:01,160 --> 00:13:08,160
LM 채택 변형에 도입된 언어 모델 목표는 이를 더욱 유용하게 만듭니다.

139
00:13:09,200 --> 00:13:11,840
명령 튜닝을 위해.

140
00:13:11,840 --> 00:13:17,840
이 모델에는 110억 개의 매개변수가 있는 인코더, 디코더 아키텍처가 있습니다.

141
00:13:18,760 --> 00:13:25,760
Flan과 마찬가지로 T0 작성자는 혼합된 데이터 풀에서 훈련 샘플을 무작위로 추출했습니다.

142
00:13:25,760 --> 00:13:26,760
세트.

143
00:13:26,760 --> 00:13:32,760
그러나 그들은 각 데이터 세트의 샘플 수를 30K가 아닌 500K로 유지했습니다.

144
00:13:32,760 --> 00:13:38,160
여기서도 입력 및 출력 토큰 길이는 1,000개의 입력이 있는 Flan과 유사합니다.

145
00:13:38,160 --> 00:13:41,080
256개의 출력 토큰.

146
00:13:41,320 --> 00:13:48,320
T0는 배치 크기가 1,000개 토큰인 총 2,500억 개의 토큰으로 훈련됩니다.

147
00:13:50,120 --> 00:13:56,120
Flan과 유사하게 Edda Factor Optimizer가 T0 훈련에 다시 사용되었습니다.

148
00:13:56,120 --> 00:14:01,120
이 슬라이드는 논문의 주요 결과를 보여줍니다.

149
00:14:01,120 --> 00:14:08,120
그림은 T0의 제로 샷 성능을 기본 모델과 비교한 것입니다.

150
00:14:08,520 --> 00:14:12,280
다양한 크기의 GPT-3.

151
00:14:12,280 --> 00:14:17,280
그리고 여기서는 4개의 서로 다른 작업 클러스터가 보류 작업으로 사용됩니다.

152
00:14:17,280 --> 00:14:24,280
이는 제로샷 평가이기 때문에 작성자는 베이스에 대한 결과를 모두 보고합니다.

153
00:14:25,600 --> 00:14:30,240
각 데이터 세트에 대한 모든 프롬프트에서 모델 및 T0.

154
00:14:30,240 --> 00:14:33,960
이것이 여러 개의 파란색 또는 녹색 점이 보이는 이유입니다.

155
00:14:33,960 --> 00:14:40,960
그들은 체리 따기를 하지 않으며 평균이나 중앙값을 보고하지 않습니다.

156
00:14:40,960 --> 00:14:47,600
이는 단일 숫자인 GPT-3의 다른 모든 데이터 포인트와 대조됩니다.

157
00:14:47,600 --> 00:14:50,680
GPT-3 논문에서 나온 것입니다.

158
00:14:50,680 --> 00:14:57,680
보시다시피 T0은 항상 기본 모델보다 성능이 뛰어납니다.

159
00:14:58,680 --> 00:15:05,680
또 다른 눈에 띄는 결과는 T0가 모든 자연어 이해에서 GPT-3보다 뛰어난 성능을 보인다는 점입니다.

160
00:15:05,800 --> 00:15:09,800
작업은 11배 더 작더라도 마찬가지입니다.

161
00:15:09,800 --> 00:15:16,160
폐지 연구에서 저자는 폐지 기간 동안 프롬프트 수의 효과를 연구했습니다.

162
00:15:16,160 --> 00:15:21,120
출력 모델의 성능에 대한 훈련.

163
00:15:21,120 --> 00:15:27,080
프롬프트가 없는 모델은 어떠한 프롬프트로도 훈련되지 않았기 때문에 기본 모델입니다.

164
00:15:27,080 --> 00:15:34,080
녹색으로 표시된 것은 T0 모델로, 1회당 평균 거의 8개의 프롬프트가 표시됩니다.

165
00:15:35,720 --> 00:15:37,560
데이터 세트.

166
00:15:37,560 --> 00:15:44,240
보시다시피 프롬프트 수를 늘리면 모델 성능이 향상됩니다.

167
00:15:44,240 --> 00:15:49,840
그리고 대부분의 경우 단일 프롬프트를 추가하는 것만으로도 성능을 향상시키기에 충분합니다.

168
00:15:49,840 --> 00:15:52,520
모델의 크게.

169
00:15:52,520 --> 00:15:59,200
이전 결과와 유사하게 여기의 T0 작성자는 모든 프롬프트에 대한 결과를 보고합니다.

170
00:15:59,200 --> 00:16:05,480
그래서 그 사이에 직사각형 사이에 간격이 있고 직사각형은

171
00:16:05,480 --> 00:16:09,440
성능의 중간 50%.

172
00:16:09,440 --> 00:16:16,440
보시다시피 프롬프트 수가 적은 경우 1개 또는 5개라고 가정해 보겠습니다.

173
00:16:16,440 --> 00:16:22,400
프롬프트가 0인 경우와 비교하여 중간 성능이 개선되었으나 여전히 크거나 큰 문제가 있습니다.

174
00:16:23,280 --> 00:16:27,000
다양한 프롬프트에서 볼 수 있는 차이.

175
00:16:27,000 --> 00:16:30,680
일부 작업의 경우 프롬프트가 많으면 성능이 저하됩니다.

176
00:16:30,680 --> 00:16:37,680
그래프에서 볼 수 있듯이 이는 프롬프트 간의 품질이 다양하기 때문일 수 있습니다.

177
00:16:38,880 --> 00:16:39,880
사용됩니다.

178
00:16:39,880 --> 00:16:44,320
따라서 프롬프트 수에 더 많은 것을 추가할수록 더 좋은 것은 아닙니다.

179
00:16:44,320 --> 00:16:46,160
결과는 될 것입니다.

180
00:16:46,160 --> 00:16:50,800
이 슬라이드는 T0과 Flan의 개요 비교를 제공합니다.

181
00:16:50,880 --> 00:16:55,920
이 두 모델 모두 명령어 튜닝을 사용하여 언어의 제로샷 성능을 향상시킵니다.

182
00:16:55,920 --> 00:16:56,920
모델.

183
00:16:56,920 --> 00:17:02,920
Flan은 디코더 전용 모델을 기본 모델로 사용하는 반면 T0은 인코더, 디코더 모델을 사용합니다.

184
00:17:04,920 --> 00:17:08,080
11배 더 작습니다.

185
00:17:08,080 --> 00:17:13,320
실제로 폐지론자 연구에서 Flan의 실험은 명령 조정이 다음과 같은 것으로 나타났습니다.

186
00:17:13,320 --> 00:17:20,320
더 작은 모델의 경우 T0가 수행한 것과 일치하지 않는 성능이 저하될 수 있습니다.

187
00:17:20,880 --> 00:17:21,800
실험.

188
00:17:21,800 --> 00:17:27,800
Flan의 데이터세트에는 10%의 비영어권 데이터가 포함되어 있는 반면 T0은 모든 비영어권 데이터를 제외했습니다.

189
00:17:31,200 --> 00:17:34,800
데이터세트와 훈련 데이터세트의 데이터.

190
00:17:34,800 --> 00:17:41,800
Flan이 번역 결과를 보고할 때 실험 결과에서 이를 확인할 수 있습니다.

191
00:17:42,520 --> 00:17:48,880
이러한 유형의 작업에서는 지도 모델과 번역의 성능을 능가합니다.

192
00:17:48,880 --> 00:17:52,160
작업은 T0 결과에 포함되지 않습니다.

193
00:17:52,160 --> 00:17:58,160
이 두 논문 모두 데이터 세트를 발표했으며 Flan V2는 Flan V2의 최신 버전입니다.

194
00:17:58,160 --> 00:18:04,160
Flan은 실제로 T0의 p3 데이터세트도 포함합니다.

195
00:18:04,160 --> 00:18:11,000
두 논문은 보류 작업에 대한 모델을 훈련하는 방법도 다릅니다.

196
00:18:11,000 --> 00:18:18,000
Flan은 보류된 작업당 단일 모델을 훈련하는 반면 T0은 3개의 보류된 작업에 대해 하나의 모델을 훈련합니다.

197
00:18:19,160 --> 00:18:26,160
작업을 함께 수행하면 다양한 세트에 대한 모델의 생성 가능성을 더 잘 추정할 수 있습니다.

198
00:18:28,840 --> 00:18:30,080
보이지 않는 임무.

199
00:18:30,080 --> 00:18:37,080
T0의 간략한 토론 섹션 외에는 두 논문을 비교하지 않습니다.

200
00:18:37,840 --> 00:18:44,840
하지만 둘 다 다음과 같은 경우 자연어 추론 작업에 대한 결과를 보고합니다.

201
00:18:45,400 --> 00:18:52,400
그들은 자신을 GPT-3과 비교합니다. 결과에 따르면, 그들은 매우 유사한 성능을 가지고 있습니다.

202
00:18:52,520 --> 00:18:57,760
대부분의 자연어 추론 데이터세트에 적용됩니다.

203
00:18:57,760 --> 00:19:04,760
단 하나의 데이터세트만 있습니다. SuperGrowl의 약속 은행인 T0이 훨씬 더 뛰어난 성능을 발휘합니다.

204
00:19:05,480 --> 00:19:12,480
Flan은 두 모델 모두 약 2,500억 개의 토큰에 대해 교육을 받았다고 가정합니다.

205
00:19:14,920 --> 00:19:21,920
Lambda PT 모델이 충분히 훈련되지 않아 유사한 성능을 가질 가능성이 있습니다.

206
00:19:21,920 --> 00:19:27,400
훨씬 더 많은 수의 매개변수를 가지면서 T0으로 변경됩니다.

207
00:19:27,400 --> 00:19:33,680
최상의 지침을 얻기 위해 두 개의 훈련 데이터 세트를 혼합한 최신 논문이 있습니다.

208
00:19:33,680 --> 00:19:40,680
결과를 대규모로 조정합니다. 이 논문에서는 Flan T5와 Flan POM도 훈련하지만 다음 논문에서는

209
00:19:41,680 --> 00:19:48,680
나는 단지 미세 조정을 확대하는 것보다 다른 방향을 취하는 것에 대해 다룰 것입니다.

210
00:19:48,680 --> 00:19:54,680
MetaAI의 정렬을 위해 Lima 이하에 대해 이야기해 보겠습니다.

211
00:19:54,680 --> 00:20:00,680
본 논문의 가설은 모델이 사전 훈련 중에 지식을 학습했다는 것입니다.

212
00:20:02,680 --> 00:20:08,680
정렬은 상호작용에 필요한 스타일이나 형식만 가르칠 뿐입니다.

213
00:20:08,680 --> 00:20:15,680
사용자. 이 가설을 사용하면 제한된 명령 미세 조정 또는 제한된 명령만

214
00:20:15,680 --> 00:20:21,680
고품질 출력을 생성하도록 모델을 교육하려면 데이터 조정이 필요합니다.

215
00:20:21,680 --> 00:20:28,680
이 가설을 테스트하기 위해 Lima는 신중하게 선별된 프롬프트 1,000개만 만드는 데 중점을 둡니다.

216
00:20:30,680 --> 00:20:36,680
다양한 입력 스타일을 가지고 있지만 유용한 보조자 형식의 통일된 출력 스타일을 가지고 있습니다.

217
00:20:37,680 --> 00:20:42,680
일체 포함. 리마 저자는 다양한 소스에서 프롬프트 데이터를 선택합니다.

218
00:20:42,680 --> 00:20:48,680
첫 번째 소스는 170개 이상의 다양한 거래소가 있는 StackExchange입니다. 스택 오버플로

219
00:20:48,680 --> 00:20:55,680
예를 들어 유명한 것 중 하나입니다. 각 StackExchange에 대해 저자는 200개를 샘플링합니다.

220
00:20:57,680 --> 00:21:03,680
질문. 가장 높은 점수를 받은 질문을 선택했지만,

221
00:21:03,680 --> 00:21:08,680
본문에 오버플로 없이 제목에 자체 포함되어 있습니다.

222
00:21:08,680 --> 00:21:14,680
또한 선택한 각 질문에 대한 상위 답변도 선택했습니다. 답변이 있는 한, 상위 답변

223
00:21:14,680 --> 00:21:21,680
최소한 플러스 10점 이상을 받았습니다. 마지막으로 그렇지 않은 답변은 제외했습니다.

224
00:21:22,680 --> 00:21:28,680
도움이 되는 어시스턴트 스타일과 일치하세요. 예를 들어 답변이 너무 짧거나 너무 길거나

225
00:21:28,680 --> 00:21:35,680
1인칭 형식으로 작성되었거나 답변이 독립적인 참조가 아닌 경우

226
00:21:39,680 --> 00:21:45,680
추가 정보를 얻기 위해 다른 게시물에 대해서는 모든 내용을 제외했습니다. 그리고 마침내 그들은 제거했습니다

227
00:21:45,680 --> 00:21:52,680
응답의 링크와 이미지. Vikiha는 저자가 작성한 두 번째 데이터 소스였습니다.

228
00:21:53,680 --> 00:22:00,680
프롬프트 생성에 사용됩니다. 그들은 2단계 샘플링 방법으로 200개의 기사를 샘플링했습니다. 와 함께

229
00:22:01,680 --> 00:22:07,680
이 방법을 사용하여 기존 17개의 기사 카테고리 중 하나의 카테고리를 먼저 샘플링했습니다.

230
00:22:08,680 --> 00:22:15,680
그런 다음 해당 카테고리 내에서 기사를 샘플링하여 전체 샘플이

231
00:22:16,680 --> 00:22:23,680
그들이 가지고 있는 수는 가능한 한 다양합니다. 기사 제목을 입력 프롬프트로 사용했습니다.

232
00:22:24,680 --> 00:22:30,680
그리고 몸이 반응합니다. 마찬가지로 그들은 이미지를 제거하기 위해 일부 처리를 수행했으며

233
00:22:31,680 --> 00:22:37,680
응답의 링크 및 기타 태그.

234
00:22:38,680 --> 00:22:44,680
Reddit 데이터 세트는 Lima 작성자가 교육 생성에 사용한 세 번째 데이터 소스였습니다.

235
00:22:47,680 --> 00:22:53,680
데이터세트. Reddit은 대부분 재미보다는 오락의 목적으로 사용되기 때문에

236
00:22:53,680 --> 00:23:00,680
실제로 도움이 되고, 최고 투표를 받은 답변은 냉소적이거나 ​​재미있을 수 있지만 반드시 그런 것은 아닙니다.

237
00:23:00,680 --> 00:23:07,680
도움이 되는 어시스턴트 스타일의 형식입니다. 그러한 답변을 피하기 위해 저자는 수동으로만 선택했습니다.

238
00:23:08,680 --> 00:23:15,680
가장 많이 투표된 상위 게시물의 예를 보여주며 이를 두 개의 하위 레딧으로만 제한했습니다. 승천,

239
00:23:16,680 --> 00:23:23,680
누군가가 전제를 제공하는 하위 레딧인 QA 및 R 프롬프트 형식의 QA입니다.

240
00:23:24,680 --> 00:23:30,680
허구의 이야기를 만들어 사용자가 그 이야기를 창의적으로 완성하도록 권장합니다.

241
00:23:34,680 --> 00:23:40,680
데이터 세트 생성의 마지막 단계로 작성자는 프롬프트를 수동으로 작성했습니다. 그들은 두 개를 사용했습니다

242
00:23:41,680 --> 00:23:46,680
교육을 위한 다양한 작성자 그룹과 평가가 유지되는지 확인하기 위한 테스트

243
00:23:47,680 --> 00:23:53,680
편견이나 오염이 없는 곳입니다. 작성자는 프롬프트에 대한 응답도 작성했습니다.

244
00:23:54,680 --> 00:23:59,680
훈련 세트에서. 그들은 유용한 보조 AI 스타일로 질문에 답변했습니다.

245
00:24:00,680 --> 00:24:05,680
질문을 인정한 후 답변을 하며, 이 형식이 도움이 될 것이라고 주장합니다.

246
00:24:06,680 --> 00:24:13,680
생각의 기차를 형성하는 모델. 훈련 세트를 더욱 다양화하기 위해 그들은 단일을 선택했습니다.

247
00:24:14,680 --> 00:24:21,680
요약과 같은 50가지 자연어 생성 작업의 예를 만들고 이를

248
00:24:22,680 --> 00:24:29,680
훈련 데이터 세트도 마찬가지입니다. 훈련을 위해 저자는 650억 개의 매개변수가 있는 Lama를 기본으로 사용했습니다.

249
00:24:30,680 --> 00:24:37,680
모델 및 입력 크기는 2000 토큰입니다. 그들은 수집된 1000개의 예시를 바탕으로 모델을 훈련시켰고

250
00:24:37,680 --> 00:24:44,680
단일 에포크를 갖기에는 데이터가 충분하지 않았기 때문에 15개의 에포크에 대해 배치 크기를 32로 했습니다. 그러나 그들은

251
00:24:45,680 --> 00:24:52,680
또한 평가를 기반으로 5번째와 10번째 신기원 사이의 모델 체크포인트를 수동으로 선택했습니다.

252
00:24:53,680 --> 00:25:00,680
50일에 15 에포크 이후 마지막 체크포인트만 사용하는 대신 적자를 버텼습니다. 그들

253
00:25:00,680 --> 00:25:07,680
또한 사용자와 어시스턴트를 구별하기 위해 특별한 턴 종료 토큰을 사용했습니다.

254
00:25:08,680 --> 00:25:15,680
역할. 그들은 Adam W를 최적화 도구로 사용했는데, 이는 다음과 같은 점을 제외하면 Adam과 매우 유사합니다.

255
00:25:16,680 --> 00:25:23,680
그들은 가중치 감소가 구현되는 방식을 수정했습니다. 그들은 드롭아웃을 사용했습니다.

256
00:25:23,680 --> 00:25:30,680
잔여 네트워크. 그들은 가장 낮은 수준에서 비율 0으로 시작하여 선형적으로 증가했습니다.

257
00:25:31,680 --> 00:25:39,680
마지막 레이어의 드롭아웃 비율은 2.5입니다. 라마의 효율성을 설정하기 위해 저자는 인간에게 물었습니다.

258
00:25:40,680 --> 00:25:47,680
평가자는 출력을 비교하고 출력을 평균 출력과 비교합니다.

259
00:25:47,680 --> 00:25:54,680
프롬프트를 표시하고 Lama의 응답을 선호하는지 다른 모델의 응답을 선호하는지 지정하는 다른 모델

260
00:25:55,680 --> 00:26:03,680
응답하거나 동점이라고 생각하는 경우. 주관적이고 매우 주관적인 작업이므로,

261
00:26:04,680 --> 00:26:10,680
저자는 또한 무작위로 선택된 일부 주석에 대해 주석자 간 일치도를 측정했으며

262
00:26:10,680 --> 00:26:17,680
그들은 주석자 간 합의가 정말 높다는 것을 보여주었습니다. 인간 평가자들은 라마를 다음과 비교했습니다.

263
00:26:18,680 --> 00:26:25,680
이 5가지 모델을 일대일로 설정하세요. 결과 요약은 오른쪽 그래프에 표시됩니다.

264
00:26:26,680 --> 00:26:34,680
74%의 경우에 라마와 알파카를 비교한 결과는 다음과 같습니다.

265
00:26:34,680 --> 00:26:43,680
74%의 사례에서 라마와 알파카를 비교했을 때 인간 평가자는 라마를 선호했습니다.

266
00:26:44,680 --> 00:26:50,680
또는 그들은 그것이 동점이라고 생각했습니다. 이것이 인상적인 것은 알파카 모델과 동일하면서도

267
00:26:51,680 --> 00:27:03,680
52배 더 많은 데이터에 대해 학습되었습니다. 또 다른 인상적인 결과는 43%에서

268
00:27:04,680 --> 00:27:12,680
이 경우 인간 평가자들은 GPT-4보다 Lama를 선호했거나 두 사람이

269
00:27:13,680 --> 00:27:21,680
동점이었습니다. 또 다른 평가에서 저자는 GPT-4를 평가자로 사용하고 비교를 수행했습니다.

270
00:27:22,680 --> 00:27:28,680
결과는 전반적으로 비슷한 경향을 보였습니다. 그러나 GPT-4는 더 큰 모델의 답변을 더 선호했습니다.

271
00:27:28,680 --> 00:27:36,680
자주. 흥미롭게도 90%의 경우 GPT-4는 실제로 Lama의 반응을 선호했습니다.

272
00:27:37,680 --> 00:27:45,680
자신의 반응보다. 명칭 연구에서 저자는 품질을 실험했습니다.

273
00:27:46,680 --> 00:27:53,680
그리고 훈련 데이터의 다양성. 그렇게 하기 위해 그들은 70억 버전의 모델에서 찾습니다.

274
00:27:53,680 --> 00:28:00,680
2,000개의 예시가 있습니다. 그래서 그들은 예시의 수를 정했지만 이러한 예시의 출처는

275
00:28:01,680 --> 00:28:07,680
다른 데이터 세트에서 나왔습니다. 그런 다음 GPT-4에 다음의 유용성을 평가해 달라고 요청했습니다.

276
00:28:08,680 --> 00:28:14,680
훈련된 각 모델은 6개가 가장 높고 1개가 가장 유용합니다.

277
00:28:15,680 --> 00:28:22,680
각 데이터 세트에 대해 테스트 분할에서 5개의 예가 이 평가를 위해 무작위로 선택되었습니다.

278
00:28:23,680 --> 00:28:30,680
막대 차트에서 볼 수 있듯이 첫 번째 데이터 세트는 Vikiha에서 나옵니다.

279
00:28:31,680 --> 00:28:37,680
이 데이터 세트는 고도로 조정된 데이터 세트이기 때문에 품질은 높지만 다양성은 낮습니다.

280
00:28:38,680 --> 00:28:43,680
품질은 높지만 모든 프롬프트는 다양성이 낮은 방법에 대한 형식으로 되어 있습니다.

281
00:28:43,680 --> 00:28:57,680
여기의 두 번째 데이터 세트는 필터링되지 않은 스택 교환입니다. 주제가 다양하기 때문입니다.

282
00:28:58,680 --> 00:29:04,680
다양성은 높지만 필터링되지 않았기 때문에 품질이 낮습니다.

283
00:29:05,680 --> 00:29:12,680
가장 성능이 좋은 마지막 열은 필터링된 스택 교환입니다.

284
00:29:13,680 --> 00:29:18,680
높은 품질과 높은 다양성을 모두 갖추고 있기 때문에 최고의 성능을 발휘합니다.

285
00:29:19,680 --> 00:29:24,680
이 실험은 데이터의 품질과 다양성 모두의 중요성을 확인시켜줍니다.

286
00:29:25,680 --> 00:29:32,680
명령어 튜닝에 매우 중요합니다. 지금까지 제 이야기를 들어주신 모든 분들께 감사드립니다.

287
00:29:32,680 --> 00:29:37,680
다음으로 호세인과 함께 강화학습 및 피드백에 대해 이야기해보겠습니다.