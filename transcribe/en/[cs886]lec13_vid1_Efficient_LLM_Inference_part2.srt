1
00:00:00,000 --> 00:00:04,080
So I'll be taking off for the next three papers.

2
00:00:04,080 --> 00:00:07,600
So essentially what we're discussing today has been,

3
00:00:07,600 --> 00:00:12,200
how do we speed up inference and techniques surrounding it?

4
00:00:12,200 --> 00:00:15,640
Nothing really locked to the particular model and

5
00:00:15,640 --> 00:00:18,320
that would be a common trend even for

6
00:00:18,320 --> 00:00:20,760
the papers which I'll be going through.

7
00:00:20,760 --> 00:00:24,480
So as a starter,

8
00:00:24,480 --> 00:00:27,960
at least for the three papers which I'm covering,

9
00:00:28,520 --> 00:00:31,480
and in the past weeks at least what we have looked at

10
00:00:31,480 --> 00:00:35,080
in improving the LLM efficiency.

11
00:00:35,080 --> 00:00:40,480
We have been mostly looking at how do we speed up the training aspects,

12
00:00:40,480 --> 00:00:44,880
and not really how do we deploy these models in scale.

13
00:00:44,880 --> 00:00:48,200
In the last lecture, we had some aspect of what is

14
00:00:48,200 --> 00:00:52,320
a good architecture to be able to deploy these models on scale.

15
00:00:52,320 --> 00:00:57,200
But that would come under most of the first three categories,

16
00:00:57,200 --> 00:01:01,120
for example, and what we'll be looking at today is the last four.

17
00:01:01,120 --> 00:01:03,160
So we'll be covering speculative,

18
00:01:03,160 --> 00:01:07,800
so we are primarily looking at reducing latency and improving throughput,

19
00:01:07,800 --> 00:01:10,360
and that's the common theme across all the papers.

20
00:01:10,360 --> 00:01:12,440
Some of them target one of those,

21
00:01:12,440 --> 00:01:14,320
some of them try to improve both of them,

22
00:01:14,320 --> 00:01:17,560
and we'll be covering speculative decoding,

23
00:01:17,560 --> 00:01:23,720
BLLMs as the solution for doing this.

24
00:01:24,720 --> 00:01:28,080
So yeah, for the first paper,

25
00:01:28,080 --> 00:01:31,920
we want to efficiently handle LLM requests that are coming in.

26
00:01:31,920 --> 00:01:36,000
So LLM applications have been something which are on the rise in the recent days,

27
00:01:36,000 --> 00:01:39,000
and let's say you have a co-pilot at Cilsrani,

28
00:01:39,000 --> 00:01:42,320
and every single time you make an iteration on the code,

29
00:01:42,320 --> 00:01:47,680
there's an API column going to the co-pilot server and it's responding to you.

30
00:01:47,680 --> 00:01:50,560
So when there are millions of requests coming in,

31
00:01:50,560 --> 00:01:53,520
how do we efficiently handle them and make use of

32
00:01:53,520 --> 00:01:56,040
the infrastructure which we have at hand?

33
00:01:56,040 --> 00:02:06,520
So yeah, so let's look at regular inference procedures.

34
00:02:06,520 --> 00:02:09,800
We already had a look at KQB at attention,

35
00:02:09,800 --> 00:02:14,040
but let's just look at what actually happens during the inference stage.

36
00:02:14,040 --> 00:02:19,440
So let's say we have an input token coming in,

37
00:02:19,440 --> 00:02:21,960
we convert it into an embedding,

38
00:02:21,960 --> 00:02:29,960
let's say x, and then we multiply it with weight matrices which we have learned.

39
00:02:29,960 --> 00:02:35,320
So we have Wk, we have Wq, we have Wv,

40
00:02:35,320 --> 00:02:39,480
so the key query and value weights,

41
00:02:39,480 --> 00:02:46,520
and then we compute K for Kv,

42
00:02:46,520 --> 00:02:50,080
and then let that just be small q,

43
00:02:50,080 --> 00:02:57,000
and then we have K and then V being computed based on these weights.

44
00:02:57,000 --> 00:03:01,800
So that's a matrix and let's say this is a vector,

45
00:03:01,800 --> 00:03:06,400
and this is the embedding of the input token coming in.

46
00:03:06,400 --> 00:03:09,960
So what we are primarily interested is in K and V,

47
00:03:09,960 --> 00:03:11,720
at least for the first paper.

48
00:03:12,040 --> 00:03:20,280
When they're computing attention through the dot product of,

49
00:03:20,280 --> 00:03:27,400
let's say qA transpose divided by root dimension of this,

50
00:03:27,400 --> 00:03:41,680
and then we take the softmax and then we multiply with the value matrix.

51
00:03:41,680 --> 00:03:47,440
So if we are doing this computation,

52
00:03:47,440 --> 00:03:49,400
the naive way to do it,

53
00:03:49,400 --> 00:03:55,520
we would end up recomputing each of the elements in the QK transpose matrix.

54
00:03:55,520 --> 00:04:00,160
So to avoid having to recompute every element,

55
00:04:00,160 --> 00:04:04,040
we have new query tokens coming in for a sequence.

56
00:04:04,040 --> 00:04:06,760
We implement something called the KvCache.

57
00:04:06,760 --> 00:04:08,720
How does the KvCache work?

58
00:04:08,760 --> 00:04:15,560
So let's say I have a sentence saying I am no good,

59
00:04:15,560 --> 00:04:20,640
and we need to do this for each of the input tokens coming in,

60
00:04:20,640 --> 00:04:22,360
and as we do that,

61
00:04:22,360 --> 00:04:26,280
we keep building a KvCache for each token.

62
00:04:26,280 --> 00:04:32,080
So let's say each of these is a token which is coming in.

63
00:04:32,280 --> 00:04:38,320
We create the K matrix and the V matrix,

64
00:04:38,320 --> 00:04:40,360
we store this in memory,

65
00:04:40,360 --> 00:04:43,200
and each of these tokens,

66
00:04:43,200 --> 00:04:47,760
so here it will correspond to a particular column in the K matrix,

67
00:04:47,760 --> 00:04:52,000
and here it will be a particular row in the V matrix.

68
00:04:52,000 --> 00:04:55,200
As we do this over time,

69
00:04:55,200 --> 00:04:57,280
these matrices are growing in size.

70
00:04:57,520 --> 00:05:03,440
So we can say that instead of

71
00:05:03,440 --> 00:05:08,520
recomputing this entire matrix or redoing that computation,

72
00:05:08,520 --> 00:05:12,120
what we can do is that for all the previous tokens,

73
00:05:12,120 --> 00:05:14,320
we can have Kcache,

74
00:05:14,320 --> 00:05:18,200
which is all the columns which have computed so far.

75
00:05:18,200 --> 00:05:20,400
I just save it into my cache.

76
00:05:20,400 --> 00:05:23,320
Then when new tokens come in,

77
00:05:23,320 --> 00:05:25,800
I just add it to my cache,

78
00:05:26,000 --> 00:05:28,560
and this is the only new computation which I do,

79
00:05:28,560 --> 00:05:30,800
and the same thing for the value matrix,

80
00:05:30,800 --> 00:05:32,680
everything which has come so far,

81
00:05:32,680 --> 00:05:35,520
I save it into my Vcache,

82
00:05:35,520 --> 00:05:40,400
and then the new computations go into a new row in the Vcache.

83
00:05:40,400 --> 00:05:42,360
So as you can see,

84
00:05:42,360 --> 00:05:44,800
as we have a large sequence length,

85
00:05:44,800 --> 00:05:48,560
our K and V matrices will keep growing in size.

86
00:05:48,560 --> 00:05:59,560
Now, in the nine implementation,

87
00:05:59,560 --> 00:06:02,160
we need contiguous chunks of memory for this

88
00:06:02,160 --> 00:06:05,400
because of the attention operations which will be happening.

89
00:06:05,400 --> 00:06:08,840
So the first paper looks at how we can break this down

90
00:06:08,840 --> 00:06:12,480
and manage memory better.

91
00:06:12,480 --> 00:06:16,520
So what exactly does the KvCache contain?

92
00:06:16,640 --> 00:06:18,240
As a high-level overview.

93
00:06:18,240 --> 00:06:20,680
So let's say this is my prompt.

94
00:06:20,680 --> 00:06:23,600
So we have that,

95
00:06:23,600 --> 00:06:26,080
and then this is the current snippet,

96
00:06:26,080 --> 00:06:29,120
and the model is generating the response,

97
00:06:29,120 --> 00:06:31,280
and we are currently stopped at IAM.

98
00:06:31,280 --> 00:06:34,720
So at this particular snapshot,

99
00:06:34,720 --> 00:06:38,120
we have four tokens,

100
00:06:38,120 --> 00:06:43,720
but let's say this is a model with a 2048 context length.

101
00:06:44,680 --> 00:06:48,480
You can also, if you have worked with the OpenAI API

102
00:06:48,480 --> 00:06:49,720
or any inference calling,

103
00:06:49,720 --> 00:06:53,360
you can specify a max tokens for how much you want to generate.

104
00:06:53,360 --> 00:06:56,440
So there is an issue of variable sequence lengths,

105
00:06:56,440 --> 00:06:58,520
which we should keep in our minds,

106
00:06:58,520 --> 00:07:03,120
but let's assume that we have a 2048 sequence length.

107
00:07:03,120 --> 00:07:09,680
So essentially, I'll be reserving memory for those 2048 tokens,

108
00:07:09,840 --> 00:07:15,080
and that's shown by the reserve token.

109
00:07:15,080 --> 00:07:19,880
And the model can also choose to terminate early.

110
00:07:19,880 --> 00:07:21,760
So if you gave me a request,

111
00:07:21,760 --> 00:07:23,000
and I'm done with my response,

112
00:07:23,000 --> 00:07:24,360
I can pass the US token,

113
00:07:24,360 --> 00:07:27,720
and then I'm done with what I needed to do.

114
00:07:27,720 --> 00:07:30,920
So this is just a snippet of what is happening into the future,

115
00:07:30,920 --> 00:07:34,040
and this particular response ends at that US token.

116
00:07:35,800 --> 00:07:36,920
And then towards the end,

117
00:07:36,920 --> 00:07:40,840
that green is another prompt which has come in,

118
00:07:40,840 --> 00:07:43,400
and the model is serving back.

119
00:07:43,400 --> 00:07:45,080
So this is just a snippet of memory,

120
00:07:45,080 --> 00:07:46,960
and what the authors are like,

121
00:07:46,960 --> 00:07:49,040
the problem which the authors are trying to solve.

122
00:07:49,040 --> 00:07:52,040
So as we can see,

123
00:07:52,040 --> 00:07:53,920
we have two issues,

124
00:07:53,920 --> 00:07:55,560
which the authors call internet fragmentation

125
00:07:55,560 --> 00:07:57,160
and external fragmentation.

126
00:07:57,160 --> 00:07:59,640
So because of the reserve tokens,

127
00:08:00,640 --> 00:08:04,040
we have a big chunk of memory being allocated

128
00:08:04,040 --> 00:08:05,680
as the request came in.

129
00:08:05,720 --> 00:08:09,360
And because we do not know the output lengths

130
00:08:09,360 --> 00:08:12,360
of to what extent this model is going to consume memory,

131
00:08:12,360 --> 00:08:16,400
we also have some padding between the next token

132
00:08:16,400 --> 00:08:17,520
being saved into memory.

133
00:08:17,520 --> 00:08:20,520
So we have external fragmentation happening as well,

134
00:08:20,520 --> 00:08:22,000
because of the different sequence lengths,

135
00:08:22,000 --> 00:08:24,040
and internal fragmentation is because you don't know

136
00:08:24,040 --> 00:08:27,680
the output length of where this will terminate.

137
00:08:27,680 --> 00:08:30,800
So what the authors find is that

138
00:08:31,800 --> 00:08:35,600
based on this naive allocation of memory,

139
00:08:35,600 --> 00:08:38,120
about 60% remains unused.

140
00:08:38,120 --> 00:08:41,400
So that's the problem which they are trying to solve.

141
00:08:41,400 --> 00:08:44,520
And code wise, if you want to turn off cache

142
00:08:44,520 --> 00:08:45,720
and turn on cache,

143
00:08:45,720 --> 00:08:48,920
we can just use the use cache in TensorFlow.

144
00:08:48,920 --> 00:08:53,800
And if you try to do that on Colab with like a T4,

145
00:08:53,800 --> 00:08:55,960
it will have a performance difference of like five X.

146
00:08:55,960 --> 00:08:57,640
So we do need KV cache,

147
00:08:57,640 --> 00:08:59,400
but we need to do it efficiently,

148
00:08:59,400 --> 00:09:01,000
and this nine way doesn't work.

149
00:09:04,080 --> 00:09:06,720
Just the serving the entire memory block

150
00:09:06,720 --> 00:09:08,480
for the entire sequence length.

151
00:09:08,480 --> 00:09:10,680
So you can free our cache.

152
00:09:10,680 --> 00:09:12,960
Yeah, we cannot be pre-allocating this memory.

153
00:09:12,960 --> 00:09:13,800
To do it efficiently,

154
00:09:13,800 --> 00:09:15,840
there should be some better way to handle this.

155
00:09:15,840 --> 00:09:18,720
And that's what the authors propose in this paper.

156
00:09:19,720 --> 00:09:22,280
Again, just to look at the scale of things.

157
00:09:22,280 --> 00:09:24,560
Before the presentation,

158
00:09:24,560 --> 00:09:27,280
I just had a look at how much code Lama consumes.

159
00:09:27,280 --> 00:09:28,800
So essentially we have,

160
00:09:30,240 --> 00:09:31,080
let's say,

161
00:09:32,720 --> 00:09:34,760
so we have two matrices.

162
00:09:34,760 --> 00:09:37,560
So K and V.

163
00:09:37,560 --> 00:09:38,880
And then we have,

164
00:09:40,360 --> 00:09:43,280
since for inference, we usually do FT16.

165
00:09:43,280 --> 00:09:45,080
So we can say that's two bytes.

166
00:09:46,600 --> 00:09:50,120
And then we need a number of layers.

167
00:09:50,120 --> 00:09:54,440
So code Lama, if you look at the biggest 60 billion

168
00:09:54,440 --> 00:09:55,280
parameter model,

169
00:09:56,200 --> 00:09:57,760
if you look at the 60 billion parameter model

170
00:09:57,760 --> 00:09:59,280
for code Lama.

171
00:09:59,280 --> 00:10:03,120
So that would be approximately 120 GB,

172
00:10:04,080 --> 00:10:05,800
just for the model rates.

173
00:10:05,800 --> 00:10:08,920
But what I'm trying to look at is how much

174
00:10:08,920 --> 00:10:10,560
the KV cache would be.

175
00:10:10,560 --> 00:10:14,400
So the number of layers for the 60 billion parameter model

176
00:10:14,400 --> 00:10:15,240
is 80.

177
00:10:16,200 --> 00:10:20,000
And the embedding dimension of the 60 billion parameter model

178
00:10:20,000 --> 00:10:20,840
is 8192.

179
00:10:21,760 --> 00:10:24,120
Unlike some of the smaller models,

180
00:10:24,120 --> 00:10:26,120
which have 510 and so forth.

181
00:10:26,120 --> 00:10:30,200
And yeah, so essentially we have multi-client on this.

182
00:10:30,200 --> 00:10:33,120
So we have two into two bytes into 80 layers

183
00:10:33,120 --> 00:10:35,200
into 8192.

184
00:10:35,200 --> 00:10:39,600
And again, we have batch size coming into the equation.

185
00:10:39,600 --> 00:10:43,400
But for inference, batch size essentially deals with

186
00:10:43,400 --> 00:10:45,680
how many requests are we processing

187
00:10:45,680 --> 00:10:47,240
rather than the training batch size.

188
00:10:47,240 --> 00:10:50,080
So even if we keep this as one,

189
00:10:50,280 --> 00:10:52,320
we are only serving one at a time.

190
00:10:52,320 --> 00:10:54,960
If you complete this, it comes out to 10 GB.

191
00:10:54,960 --> 00:10:58,840
So just, so the KV cache,

192
00:10:58,840 --> 00:11:01,440
assuming full utilization of the context length

193
00:11:01,440 --> 00:11:03,400
would be around 10 GB.

194
00:11:04,360 --> 00:11:07,600
So yeah, so this is like a parent model

195
00:11:07,600 --> 00:11:08,800
which we have deployed, right?

196
00:11:08,800 --> 00:11:10,840
So code Lama 2 is one of the best,

197
00:11:10,840 --> 00:11:12,640
I wouldn't say best, but for some tasks,

198
00:11:12,640 --> 00:11:14,240
a good open source model.

199
00:11:14,240 --> 00:11:18,680
Is that 10 GB per batch or 10 GB?

200
00:11:18,680 --> 00:11:20,920
Yeah, so it is per request, in a sense.

201
00:11:20,920 --> 00:11:24,320
Is that reused or does he pilot it?

202
00:11:25,600 --> 00:11:28,360
So once you're done, you can obviously free it.

203
00:11:28,360 --> 00:11:30,320
But let's say like...

204
00:11:32,320 --> 00:11:34,440
Yeah, that is the maximum you can consume

205
00:11:34,440 --> 00:11:35,280
for a request.

206
00:11:35,280 --> 00:11:38,720
How much is the cost of 8,192?

207
00:11:38,720 --> 00:11:42,160
Yeah, and this becomes even bigger of an issue

208
00:11:42,160 --> 00:11:44,080
if you consider the new Gemini models which came out

209
00:11:44,080 --> 00:11:46,720
because they boast like 10 million context length.

210
00:11:47,720 --> 00:11:49,120
So yeah, you can see like,

211
00:11:49,120 --> 00:11:51,680
we cannot allocate that much memory contiguously

212
00:11:51,680 --> 00:11:54,840
and we should only be allocating what is required

213
00:11:54,840 --> 00:11:55,680
at this point of time

214
00:11:55,680 --> 00:11:58,720
so that we can process multiple requests as they're happening.

215
00:11:58,720 --> 00:12:00,440
If you exceed this memory, obviously it will crash

216
00:12:00,440 --> 00:12:02,480
and you cannot do anything about it.

217
00:12:02,480 --> 00:12:06,680
So as far as what the authors concern,

218
00:12:06,680 --> 00:12:09,200
they look at 13 billion parameter model

219
00:12:09,200 --> 00:12:12,520
and it consumes around 30% of the available memory

220
00:12:12,520 --> 00:12:13,640
for the KV cache.

221
00:12:14,880 --> 00:12:16,120
I was just trying to make the point

222
00:12:16,120 --> 00:12:18,080
that it's much bigger of an issue right now

223
00:12:18,080 --> 00:12:19,160
if you do it naively.

224
00:12:20,520 --> 00:12:23,480
Yeah, so we cannot have contiguous spaces of memory

225
00:12:23,480 --> 00:12:25,440
being allocated for such huge sizes

226
00:12:25,440 --> 00:12:29,080
and memory sharing is something which cannot be done

227
00:12:29,960 --> 00:12:31,200
in the naive implementation.

228
00:12:31,200 --> 00:12:32,120
Why do we need to do that?

229
00:12:32,120 --> 00:12:34,680
We'll come to in a later slide.

230
00:12:34,680 --> 00:12:39,400
Yeah, so this sounds like something we study in OS

231
00:12:39,400 --> 00:12:41,440
about memory fragmentation and sharing.

232
00:12:41,440 --> 00:12:44,240
Like that's essentially what the authors do as well.

233
00:12:44,280 --> 00:12:47,360
So you can make a logical simile.

234
00:12:47,360 --> 00:12:51,000
So you're considering incoming requests as processes.

235
00:12:51,000 --> 00:12:53,680
You're considering that we define something

236
00:12:53,680 --> 00:12:54,760
as logical KV blocks

237
00:12:54,760 --> 00:12:56,760
which is essentially same as virtual memory

238
00:12:56,760 --> 00:12:58,800
and you also define physical KV blocks

239
00:12:58,800 --> 00:13:01,080
which is the representation in actual physical memory

240
00:13:01,080 --> 00:13:02,640
and that physical memory.

241
00:13:02,640 --> 00:13:05,640
And similar to a page table, you also define a block table.

242
00:13:07,080 --> 00:13:10,360
So what do the authors propose?

243
00:13:10,440 --> 00:13:15,440
Instead of allocating the entire contiguous block

244
00:13:15,680 --> 00:13:17,600
which we just computed the size of,

245
00:13:17,600 --> 00:13:21,400
we instead break it down into smaller blocks

246
00:13:21,400 --> 00:13:24,200
of let's say size four in this particular example

247
00:13:24,200 --> 00:13:28,040
but it's better to use 16 or 32 based on experiments

248
00:13:28,040 --> 00:13:29,320
which we'll look at later.

249
00:13:29,320 --> 00:13:34,320
And if you have blocks of size four

250
00:13:34,640 --> 00:13:36,360
and we're kind of abstracting out

251
00:13:36,360 --> 00:13:38,000
some implementation level details of like

252
00:13:38,040 --> 00:13:41,120
we are storing K and V for each of these KV blocks.

253
00:13:41,120 --> 00:13:46,120
But so essentially you have in the logical table,

254
00:13:46,320 --> 00:13:49,200
everything's contiguous but in the physical table,

255
00:13:49,200 --> 00:13:51,280
you're essentially mapping it using the block table

256
00:13:51,280 --> 00:13:54,400
to non-contiguous chunks within the physical table.

257
00:13:54,400 --> 00:13:59,400
And yeah, so as the model is generating a new token,

258
00:14:01,640 --> 00:14:05,120
it has this block reserved for it.

259
00:14:05,120 --> 00:14:07,720
So it will add it to the block

260
00:14:07,760 --> 00:14:09,600
because it has available memory.

261
00:14:09,600 --> 00:14:11,800
But now I want to add a new token.

262
00:14:11,800 --> 00:14:14,720
I do not have space anymore in the two blocks

263
00:14:14,720 --> 00:14:15,640
which I have reserved.

264
00:14:15,640 --> 00:14:18,480
So now I need to request for another block

265
00:14:18,480 --> 00:14:20,640
and that block is allocated to it.

266
00:14:20,640 --> 00:14:22,920
And to complete this particular request,

267
00:14:22,920 --> 00:14:27,840
I use two more blocks in that four block group.

268
00:14:27,840 --> 00:14:32,840
So the block table is just enabling you to make that mapping

269
00:14:33,320 --> 00:14:36,200
and keep track of as you need to expand.

270
00:14:38,440 --> 00:14:41,320
So obviously if you scale this to multiple requests,

271
00:14:41,320 --> 00:14:43,040
you can do the same thing.

272
00:14:43,040 --> 00:14:45,000
You will have individual block tables

273
00:14:45,000 --> 00:14:46,240
for each of the requests,

274
00:14:46,240 --> 00:14:48,320
but it can essentially do the same thing.

275
00:14:48,320 --> 00:14:51,040
But something to note here is this is a particular

276
00:14:51,040 --> 00:14:52,920
kind of example which I picked.

277
00:14:52,920 --> 00:14:55,320
You have the same prompt,

278
00:14:55,320 --> 00:14:58,280
but there are two different responses happening.

279
00:14:58,280 --> 00:15:01,280
So which is a kind of decoding which you can do

280
00:15:01,280 --> 00:15:06,280
of parallel decoding or and if you want to do,

281
00:15:07,280 --> 00:15:10,520
for example, if you want open AI

282
00:15:10,520 --> 00:15:12,680
to give you two choices of responses,

283
00:15:12,680 --> 00:15:15,800
like if you have just interface with the chat GPT API

284
00:15:15,800 --> 00:15:18,080
using GPT-4 or even GPT-3.5,

285
00:15:18,080 --> 00:15:19,760
it will sometimes show you like,

286
00:15:19,760 --> 00:15:21,640
hey, these are two responses which I have.

287
00:15:21,640 --> 00:15:22,840
Which one do you think is better

288
00:15:22,840 --> 00:15:24,680
and then they use it to improve their model.

289
00:15:24,680 --> 00:15:27,800
But it is something which is actively used.

290
00:15:27,800 --> 00:15:29,200
And if you want to do that,

291
00:15:29,200 --> 00:15:31,520
this doesn't seem like a good way to go about it

292
00:15:31,520 --> 00:15:32,920
because you are again wasting memory

293
00:15:32,960 --> 00:15:36,800
for the common bits of these requests.

294
00:15:36,800 --> 00:15:41,800
So, and yeah, before I jump into that,

295
00:15:42,760 --> 00:15:44,280
we do have internal fragmentation,

296
00:15:44,280 --> 00:15:48,600
but it is limited because it is limited by the block size

297
00:15:48,600 --> 00:15:50,280
because you only reserve that much.

298
00:15:50,280 --> 00:15:53,160
And then if any internal fragmentation does happen,

299
00:15:53,160 --> 00:15:55,000
it's because you're not consuming

300
00:15:55,000 --> 00:15:56,920
that much of the block size.

301
00:15:56,920 --> 00:15:59,600
And there is no external fragmentation by this logic

302
00:15:59,600 --> 00:16:03,280
because you're requesting more memory assignment required.

303
00:16:03,280 --> 00:16:06,240
So, there is no concept of external fragmentation.

304
00:16:07,440 --> 00:16:10,920
Now, how do we share KB blocks?

305
00:16:10,920 --> 00:16:13,960
So, let's say we are doing parallel recording

306
00:16:13,960 --> 00:16:17,400
and we want two responses for this particular problem.

307
00:16:17,400 --> 00:16:21,280
What the authors propose is we have something

308
00:16:21,280 --> 00:16:23,020
like reference counts.

309
00:16:23,020 --> 00:16:25,960
So, for both of these sequences,

310
00:16:25,960 --> 00:16:29,600
we have blocks with a reference count of two.

311
00:16:29,600 --> 00:16:32,280
I said two requests are depending on this right now.

312
00:16:32,280 --> 00:16:34,880
And let's say at this point, I choose to diverge.

313
00:16:34,880 --> 00:16:38,240
I choose to have two different tracks of responses.

314
00:16:39,120 --> 00:16:42,680
What I'd do is reduce my reference count to one.

315
00:16:42,680 --> 00:16:44,120
And then there is an operation

316
00:16:44,120 --> 00:16:45,920
which is allowed called copy on write,

317
00:16:45,920 --> 00:16:48,400
which essentially copies the block

318
00:16:48,400 --> 00:16:49,480
which you're working with.

319
00:16:49,480 --> 00:16:52,840
And then only that is repeated.

320
00:16:52,840 --> 00:16:55,160
And then today we are learning

321
00:16:55,160 --> 00:16:56,960
what still has a reference count of two.

322
00:16:56,960 --> 00:17:01,480
And then you can continue on this in the normal flow.

323
00:17:03,320 --> 00:17:06,400
Another case which the authors try to demonstrate

324
00:17:06,400 --> 00:17:09,480
of like they can support is beam searching.

325
00:17:09,480 --> 00:17:14,480
So, it is supported through fork append and free operations

326
00:17:14,680 --> 00:17:18,240
which is again part of their implementation.

327
00:17:18,240 --> 00:17:21,240
And what exactly is beam searching?

328
00:17:21,240 --> 00:17:24,560
So, if you're doing something like machine translation,

329
00:17:24,560 --> 00:17:28,320
there are often like as you progress through the sequence

330
00:17:28,320 --> 00:17:30,880
and you are creating the,

331
00:17:30,880 --> 00:17:33,480
like let's say I'm computing English to German.

332
00:17:33,480 --> 00:17:35,880
And as you proceed through the sequence,

333
00:17:35,880 --> 00:17:37,920
the model may decide that,

334
00:17:37,920 --> 00:17:39,600
oh, this particular interpretation

335
00:17:39,600 --> 00:17:41,540
which I thought of it is not really the best way.

336
00:17:41,540 --> 00:17:44,120
I should probably have thought of it in a different manner.

337
00:17:44,120 --> 00:17:46,400
So, beam search allows you to maintain

338
00:17:46,400 --> 00:17:48,400
particular beams of thought.

339
00:17:48,400 --> 00:17:51,160
And the issue with this as compared to parallel

340
00:17:51,160 --> 00:17:53,000
is that beams can be deleted

341
00:17:53,000 --> 00:17:55,400
and you can have new beams just up here.

342
00:17:55,400 --> 00:17:58,000
So, like beam one got deleted

343
00:17:58,000 --> 00:18:02,680
and now the new beam one is the earlier beam zero

344
00:18:02,680 --> 00:18:04,440
and a branch of it.

345
00:18:04,440 --> 00:18:09,440
So, this is again supported by this model of memory sharing.

346
00:18:10,540 --> 00:18:15,540
And yeah, another thing which is required for one,

347
00:18:17,440 --> 00:18:19,280
the reason that there was a reason

348
00:18:19,280 --> 00:18:20,840
why we had contiguous memory.

349
00:18:20,840 --> 00:18:22,800
And that was to like efficiently support

350
00:18:22,800 --> 00:18:24,760
these attention computations, right?

351
00:18:24,760 --> 00:18:28,480
So, the authors also introduce page detention

352
00:18:28,480 --> 00:18:31,720
which is, they just implement the kernels

353
00:18:31,720 --> 00:18:34,760
to handle the different blocks in this case.

354
00:18:34,760 --> 00:18:39,760
So, essentially it's just a rewrite

355
00:18:39,880 --> 00:18:41,400
of the original equation.

356
00:18:41,400 --> 00:18:44,000
You compute it per block

357
00:18:44,000 --> 00:18:47,520
and even the values are for that particular block

358
00:18:47,520 --> 00:18:50,520
and then you assimilate it back into the attention score.

359
00:18:50,520 --> 00:18:53,080
So, you can see the attention computations happening

360
00:18:53,080 --> 00:18:55,520
for each block, so I move through it.

361
00:18:57,240 --> 00:19:00,680
And yeah, then you can just iterate over all the blocks

362
00:19:00,680 --> 00:19:02,600
and compute the attention score for it.

363
00:19:04,200 --> 00:19:06,400
So, what was the results of the authors, fine?

364
00:19:06,400 --> 00:19:10,600
The authors re-implement ORCA.

365
00:19:10,600 --> 00:19:11,960
So, ORCA is not completely available.

366
00:19:11,960 --> 00:19:14,680
So, they re-implement ORCA and then compare

367
00:19:14,680 --> 00:19:17,240
their model serving technique with ORCA

368
00:19:17,240 --> 00:19:22,240
and they also have fast tensors

369
00:19:23,440 --> 00:19:26,280
which they compare against at Iliuth-Reynman X graph.

370
00:19:26,280 --> 00:19:29,760
But, as you can see, the VLLM implementation

371
00:19:29,760 --> 00:19:31,640
which is what's discussed so far

372
00:19:32,600 --> 00:19:35,800
supports a much higher batch size

373
00:19:35,800 --> 00:19:38,320
of like, they're supporting like 30 requests

374
00:19:38,320 --> 00:19:40,040
in parallel, right, in a sense.

375
00:19:40,040 --> 00:19:42,600
So, yeah.

376
00:19:42,680 --> 00:19:45,000
Yeah, so, yeah.

377
00:19:45,000 --> 00:19:47,440
Well, how do they, how do they implement it?

378
00:19:47,440 --> 00:19:49,560
Is it like on Python level or is it on?

379
00:19:49,560 --> 00:19:51,040
No, it has to be CUDA level.

380
00:19:51,040 --> 00:19:52,360
It has to be like CUDA level.

381
00:19:52,360 --> 00:19:54,560
Because you're messing with that VM right now, right?

382
00:19:54,560 --> 00:19:56,000
No, no, no, okay.

383
00:19:56,000 --> 00:19:59,640
So, there is like a way for YouTube defining

384
00:19:59,640 --> 00:20:04,640
like the block mapping stuff like in CUDA level, right?

385
00:20:04,920 --> 00:20:05,760
Yeah.

386
00:20:05,760 --> 00:20:08,080
Oh, okay, I see what you're saying.

387
00:20:08,080 --> 00:20:10,320
So, you had to block people on that slide

388
00:20:10,400 --> 00:20:12,400
because you know, there's high probability

389
00:20:12,400 --> 00:20:14,240
of this and what happens in ours.

390
00:20:14,240 --> 00:20:16,840
Do you know if they also try to use something similar

391
00:20:16,840 --> 00:20:21,840
to TFB for the, like the table, block table?

392
00:20:22,840 --> 00:20:24,960
Like, is there a sort of passion for that

393
00:20:24,960 --> 00:20:27,240
when you think it will be more or?

394
00:20:27,240 --> 00:20:30,240
I don't think the answer to this question is fine.

395
00:20:35,240 --> 00:20:40,040
Yeah, so, I'll be moving on to the next paper from there.

396
00:20:40,080 --> 00:20:41,960
So, for that, we are done.

397
00:20:41,960 --> 00:20:43,880
So, I had a really good question.

398
00:20:43,880 --> 00:20:46,760
So, this mechanism is built on the attention night mechanism.

399
00:20:46,760 --> 00:20:49,720
The idea is that we can append on subsequent tokens

400
00:20:49,720 --> 00:20:52,800
and keep the computations from earlier.

401
00:20:52,800 --> 00:20:54,720
As you stack transformer layer,

402
00:20:54,720 --> 00:20:56,440
like these attention blocks, right?

403
00:20:56,440 --> 00:21:00,280
The input to the next layer is the output from previous layer

404
00:21:00,280 --> 00:21:02,560
and we sum through Qs.

405
00:21:02,560 --> 00:21:05,000
So, that means that the X from previous layers

406
00:21:05,000 --> 00:21:07,040
is not necessarily the same X

407
00:21:07,040 --> 00:21:09,040
as you get deeper in the model.

408
00:21:09,040 --> 00:21:11,080
Even as you can already continuously.

409
00:21:11,080 --> 00:21:13,440
So, what happens to performance if we enforce a prior

410
00:21:13,440 --> 00:21:16,000
that like that block is gonna be the same?

411
00:21:17,880 --> 00:21:22,880
So, the KV cache per se is not what we are modifying.

412
00:21:24,040 --> 00:21:26,680
Like, the KV cache already existed long before.

413
00:21:26,680 --> 00:21:29,760
We are just changing, like, we are essentially

414
00:21:29,760 --> 00:21:31,640
just breaking down KV cache from like

415
00:21:31,640 --> 00:21:35,720
contiguous blocks of memory to like smaller pieces, right?

416
00:21:35,720 --> 00:21:37,560
Right.

417
00:21:37,560 --> 00:21:39,520
Yeah, so all of this are based on the fact

418
00:21:39,520 --> 00:21:42,880
that it's not binary, you know, it's causal.

419
00:21:42,880 --> 00:21:45,920
So, you have to, because this generation, right?

420
00:21:45,920 --> 00:21:49,960
And also, like these base models are not encoders.

421
00:21:49,960 --> 00:21:50,800
They are decoders.

422
00:21:50,800 --> 00:21:51,920
Yeah, they're decoders only.

423
00:21:51,920 --> 00:21:54,400
Yeah, that's why you can do this.

424
00:21:55,840 --> 00:21:58,160
So, moving on to the next one.

425
00:21:58,160 --> 00:21:59,880
So, we already looked at flash attention

426
00:21:59,880 --> 00:22:03,600
and the only reason I'm bringing it back up is

427
00:22:03,600 --> 00:22:07,040
because the next paper derives from some of the results

428
00:22:07,040 --> 00:22:09,960
in flash attention and like tries to bring

429
00:22:09,960 --> 00:22:13,200
what flash attention did for the training aspects

430
00:22:13,200 --> 00:22:14,880
into the inference aspects.

431
00:22:14,880 --> 00:22:19,600
So, the reason why flash attention was able

432
00:22:19,600 --> 00:22:23,080
to improve the output was, one, because

433
00:22:24,440 --> 00:22:27,760
they were able to break down the softmax computation

434
00:22:27,760 --> 00:22:32,760
into user terms and the key inside which they brought

435
00:22:33,600 --> 00:22:35,960
was that we are not really computation bound,

436
00:22:35,960 --> 00:22:37,640
we are memory bound of like,

437
00:22:39,000 --> 00:22:43,400
and the memory operations are what's slowing us down here

438
00:22:43,400 --> 00:22:47,480
and the graph again from the flash attention paper,

439
00:22:47,480 --> 00:22:51,680
they showed that instead of writing from the HVM

440
00:22:51,680 --> 00:22:54,200
into the SRAM and back again and again,

441
00:22:54,200 --> 00:22:56,960
if you create a few scramble which does

442
00:22:56,960 --> 00:22:58,560
all of these operations together,

443
00:22:58,560 --> 00:23:01,440
we can significantly speed up on attention computations.

444
00:23:02,280 --> 00:23:07,280
So, and also that the matrix multiplication wasn't

445
00:23:07,360 --> 00:23:09,160
what was consuming most of the time

446
00:23:09,160 --> 00:23:11,760
if you look at the breakdown of the times involved.

447
00:23:13,480 --> 00:23:16,160
So, what we are looking at is flash decoding

448
00:23:16,160 --> 00:23:21,160
and so the animation is technically

449
00:23:21,520 --> 00:23:23,760
destroying flash attention and how it works,

450
00:23:25,160 --> 00:23:29,120
but the concern which the authors are trying to address here

451
00:23:29,160 --> 00:23:32,800
is that if you're just doing plain inference

452
00:23:32,800 --> 00:23:36,400
and you don't have a high batch size,

453
00:23:36,400 --> 00:23:39,440
much of your GPU can remain unused

454
00:23:39,440 --> 00:23:43,240
and the way flash attention was implemented,

455
00:23:43,240 --> 00:23:46,760
you break down the key and B,

456
00:23:46,760 --> 00:23:51,040
but you don't really try to break it across batches.

457
00:23:51,040 --> 00:23:55,560
So, the modification which the authors propose

458
00:23:55,560 --> 00:23:59,600
is just split it into smaller chunks and recompute it.

459
00:23:59,600 --> 00:24:03,880
So, in flash attention, you had this scalar value

460
00:24:03,880 --> 00:24:07,120
which we were computing along with the usual computation

461
00:24:07,120 --> 00:24:10,400
so that, like the alpha and beta in a sense,

462
00:24:10,400 --> 00:24:14,640
so that you can combine these back in the final set.

463
00:24:14,640 --> 00:24:19,640
So, the same thing happens here as well across splits.

464
00:24:19,880 --> 00:24:22,120
So, you need to keep track of these values

465
00:24:22,120 --> 00:24:23,960
for each of the splits so that you can combine it

466
00:24:23,960 --> 00:24:24,800
at the last page.

467
00:24:29,040 --> 00:24:32,760
Yeah, so on a high level, that is the only change

468
00:24:32,760 --> 00:24:35,400
within flash decoding, implement splits

469
00:24:35,400 --> 00:24:38,760
so that you're processing everything in smaller chunks

470
00:24:38,760 --> 00:24:41,960
and you need to keep track of this extra scalar

471
00:24:41,960 --> 00:24:43,760
which is the log sum exponential

472
00:24:43,760 --> 00:24:46,560
and you can compute it back into the final value

473
00:24:46,560 --> 00:24:48,520
which you should have obtained.

474
00:24:48,520 --> 00:24:53,520
And by doing this, the reason for doing this

475
00:24:54,160 --> 00:24:55,720
is increasing context lengths.

476
00:24:55,720 --> 00:24:57,680
So, you can have a single request coming in

477
00:24:57,680 --> 00:24:59,320
which is obviously like the Gemini model

478
00:24:59,320 --> 00:25:01,360
which I stated with like 10 million context length.

479
00:25:01,360 --> 00:25:05,400
So, if you have such huge prompt lengths coming in,

480
00:25:06,520 --> 00:25:09,720
flash attention by itself does not do well

481
00:25:09,720 --> 00:25:11,320
because of the reason stated that

482
00:25:12,200 --> 00:25:15,680
you're not efficiently utilizing the GPU as you could.

483
00:25:15,680 --> 00:25:18,000
And you can see that from the graphs.

484
00:25:18,000 --> 00:25:22,280
So, they try naive PyTorch primitive.

485
00:25:22,280 --> 00:25:24,640
So, this paper is coming from PyTorch itself.

486
00:25:24,640 --> 00:25:27,760
So, they try PyTorch's own primitives

487
00:25:27,760 --> 00:25:30,880
and that actually works better than flash attention

488
00:25:30,880 --> 00:25:32,240
in some of the cases.

489
00:25:32,240 --> 00:25:35,200
And with flash decoding, they are able to preserve

490
00:25:35,200 --> 00:25:37,600
the performance of the model even on significantly higher

491
00:25:37,600 --> 00:25:41,160
context lengths by just breaking down this computation.

492
00:25:41,160 --> 00:25:45,040
And a significant amount of the speedup is just

493
00:25:45,040 --> 00:25:47,800
because of the way flash attention spread it up

494
00:25:47,800 --> 00:25:49,680
with the Fuse kernels and stuff like that.

495
00:25:49,680 --> 00:25:52,520
So, they are utilizing like the 50X speedup

496
00:25:52,520 --> 00:25:55,600
which just comes from using flash attention

497
00:25:55,600 --> 00:25:59,120
as opposed to the naive implementation of attention,

498
00:25:59,120 --> 00:26:01,400
the computations which were reading back and forth

499
00:26:01,400 --> 00:26:02,240
into memory.

500
00:26:03,280 --> 00:26:05,440
Yeah, and it's still better for larger context lengths

501
00:26:05,440 --> 00:26:08,000
which is the main point here.

502
00:26:09,560 --> 00:26:10,400
Yeah.

503
00:26:10,400 --> 00:26:15,560
So, what do you mean when you say it's splitting across

504
00:26:15,560 --> 00:26:16,400
KMP?

505
00:26:17,400 --> 00:26:18,240
So,

506
00:26:23,680 --> 00:26:27,000
In flash attention, you can see like this is happening

507
00:26:27,000 --> 00:26:28,240
as a sequential operation, right?

508
00:26:28,240 --> 00:26:33,240
For that entire KMB block and in which is fine for training

509
00:26:34,880 --> 00:26:38,080
but when you're doing inference and you don't have

510
00:26:40,280 --> 00:26:42,360
like you don't have enough batch size

511
00:26:42,360 --> 00:26:44,080
to populate your entire GPU.

512
00:26:44,120 --> 00:26:46,480
So, what your GPU would essentially end up doing

513
00:26:46,480 --> 00:26:49,160
is doing this sequentially even though you are essentially

514
00:26:49,160 --> 00:26:54,160
breaking down it into smaller operations, right?

515
00:26:54,200 --> 00:26:55,040
So,

516
00:26:56,560 --> 00:26:57,800
I don't know if that.

517
00:26:57,800 --> 00:26:58,640
So,

518
00:26:58,640 --> 00:26:59,640
it breaks that down into like multiple for loops going through.

519
00:26:59,640 --> 00:27:12,600
So, even if you have a single query coming in,

520
00:27:12,640 --> 00:27:14,840
you can handle it better.

521
00:27:14,840 --> 00:27:17,040
Like you don't have like 10 queries,

522
00:27:18,200 --> 00:27:21,320
like while training you can easily create like 128 batch size

523
00:27:21,320 --> 00:27:26,000
but if you are like there is an aspect of latency

524
00:27:26,000 --> 00:27:26,840
involved here, right?

525
00:27:26,840 --> 00:27:29,760
You don't wanna be waiting on batching up like 20 queries

526
00:27:29,760 --> 00:27:32,120
that are coming in to respond to the first guy.

527
00:27:32,120 --> 00:27:37,120
Yeah, I guess then in this case you would have to do something

528
00:27:37,120 --> 00:27:39,080
you have to change that out a little bit actually, right?

529
00:27:39,080 --> 00:27:43,880
Because before you need to do computation,

530
00:27:43,880 --> 00:27:46,400
you already have your first thing, your new first.

531
00:27:46,400 --> 00:27:50,880
You put your computation on the first count,

532
00:27:50,880 --> 00:27:52,320
but it's in the output data.

533
00:27:52,320 --> 00:27:53,320
Yeah.

534
00:27:53,320 --> 00:27:54,640
But now you're trying to do everything

535
00:27:54,640 --> 00:27:56,280
and aggregating them together.

536
00:27:56,280 --> 00:27:57,120
Yeah.

537
00:27:57,120 --> 00:27:59,480
So, there's some reduction stuff here or?

538
00:27:59,480 --> 00:28:01,320
Yeah, there is a final reduction set.

539
00:28:01,320 --> 00:28:02,160
I see.

540
00:28:05,880 --> 00:28:08,600
And that scalar which we are tracking for like the weights

541
00:28:08,600 --> 00:28:10,320
of each of these individual components

542
00:28:10,320 --> 00:28:12,080
allows us to do that reduction set.

543
00:28:13,080 --> 00:28:13,920
I see.

544
00:28:13,920 --> 00:28:15,240
Yeah.

545
00:28:15,240 --> 00:28:17,200
So, as far as I understand,

546
00:28:17,200 --> 00:28:21,840
so this is basically roughly the same as flash attention

547
00:28:21,840 --> 00:28:24,000
just like in some way or place.

548
00:28:24,000 --> 00:28:26,400
Yeah, this is not a full paper, it's just a blog post.

549
00:28:26,400 --> 00:28:27,240
Yeah.

550
00:28:27,240 --> 00:28:30,160
I'm kind of, so this is for processing,

551
00:28:30,160 --> 00:28:31,680
like will it long?

552
00:28:31,680 --> 00:28:35,440
Yeah, so the intention is to perform better

553
00:28:35,440 --> 00:28:37,360
on like long context lengths and?

554
00:28:37,360 --> 00:28:38,280
Long context lengths.

555
00:28:38,280 --> 00:28:39,840
Small bad sizes.

556
00:28:39,840 --> 00:28:40,840
For inference.

557
00:28:40,840 --> 00:28:42,160
Oh, okay.

558
00:28:42,160 --> 00:28:45,120
But does it do better if you're gonna generate

559
00:28:45,120 --> 00:28:46,440
that long?

560
00:28:46,440 --> 00:28:47,280
Is that what it is?

561
00:28:47,280 --> 00:28:48,120
Yeah.

562
00:28:48,120 --> 00:28:52,000
Just specialize for the process and the long input?

563
00:28:52,000 --> 00:28:53,600
Oh, no, it should work for both.

564
00:28:53,600 --> 00:28:55,960
It should work for both for the output.

565
00:28:55,960 --> 00:28:56,800
Yeah.

566
00:28:56,800 --> 00:28:57,640
But like for the output,

567
00:28:57,640 --> 00:29:00,240
you have to generate this long by having this period.

568
00:29:01,520 --> 00:29:03,600
For the discussion, they only keep it to prompt lengths.

569
00:29:03,600 --> 00:29:07,240
Oh, they only make a term of it's a long prompt.

570
00:29:07,240 --> 00:29:08,080
Yeah.

571
00:29:08,880 --> 00:29:10,160
Not with the responses.

572
00:29:10,160 --> 00:29:11,000
Oh, okay.

573
00:29:12,200 --> 00:29:14,320
Yeah, I just assume the same thing could be done

574
00:29:14,320 --> 00:29:16,600
for response, but yeah,

575
00:29:16,600 --> 00:29:18,520
since you're building it up incrementally.

576
00:29:18,520 --> 00:29:20,080
Oh, right, right, right.

577
00:29:20,080 --> 00:29:20,920
So same thing.

578
00:29:20,920 --> 00:29:21,760
Yeah.

579
00:29:21,760 --> 00:29:23,560
What is the question?

580
00:29:23,560 --> 00:29:27,400
So yeah, just wondering, like if this is only applied

581
00:29:27,400 --> 00:29:30,160
to like long prompt or long response?

582
00:29:31,160 --> 00:29:32,000
Right.

583
00:29:32,000 --> 00:29:33,320
Oh, I see.

584
00:29:33,320 --> 00:29:35,400
But it seems like it's the same thing.

585
00:29:35,400 --> 00:29:36,240
I see.

586
00:29:39,080 --> 00:29:42,920
So the last paper which I'll be diving into

587
00:29:42,920 --> 00:29:45,960
is a continuation of what Rishabh was looking at

588
00:29:45,960 --> 00:29:48,520
with respect to speculative decoding.

589
00:29:48,520 --> 00:29:51,400
So essentially what you're trying to do

590
00:29:51,400 --> 00:29:53,080
with speculative decoding was

591
00:29:54,720 --> 00:29:57,320
instead of having a decoding step

592
00:29:57,320 --> 00:29:59,360
which only gives you out one token,

593
00:29:59,360 --> 00:30:02,880
you want to have scenarios where some of the decoding steps

594
00:30:02,880 --> 00:30:06,480
give you a few more tokens than just a single token.

595
00:30:06,480 --> 00:30:09,320
So in effect, you can speed up your inference.

596
00:30:09,320 --> 00:30:14,320
So if you just like look back at just like build it up

597
00:30:16,760 --> 00:30:21,680
to this point, the Lama tokenizer has 32,000 tokens.

598
00:30:21,680 --> 00:30:25,440
If you randomly guess what your next token should be,

599
00:30:25,440 --> 00:30:28,280
that's like a one in 32,000 chance that you get it right.

600
00:30:28,280 --> 00:30:32,920
But you can do something like in language,

601
00:30:32,920 --> 00:30:35,160
you don't have, all tokens are not equally likely

602
00:30:35,160 --> 00:30:38,640
and you can use that distribution to pick something.

603
00:30:38,640 --> 00:30:41,280
Or another thing is which there are other papers

604
00:30:41,280 --> 00:30:43,680
that look at it is use the prompt

605
00:30:43,680 --> 00:30:45,240
to speed up your inference.

606
00:30:45,240 --> 00:30:50,240
So essentially the idea is that there will be tokens

607
00:30:50,880 --> 00:30:53,440
from the prompt which repeat within your response.

608
00:30:53,440 --> 00:30:55,600
So you can just speed it up based on that.

609
00:30:56,600 --> 00:30:59,280
That is something which authors have looked at us

610
00:31:00,280 --> 00:31:01,720
and using ngrams as a lookup table

611
00:31:01,720 --> 00:31:04,200
which we will come to again even in this approach.

612
00:31:04,240 --> 00:31:09,240
And the speculative decoding much of the aspects have gone

613
00:31:09,480 --> 00:31:11,880
into using a smaller draft model or a helper model.

614
00:31:11,880 --> 00:31:14,920
But it hasn't been widely adopted

615
00:31:14,920 --> 00:31:17,080
because of the issues which were again brought up.

616
00:31:17,080 --> 00:31:22,080
And the complexity of having to manage two different models

617
00:31:23,480 --> 00:31:25,480
as well as training it.

618
00:31:25,480 --> 00:31:27,280
So there was a paper which came out

619
00:31:27,280 --> 00:31:30,240
after speculative decoding called online speculative decoding

620
00:31:30,240 --> 00:31:33,480
which essentially just tries to improve your helper model

621
00:31:33,480 --> 00:31:36,000
to perform better over time.

622
00:31:36,000 --> 00:31:40,640
And there's also Medusa which tried to look at using

623
00:31:40,640 --> 00:31:45,160
multiple attention heads for doing the process

624
00:31:45,160 --> 00:31:48,000
in a manner that's like easily adoptable.

625
00:31:48,000 --> 00:31:50,080
And Medusa is a more applied approach

626
00:31:50,080 --> 00:31:53,840
than speculative decoding with draft models.

627
00:31:53,840 --> 00:31:58,840
So with that background, we can look at look ahead decoding.

628
00:31:59,680 --> 00:32:03,920
This is just a graphic which they gave on their website

629
00:32:03,920 --> 00:32:07,160
of how fast and inference we are looking at

630
00:32:07,160 --> 00:32:09,280
as compared to the normal decoding process.

631
00:32:10,960 --> 00:32:15,240
And how do we take better guesses is the question

632
00:32:15,240 --> 00:32:16,560
which we are trying to answer, right?

633
00:32:16,560 --> 00:32:21,560
So yeah, so we're trying to essentially improve

634
00:32:21,880 --> 00:32:23,360
the token acceptance rate.

635
00:32:23,360 --> 00:32:26,640
So if you have your draft models putting out

636
00:32:26,680 --> 00:32:29,360
five different tokens and then all of them

637
00:32:29,360 --> 00:32:30,720
just essentially get discarded

638
00:32:30,720 --> 00:32:32,920
because the main token says this is not what I want.

639
00:32:32,920 --> 00:32:34,280
So it's wasted compute.

640
00:32:34,280 --> 00:32:37,160
So we want to be improving the acceptance rate.

641
00:32:40,720 --> 00:32:44,640
And with that, one of the techniques is Jacobi iteration.

642
00:32:45,680 --> 00:32:48,400
So the Jacobi method is something

643
00:32:48,400 --> 00:32:51,440
which we would have gone over in high school

644
00:32:51,440 --> 00:32:54,160
if you remember that from a linear algebra.

645
00:32:54,200 --> 00:32:57,880
But so essentially what the authors propose

646
00:32:57,880 --> 00:33:01,760
is that the core problem which you're trying to solve

647
00:33:01,760 --> 00:33:06,600
is predicting the M tokens in the response sequence.

648
00:33:06,600 --> 00:33:09,480
So you have an X, you have a Y, which is your response

649
00:33:09,480 --> 00:33:10,720
and you have M tokens there

650
00:33:10,720 --> 00:33:14,040
and you're trying to decode that regressively.

651
00:33:14,040 --> 00:33:18,760
And can we do that faster?

652
00:33:18,760 --> 00:33:23,240
Can we just compute all of the tokens in one go

653
00:33:23,240 --> 00:33:25,560
or like a specific set of them in one go?

654
00:33:26,600 --> 00:33:30,920
For that, they rewrite the regressive decoding model.

655
00:33:30,920 --> 00:33:34,280
So essentially for the regressive step,

656
00:33:34,280 --> 00:33:36,880
you're trying to look at what is the token

657
00:33:36,880 --> 00:33:40,160
which I should output given that

658
00:33:40,160 --> 00:33:42,080
I have this particular input, right?

659
00:33:42,080 --> 00:33:45,920
And as we go for like further steps

660
00:33:45,920 --> 00:33:47,400
as you're building out your response,

661
00:33:47,400 --> 00:33:49,200
you do that for each particular token.

662
00:33:49,200 --> 00:33:50,720
So they define a function

663
00:33:51,720 --> 00:33:53,880
which is shown here

664
00:33:53,880 --> 00:33:57,040
and they convert it into that form.

665
00:33:58,880 --> 00:34:02,400
So you essentially have a system of nonlinear equations

666
00:34:02,400 --> 00:34:03,720
which you're trying to solve.

667
00:34:04,960 --> 00:34:07,680
The Jacobi method, if you do remember linear algebra

668
00:34:07,680 --> 00:34:10,040
is applied to a system of linear equations.

669
00:34:11,640 --> 00:34:14,680
And that is actually an issue in the description

670
00:34:14,680 --> 00:34:18,360
of this paper because they link to the normal Jacobi method

671
00:34:18,360 --> 00:34:19,640
like the link to the Wikipedia page

672
00:34:19,640 --> 00:34:22,120
with the normal Jacobi method, which is this.

673
00:34:22,120 --> 00:34:23,840
Like this is what you study in high school

674
00:34:23,840 --> 00:34:25,640
of how to use the Jacobi method,

675
00:34:25,640 --> 00:34:28,800
but it applies to only linear equations.

676
00:34:28,800 --> 00:34:32,200
And what you essentially end up doing for a refresher

677
00:34:32,200 --> 00:34:35,920
is that once you rewrite the system of equations

678
00:34:35,920 --> 00:34:40,000
into each variable, once you represent it in this form,

679
00:34:40,000 --> 00:34:43,120
you take an initial guess of what each token is

680
00:34:43,120 --> 00:34:46,560
and then you iteratively, like similar to Newton-Raphson

681
00:34:46,560 --> 00:34:47,720
or any of those iterative methods,

682
00:34:47,720 --> 00:34:50,480
you try to iteratively solve the system of equations

683
00:34:50,480 --> 00:34:53,160
and once your method converges,

684
00:34:53,160 --> 00:34:56,160
you have all the tokens which you need.

685
00:34:56,160 --> 00:35:01,160
And but this is for linear systems

686
00:35:01,480 --> 00:35:03,400
and the only reference which I could find to do this

687
00:35:03,400 --> 00:35:06,800
for nonlinear equations was this particular book.

688
00:35:06,800 --> 00:35:09,680
And yeah, they essentially say that

689
00:35:09,680 --> 00:35:12,480
you can do the linear method for the nonlinear system

690
00:35:12,480 --> 00:35:14,040
and it does work the same way.

691
00:35:14,040 --> 00:35:16,120
And one property of this computation

692
00:35:16,160 --> 00:35:19,000
is that with every step, you solve one of the variables

693
00:35:20,080 --> 00:35:24,920
and with m steps, you can solve the entire equation,

694
00:35:24,920 --> 00:35:28,320
but that is essentially same as regressively doing it

695
00:35:28,320 --> 00:35:30,320
because if you had to take m steps,

696
00:35:32,280 --> 00:35:35,000
it is same as just a sequentially,

697
00:35:35,000 --> 00:35:37,840
but in this case, you're solving a system of equations

698
00:35:37,840 --> 00:35:38,680
to get to that.

699
00:35:38,680 --> 00:35:41,240
That's the difference in Jacobi iteration

700
00:35:41,240 --> 00:35:43,880
and this is algorithm from that book

701
00:35:43,880 --> 00:35:46,560
about how do you do parallelize

702
00:35:46,560 --> 00:35:48,160
the nonlinear Jacobi iteration.

703
00:35:49,680 --> 00:35:52,720
Yeah, so just to do that visually,

704
00:35:52,720 --> 00:35:56,080
this is what would happen if we do Jacobi iteration.

705
00:35:56,080 --> 00:36:00,960
You have your prompt and then you take a guess

706
00:36:00,960 --> 00:36:02,360
of what the other tokens are.

707
00:36:02,360 --> 00:36:04,960
This is like your initial guess in the Jacobi method

708
00:36:04,960 --> 00:36:08,200
and you just are randomly filling that out

709
00:36:08,200 --> 00:36:11,920
of what you think the next token should be.

710
00:36:11,920 --> 00:36:15,000
And then the issue again here is that

711
00:36:15,000 --> 00:36:17,720
you're saying that the operations are,

712
00:36:17,720 --> 00:36:20,840
like the memory operations are what are taking time,

713
00:36:20,840 --> 00:36:24,280
so you want to optimize the GPU as much as possible.

714
00:36:24,280 --> 00:36:27,520
So computations is not a concern in this particular case

715
00:36:27,520 --> 00:36:30,280
and we are particularly looking at models

716
00:36:30,280 --> 00:36:33,000
where you want to improve the latency

717
00:36:33,000 --> 00:36:38,000
and there is an increased amount of computations

718
00:36:39,160 --> 00:36:41,600
which will happen as we go into the actual model,

719
00:36:41,600 --> 00:36:44,320
but that is not something you should look at here

720
00:36:44,320 --> 00:36:47,640
because the concern is latency in this particular case.

721
00:36:47,640 --> 00:36:52,640
So now you don't have a draft model again in this case,

722
00:36:52,640 --> 00:36:55,320
it's just your main model predicting these.

723
00:36:55,320 --> 00:37:00,320
So your main model says that the next token

724
00:37:00,600 --> 00:37:05,600
to Alan Turing is like the next token should be is

725
00:37:07,000 --> 00:37:10,160
and then you have parallelized the operation

726
00:37:10,160 --> 00:37:12,760
by having a few random tokens which you guessed

727
00:37:12,760 --> 00:37:15,200
and then you're asking the model

728
00:37:15,200 --> 00:37:17,320
to parallelly compute these as well.

729
00:37:17,320 --> 00:37:21,320
So the model says if this was who,

730
00:37:21,320 --> 00:37:22,880
then the next token should be yes.

731
00:37:22,880 --> 00:37:26,120
If it's yes, it should be A and so on

732
00:37:26,120 --> 00:37:29,800
for as many tokens you guessed.

733
00:37:29,800 --> 00:37:32,120
So this is the parameter which we'll set later

734
00:37:32,120 --> 00:37:34,640
of like how many do we want to guess forward,

735
00:37:34,640 --> 00:37:38,120
but the model computes what the next should be

736
00:37:38,160 --> 00:37:42,720
and if we let it continue,

737
00:37:43,920 --> 00:37:47,920
we use the new generated tokens as the,

738
00:37:47,920 --> 00:37:50,520
like we updated with the random selection

739
00:37:50,520 --> 00:37:55,520
and now we have one step forward.

740
00:37:56,400 --> 00:37:57,240
Like in the previous step,

741
00:37:57,240 --> 00:37:58,680
we only got one accepted token

742
00:37:58,680 --> 00:38:00,560
and the rest of it was wasted computation.

743
00:38:00,560 --> 00:38:03,040
So there was no merit to doing that,

744
00:38:03,040 --> 00:38:08,040
but except we updated our randomly selected initial guess

745
00:38:08,720 --> 00:38:10,520
and similar to the computation,

746
00:38:10,520 --> 00:38:11,880
we solved one variable,

747
00:38:11,880 --> 00:38:14,040
but the remaining variables are still kind of random

748
00:38:14,040 --> 00:38:16,160
and we still haven't controls yet.

749
00:38:16,160 --> 00:38:19,600
And now the model has to look at all of these variables

750
00:38:19,600 --> 00:38:23,840
and try to compute what the next open would be

751
00:38:23,840 --> 00:38:27,360
if this was the particular variable in this spot.

752
00:38:27,360 --> 00:38:31,400
So again, we do the same step, we continue

753
00:38:31,400 --> 00:38:36,240
and then in cases where

754
00:38:39,080 --> 00:38:40,880
yeah, I believe this step fails as well.

755
00:38:40,880 --> 00:38:42,680
So in this particular case,

756
00:38:43,760 --> 00:38:46,000
you're essentially just rewriting and every step

757
00:38:46,000 --> 00:38:48,480
you're only getting one new token accepted,

758
00:38:48,480 --> 00:38:53,480
but when you have a scenario where the next token,

759
00:38:54,680 --> 00:38:59,200
which the main model predicts is same as that

760
00:38:59,200 --> 00:39:01,440
which you're getting via speculative decoding,

761
00:39:01,440 --> 00:39:04,680
you can have multiple tokens be accepted,

762
00:39:04,680 --> 00:39:06,840
which was again the objective which we set out with

763
00:39:06,840 --> 00:39:09,040
of like having a few steps

764
00:39:09,040 --> 00:39:11,120
where more than one token gets accepted.

765
00:39:11,120 --> 00:39:13,560
And the rest of it is just wasted computation.

766
00:39:13,560 --> 00:39:15,040
So we have re-rated there,

767
00:39:15,040 --> 00:39:17,920
but on the last step, we accepted two tokens.

768
00:39:17,920 --> 00:39:20,920
So this is naively doing Jacobi iteration.

769
00:39:23,200 --> 00:39:26,280
What the authors propose is a bit more complicated.

770
00:39:26,280 --> 00:39:29,680
So instead of all of that wasted computation,

771
00:39:29,680 --> 00:39:32,000
what if you saved the trajectory

772
00:39:32,000 --> 00:39:34,360
which the model was trying to take?

773
00:39:34,360 --> 00:39:36,080
As in you save the n-grams.

774
00:39:36,120 --> 00:39:37,920
If you're computing using the main model

775
00:39:37,920 --> 00:39:39,920
what the next token should be,

776
00:39:39,920 --> 00:39:41,920
you just see it like in this case,

777
00:39:41,920 --> 00:39:44,800
it's a two-gram, so it's very similar to recuperation

778
00:39:44,800 --> 00:39:48,120
and we will be dealing with higher ends,

779
00:39:48,120 --> 00:39:50,680
but just for visualization,

780
00:39:50,680 --> 00:39:52,440
you have a two-gram being created

781
00:39:52,440 --> 00:39:56,840
and what you can do is before discarding everything,

782
00:39:56,840 --> 00:40:00,120
you can have a look at the two-grams which you have stored

783
00:40:00,120 --> 00:40:04,320
and then use that as your vet of what the next token is

784
00:40:04,320 --> 00:40:07,800
and use that to continue forward.

785
00:40:07,800 --> 00:40:08,880
In the iteration.

786
00:40:08,880 --> 00:40:11,960
What happens when there are conflicting ones?

787
00:40:11,960 --> 00:40:13,080
Is that on the next slide?

788
00:40:13,080 --> 00:40:15,880
But at least for the visualization,

789
00:40:15,880 --> 00:40:19,080
you have a case where you only have one case

790
00:40:19,080 --> 00:40:20,480
which matches that one.

791
00:40:20,480 --> 00:40:22,760
Oh, so basically this is like a cache, right?

792
00:40:22,760 --> 00:40:24,440
Yeah, you have a cache implemented.

793
00:40:24,440 --> 00:40:28,240
You roughly know what the target model is based on.

794
00:40:28,240 --> 00:40:29,600
Yeah, because we don't have a draft model

795
00:40:29,600 --> 00:40:32,320
and it's all the target model's distribution happening, right?

796
00:40:33,320 --> 00:40:36,920
But there are some failure cases

797
00:40:36,920 --> 00:40:40,600
where this two-gram can happen in two places.

798
00:40:40,600 --> 00:40:42,840
Yeah, so when it happens in two places,

799
00:40:42,840 --> 00:40:44,640
you need to do a verification step

800
00:40:44,640 --> 00:40:47,560
and the actual implementation is way more complex than this

801
00:40:47,560 --> 00:40:48,560
because this is a two-gram

802
00:40:48,560 --> 00:40:50,800
which makes it very similar to speculative decoding,

803
00:40:50,800 --> 00:40:53,240
but the actual case is an n-gram.

804
00:40:54,480 --> 00:40:56,360
So in this particular case,

805
00:40:56,360 --> 00:40:58,440
you have the n being set to four,

806
00:40:58,440 --> 00:41:00,520
so you end up with a four-gram

807
00:41:01,440 --> 00:41:02,440
with like, yeah.

808
00:41:02,440 --> 00:41:04,280
So the numbers are meaningless,

809
00:41:04,280 --> 00:41:05,120
please don't look at that.

810
00:41:05,120 --> 00:41:06,640
The numbers essentially just say

811
00:41:06,640 --> 00:41:08,440
what is the position of this particular token

812
00:41:08,440 --> 00:41:10,320
with respect to the initial token

813
00:41:10,320 --> 00:41:11,880
in the prediction process.

814
00:41:11,880 --> 00:41:16,880
And so a valid n-gram in this particular case would be,

815
00:41:20,760 --> 00:41:22,880
let's say you have one, two, three,

816
00:41:22,880 --> 00:41:24,400
and then whatever's happening at this step.

817
00:41:24,400 --> 00:41:27,320
So we are on the last step of the Jacob iteration

818
00:41:27,320 --> 00:41:29,120
where the verification step will happen.

819
00:41:29,160 --> 00:41:30,560
So that's why the fourth token

820
00:41:30,560 --> 00:41:32,240
is not really there in the diagram,

821
00:41:32,240 --> 00:41:35,800
but you essentially have an n-gram being created

822
00:41:35,800 --> 00:41:39,920
and you also have to predict the next token

823
00:41:39,920 --> 00:41:41,320
based on what has happened.

824
00:41:42,440 --> 00:41:44,200
And there are two parameters in this entire problem.

825
00:41:44,200 --> 00:41:47,840
There's window size of how far we are looking at.

826
00:41:47,840 --> 00:41:49,920
So that determines how many new tokens

827
00:41:49,920 --> 00:41:51,280
you are generating on each step.

828
00:41:51,280 --> 00:41:52,440
So in this case, you have five,

829
00:41:52,440 --> 00:41:54,360
so that's why the window size is five.

830
00:41:54,360 --> 00:41:56,400
And the n-gram size is controlling

831
00:41:56,400 --> 00:41:58,200
how much of the history you're retaining.

832
00:41:58,200 --> 00:41:59,960
So you are constructing a four-gram,

833
00:41:59,960 --> 00:42:02,600
so that's why the n-gram size is four there.

834
00:42:02,600 --> 00:42:07,600
And now if you have scenarios

835
00:42:08,200 --> 00:42:12,600
where there are multiple correct n-grams to choose from,

836
00:42:12,600 --> 00:42:14,160
then you need to do verification.

837
00:42:14,160 --> 00:42:15,680
So in this particular case,

838
00:42:15,680 --> 00:42:18,240
there is an additional parameter called g,

839
00:42:18,240 --> 00:42:21,360
which is how many you choose to verify.

840
00:42:21,360 --> 00:42:25,400
In all the experiments, the authors choose set it to w,

841
00:42:25,400 --> 00:42:29,440
the window size is same as the number of things

842
00:42:29,440 --> 00:42:30,520
you choose to verify.

843
00:42:30,520 --> 00:42:33,920
And what this shows is the attention mask.

844
00:42:33,920 --> 00:42:36,520
The only thing it is essentially saying is that,

845
00:42:36,520 --> 00:42:37,480
same as causal attention,

846
00:42:37,480 --> 00:42:39,800
you are only looking at the tokens before it.

847
00:42:39,800 --> 00:42:43,720
So the token one can only see itself and zero.

848
00:42:43,720 --> 00:42:47,640
And the same thing is done for all of the steps.

849
00:42:47,640 --> 00:42:52,640
So, and the verification branch cannot look at anything

850
00:42:53,720 --> 00:42:54,640
in the lookahead branch,

851
00:42:54,640 --> 00:42:56,360
which is why it's all masked out.

852
00:42:56,360 --> 00:42:59,040
And then you have it, again,

853
00:42:59,040 --> 00:43:01,080
only attending to its previous tokens

854
00:43:01,080 --> 00:43:02,560
and just starting to do this.

855
00:43:02,560 --> 00:43:05,200
You are verifying, you only have zero at this step

856
00:43:05,200 --> 00:43:07,120
and everything else is happening through

857
00:43:08,080 --> 00:43:10,160
like iteratively.

858
00:43:10,160 --> 00:43:13,440
So this is for the verification branch

859
00:43:13,440 --> 00:43:17,840
and this is the lookahead stuff which is happening.

860
00:43:17,840 --> 00:43:21,560
And they just combine it together into one parallel step

861
00:43:21,560 --> 00:43:22,560
to do it efficiently.

862
00:43:24,640 --> 00:43:28,520
And yeah, so this is just a comparison

863
00:43:28,520 --> 00:43:30,840
of what the attention mask looks like

864
00:43:30,840 --> 00:43:32,440
when you compare it with Medusa.

865
00:43:32,440 --> 00:43:35,520
So in Medusa, you have multiple heads

866
00:43:35,520 --> 00:43:37,640
and like if the first head says that

867
00:43:37,640 --> 00:43:39,560
the next token can be gate par i

868
00:43:39,560 --> 00:43:42,840
of like a case where we are taking only the two ones,

869
00:43:42,840 --> 00:43:44,880
the next head will again continue from it

870
00:43:44,880 --> 00:43:47,680
and say three tokens.

871
00:43:47,680 --> 00:43:50,920
And for the i case, it will again generate three tokens

872
00:43:50,920 --> 00:43:53,360
and that's what the attention mask looks like there.

873
00:43:54,360 --> 00:43:56,960
To contrast with what this attention mask looks like.

874
00:43:59,080 --> 00:44:00,520
Yeah, I'm mostly but I've been up.

875
00:44:00,520 --> 00:44:03,280
So the scaling law is something which they discuss

876
00:44:03,280 --> 00:44:05,520
based on the results of this paper.

877
00:44:05,520 --> 00:44:10,520
So what they find is that if you have sufficiently large N

878
00:44:10,960 --> 00:44:13,800
and you exponentially increase the builder size,

879
00:44:13,800 --> 00:44:17,680
you can get a linear trend in the step compression.

880
00:44:17,680 --> 00:44:20,960
So you can linearly reduce the number of decoding steps

881
00:44:20,960 --> 00:44:21,960
which is required.

882
00:44:23,360 --> 00:44:24,880
So that's a result which they find

883
00:44:24,880 --> 00:44:26,480
with their experimentation.

884
00:44:26,480 --> 00:44:28,520
And what impact does this have?

885
00:44:29,600 --> 00:44:32,680
So they applied it on Lama Chat model

886
00:44:32,680 --> 00:44:35,880
and they observed a speedup of 1.5x

887
00:44:35,880 --> 00:44:39,280
and latency reductions on human eval

888
00:44:39,280 --> 00:44:41,680
as well as the math problems.

889
00:44:41,680 --> 00:44:43,600
Yeah, that's GSM 8K.

890
00:44:43,600 --> 00:44:47,400
So yeah, so it does reduce latency

891
00:44:47,400 --> 00:44:49,400
which was the primary target of this entire thing.

892
00:44:49,400 --> 00:44:53,560
So wasted computations again, it's not a concern here.

893
00:44:53,560 --> 00:44:56,760
You're just trying to efficiently utilize most of the GPS.

894
00:44:58,680 --> 00:45:02,040
So for my part, we see that LLM inference

895
00:45:02,040 --> 00:45:03,720
was bottlenecked by memory constraints

896
00:45:03,720 --> 00:45:06,040
and we saw how we can go about building

897
00:45:06,040 --> 00:45:07,800
a more efficient KV cache.

898
00:45:09,960 --> 00:45:12,040
The authors also had to implement like the kernel

899
00:45:12,040 --> 00:45:14,800
for blockwise attention or like page detention

900
00:45:14,800 --> 00:45:15,640
as they call it.

901
00:45:16,560 --> 00:45:18,720
The next paper looked at flash decoding

902
00:45:18,720 --> 00:45:20,840
which was trying to bring the enhancements

903
00:45:20,840 --> 00:45:24,200
of flash attention V1 and V2 into the inference step.

904
00:45:24,200 --> 00:45:26,160
And the last paper which we looked at

905
00:45:26,160 --> 00:45:28,040
was look ahead decoding

906
00:45:28,040 --> 00:45:30,920
which is trying to improve speculative decoding

907
00:45:30,920 --> 00:45:32,720
but take a different approach

908
00:45:32,720 --> 00:45:34,240
rather than using draft models

909
00:45:34,240 --> 00:45:37,720
and yeah, for latency critical applications

910
00:45:37,720 --> 00:45:40,720
because there's a lot of wasted computations happening.

911
00:45:40,720 --> 00:45:41,560
Thank you.

912
00:45:41,560 --> 00:45:42,400
Thank you.

913
00:45:42,400 --> 00:45:43,240
Thank you.

