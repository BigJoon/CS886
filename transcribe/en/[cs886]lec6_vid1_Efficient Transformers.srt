1
00:00:00,000 --> 00:00:07,600
Hi everyone, my name is Albert and I will be presenting the lecture on efficient transformer

2
00:00:07,600 --> 00:00:11,840
for CS86.

3
00:00:11,840 --> 00:00:13,760
So what are we covering today?

4
00:00:13,760 --> 00:00:19,360
I'll give a quick review of the attention mechanism in transformers and then explain

5
00:00:19,360 --> 00:00:24,680
the time and space complexity of a traditional transformer and why that would be a problem.

6
00:00:24,680 --> 00:00:31,560
I've finally presented several solutions proposed over the past, including sparse transformers,

7
00:00:31,560 --> 00:00:39,000
lean former, linearized tension, long former, and resonant feature tension and so on.

8
00:00:39,000 --> 00:00:44,520
These are of course not all of them, but due to time and constraint, these are all we will

9
00:00:44,520 --> 00:00:45,520
be covering today.

10
00:00:45,520 --> 00:00:50,640
In the end, I will present the get-up project comparing the performance of all efficient

11
00:00:50,640 --> 00:00:53,320
transformers.

12
00:00:53,320 --> 00:00:57,040
So just a quick review of the attention mechanism.

13
00:00:57,040 --> 00:01:04,600
On the left, we define a bunch of values, begin as a sequence length, Lh is the number

14
00:01:04,600 --> 00:01:12,440
of pairs, usually 8, there's some model in the embedding size, already 512, and this

15
00:01:12,440 --> 00:01:18,840
k, which is called to demodel divided by h, is 64.

16
00:01:18,840 --> 00:01:27,920
So the input sequence is embedded into x, which is dimension n times demodel, and then

17
00:01:27,920 --> 00:01:34,040
the input is put through 8 separate heads, each containing weaknesses which x will multiply

18
00:01:34,040 --> 00:01:44,120
by, and the dimensions are of demodel times decay, and to obtain so that we can obtain

19
00:01:44,120 --> 00:01:48,400
the accurate key and value.

20
00:01:48,400 --> 00:01:54,560
Then the attention is calculated by taking the softmax of q times k transposed divided

21
00:01:54,560 --> 00:02:01,920
by square root of decay times, then closing time to iv, and finally the results from each

22
00:02:01,920 --> 00:02:09,240
head are concedinated and multiplied by a final width matrix to obtain the output.

23
00:02:09,240 --> 00:02:15,760
So let's then look at where the complexity comes from, recall the attention function,

24
00:02:15,760 --> 00:02:23,800
the attention q, k, and v is called to soften the x, blah, blah, blah, where the dimension

25
00:02:23,800 --> 00:02:30,680
of q is called dimension of k, which is called to n times decay, so the calculation of q

26
00:02:30,680 --> 00:02:40,360
times k transposed will thus cause an order of n squared times decay.

27
00:02:40,360 --> 00:02:53,640
As for the space complexity, storing the attention matrix, which is this part, would

28
00:02:53,640 --> 00:03:03,360
cause all n squared space complexity for each head, so the total of space complexity

29
00:03:03,360 --> 00:03:11,280
would be all of n squared h, therefore if we have a very long sequence, much longer

30
00:03:11,280 --> 00:03:16,960
when there are much longer than decay and sub-h, time and space cost would be quadratic

31
00:03:16,960 --> 00:03:22,360
to the sequence length, which would be very terrible if the sequence is very long, this

32
00:03:22,360 --> 00:03:27,160
would have a bad impact on the scalability of the transformer and really increase its

33
00:03:27,160 --> 00:03:32,960
resource consumption, this would also effect this adoption to all sorts of situations such

34
00:03:32,960 --> 00:03:39,960
as image completion and audio recognition and so on.

35
00:03:39,960 --> 00:03:45,520
So to counter this situation, many has come over the solutions, here we list a bunch of

36
00:03:45,520 --> 00:03:48,400
them and their improved cost.

37
00:03:48,400 --> 00:03:58,160
First is the smart transformer proposed by Childed L in April of 2019 achieved all of

38
00:03:58,160 --> 00:04:06,480
n times pth root of n times space cost, where p is a hyperparameter independent of n, in

39
00:04:06,480 --> 00:04:11,680
their work they gave examples when pth could too, but they claimed that higher dimensions

40
00:04:11,680 --> 00:04:13,000
were also feasible.

41
00:04:13,920 --> 00:04:20,120
Next we have the lean former proposed by Wang Edel in June 2020 and now raised the tension

42
00:04:20,120 --> 00:04:31,240
by Kassarou Polos Edel in August 2020, long former by Delta G in December of 2020, random

43
00:04:31,240 --> 00:04:41,280
feature tension RFA by Peng Edel in March of 2021.

44
00:04:41,280 --> 00:04:49,360
See all those four solutions achieved linear time and space cost in various ways and some

45
00:04:49,360 --> 00:04:53,960
even have constant space cost which we will cover later.

46
00:04:53,960 --> 00:05:04,800
So let's talk about the one by one, first the smart transformers, the motivation behind

47
00:05:04,800 --> 00:05:10,840
the smart transformer can be explained by the following image, these are learned attention

48
00:05:10,840 --> 00:05:21,960
patterns from a 1228 layer network on the C4-10 reset train with full attention, divide

49
00:05:21,960 --> 00:05:29,880
areas such as those ones highlights the attention base for head while generating given pixel

50
00:05:29,880 --> 00:05:36,880
while black indicates masked pixels and an ultra-regressive task such as those areas.

51
00:05:37,840 --> 00:05:42,720
It's interesting to note here that the layers are able to learn a variety of specialized

52
00:05:42,720 --> 00:05:51,040
sparse patterns, so for example in image set A which are from the initial layers we noticed

53
00:05:51,040 --> 00:06:01,280
that black areas surrounded the target pixel for example this one and which resembles

54
00:06:01,280 --> 00:06:09,520
convolution and in image set B which is from layer 1920 we notice a row and column attention

55
00:06:09,520 --> 00:06:19,280
such as these ones. In further layers such as C we notice a data dependent access patterns

56
00:06:20,560 --> 00:06:25,840
and so for example the white areas here surround the bird

57
00:06:26,640 --> 00:06:39,360
and in the final set from layer 64 to 128 whites only appear in a few specific locations as we can see here

58
00:06:41,760 --> 00:06:52,480
and sometimes I'm not sure if that one is here so maybe we don't need the full attention maybe

59
00:06:52,480 --> 00:06:59,520
sparse attention is enough. Here we explain the motivation in a more mathematical way

60
00:07:00,480 --> 00:07:06,000
if you recall the attention function the softmax function and what you're seeing here is just a

61
00:07:06,000 --> 00:07:12,320
token by token form or the function with the output token only being able to attend the tokens

62
00:07:12,320 --> 00:07:19,120
before it. Notice that this is basically the decoder part of the transformer with all the inputs

63
00:07:19,120 --> 00:07:29,600
after the location of target output masked so as you can see here KSI, J can only reach

64
00:07:30,560 --> 00:07:41,520
the index specified in SI. So in the full attention mechanism SI here is equal to J

65
00:07:41,520 --> 00:07:47,920
where J is less than or equal to I meaning that the output token for example here can only attend

66
00:07:47,920 --> 00:07:58,320
to all of the previous input vectors. However from what we have seen before this might not be

67
00:07:58,320 --> 00:08:05,760
necessary so therefore the idea is to separate attention to pay attention has and find efficient

68
00:08:05,760 --> 00:08:15,360
subset of this SI such that the size of A is proportional to pth root of N.

69
00:08:19,840 --> 00:08:27,440
Here the author proposes two solutions when p is equal to 2. To the left is the full attention

70
00:08:27,760 --> 00:08:38,400
and in the middle they choose the hyperparameter L and in this case 4 and an output token would

71
00:08:38,400 --> 00:08:45,680
attend to the three tokens before them. For example you have this one attending to the three before

72
00:08:45,680 --> 00:08:59,440
them and also the fourth, the A's before them. The reason for this, the reason for like these

73
00:08:59,440 --> 00:09:08,560
light blue ones is to maintain full connectivity since this token are not connected to this one

74
00:09:09,520 --> 00:09:17,600
or this one. But through the light blue ones since this token, these tokens are connected to

75
00:09:19,120 --> 00:09:24,320
the three before it, if this token is connected to it then

76
00:09:27,920 --> 00:09:29,120
we have full connectivity.

77
00:09:29,280 --> 00:09:37,520
So this one is named the strided sparse transformer due to its shape.

78
00:09:40,880 --> 00:09:46,160
So to the right we think proposed another solution called the fixed sparse transformer.

79
00:09:46,960 --> 00:09:50,960
In this example the fifth, ninth and thirteenth token

80
00:09:50,960 --> 00:09:59,600
and also the seventeenth token would attend to the previous three tokens.

81
00:10:01,920 --> 00:10:06,400
By the fourth, the eighth and the top tokens attending to the previous two

82
00:10:08,320 --> 00:10:16,480
and also the top. Yeah and two and so on. These are indicated by the dark blue areas as you can see

83
00:10:17,440 --> 00:10:25,760
here. So now to maintain full connectivity, the set of four tokens should attend to the last token

84
00:10:25,760 --> 00:10:33,920
from the previous set. So this one for example should attend to this one and this one should also

85
00:10:33,920 --> 00:10:43,920
attend to this one to maintain full connectivity indicated by the light blue areas. So these

86
00:10:43,920 --> 00:10:50,560
these are just two examples when P equals to two. High dimensions are also possible according to the

87
00:10:50,560 --> 00:10:58,800
authors and the calculation here now then is reduced to over and square root of n in this case

88
00:10:59,440 --> 00:11:04,480
because the size of these areas are proportional to square root of n.

89
00:11:06,160 --> 00:11:12,080
So how about the space capacity? But instead of merely remembering every attention matrices

90
00:11:12,720 --> 00:11:18,720
in here they perform recalculation for the attention at the lower network and instead

91
00:11:18,720 --> 00:11:26,160
only seeming the results of the calculation here and here indicated by the gray areas.

92
00:11:28,160 --> 00:11:31,360
So this is called gradient checkpoint and recomputation.

93
00:11:34,000 --> 00:11:41,600
Therefore the space capacity is also reduced n times the model which is linear to n.

94
00:11:42,800 --> 00:11:44,960
Which is good when n gets really big.

95
00:11:47,840 --> 00:11:52,640
So now let's look at its performance against the state-of-the-art model at the time.

96
00:11:53,760 --> 00:12:00,400
So train and save our 10 new sets and we get the text data set and the means net

97
00:12:00,400 --> 00:12:07,360
60 by 4 by 64 new set and a 5 second 12 kHz classical music like classical music.

98
00:12:07,920 --> 00:12:15,040
So the bits but bits provide here means the number of bits required to generate a byte or a pixel.

99
00:12:16,160 --> 00:12:19,120
The lowest number is the value of the performances.

100
00:12:20,720 --> 00:12:22,720
So we can see here the sparse transformers here

101
00:12:24,800 --> 00:12:29,360
outperformed other models in a single test.

102
00:12:29,760 --> 00:12:41,760
The speed is also increased as indicated to the right as you can see here.

103
00:12:46,240 --> 00:12:53,840
And at the bottom right you can see that even with long contacts up to 12,160 tokens

104
00:12:54,560 --> 00:12:57,760
the performance is still good.

105
00:13:03,200 --> 00:13:05,760
Next we talk about Neoformer.

106
00:13:09,680 --> 00:13:14,720
The idea behind Neoformer is as follows. If you remember the intention function,

107
00:13:15,360 --> 00:13:21,280
the idea behind Neoformer is basically to project the key and value into lower dimensional

108
00:13:21,280 --> 00:13:29,120
matrices key time of dimension key times dk with k much smaller than n.

109
00:13:31,520 --> 00:13:38,000
This way the computation cost will be reduced to all of n times k.

110
00:13:39,200 --> 00:13:45,440
As soon as the dimension of the addition matrix would become n times k the space complexity

111
00:13:46,160 --> 00:13:50,960
also reduced to n times k times h.

112
00:13:52,800 --> 00:13:56,560
Now the question comes whether such predictions exist.

113
00:14:00,160 --> 00:14:03,680
Here the author proposed a theorem to show that they do.

114
00:14:05,040 --> 00:14:11,040
The theorem basically showed that for a certain value of k such projections E and F exists.

115
00:14:11,920 --> 00:14:24,080
At here k is dependent on n but I noted that the rank of qi k are composed

116
00:14:26,160 --> 00:14:32,960
divided by square root of dk is at most dk since the dimension of qi is equals to the

117
00:14:32,960 --> 00:14:35,600
dimension of ki which equals to n times dk.

118
00:14:35,840 --> 00:14:41,360
So this theorem combined with that fact gives the following.

119
00:14:43,680 --> 00:14:50,720
That k equals to 9 times log of dk divided by epsilon squared minus epsilon cubed

120
00:14:56,480 --> 00:15:03,440
which is independent of n and ei is equal to delta times r fi is equal to

121
00:15:04,160 --> 00:15:14,320
e to the power of negative delta times r where r is n by k matrix with identically

122
00:15:14,320 --> 00:15:24,560
independent entries from id entries from normal distribution with a mean of zero

123
00:15:24,560 --> 00:15:30,880
and the variance of one over k and delta which is equal to one over two to the power of n.

124
00:15:31,840 --> 00:15:39,840
This theorem still holds. So the proof of this theorem combines a bunch of inequality lemmas

125
00:15:39,840 --> 00:15:45,440
and so on and due to the time constraint I will not delve into the proof in too much details but

126
00:15:45,440 --> 00:15:54,000
just know that we can find a concrete k delta ei and fi so that we can transform

127
00:15:54,880 --> 00:16:02,720
this epsilon max function into the left where ki and vi are predicted into lower dimensions.

128
00:16:03,760 --> 00:16:10,960
So now we look at some examples of these theorems that I have performed to evaluate

129
00:16:10,960 --> 00:16:19,280
support formulas. Here the value tpl means the validation complexity and the lower that is

130
00:16:19,280 --> 00:16:23,600
the less uncertain the model is when predicting the better the performance.

131
00:16:25,520 --> 00:16:32,320
So first to compare the performance with the fixed sequence length but varying k so as we can see

132
00:16:33,040 --> 00:16:39,920
here and here I can see that the validation complexity of the lean former merged very close

133
00:16:41,280 --> 00:16:48,080
to the standard transformer and even not very much the larger k is the better the performance

134
00:16:48,080 --> 00:16:54,960
as we can see very slightly here if we zoom in a little bit we can see that it is a bit better.

135
00:16:59,680 --> 00:17:11,440
Yeah and next to compare the three different ways to visualize enf so first is the otherwise

136
00:17:11,920 --> 00:17:16,800
where they use the same e and f for each head

137
00:17:20,960 --> 00:17:27,600
but varying between layers and second is the key value wise we still use the same e for both

138
00:17:27,600 --> 00:17:34,560
key and values for each head varying between layers and third one is layer wise where they use a single

139
00:17:34,560 --> 00:17:42,000
e for all layers and all heads the performance of these three ways of sharing did not very much

140
00:17:42,640 --> 00:17:49,840
but layer rise chain was just a little bit more stable as we can see here finally

141
00:17:52,240 --> 00:18:01,360
for fixed k and varying n the bigger n is that's lower the complexity decreases but eventually

142
00:18:02,240 --> 00:18:04,480
emerge to similar values.

143
00:18:08,080 --> 00:18:14,400
Here let's look at some performances compared to other models they used a Robert A based model

144
00:18:14,400 --> 00:18:20,880
as a base model trained on the same corpus as burnt they're tested on four different benchmarks

145
00:18:21,440 --> 00:18:30,720
benchmark natural language understanding tasks is varying n and for each n varying k and the same

146
00:18:30,720 --> 00:18:36,080
strategy so we can see that the performance here and here

147
00:18:38,560 --> 00:18:45,680
are quite well compared to the baseline and except for this one here but overall on average

148
00:18:46,560 --> 00:18:55,520
the infomerate performed much better than the base lines well not much better but yeah

149
00:18:56,160 --> 00:18:59,360
there is there are like slight improvements

150
00:19:03,600 --> 00:19:04,480
and for this one

151
00:19:07,600 --> 00:19:13,360
we look at the time and memory efficiency improvements so lie the left to left is the

152
00:19:13,360 --> 00:19:20,640
time saved the right is memory saved the baseline here is the basic transformer

153
00:19:21,280 --> 00:19:27,040
and we can see here we can observe a great improvement over the basic transformer both

154
00:19:27,040 --> 00:19:31,520
in time and space as it increases

155
00:19:34,480 --> 00:19:42,080
so in the longer sequence it is the more obvious improvement is and the higher the project dimension

156
00:19:42,080 --> 00:19:51,280
k is the less obvious improvement is because as the dimension grows it takes a lot more time

157
00:19:52,160 --> 00:19:59,920
since like time dependencies like n times k it's going to be slower as k grow

158
00:20:03,600 --> 00:20:06,240
and next we talk about the linearization

159
00:20:07,200 --> 00:20:12,000
by kathropoulos flows in august 2020

160
00:20:14,960 --> 00:20:18,000
here we could recall the definition of the softmax function

161
00:20:19,600 --> 00:20:27,520
and given the sequence x is equal to x1 all the way to xn the i's position of the softmax of x

162
00:20:28,480 --> 00:20:35,920
is equal to e to the power of xi divided by the sum of the exponents of all xi

163
00:20:37,120 --> 00:20:44,640
so if i let i denote i's row of the matrix y denote the output of the attention mechanism

164
00:20:45,680 --> 00:20:49,120
then i's row of y can be calculated by the following

165
00:20:51,120 --> 00:20:56,240
y equals the softmax of qi times k divided by square root of dk times v

166
00:20:57,600 --> 00:21:01,040
talking in the softmax function gives the next part

167
00:21:01,920 --> 00:21:10,640
and finally we guess that the top part is can be seen as follows if qi is a vector like this

168
00:21:12,240 --> 00:21:19,680
and k oh sorry this should be a transpose here and k is like this

169
00:21:20,640 --> 00:21:24,400
then we have the first row of the

170
00:21:28,000 --> 00:21:34,000
this one's q1 this one's q2 the first row of the

171
00:21:34,960 --> 00:21:47,920
the attention matrix would be q1.k1 and q1.k2 and this one times by v

172
00:21:50,800 --> 00:21:56,000
this is v1 this is v2 would give v1

173
00:21:56,960 --> 00:21:59,760
times

174
00:22:02,080 --> 00:22:02,400
first

175
00:22:07,520 --> 00:22:16,000
would be v1 times q1.k1 plus v2 times q1.k2

176
00:22:17,840 --> 00:22:23,920
for the whole row all right so this is where we come from

177
00:22:23,920 --> 00:22:32,400
where the sum of q and k is equal to the exponents of q times k divided by square root of dk

178
00:22:42,000 --> 00:22:48,160
here the only constraint by the definition of an attention function is that their

179
00:22:48,240 --> 00:22:56,080
formation the same function is has to be non-negative so here we do not need to specify the kernel

180
00:22:57,920 --> 00:23:02,000
so we only need to specify the feature mark or the feature representation

181
00:23:02,960 --> 00:23:10,560
so a feature representation phi of a kernel k is a function such that k of x and y equals

182
00:23:10,560 --> 00:23:13,360
is equal to phi of x times phi of y

183
00:23:15,920 --> 00:23:20,080
so plugging that into the previous equation we have gives the following

184
00:23:22,720 --> 00:23:31,840
you can take out phi of qi transposed here because the sum does not affect i

185
00:23:32,800 --> 00:23:44,000
and notice that these sums can be computed once these sums can be treated computed once for every

186
00:23:44,000 --> 00:23:51,600
i and reused now like for for once and you've reused for every i the order should be reversed

187
00:23:52,240 --> 00:23:59,520
yeah the cost for computing the sums depend on n linearly and computing this

188
00:24:02,320 --> 00:24:12,080
cost o of n since the top sum produce a constant and the bottom sum at the bottom is a dot project

189
00:24:12,080 --> 00:24:23,040
and y sub i has d mod rows in total so in the end the modest computational cost is linear to n

190
00:24:26,880 --> 00:24:34,800
and the the space cost would also be reduced linear to n since we do not need to save the

191
00:24:34,800 --> 00:24:43,520
floatation restriction anymore each of these requires our friend to save

192
00:24:46,320 --> 00:24:53,360
and the choice of phi x was the in this paper was the exponential linear unit activation function

193
00:24:53,360 --> 00:24:58,960
plus one to avoid setting the gradient to zero and xx negative

194
00:25:04,400 --> 00:25:11,680
so let's come when it's come to causal masking which is a technique used in autoregressive feature

195
00:25:11,680 --> 00:25:17,600
generation mentioned before basically mask the tokens after the target token making only the

196
00:25:17,600 --> 00:25:24,640
tokens before it visible to that token in this case our equation becomes the following

197
00:25:25,520 --> 00:25:28,000
we simply replace n by i

198
00:25:30,640 --> 00:25:39,200
and replace the top sum by s i and the bottom sum by z i so notice that s i can be computed from

199
00:25:39,200 --> 00:25:48,640
s i minus one and by adding phi k i phi of k i times v i transpose and that i can become the

200
00:25:48,640 --> 00:25:56,480
computed from z i minus one by adding phi of k i since the computation of the sums are linear to n

201
00:25:56,480 --> 00:26:01,360
and adding those are constant to n we still have a linear time complexity

202
00:26:04,160 --> 00:26:06,000
but how about the space complexity

203
00:26:06,320 --> 00:26:14,880
here during the back propagation the gradients with respect to q k and v

204
00:26:15,600 --> 00:26:24,480
needs to be computed from the floatation matrix giving a space complexity of o of s square in the

205
00:26:24,480 --> 00:26:31,920
standard transformer but here it also derives the gradients so that the time cost is linear

206
00:26:32,560 --> 00:26:35,040
to the space cost and the space cost is constant

207
00:26:36,960 --> 00:26:44,080
given the numerator from the previous equation and the gradient of the loss function

208
00:26:48,160 --> 00:26:55,840
nebulous of y bar of l the authors were able to derive the gradients with respect to phi of

209
00:26:56,640 --> 00:27:00,800
q phi phi q of k and v as follows

210
00:27:03,200 --> 00:27:07,840
we can see that these gradients all require linear to n time

211
00:27:10,720 --> 00:27:15,600
as shown here here and here these calculations

212
00:27:15,920 --> 00:27:25,600
are only like linear to n just because the sum here are there and that many

213
00:27:27,120 --> 00:27:31,520
operations these operations are not dependent on n even

214
00:27:33,280 --> 00:27:40,480
and solving them require constant time because the final values does not depend on n

215
00:27:41,360 --> 00:27:50,720
and a total time and completion cost does all linear overall if combined with

216
00:27:51,440 --> 00:27:57,520
previous steps we will omit the discussion regarding the derivation of the gradients because

217
00:27:58,320 --> 00:28:04,480
time and space are limited but if you're interested you can go to the paper and have a read

218
00:28:05,360 --> 00:28:09,360
so

219
00:28:12,000 --> 00:28:19,120
let's move on from what if you have gathered above they formalize the transformers as an

220
00:28:19,120 --> 00:28:25,760
recurrent neural network this and i quote is a first step towards better understanding

221
00:28:26,400 --> 00:28:32,240
the relationship between transformers and popular recurrent networks and the process

222
00:28:32,960 --> 00:28:40,320
is used for storing and retrieving information the authors do not specify like why rnn means

223
00:28:40,320 --> 00:28:46,320
linear time space complexity their goal for formulating transformers in this way of another

224
00:28:46,320 --> 00:28:53,840
n is just to emphasize the idea that transformers and rnn can be closely related to each other

225
00:28:54,320 --> 00:29:06,640
so here s of s and z are hidden states x i are easy i's output and y i is the i theme

226
00:29:07,200 --> 00:29:10,880
okay sorry s i is the output and y i is i's output

227
00:29:13,040 --> 00:29:18,400
now let's look at some experimental results we just quickly run through them and

228
00:29:19,040 --> 00:29:27,120
the black line represents all those models and the blue lines are the from the reformer model

229
00:29:27,120 --> 00:29:34,160
which was another efficient transformer model and the red line here is then a transform

230
00:29:35,120 --> 00:29:38,480
and we can see that the linear attention model performs the rest in both

231
00:29:39,280 --> 00:29:43,120
time and space requirements as the sequences grows

232
00:29:43,440 --> 00:29:51,600
next we will look at the convergence of cross entropy loss

233
00:29:52,960 --> 00:29:56,720
we can see that in the step growth the linear addition model loss

234
00:29:57,520 --> 00:30:04,960
converged to that of a standard transformer while the reformer did not quite reach there

235
00:30:05,280 --> 00:30:15,600
so comparing image generation we can see that the linear attention model significantly

236
00:30:19,280 --> 00:30:26,640
affirms the other models in terms of speed as seen here although the performance is not the best

237
00:30:27,200 --> 00:30:31,680
the base per dimension is not the lowest they're still closed to the baseline

238
00:30:35,920 --> 00:30:45,520
and here are just some illustrations of the image generation loss

239
00:30:46,320 --> 00:30:50,560
and we pause to observe some of them but i will not delve into too much details

240
00:30:53,520 --> 00:30:57,360
and finally on speech recognition tasks the linear transformer

241
00:30:58,160 --> 00:31:02,640
model still outperforms the rest in terms of time spent per epoch

242
00:31:02,640 --> 00:31:10,160
it also has lower error rate than two of the baseline models although not outperforming the

243
00:31:10,160 --> 00:31:13,600
standard transformer there is a self max function

244
00:31:16,720 --> 00:31:23,200
next we will talk about law former by belt g in december 2020

245
00:31:25,920 --> 00:31:32,160
the idea here is quite similar to that of the sparse transformer so we will not

246
00:31:32,160 --> 00:31:39,280
delve too deep into it so basically they define a fixed window size independent of n

247
00:31:39,280 --> 00:31:44,720
instead of dependent to n so that the total computational cost and space cost will be linear to

248
00:31:44,720 --> 00:31:56,960
n here we show their approaches and they propose three forms of sliding window attention

249
00:31:57,920 --> 00:32:03,600
first one simply called the sliding window attention and they are given like they provide

250
00:32:04,240 --> 00:32:10,560
at the hyper parameter of like a fixed window size w and each token would attend to one half

251
00:32:10,560 --> 00:32:19,760
of w token on each side like the one here the second one is called the dilated sliding window

252
00:32:19,760 --> 00:32:29,200
which is sliding window with gaps of size dilation d as seen here and the global attention on the

253
00:32:29,200 --> 00:32:37,200
other hand specify certain tokens that both attend to all other tokens and receive attention from

254
00:32:37,200 --> 00:32:45,360
all other tokens for example for classification global attention is used for the cls token

255
00:32:45,680 --> 00:32:52,080
while specifying the global attention is tax specific it is an easy way to add

256
00:32:52,640 --> 00:33:00,560
inductive bias to the model's attention and it is much simpler than existing task specific

257
00:33:00,560 --> 00:33:06,400
approaches that use complex architecture to combine information across similar

258
00:33:06,400 --> 00:33:14,160
like smaller input chunks so what it did is to combine the diet the sliding window and global

259
00:33:14,160 --> 00:33:21,520
attention for example in those tokens here and here and the ones here

260
00:33:24,000 --> 00:33:29,920
and since those are independent of n because the global sliding window the

261
00:33:33,440 --> 00:33:39,200
sorry for the global plus sliding window approach they use two sets of q k and v

262
00:33:40,080 --> 00:33:50,480
similar to that of the sparse transformer yes but they are all initialized to be the same

263
00:33:51,280 --> 00:33:56,240
and there's a number of such global tokens as smaller relative to n so the total computational

264
00:33:56,240 --> 00:34:01,280
cost is still independent to n and therefore is linear to n

265
00:34:09,200 --> 00:34:20,720
now let's come to some experiments here dbc is dpc is equal to the equivalent concept to

266
00:34:21,520 --> 00:34:29,440
base per pixel in terms of text text base task you can see that long former has the best

267
00:34:30,400 --> 00:34:39,200
small model of vpc on tx8 and nvk data set and close to best dpc for large models on

268
00:34:40,560 --> 00:34:51,040
nvk so yes we can see here the right it shows comparison within the model for different

269
00:34:51,040 --> 00:34:55,840
mode of sliding windows and dilation

270
00:35:01,760 --> 00:35:04,800
and to the bottom right it tests several of our configurations

271
00:35:11,280 --> 00:35:14,960
here we show some other accuracy results for q and a

272
00:35:15,520 --> 00:35:22,560
for referencing classification compared to barbata and current leaderboard

273
00:35:24,720 --> 00:35:27,760
as of current as of may of 2020

274
00:35:30,320 --> 00:35:38,880
so you'll see here an offmer based how to perform the robot based and the score is pretty good

275
00:35:38,880 --> 00:35:49,760
and the final final uh just more results you may pause and read them if you want to but

276
00:35:50,560 --> 00:35:58,960
i think i should move on due to the limited time so lastly we come to running feature tension of

277
00:35:58,960 --> 00:36:11,360
a proposed to proposed by punk l in march of 2021 and the idea here is inspired by linearized

278
00:36:11,360 --> 00:36:20,880
attention first we introduce the preliminary theorem used if we let wi be independently

279
00:36:20,880 --> 00:36:30,480
sampled vectors from a normal distribution with zero mean and sigma squared times the identity

280
00:36:30,480 --> 00:36:40,640
matrix outside the variance in this like in other words like let each entry in wi be sampled from

281
00:36:41,600 --> 00:36:45,600
the normal distribution we and let

282
00:36:48,400 --> 00:36:58,960
phi i at flip five be a non-linear transformation from r small r small d to r two times big d

283
00:37:00,560 --> 00:37:09,280
where phi of vector x is equal to square root of one over big d times the transpose of sign of

284
00:37:09,280 --> 00:37:20,080
w one dot x all the way to sign of w d dot x and cosine of w one dot x all the way to cosine of w

285
00:37:20,080 --> 00:37:32,640
d x transpose then we take as expected values of phi i time the phi x times y is

286
00:37:36,160 --> 00:37:45,600
i excuse me given wi is the exponent of the length of x minus y squared divided by two

287
00:37:45,600 --> 00:37:53,600
times sigma squared negated and here we notice that big d is a hyper parameter

288
00:37:54,320 --> 00:38:03,920
independent of sequence length so how does this theorem help us well now we can perform the following

289
00:38:03,920 --> 00:38:12,720
operation the exponents of q are transpose times k i divided by square root of dk equals to this

290
00:38:12,720 --> 00:38:20,800
formula here we simply just square the inside and separate them

291
00:38:26,080 --> 00:38:34,960
and here we notice the second components is second the second components can be replaced

292
00:38:35,760 --> 00:38:41,120
like now we can use the theorem before to replace it with its kind of representation

293
00:38:41,760 --> 00:38:49,920
to obtain this final form now we can see why this work is inspired by linearization

294
00:38:51,840 --> 00:38:54,960
we call that the attention function from the linear attention

295
00:38:56,880 --> 00:39:06,960
so right here we just replace the same function with what we have here and define what

296
00:39:07,680 --> 00:39:12,960
we define what we obtain as r f a of g i k v

297
00:39:16,240 --> 00:39:26,560
so all times here this all times here denote the out the output which produced matrix whose

298
00:39:26,560 --> 00:39:34,320
entries are all products of n element in the first vector with n element in the second vector

299
00:39:34,880 --> 00:39:42,560
corresponding so for example if i have two vectors they produce the cross product produce this matrix

300
00:39:46,480 --> 00:39:47,680
and so on and so forth

301
00:39:51,520 --> 00:39:59,120
and here we will discuss the time and space complexity of r f a here we just use a different

302
00:39:59,120 --> 00:40:06,160
letter but it's basically the same idea in this case which is called the gaussian

303
00:40:06,960 --> 00:40:18,560
chrono five qt or five qi takes time 2d 2 times dk 2 times c times dk and space of 2d

304
00:40:18,560 --> 00:40:31,360
and this product cross product here takes time 2 times n times d times dk and space of 2 times d

305
00:40:32,160 --> 00:40:34,720
times dk and

306
00:40:37,200 --> 00:40:47,120
summation here alone takes time of all of 2n times dk d times dk and space of all of 2d

307
00:40:48,800 --> 00:40:57,680
and the opposition with five qt the dot products here and here takes time 2d k therefore the time

308
00:40:58,240 --> 00:41:08,960
is the full time is linear to n and the memory total memory is 4d plus 2d

309
00:41:09,840 --> 00:41:16,000
dk which is constant but it highly depends on d

310
00:41:16,080 --> 00:41:23,840
and the virtual experiment shows that when d is greater than 2 times little d

311
00:41:24,480 --> 00:41:34,320
depending on this is marginal so like the linearized attention this again gives us an

312
00:41:34,320 --> 00:41:40,960
argument in case of causal attention now i need to remind everyone here that this does not explain

313
00:41:40,960 --> 00:41:48,080
why time as speaks complexity is linear and this is this simply just strengthen the relationship

314
00:41:48,080 --> 00:41:58,320
between a transformer and an rnn so due to this rnn like property rfa can be made stateful

315
00:41:59,280 --> 00:42:05,520
so that is we can pass last in the states

316
00:42:08,480 --> 00:42:16,960
st and zt to the next mini batch during both training and devaluation we will see later that

317
00:42:16,960 --> 00:42:26,400
this improves the problems quite a bit so another benefits that rnn like form gives us is the ability

318
00:42:26,400 --> 00:42:41,040
to learn previously bias with our gt mechanism here wg and bg are learning parameters and gt is

319
00:42:41,040 --> 00:42:48,720
between 0 and 1 the hidden states are multiplied by gt which is the learned scatter so that the

320
00:42:48,720 --> 00:42:53,840
history is exponentially decayed if you were in the most recent context

321
00:42:58,240 --> 00:43:06,960
so those are besides the gaussian kernel also proposed an arc cosine kernel

322
00:43:07,680 --> 00:43:12,880
they chose from a family of chipped variant kernel to rectify linear unity activation function

323
00:43:12,880 --> 00:43:24,400
here the redo function is simply the max of 0 and x this achieves no results and is

324
00:43:25,440 --> 00:43:34,160
supplements to alternative to self max again besides the gaussian kernel

325
00:43:34,480 --> 00:43:40,800
so here let's look at some experimentation results

326
00:43:42,640 --> 00:43:44,960
and the baseline used here is the linearized attention

327
00:43:46,560 --> 00:43:54,800
here compared with the variances proposed in this one we can see that in terms of

328
00:43:54,800 --> 00:44:04,720
propensity the gt variant performs much better than the baseline and on gt ones as we can see here

329
00:44:07,680 --> 00:44:10,480
and yeah the city of awareness performs even better

330
00:44:14,240 --> 00:44:20,000
and when it comes to machine translation where the baseline is the standard transformer we're

331
00:44:20,000 --> 00:44:25,920
going to see that the speed of rfa is nearly as smooth as the linearized attention

332
00:44:27,120 --> 00:44:32,480
shown here while the performance is near the baseline and better than linear

333
00:44:33,840 --> 00:44:37,920
attention as shown for example here

334
00:44:42,000 --> 00:44:45,840
and here is just a comparison with other models on different text data sets

335
00:44:46,720 --> 00:44:53,120
and we can see that it has like an outstanding accuracy among the test models

336
00:44:53,920 --> 00:44:59,680
and it did not but it did not lead in terms of speed and memory consumption

337
00:45:00,800 --> 00:45:06,720
but still it is reached quite up there for example here

338
00:45:06,720 --> 00:45:13,840
and finally we look at the comparison of to the softmax function

339
00:45:15,520 --> 00:45:20,720
of the two rfa variances in terms of decoding speed and memory consumption

340
00:45:21,840 --> 00:45:24,480
notice that the x scale is log 2 based

341
00:45:27,040 --> 00:45:33,200
as questions growth the speed of softmax clearly shows a quadratic decay

342
00:45:34,080 --> 00:45:40,000
while the speed of rfa in both variances did not vary much it even dropped a little bit

343
00:45:42,480 --> 00:45:47,040
oh sorry it went up a little bit but then dropped a little bit again

344
00:45:48,720 --> 00:45:58,320
and in terms of memory consumption softmax grows correctly while rfa remains almost constant

345
00:45:59,200 --> 00:46:07,280
so that's all the solutions proposed I will use today and as a final note I will present

346
00:46:07,920 --> 00:46:10,640
a github page comparing all the efficient transformers

347
00:46:12,080 --> 00:46:18,960
shot to Professor Wen Hu Chen for this page

348
00:46:21,040 --> 00:46:26,720
and this website lists all the efficient transformers as of the 8th of November 2020

349
00:46:26,800 --> 00:46:30,240
and the performance comparisons as shown in this chart

350
00:46:31,680 --> 00:46:34,720
you can go to this link for more information about it

351
00:46:36,960 --> 00:46:39,840
and that's it thank you for your time

