1
00:00:00,000 --> 00:00:10,800
Hello, and then it's kind of along with me, it's Jerry. We will be talking about large

2
00:00:10,800 --> 00:00:20,240
language models in lecture 9 of CEF 886 recent analysis on foundation model. What is large

3
00:00:20,240 --> 00:00:29,720
language model? Now, in order to cover our agenda for today, we will focus on several

4
00:00:29,720 --> 00:00:36,200
different types of large language models in terms of architecture. We will be focusing

5
00:00:36,200 --> 00:00:45,080
on DPT3, T5, Codex, Lama2, and so on. We also focus on supervised learning and compare them

6
00:00:45,080 --> 00:00:52,200
to other types of learning. You're free to stop aside any time and ask questions. We

7
00:00:52,200 --> 00:00:56,920
do have a set of questions at the end of this session for that discussion.

8
00:00:57,720 --> 00:01:05,800
Now, for large models, we are focusing on deep learning, which is really natural language

9
00:01:05,800 --> 00:01:13,000
pressing tasks. Large language models are computational models that have the capacity,

10
00:01:13,000 --> 00:01:17,400
capability to understand and generate human models. Large language models

11
00:01:20,120 --> 00:01:25,160
have the transmittive ability to predict the likelihood of work sequences or during a text

12
00:01:25,160 --> 00:01:30,680
based on a given input. Example of how this has been done in the past, in Ngram models,

13
00:01:30,680 --> 00:01:37,480
FKM will probably teach that actually based on traditional text, we try to estimate the right

14
00:01:39,640 --> 00:01:45,400
word probability. They are usually trained on massive amount of public-care legal data.

15
00:01:46,360 --> 00:01:50,520
Large language models have the transmittability to predict likelihood of work sequences or during

16
00:01:50,600 --> 00:01:55,400
a text based on a given input. They are advanced programming models with multi-parameter

17
00:01:56,040 --> 00:02:01,880
and exceptional learning capabilities. Separation model in transformers serves as a fundamental

18
00:02:01,880 --> 00:02:07,880
building block for large language model tasks. Transmortem has a reference to visualize the

19
00:02:07,880 --> 00:02:14,120
field of finality with the ability to handle sequential data efficiently, align for parallelization

20
00:02:14,120 --> 00:02:22,040
and capturing language, penises, and text. One key feature of fellow learners is in-contact

21
00:02:22,040 --> 00:02:26,760
learning, which we will talk about later. Now, we want to do a comparison of large

22
00:02:26,760 --> 00:02:33,480
language models in comparison to deep learning on machine learning and deep learning. With

23
00:02:36,760 --> 00:02:42,200
large language models, usually the training data are very large in the orders of

24
00:02:42,840 --> 00:02:51,960
terabytes, basically. You don't have to directly identify any of the features or any of the variables,

25
00:02:51,960 --> 00:02:56,920
it does automatically. And the models tend to be very complex. They are

26
00:02:59,240 --> 00:03:04,200
complex because all they're dealing with are really complex problems. Now, one of the problems

27
00:03:04,200 --> 00:03:09,960
they have, though, is that they are not easy to interpret. It is harder to interpret the

28
00:03:09,960 --> 00:03:16,280
conclusions that they have. This is the concept with large language models, and we'll talk about

29
00:03:16,280 --> 00:03:21,000
that later. They also have very high platform and they require a massive amount of compute power in

30
00:03:21,000 --> 00:03:28,200
order to run. Really, we're talking another petafloat per second per day here, and they do require

31
00:03:29,080 --> 00:03:34,760
a lot of money in order to do that. Now, natural language present tasks tend to be

32
00:03:35,560 --> 00:03:45,000
in three major areas, which is understanding reasoning and then generation of new materials.

33
00:03:45,000 --> 00:03:52,520
On understanding the four major areas there, we're looking at sentiment analysis,

34
00:03:52,520 --> 00:03:59,640
this is where it takes a paragraph or a body of text and then interprets it to determine

35
00:03:59,640 --> 00:04:05,960
the emotional condition. So, this was usually binary or a triple. It could be, oh, yeah,

36
00:04:05,960 --> 00:04:12,840
this is a prophecy half. And it meant on this or negative sentiment or neutral.

37
00:04:13,560 --> 00:04:21,480
They also can handle text classification. So, it's similar to sentiment analysis, but then

38
00:04:21,480 --> 00:04:28,280
it requires a definite classification of that text. And one of the things it can also do is

39
00:04:28,280 --> 00:04:34,280
natural language inference, which is the termination of given, whether it give or protest this

40
00:04:35,000 --> 00:04:39,720
follow-up based on the given premise of the text in which they're dealing with.

41
00:04:40,840 --> 00:04:46,120
Yes, of course, they might be understanding. And then in terms of reasoning, this is really where

42
00:04:47,160 --> 00:04:52,120
the model must be able to comprehend any provided information that which

43
00:04:52,520 --> 00:04:57,480
utilize reasoning to deduce answer where next year at least I have checked.

44
00:04:57,480 --> 00:05:02,360
Then they can generate text. So, they can do summarization. You know, they can take a whole

45
00:05:02,360 --> 00:05:09,720
concise, they can generate a concise abstract from a large body of text and they can do question

46
00:05:09,720 --> 00:05:18,360
answering. So, the first model we'll talk about here is P5. P5 stands for text-to-text transfer

47
00:05:18,440 --> 00:05:24,840
transformer. So, it can perform tasks such as translation, question answering, classification,

48
00:05:24,840 --> 00:05:30,840
signature score or summarization. Now, it has a unified approach to problems. It basically

49
00:05:30,840 --> 00:05:36,520
creates everything as text and then outputs as text as well. What is thought that allows

50
00:05:37,400 --> 00:05:43,400
how to use the researcher of P5 to use the same model, the same post-function,

51
00:05:43,400 --> 00:05:49,960
hop-up parameters, layers, SSTURA, across a diverse set of tasks. So, they treat every text

52
00:05:50,520 --> 00:05:55,960
problem on the text and they use what they talk about, what are the SSTURA representation,

53
00:05:55,960 --> 00:06:00,920
which is usually numbers. And the input is a common core project which produces about

54
00:06:01,560 --> 00:06:06,600
really 20 terabytes of text extra from web edges each month. They clean up that in order to make

55
00:06:06,600 --> 00:06:12,840
its process usable. The model itself is first portrayed on the data rich task before being

56
00:06:12,840 --> 00:06:19,160
fact-tuned or in the downstream task. And the building block is safe attention. So, for example,

57
00:06:19,160 --> 00:06:26,040
if you look at what we have in diagram here, the first top sentence on the left side has a prefix

58
00:06:26,040 --> 00:06:30,760
of translating this to German, that is good. And it goes through the transformer and the

59
00:06:31,560 --> 00:06:40,120
correct German response comma, that is good. Now, let's talk about the architecture of this T5.

60
00:06:40,840 --> 00:06:48,440
So, confirmation fair was first introduced in the terminal, 37 people called at the

61
00:06:48,440 --> 00:06:54,440
show it's all in it. That was by one analyst. It was a groundbreaking architecture that used

62
00:06:54,440 --> 00:07:02,440
attention to add context to data. The encoding, in this case, has an encoding component

63
00:07:03,160 --> 00:07:08,040
and a decoding component. So, the encoding component accepts input sequence and a map

64
00:07:08,200 --> 00:07:12,360
after a continuous repetition that holds all the learning functions about the input.

65
00:07:13,000 --> 00:07:17,160
And then the decoding component then takes the continuity of the token and then sequentially

66
00:07:17,160 --> 00:07:23,720
generates output while also being fed its previous output. So, the encoding component accepts all

67
00:07:23,720 --> 00:07:27,640
vectors in the input sequence at the same time, but for function encoding so that it knows the

68
00:07:27,640 --> 00:07:35,480
order of input within the sequence of the input. And then it does that by adding extra vector to

69
00:07:35,480 --> 00:07:41,880
each input by embedding it within the vector. And then for every odd input within the sequence,

70
00:07:41,880 --> 00:07:48,440
it added vector created by cosine function and for every even, it adds a vector created by sine

71
00:07:48,440 --> 00:07:55,160
function. Now, there is a linear layer just as you will see in the diagram on the bottom right.

72
00:07:55,160 --> 00:07:59,720
This in the layer is a neural network that provides the vector, that projects the vector

73
00:07:59,720 --> 00:08:05,720
produced by decoding component onto a much larger vector called a logic vector. Now,

74
00:08:05,720 --> 00:08:10,760
the logic vector size is determined by a number of unique words in the model's vocabulary.

75
00:08:10,760 --> 00:08:16,200
So, the job of the softbox layer, which is the last one before the output on the bottom right,

76
00:08:17,320 --> 00:08:20,920
is to convert the logic score into probability. So, the word

77
00:08:22,120 --> 00:08:26,440
or this input sequence with the output sequence with the highest probability in

78
00:08:26,440 --> 00:08:30,840
the model vocabulary is chosen and then it's predicted as output during this time step.

79
00:08:31,400 --> 00:08:36,680
The output is then fed back to the decoding component to help inform or provide context

80
00:08:36,680 --> 00:08:46,200
to each prediction for the next layer. This is known as auto regression. Know that as input

81
00:08:46,200 --> 00:08:52,600
flow from one encoder to the next, it also puts the next encoder in the stack. It's

82
00:08:52,600 --> 00:09:00,200
simply happening with the stack of decoders. Now, let's go into details of the encoding and

83
00:09:00,200 --> 00:09:05,160
decoding layer. So, within the encoder layer, you have what we call the safe attention. This

84
00:09:05,160 --> 00:09:10,920
mechanism enables what you interact with each other, which is a part of that so that they can

85
00:09:10,920 --> 00:09:17,160
figure out which other words they should pay more attention to. The encoder decoder attention

86
00:09:18,120 --> 00:09:22,840
layer, which is within the decoding component, the goal of that layer is to help

87
00:09:24,280 --> 00:09:27,800
the model align the words in the input sequence with the words in the output sequence.

88
00:09:28,440 --> 00:09:34,600
And then the linear layer, which is the neural network layer, it's a neural network. But

89
00:09:34,600 --> 00:09:40,200
in that project, the vector produced by the decoding component into a much larger

90
00:09:40,200 --> 00:09:45,320
vector called a logic. And then the size of the logic, of course, as mentioned earlier,

91
00:09:45,320 --> 00:09:51,160
is something but not a unique word in a word or vocabulary. The development of the

92
00:09:51,160 --> 00:10:00,200
top model is to provide logistic probabilities. And that's really what happens within the

93
00:10:00,200 --> 00:10:08,520
data form model. Now, you've seen this before, this is a standard transformer model architecture,

94
00:10:08,520 --> 00:10:18,120
which we've had in earlier lectures. So, I won't really go into individual details of how this

95
00:10:18,120 --> 00:10:26,840
works. Now, it's, as we mentioned, consists of both an encoder, it's because an encoder

96
00:10:26,840 --> 00:10:32,200
decoder architecture, it's the encoder and encoder are really similar in size. They each

97
00:10:32,200 --> 00:10:37,160
consist of 12 blocks each. And then each block comprises the safe attention, the optional encoder

98
00:10:37,160 --> 00:10:45,000
decoder attention and a feed forward network. And that's, this is a standard transformer

99
00:10:45,000 --> 00:10:53,960
architecture. Now, within the T5 transformer, attention is handled really in a unique way.

100
00:10:53,960 --> 00:10:58,760
So, the model process, each input in a input sequence, each token input sequence, and they

101
00:10:58,760 --> 00:11:03,000
assign different attentional scores to other tokens in the safe sequence. Now, this calls

102
00:11:03,000 --> 00:11:08,280
determining patterns of each token's contribution to the model's understanding of the context.

103
00:11:08,280 --> 00:11:15,160
And so, within that encoder decoder attention, so let's talk about this. So, what happens that

104
00:11:15,160 --> 00:11:18,920
the queries come from a previous decoder layer and then the memory and keys come from the output

105
00:11:18,920 --> 00:11:23,240
of the encoder. And it allows every question encoder to attach all the questions in the

106
00:11:23,240 --> 00:11:27,480
input sequence. Now, that helps aligns the world in the input sequence with the words in the

107
00:11:27,560 --> 00:11:36,440
input sequence. And we also have the encoder also contains safe attention. And the other part,

108
00:11:36,440 --> 00:11:42,760
which is the marked safe attention in the decoder. There could be variance in the T5 architecture.

109
00:11:42,760 --> 00:11:50,280
So, they are slightly three different variants that people talked about,

110
00:11:50,280 --> 00:11:55,560
don't you know people that wish this was based upon? Now, the fact is really that

111
00:11:56,920 --> 00:12:03,320
this distinguishing factor really is, it must be used by different attention mechanism in the mode.

112
00:12:05,000 --> 00:12:11,720
Now, if you look at this, we have dark colors and then light colors, right? So,

113
00:12:12,520 --> 00:12:19,320
the different color indicates the different performance. Dark, gray lines correspond to the

114
00:12:19,400 --> 00:12:25,240
fully visible masking and then the light gray lines correspond to casual masking. So, we used the

115
00:12:25,240 --> 00:12:31,160
dot that you can see in that diagram to denote a special end of sequence token that represent

116
00:12:31,160 --> 00:12:36,760
the end of the prediction. The input and output are represented as x and y respectively. On the left

117
00:12:37,320 --> 00:12:42,200
side, which is the first model we have on the left side, you have just the standard encoder,

118
00:12:42,200 --> 00:12:46,920
decoder architecture, then you see it's usually a fully visible masking encoder and then the encoder,

119
00:12:47,800 --> 00:12:53,400
decoder characterization with careful markings in the decoder, which is a light gray line.

120
00:12:53,400 --> 00:12:58,200
In the middle, we have a language model consisting of a single transformer layer stack

121
00:13:00,600 --> 00:13:06,600
and that is spread the concatenation of the input and target crossing using a car roll,

122
00:13:07,960 --> 00:13:14,440
a causal mask throughout. On the right side, you have a prefix language model. This has

123
00:13:14,760 --> 00:13:19,800
a prefix language model corresponding to a line with visible masking over the input.

124
00:13:21,000 --> 00:13:26,040
So, the attention, this has the different attention mask patterns that you can have. The

125
00:13:26,040 --> 00:13:31,480
attention mask pattern is a crucial component in large language models itself as a mechanism

126
00:13:31,480 --> 00:13:38,120
to control which part of the input sequence the model should pay attention to during the training

127
00:13:38,120 --> 00:13:45,560
or inference process. So, there may be four types of attention masks. You have what we call

128
00:13:45,560 --> 00:13:52,600
full attention. So, in some models, every token is allocated to every other token input.

129
00:13:53,560 --> 00:13:59,000
This is known as full attention and is often used in smaller models. If you have a scale

130
00:13:59,720 --> 00:14:04,920
dot product attention to make completion more efficient, large models typically use scale

131
00:14:05,480 --> 00:14:10,200
dot attention, scale dot product attention. This means that the tokens only attend to other

132
00:14:10,200 --> 00:14:15,800
tokens within a certain distance and that distance is determined by a window size fixed

133
00:14:15,800 --> 00:14:22,120
context length and you have what we call the masked attention. In this case, the model should

134
00:14:22,120 --> 00:14:27,960
not tend to put your tokens to avoid the time leakage during training and a triangular attention

135
00:14:27,960 --> 00:14:32,840
mask is used. This marking shows that each token can only attend to previous tokens in the sequence

136
00:14:32,840 --> 00:14:37,800
and then you can have the case of attention heads. So, large language models like GPT 3.5,

137
00:14:38,520 --> 00:14:42,760
employee multi-potential head, each head lands different attention patterns along the model

138
00:14:42,760 --> 00:14:47,480
to capture various information within the data. I now have a couple of different aspects like

139
00:14:47,480 --> 00:14:56,360
thinker, semantics, or sentiment. So, in this diagram that we showed here, the dark cell

140
00:14:57,240 --> 00:15:05,080
at row i and column j, in the case of the second semicolon, it's allowed to attend

141
00:15:05,800 --> 00:15:13,720
to input element j at time step i. So, for example, it has to input x1 at y1.

142
00:15:14,760 --> 00:15:21,480
Now, looking at the performance of the various different models, you can see here that the

143
00:15:22,120 --> 00:15:31,480
encoder, the encoder architecture, it's referred to the previous architecture

144
00:15:31,480 --> 00:15:36,360
variants I would describe. P refers to the number of parameters in the 12-layer

145
00:15:37,240 --> 00:15:44,360
base transformer stack and M refers to the complete power, the flux required

146
00:15:45,240 --> 00:15:51,240
to process a sequence of using the encoder-decoder model. So, we evaluate each actor

147
00:15:51,240 --> 00:15:55,640
variant using a denoicing objective and then autoregressive, which is commonly used

148
00:15:56,440 --> 00:16:01,320
between objects. We consider the encoder-decoder texture with denoicing. Denoicing helps reduce

149
00:16:01,320 --> 00:16:06,360
the randomness and corresponding input data. How is it there for fun? Now, when we see noise here,

150
00:16:06,360 --> 00:16:13,320
we refer to, noise refers to random or unpredictable fluxion data that disrupts the ability

151
00:16:14,120 --> 00:16:20,440
to identify type of path transmission. So, that when you have to watch noise in the system,

152
00:16:20,440 --> 00:16:27,880
it results in reduced accuracy and then relapses of the motor's protection or output.

153
00:16:28,920 --> 00:16:35,160
Now, in terms of the data that T5 uses, it uses what we call closer, clean, crawl, couples,

154
00:16:36,120 --> 00:16:40,440
which I mentioned much earlier at the beginning of this. It's really a set of

155
00:16:40,840 --> 00:16:49,560
friendship by text data that's extra for the web. And it's number to clean up. It uses

156
00:16:50,120 --> 00:16:56,040
some simple heuristic operator to remove boiler text places, to duplicate, agree, retain line.

157
00:16:56,040 --> 00:17:00,840
Each line must continue at least five words. And it discusses any page that's

158
00:17:00,840 --> 00:17:06,840
right as three sentences and then removes any page also that may contain any item from the

159
00:17:07,800 --> 00:17:13,800
list of dirty, not your obscene or the worst bad word. And of course, it also removes a

160
00:17:13,800 --> 00:17:21,480
duplicating data. Now, what are these type of tasks that T5 does in downstream form? I think

161
00:17:21,480 --> 00:17:27,320
they don't really serve. It can be used to measure general language and abilities. You can

162
00:17:28,120 --> 00:17:31,640
check for sentences like subject judgment, different assessment analysis with it.

163
00:17:32,200 --> 00:17:35,640
You can use it to paraphrase sentence or find some narrative between them.

164
00:17:36,760 --> 00:17:42,200
You could also find natural language inferences to figure out what is happening within the

165
00:17:42,200 --> 00:17:48,760
particular sentence. And then you can do sentence completion or core reference resolution or what

166
00:17:48,760 --> 00:17:51,720
says at the end of the question. It's also got to help answer the question.

167
00:17:52,040 --> 00:18:02,920
In order to train the model, we basically follow this very simple text. It goes ahead and

168
00:18:03,640 --> 00:18:14,520
pretend using a pretend step of 2 to power 19 or 524,000 of variety of steps. And

169
00:18:15,320 --> 00:18:26,680
it has a linear rate of 0.01 for the first 10 to power 4 or 10,000 steps. And then it reduces

170
00:18:26,680 --> 00:18:33,480
the linear rate of 0.01 when it's pointing it. One of the things that T5 can do is also

171
00:18:33,480 --> 00:18:41,880
progress objectives. It can figure out, it's sort of a mechanism where the model can gain

172
00:18:41,880 --> 00:18:47,800
general purpose knowledge to apply to downstream steps. You can just see a sequence of token ideas

173
00:18:47,800 --> 00:18:55,320
corresponding to tokenized text from only build text data, and then it can use this

174
00:18:55,320 --> 00:19:01,080
token sequence to process, to produce both a corrector input sequence and a corresponding

175
00:19:01,080 --> 00:19:07,560
output. And then it uses the sentence piece to encode the text for the token. So it's actually

176
00:19:07,560 --> 00:19:18,120
quite good in doing that. And similar to the way the products are on supervised objective,

177
00:19:18,120 --> 00:19:24,440
the prerate data set is a crucial component of transfer learning pipeline. And then on like

178
00:19:24,440 --> 00:19:29,320
objectives of 8Mark, new pre-training data that I usually not should ask because I'm sure they are

179
00:19:30,280 --> 00:19:36,600
so, but these four, these T5 uses high level approaches where it does

180
00:19:37,160 --> 00:19:42,120
bust out and dish out and log in the model. It then uses different corruption strategies.

181
00:19:42,760 --> 00:19:51,240
And it's trying to figure out what corruption sequence of data is. Now for performances,

182
00:19:52,840 --> 00:19:57,240
these are performance resulting from pre-training on different data sets. So the first

183
00:19:57,880 --> 00:20:04,600
public events are based on a new C4 database. Data set and the best performance comes from real

184
00:20:07,000 --> 00:20:10,920
news like data set, but it was not the largest. Now this is a surprise because we

185
00:20:12,360 --> 00:20:17,320
expect that the larger data would give best performance, but that's not the case.

186
00:20:18,280 --> 00:20:24,440
The hypothesis is that the real news data is a lot more random

187
00:20:25,080 --> 00:20:36,600
and it's a lot more reflective of real life. So in pre-training loss, we find out that really the

188
00:20:36,600 --> 00:20:41,320
number of steps that it takes, if you look at what we have diagram here, we're having like

189
00:20:41,320 --> 00:20:48,520
100 steps or 300, and the bigger number of steps, the less the training loss,

190
00:20:49,640 --> 00:21:00,760
it's basically gets better. So the pre-training loss for C4 data set was as well as

191
00:21:00,760 --> 00:21:05,240
a four-attachable trope data version. The size of the state of the person above

192
00:21:05,240 --> 00:21:12,600
to keep in each data set. The first one, and that corresponds to the data set between 64 and 4000

193
00:21:12,600 --> 00:21:19,320
times of 4k, and then using a smaller data set because it's a smaller training loss. So

194
00:21:20,200 --> 00:21:24,040
the bigger data set, then the higher the data loss that you get.

195
00:21:24,040 --> 00:21:35,000
G5 scaling. So G5 scaling refers to the

196
00:21:39,320 --> 00:21:49,720
general expense at the cost in order to really maintain, to run a model. They are large,

197
00:21:49,720 --> 00:21:55,080
they are very expensive, and generally, when you increase the compute power, you get better

198
00:21:55,080 --> 00:22:03,400
performance. So with neural networks, it's usually the case that the bigger your power,

199
00:22:03,400 --> 00:22:08,520
the more computing you have, the better. If you looked at Microsoft, I invested about $13

200
00:22:08,520 --> 00:22:16,520
billion in OpenAI because of the sensitivity, a large portion of that fund was really for cloud

201
00:22:16,600 --> 00:22:25,560
compute power instead of direct cash injection. So some reflection on G5. It's text-to-text.

202
00:22:27,640 --> 00:22:31,880
It's really straightforward and high. It does what it does, and it's really good at it. It has

203
00:22:31,880 --> 00:22:36,920
a user, the original Ecuador decoder, personal architecture, which works really well.

204
00:22:37,720 --> 00:22:43,640
It uses twice as many parameters as Ecuador on the model like that, and it can do also

205
00:22:43,640 --> 00:22:51,720
a supervised objective. So let's talk about in-contact learning. Now, the way large-language

206
00:22:51,720 --> 00:22:57,240
model learns is they can take a new task from a small set of example that's presented within

207
00:22:57,240 --> 00:23:06,360
the context, and the idea really is that they can learn from analogy. So if you're looking to have

208
00:23:06,360 --> 00:23:11,320
very large data set for every task possible, it's not really applicable because it's not practical.

209
00:23:11,320 --> 00:23:19,320
You can't have all of that. So what you need to have is a way for the model to learn

210
00:23:19,320 --> 00:23:24,920
from what it is doing so it knows how to improve on it. And the way it wants to do that is really

211
00:23:25,480 --> 00:23:30,040
to be able to figure out if there's serious correlation in the training data so it can

212
00:23:31,560 --> 00:23:36,600
do that. Now, for humans, we don't really require large data set to learn most tasks. So

213
00:23:36,600 --> 00:23:41,640
we can actually learn. And the idea really in meta-linear or zero-share transfer is to allow

214
00:23:41,640 --> 00:23:49,640
the model to develop a set of broad skillset of particular abilities so that it can figure out

215
00:23:49,640 --> 00:23:59,480
things at a good time. So if we take an example of some of the items that we in meta-learning,

216
00:24:00,200 --> 00:24:12,520
basically we have the model learning via stochastic gradient descent where it's able to

217
00:24:13,560 --> 00:24:18,680
develop on survival pre-training. It's able to develop a broad skills and for our permission

218
00:24:18,680 --> 00:24:25,720
using this ability as interest time to address the data of a new task. So it's really, so if you

219
00:24:25,720 --> 00:24:32,520
look at this on a diagram, the learning goes from left to right and then that's more or less an

220
00:24:32,520 --> 00:24:39,880
aftereffect so you can have different effort coverage. With it in a loop, if we know how to do,

221
00:24:39,880 --> 00:24:45,320
if we show you the example of 5 plus 80 plus 13, 7 plus 3 plus 9, if you have to figure out

222
00:24:45,320 --> 00:24:53,640
a 1 plus 0 equals to 1, or a 3 plus 4 equals to 7, that is the in-context learning that we're

223
00:24:53,640 --> 00:25:00,600
referring to here. There are different languages that are better at in-context learning. So

224
00:25:02,760 --> 00:25:07,640
for example, one task that has been used is to figure out how to remove random symbols from a

225
00:25:07,640 --> 00:25:14,440
word and then you can have large models making really effective use of that in-context information.

226
00:25:14,440 --> 00:25:22,520
If we look at this place actually, we're talking about GA3 and it has like the large

227
00:25:22,520 --> 00:25:33,160
ones that have 1.5 billion parameters require just a few shots. In fact, the few shots and the no prompt

228
00:25:35,720 --> 00:25:43,960
tend to even work well, even though you don't have, the few shot is always slightly better than

229
00:25:43,960 --> 00:25:51,160
the one shot approach that you have here. And if you go on the no prompt, it will eventually match

230
00:25:52,120 --> 00:25:59,400
as none of the examples brought, eventually match the few shots as well. So let's talk about

231
00:25:59,400 --> 00:26:12,600
GP3, which is a last set of models that I want to talk about. We know that even though GP3 is

232
00:26:12,600 --> 00:26:18,200
based on transformer architecture, it's really a decoder only neural network. It does have embedding

233
00:26:18,200 --> 00:26:23,320
really in order to be able to take input. It has and pass all of it on to the network, but it does

234
00:26:23,320 --> 00:26:30,440
not have an encoder in the traditional encoder decoder transformer layer. So the goal really

235
00:26:30,440 --> 00:26:35,720
for GP3 is to be able to predict the next test, given all previous tests. And it uses a very,

236
00:26:35,720 --> 00:26:41,000
it's a very large autoregressive language. It can be used for a few short predictions. So you only

237
00:26:41,000 --> 00:26:47,400
need to claim the few set of data and it knows exactly what to do. So if you look at the diagram

238
00:26:47,400 --> 00:26:55,800
that we have here, it has a very similar to what we showed for T5. The only difference is that

239
00:26:56,680 --> 00:27:01,240
it doesn't have an encoder. So if you take a transformer block and then you take the transformer

240
00:27:01,240 --> 00:27:10,280
block, which is blown up for down the right, you see that it has the normalization layer that comes

241
00:27:10,360 --> 00:27:16,600
with the multiple attention head and then it goes through the model that you have, which is really

242
00:27:17,400 --> 00:27:26,280
the metrics, but the masking, the soft dropbox, it goes into the linear layer and then

243
00:27:26,920 --> 00:27:32,680
does everything in tunnel layer and then goes out and passes the other layer. So it's,

244
00:27:33,800 --> 00:27:40,040
this is the structure of what we have there. Now in terms of the GP3 training layer,

245
00:27:40,040 --> 00:27:46,680
we can see here that it does, it uses, quite to me, you can have zero short, one short,

246
00:27:47,320 --> 00:27:55,080
or even a few short, but the different layer, the GP3 that uses traditional,

247
00:27:55,080 --> 00:28:02,760
uses traditional, fine tuning, which is not, which is not to use normally for GP3. It basically,

248
00:28:03,400 --> 00:28:07,000
it's, the model itself is trained by repeated gradient of this using a large

249
00:28:07,720 --> 00:28:09,960
data, and that's what it uses to control it.

250
00:28:13,160 --> 00:28:17,000
The training data set is similar, it's based on the common crawl, which is up to

251
00:28:18,120 --> 00:28:25,560
one terabyte of what is claimed and if it's used, it's an effective thing. So the training that I

252
00:28:25,560 --> 00:28:30,520
tend to be, GP3 has several different training that can be small, can be large, medium,

253
00:28:31,240 --> 00:28:37,000
it can be just the, the, the best known one, which is the GP3, what's on the five billion

254
00:28:38,040 --> 00:28:43,480
parameters there. So for the columns there, you see the number parameter, number of layer,

255
00:28:43,480 --> 00:28:48,440
the dimension of the model, which shows how many neurons you have in that particular layer,

256
00:28:48,440 --> 00:28:53,240
and then the number of heads, and then the dimension of the heads there, and the batch size,

257
00:28:53,880 --> 00:28:57,080
which is how was it started to talk of the data processing.

258
00:28:59,160 --> 00:29:03,800
And I noticed that all the models here were trained for a total of 200 billion token.

259
00:29:06,200 --> 00:29:12,520
And in terms of compute consumption, we can see here that the, the scaling law

260
00:29:13,640 --> 00:29:22,360
shows that training for much larger models, you only use more than those for fewer tokens in

261
00:29:22,440 --> 00:29:31,080
terms of CPU. So even though GP3 is almost three times larger, or 10 times larger than

262
00:29:31,080 --> 00:29:37,960
Roberta, I'll show you in the diagram, both models took roughly 58 pF per day,

263
00:29:37,960 --> 00:29:42,840
pF per second per day of computing during training.

264
00:29:43,000 --> 00:29:53,880
Even though GP3 is bigger, it has, it is three, it has three billion parameters compared to

265
00:29:53,880 --> 00:30:03,480
three 155 million for Roberta. So the last thing I want to talk about here is really the limitations

266
00:30:03,480 --> 00:30:11,240
that we have for GP3. GP3, as wonderful as it is, it still has no limitations. It's

267
00:30:15,640 --> 00:30:23,720
it's very well what it does, it has text synthesis, the quality of the text is high,

268
00:30:23,720 --> 00:30:26,760
but sometimes if it is self, it does that

269
00:30:27,480 --> 00:30:37,800
because it doesn't really have an understanding of, from a human point of view of the text of

270
00:30:37,800 --> 00:30:43,000
this process. And so sometimes repeat itself semantically at a document level. It also can

271
00:30:43,000 --> 00:30:49,640
sometimes lose coherence over sufficiently large, long passages. And in some cases it can

272
00:30:49,640 --> 00:30:55,560
contradict itself. So occasionally it can contain non-sequital synthesis or paragraph.

273
00:30:56,680 --> 00:31:00,520
For example, the sky blue and the sun is shining, the kid did not drink his milk.

274
00:31:01,080 --> 00:31:06,040
Those sentences have nothing to do with digital. The first sentence sky blue and the sun is shining,

275
00:31:06,040 --> 00:31:10,840
that's good, but the kid did not drink his milk has nothing to do with the first sentence.

276
00:31:11,800 --> 00:31:17,800
It also has a problem lack of interpretability. That is not a problem that is unique,

277
00:31:18,600 --> 00:31:23,880
this is a problem that is common to all large language models.

278
00:31:25,480 --> 00:31:31,640
That's because it's not easily understood or not kind of interpreted. Now it may not work

279
00:31:31,640 --> 00:31:38,680
by the novel input because it's based, it's resolved on what has been injected into it.

280
00:31:38,680 --> 00:31:43,080
It retains biases of data that has been traded on. And that's actually quite important to know.

281
00:31:44,040 --> 00:31:49,400
One of the major concerns of model is that they can transfer the

282
00:31:50,280 --> 00:31:56,200
bias as in real world onto the large language model. And when we start trusting large

283
00:31:56,200 --> 00:32:02,920
language models without questioning, we are making that bias a prejudice and

284
00:32:03,640 --> 00:32:10,680
stereotyping into the large language model. And that is a special concern from a societal

285
00:32:11,240 --> 00:32:17,400
perspective. So we need to be careful when relying, if I rely too much on GP3,

286
00:32:17,400 --> 00:32:24,520
without thinking, as work continues to improve on GPT and large language models generally,

287
00:32:25,640 --> 00:32:29,160
my belief is that some of these issues may no longer be there in the future.

288
00:32:29,160 --> 00:32:34,200
They would result, people would find out how to resolve those problems.

289
00:32:35,160 --> 00:32:40,440
You know, it's, some of the problems really is due to poor sample efficiency during pre-training.

290
00:32:40,440 --> 00:32:45,400
If you don't have representative data, then there's no guarantee that you're going to have

291
00:32:45,400 --> 00:32:53,240
representative model that can serve the society equally well. And I've talked about data perpetuate

292
00:32:53,240 --> 00:33:00,520
and amplified symbiosis. Now there's another problem which, still there, which is multilingual,

293
00:33:00,520 --> 00:33:07,160
is a majority of language model researchers are done in English. So that's where most of the body

294
00:33:07,160 --> 00:33:15,960
of images of text and the purpose of the body of knowledge is. Now what I mean is that when we

295
00:33:15,960 --> 00:33:20,840
try to process non-English languages, we don't have enough support and enough

296
00:33:22,280 --> 00:33:29,560
data to be a good work of it. Thank you. I'll now hand over to Jerry to continue. Thank you.

297
00:33:30,760 --> 00:33:38,200
VPT-3 was found to be able to generate programs, which was surprising because it was never

298
00:33:38,200 --> 00:33:45,000
explicitly trained on code. However, when evaluated on the human eval benchmark,

299
00:33:45,000 --> 00:33:51,400
which will be discussed later, it had a pass rate close to zero. And this begs the question,

300
00:33:51,960 --> 00:33:58,120
is there, can there be language models that specializes in generating programs?

301
00:33:58,760 --> 00:34:05,160
And to this end, there has been recently lots of progress in generating programs from language

302
00:34:05,160 --> 00:34:11,800
models. And one such model is called CodeX, which is a specialized GPT model trained on code.

303
00:34:13,160 --> 00:34:23,480
Now, there are many ways to evaluate the performance of generative models. And

304
00:34:24,440 --> 00:34:34,200
usually the metric to evaluate this is called match-based metrics, which compares output

305
00:34:35,320 --> 00:34:45,240
to a reference solution. However, this may not be viable for models that generate code,

306
00:34:45,240 --> 00:34:51,000
because there may be too many different possible output programs that are functionally equivalent

307
00:34:51,000 --> 00:34:58,200
to the reference solution for this metric to account for. So for example, functionally

308
00:34:58,840 --> 00:35:05,400
equivalent code may use different methods to solve the problem. So for example, for sorting

309
00:35:05,400 --> 00:35:12,200
algorithms, there are many different ways to implement one. For example, by using

310
00:35:12,840 --> 00:35:22,840
deep sort, merge sort, quick sort, etc., they all achieve the same thing, but their code is

311
00:35:22,840 --> 00:35:31,480
completely different. So their match-based metrics would be low, even though they should be very high.

312
00:35:31,480 --> 00:35:37,480
And indeed, it was found that one such match-based metric called the blue score was found to be

313
00:35:37,480 --> 00:35:44,040
unreliable. And so an alternative method is to use functional correctness instead,

314
00:35:44,040 --> 00:35:51,080
which runs the output code on test cases to measure performance. And it is preferable

315
00:35:52,680 --> 00:36:01,080
because it's similar to how humans judge code. So for example, code is usually evaluated based

316
00:36:01,080 --> 00:36:09,880
on test cases passed instead of comparing to solution code in, for example, coding contests

317
00:36:09,880 --> 00:36:15,880
or coding assignments in CS courses, for example.

318
00:36:18,360 --> 00:36:24,120
And one way to evaluate functional correctness is to use the pass at K metric,

319
00:36:24,120 --> 00:36:29,480
which calculates the fraction of problems such that at least one of K sample codes generated

320
00:36:29,480 --> 00:36:35,960
passed all test cases for that problem. And this can be formulated as pass at K equals one minus,

321
00:36:35,960 --> 00:36:42,440
and then brackets one minus pass at one to the power of K, where one minus pass at one is the

322
00:36:42,440 --> 00:36:48,760
probability of an output code failing at least one test case. And this can be interpreted as the

323
00:36:48,760 --> 00:36:56,360
result of evaluating the best of K samples. Now directly calculating the pass at K results

324
00:36:56,360 --> 00:37:03,720
in a high variance. So instead N sample codes are generated, which is greater than or equal to K.

325
00:37:04,600 --> 00:37:11,400
And then the number of past codes C is counted. And the estimator one minus and minus C choose

326
00:37:11,400 --> 00:37:19,480
K divided by N choose K is calculating is calculated for each problem and averaged. Now this estimator

327
00:37:19,480 --> 00:37:26,680
can be interpreted as one minus the total number of ways to choose K failed codes,

328
00:37:27,320 --> 00:37:34,520
and divided by the total number of ways to choose K codes. And

329
00:37:37,800 --> 00:37:43,880
this gives intuition that that estimator is unbiased. However, one can prove this rigorously

330
00:37:43,880 --> 00:37:51,320
by considering that C is distributed by the binomial distribution with N trials and probability

331
00:37:51,320 --> 00:38:02,680
pass at one. And then one can take the expectation over C and expand it to a sum and solve to get the

332
00:38:03,400 --> 00:38:13,480
formula shown on the above. Now, even though that estimator is unbiased,

333
00:38:14,760 --> 00:38:21,400
just directly plugging in the empirical estimate of pass at one into the formula for pass at K

334
00:38:22,120 --> 00:38:32,440
would lead to a biased estimator, particularly it under estimates pass at K, because distributions

335
00:38:32,440 --> 00:38:39,720
are skewed more to the right when taken and exponent greater than one. But its mode is

336
00:38:39,720 --> 00:38:50,200
still to the power of K, so its mean would be greater than that. So a set of 164 problems

337
00:38:50,200 --> 00:38:57,080
called human eval was created, each containing a function signature, doc string, body, and average

338
00:38:57,080 --> 00:39:04,200
of 7.7 unit tests. And these were handwritten to avoid training on potential solutions found in

339
00:39:04,200 --> 00:39:18,520
the training data set. Now, for the training, it involves fine tune GPT models up to 12 billion

340
00:39:18,600 --> 00:39:27,800
parameters on code and is trained on a 159 gigabyte data set of unique Python files on GitHub.

341
00:39:28,520 --> 00:39:37,880
And the learning rate was used was the same as the corresponding GPT model with a 175 step

342
00:39:37,880 --> 00:39:45,160
linear wound up and cosine learning decay and trained on a total of 100 billion tokens with the

343
00:39:45,720 --> 00:39:54,920
atom opt an optimizer with weight decay. And for the results, it was shown that the cross entropy

344
00:39:54,920 --> 00:40:05,320
test loss on a held out validation set fowers a smooth power law. And when only one sample can be

345
00:40:05,320 --> 00:40:13,000
evaluated, it was found that compared to randomly choosing a sample to evaluate, it's better to

346
00:40:13,080 --> 00:40:21,640
choose the one with the highest mean log probability as it performs better. But choosing the one with

347
00:40:21,640 --> 00:40:30,520
the highest sum log probability would perform slightly worse than random choosing. And the plot

348
00:40:30,520 --> 00:40:41,720
on the left shows the pass at K versus K and the temperature. And it was found that the sampling

349
00:40:41,720 --> 00:40:48,360
temperature that maximizes pass at K increases with the number of samples K. And this makes

350
00:40:48,360 --> 00:40:55,240
sense because pass at K only cares about if one sample passes and sampling temperature controls

351
00:40:55,240 --> 00:41:03,800
randomness when sampling, which in the context of generative models, it generates the next

352
00:41:03,800 --> 00:41:13,240
token from a distribution of softmax of the logits, which are the unnormalized scores of tokens

353
00:41:13,240 --> 00:41:26,680
divided by the temperature. So in this case, the highest pass at K for the best temperature

354
00:41:26,680 --> 00:41:36,760
for pass at one is 0.2 and the highest for pass at 100 is 0.8. And these were used in the plot on

355
00:41:36,760 --> 00:41:44,040
the right to plot the pass rate versus the model size as shown in the plot to the right.

356
00:41:44,040 --> 00:41:58,760
Now Codex was compared to other models, including GPT models and the largest free model from

357
00:41:58,760 --> 00:42:07,480
tab 9. And this was done by evaluating all of them on human eval with temperatures of 0.2,

358
00:42:07,560 --> 00:42:15,640
0.4, or 0.8. And then the one with the best score was chosen from these three temperatures.

359
00:42:18,840 --> 00:42:29,720
And then it was found that only GPT-Neal and GPT-J were the only GPT models that had pass rates

360
00:42:29,800 --> 00:42:38,360
that are not close to zero. And these models are similar to Codex and trained on the pile

361
00:42:38,360 --> 00:42:47,880
data set, which has 8% GitHub code. And it was found that as shown in the table to the left,

362
00:42:48,600 --> 00:42:59,240
Codex 300M L performs GPT-J 6B, even though it has 20 times less parameters.

363
00:43:02,440 --> 00:43:09,000
Now there may be code unrelated to translating natural language to code in the data set.

364
00:43:09,960 --> 00:43:17,560
For example, there may be scripts or class implementations and so on. And these may

365
00:43:17,560 --> 00:43:26,040
lower their performance. And so a set of training problems from relevant code was obtained from

366
00:43:26,040 --> 00:43:32,360
two sources, competitive programming websites and repositories with continuous integration.

367
00:43:33,240 --> 00:43:42,600
And these were used to do supervised fine tuning on Codex to create a version called CodexS,

368
00:43:43,560 --> 00:43:54,600
which has shown to have improved performance. And it was found to prefer slightly higher

369
00:43:54,600 --> 00:44:05,480
sampling temperatures than Codex, at least for a K greater than one, possibly due to a narrow

370
00:44:06,440 --> 00:44:15,720
narrower distribution. And also it was found to outperform Codex by 6.5% on pass at one and

371
00:44:15,720 --> 00:44:25,640
15.1% on pass at 100. And this was shown in the plot on the left. Now for the plot on the right,

372
00:44:26,360 --> 00:44:35,640
it was found that the CodexS also significantly outperforms Codex,

373
00:44:36,680 --> 00:44:46,200
when only a single sample can be evaluated. And the random sample is chosen, or when a mean

374
00:44:46,200 --> 00:44:56,040
log is chosen, and also the theoretical pass rates, which is shown in the blue vines.

375
00:44:58,600 --> 00:45:06,040
And so far we have discussed how CodexX, Codex generates code from doc strings.

376
00:45:06,040 --> 00:45:13,880
But what about the other way around? And to this end, CodexD was created, and it's a version of

377
00:45:13,880 --> 00:45:21,960
CodexX that generates doc strings from code. And each training problem contains the function

378
00:45:21,960 --> 00:45:29,080
signature, the reference solution, and doc string. And there's no way to measure functional correctness

379
00:45:29,080 --> 00:45:41,000
for doc strings. So instead, they graded 10 samples for each of 1,640 problems by hand manually,

380
00:45:41,560 --> 00:45:46,760
which takes a long time, which is why only 10 samples for each problem was graded.

381
00:45:47,320 --> 00:45:59,720
And then it was found that the pass at one and pass at 10 was found was 20.3% and 46.5%

382
00:45:59,720 --> 00:46:08,600
respectively, which is slightly lower than that of CodexS. And some limitations of Codex

383
00:46:08,600 --> 00:46:17,000
includes not enough or not sample efficient to train as the training data totaled hundreds of

384
00:46:17,000 --> 00:46:25,160
millions of lines of code. And also the performance decreases exponentially in doc string length.

385
00:46:25,800 --> 00:46:31,800
So synthetic problems created from assembling 13 building blocks that deterministically

386
00:46:31,800 --> 00:46:40,920
modifies string input. So for example, convert to lowercase or remove every third char.

387
00:46:42,680 --> 00:46:54,840
These were created, and it was found that when the number of chain components from these building

388
00:46:54,840 --> 00:47:05,880
blocks increases, the pass rate decreases as shown in the plot. And finally, it can make mistakes

389
00:47:05,880 --> 00:47:13,640
binding variables to operations, especially when there are a lot of them. The next model I will

390
00:47:13,640 --> 00:47:22,040
be discussing is Llama2. Now Llama2 is a family of pre-trained and fine-tuned LLMs with billions

391
00:47:22,040 --> 00:47:29,720
of parameters. And it can outperform other open source LLMs and be on par with some closed source

392
00:47:29,720 --> 00:47:40,200
LLMs. Now the pre-training architecture and hyperbarreners were mostly adopted from Llama1.

393
00:47:40,200 --> 00:47:48,760
So for example, it has a standard transformer architecture with renormalization, where the

394
00:47:49,720 --> 00:47:56,120
inputs of each sublayer was normalized instead of the output. And this was shown to improve

395
00:47:56,120 --> 00:48:07,720
training stability, and this was inspired by GPT3. And also the swiglu activation function,

396
00:48:07,720 --> 00:48:16,440
which uses the swish function, was used. And it was found to allow more flexibility and

397
00:48:16,440 --> 00:48:23,240
expressiveness in the feed forward layers, and therefore improve performance of transformer

398
00:48:23,240 --> 00:48:30,680
models compared to standard activations like Rilu. And finally, instead of absolute positional

399
00:48:30,680 --> 00:48:39,720
embeddings, they use rotary positional embeddings, which uses a rotation matrix instead of just

400
00:48:40,680 --> 00:48:48,440
vectors for each token for the positional embeddings. And this shows relative positions

401
00:48:48,440 --> 00:48:54,360
of tokens and allows a model to capture their dependencies. And this can improve performance

402
00:48:54,360 --> 00:48:58,120
because changing positions of words in a sentence can change its meaning,

403
00:48:58,280 --> 00:49:15,160
especially their relative positions. And now 7, 13, 34, and 70 billion parameters

404
00:49:16,200 --> 00:49:23,960
were used for the family of models. And the context length, which is the amount of text that

405
00:49:24,040 --> 00:49:34,120
can be processed at a time, was doubled from Lama 1. So now it's 4k. And the learning rate of

406
00:49:35,720 --> 00:49:45,560
use was 0.0003 for smaller models and 0.00015 for larger models with group query attention,

407
00:49:45,640 --> 00:49:54,440
which is a method to stabilize the performance for larger models.

408
00:49:55,240 --> 00:50:03,720
And also it's trained using atom optimizer with weight decay with a cosine learning rate schedule,

409
00:50:03,720 --> 00:50:10,120
and it was trained on two trillion tokens of data, which is 40% more than Lama 1.

410
00:50:10,520 --> 00:50:21,800
And now the Lama 1 and 2 base models, along with other open source models, such as

411
00:50:23,080 --> 00:50:32,760
MPT and Falcon, were evaluated on several benchmarks for comparison. So for code,

412
00:50:32,760 --> 00:50:40,120
the average pass-at-one score on human EFL and another data set called MBPP is reported.

413
00:50:40,840 --> 00:50:45,880
And then for common sense reasoning, world knowledge, reading, comprehension, and math,

414
00:50:46,600 --> 00:50:53,560
the average of scores from different methods were reported. And finally,

415
00:50:55,160 --> 00:51:01,960
the massive multitask language understanding, which evaluates a wide variety of tasks,

416
00:51:02,840 --> 00:51:12,040
big bench hard, which evaluates a subset of big bench tasks, which evaluates a wide variety of

417
00:51:12,040 --> 00:51:17,160
tasks that were found to be beyond the capabilities of current language models.

418
00:51:18,920 --> 00:51:27,080
And also artificial general intelligence evaluation, which evaluates models based on

419
00:51:27,080 --> 00:51:32,440
their performance on human-centric standardized exams, were also reported.

420
00:51:34,840 --> 00:51:45,560
And this table summarizes the results discussed previously. And as shown, it was found that

421
00:51:45,560 --> 00:51:54,600
generally the Lama 2.70 billion performs the best.

422
00:51:57,960 --> 00:52:07,400
And now Lama 2.Chat is a version of Lama 2 with supervised fine-tuning through alignment techniques

423
00:52:08,040 --> 00:52:17,080
and supervised fine-tuning with publicly available instruction fine-tuning data.

424
00:52:18,120 --> 00:52:27,560
It was found that that quality is all you need as using less, i.e. thousands instead of millions,

425
00:52:27,560 --> 00:52:36,040
but higher quality examples was shown to have improved results. And also RLHF,

426
00:52:36,120 --> 00:52:41,560
which is reinforcement learning with human feedback, is applied after fine-tuning so that

427
00:52:41,560 --> 00:52:47,560
the model can understand the user intentions to further align the model with human preferences.

428
00:52:49,880 --> 00:52:57,400
Now, data of human preferences obtained from human feedback was collected and

429
00:52:58,680 --> 00:53:05,720
for more diversity of collected prompts, binary comparison protocol was used to collect

430
00:53:05,720 --> 00:53:11,080
feedback. So annotators would write prompts for the models and then for each prompt,

431
00:53:11,080 --> 00:53:17,080
they would choose one of two responses from different variants of the model. So, for example,

432
00:53:17,080 --> 00:53:26,840
with different temperature based on provided criteria, which in this case is healthfulness or

433
00:53:26,840 --> 00:53:32,840
safety, and then they would rank how much better their chosen response is on a scale of four points.

434
00:53:33,080 --> 00:53:45,720
And then, as I just said, the preference, it was given to healthfulness and safety,

435
00:53:46,440 --> 00:53:54,200
and this data was received in batches over time, so at the reward model improved over time.

436
00:53:54,760 --> 00:54:01,960
Now, human preference data was used to train reward model,

437
00:54:03,800 --> 00:54:10,040
so that patterns in the preference can be learned by changing internal text distribution

438
00:54:10,040 --> 00:54:17,320
of the base model, and it outputs a score based on prediction of the quality of the model based

439
00:54:17,320 --> 00:54:24,200
on human preference given a prompt and model response, and these scores were used as rewards for

440
00:54:24,840 --> 00:54:34,920
RLHF, and the reward model was initialized from the pre-trained model to avoid situations where

441
00:54:34,920 --> 00:54:45,880
the model would end up favoring hallucinations, so i.e., giving the wrong incorrect or misleading

442
00:54:45,880 --> 00:54:54,360
responses, and the same architecture and hyperparameters were used as well, but with

443
00:54:54,360 --> 00:55:00,760
a regression head for outputting rewards instead of a classification head, as the rewards are real

444
00:55:00,760 --> 00:55:11,320
numbers, and two RMS were used, one for healthfulness and one for safety, and later on we will be

445
00:55:11,320 --> 00:55:22,040
discussing why two are used instead of one, so a binary ranking loss with the margin component was

446
00:55:22,040 --> 00:55:33,080
used, where the margin component is a function of the preference rating, which is different for

447
00:55:33,080 --> 00:55:39,960
each reward model, and this helps the reward model give more distinct scores for more different

448
00:55:39,960 --> 00:55:46,840
responses, and the preference data was increased by combining with open sourced ones,

449
00:55:49,720 --> 00:55:57,480
and it was trained with the same parameters as the base model, but only ran one epoch, as previous

450
00:55:58,280 --> 00:56:08,040
studies have shown that it may overfit if more than one epoch was ran, and also slightly lower

451
00:56:08,120 --> 00:56:22,760
learning rate was used, and here are the results for the chat reward modeling, so these reward

452
00:56:22,760 --> 00:56:33,800
models were found to help perform STEAM SHP XL open assistant and GPT-4 on several human preference

453
00:56:33,800 --> 00:56:44,040
benchmarks, so specifically the helpfulness reward model and safety reward model each

454
00:56:44,040 --> 00:56:53,080
performed best in their own domain, so for example the helpfulness reward model performed best on

455
00:56:53,640 --> 00:57:02,840
the meta-helpful data, the safety reward model performed best on meta harmless data and etc,

456
00:57:03,480 --> 00:57:10,600
and this makes sense because they may sometimes have conflicts, so for example if a user were to

457
00:57:12,040 --> 00:57:21,800
give the prompt how to make a bond, then it's impossible to have a response that is both helpful

458
00:57:21,800 --> 00:57:31,240
and safe at the same time, and optimizing one reward model with both objectives would have

459
00:57:31,240 --> 00:57:35,800
confused the model in that case, and therefore it would not perform well.

460
00:57:40,280 --> 00:57:48,760
Now successive versions of RLHF were trained as more batches of human preference data were

461
00:57:48,760 --> 00:58:00,840
received, and these were labeled v1 to v5, and RLHF fine-tuning was explored with two algorithms,

462
00:58:00,840 --> 00:58:09,000
the first one is rejection sampling fine-tuning, so at each iteration k samples generated from

463
00:58:09,000 --> 00:58:15,400
the model, and then the best one was selected using the reward model, these best samples were

464
00:58:15,400 --> 00:58:23,800
used for a gradient update to tune the model for the next iteration, and this was only applied to

465
00:58:23,800 --> 00:58:32,520
the 7b model, while the smaller models were fine-tuned on rejection sample data from the 7b model,

466
00:58:33,320 --> 00:58:40,120
and later versions include best samples from all previous models and not just the proceeding one,

467
00:58:40,120 --> 00:58:46,920
since for example the v3 was trained using only samples from v2 that was found that its

468
00:58:46,920 --> 00:58:53,640
performance worsened in certain tasks, and sampling temperature was also adjusted for each

469
00:58:53,640 --> 00:58:59,320
iteration because its optimal value significantly changed over iterations.

470
00:59:01,800 --> 00:59:08,840
The second method is a standard method in reinforcement learning called proximal policy

471
00:59:08,840 --> 00:59:18,920
optimization, which is on policy method that uses policy gradient updates to update the policy,

472
00:59:19,720 --> 00:59:26,440
and it uses the reward model as an estimate of the reward function, and the language model

473
00:59:26,440 --> 00:59:33,240
as the policy to optimize, and the policy was iteratively improved by sampling prompts from

474
00:59:33,240 --> 00:59:43,960
the dataset and generations from policy, and then the policy, proximal policy optimization is applied.

475
00:59:46,680 --> 00:59:54,760
A new technique from the paper called ghost attention or GAT was introduced,

476
00:59:55,720 --> 01:00:01,320
and it helps control dialogue flow over multiple turns.

477
01:00:02,920 --> 01:00:09,240
So initially the RLHF model sometimes forgets instructions in a dialogue after a few turns.

478
01:00:09,960 --> 01:00:16,600
For example, when the first prompt is to answer with only emojis,

479
01:00:17,320 --> 01:00:26,600
the model may do it for the first few prompts, but later on it would forget it and answer with

480
01:00:27,160 --> 01:00:39,960
actual words, and this issue can be fixed with GAT and was applied after the version 3 of RLHF,

481
01:00:40,760 --> 01:00:47,880
and the method works by synthetically concatenating the instruction to user messages

482
01:00:47,880 --> 01:00:57,160
once it's defined, and is it was found to be consistent up to over 20 turns up to the context

483
01:00:57,160 --> 01:01:07,960
length. Now for each prompt and a test set of helpfulness and safety,

484
01:01:08,680 --> 01:01:15,720
three annotators judge the quality on a seven point scale, and the figure below shows the

485
01:01:15,720 --> 01:01:24,360
when gray percent against GPT-4 for different versions of Lama 2 chat during the supervised

486
01:01:24,360 --> 01:01:34,040
fine tuning and RLHF. So the plot on the left shows that when using the meta rewards models

487
01:01:34,040 --> 01:01:43,000
judge the RLHF v3 and later versions L performs GPT-4 on both harmlessness and helpfulness,

488
01:01:43,640 --> 01:01:49,080
however this may be unfair because the judge was made by the same company as Lama 2,

489
01:01:49,800 --> 01:02:03,080
so the plot on the right shows the results when using the GPT-4 judge, and it was found that

490
01:02:03,080 --> 01:02:14,120
still RLHF v5 both with no proximal policy optimization and with that L performs GPT-4

491
01:02:14,120 --> 01:02:22,920
on harmlessness and helpfulness. Now Lama 2 chat was also compared with several other models

492
01:02:22,920 --> 01:02:29,000
on human evaluation with over 4,000 diverse prompts with multi-turn prompts generated by

493
01:02:29,000 --> 01:02:35,240
Lama 2 chat and or chat GPT, and for each prompt three human annotators rate how much better or

494
01:02:35,240 --> 01:02:43,880
worse one model is than the other on a seven point scale, and this is shown in the plot below,

495
01:02:44,520 --> 01:02:57,160
and it shows that when put against the other models shown here Lama 2 chat had a higher

496
01:02:57,160 --> 01:03:08,920
win rate and a loss rate. Now for safety harmful data was filtered out during pre-training and

497
01:03:08,920 --> 01:03:17,400
fine-tuning, and compared to before fine-tuning Lama 2 chat showed great improvement in truthfulness

498
01:03:17,400 --> 01:03:30,040
and toxicity as its scores for truthful QA and toxigen improved, and also its scores for bold

499
01:03:30,040 --> 01:03:38,360
also improved which shows a lowered bias, and to test robustness against attackers

500
01:03:38,360 --> 01:03:50,280
red teaming was performed with over 300 people of different demographics and fields,

501
01:03:51,080 --> 01:03:58,680
and they probed the models in various unsafe situations with simulated prompts,

502
01:03:58,680 --> 01:04:03,480
and these insights were used for fine-tuning and feedback training to lower the rate of

503
01:04:03,480 --> 01:04:10,280
violating responses. So for example for the 7B model the rate was lowered by four times,

504
01:04:11,960 --> 01:04:22,200
and overall it outperforms many other models in helpfulness and safety based on human

505
01:04:22,200 --> 01:04:29,320
graders judging. However a limitation is that while these safety tuning approaches fixed most

506
01:04:29,320 --> 01:04:36,600
safety issues it goes too far in some instances which can cause the model to be overly cautious

507
01:04:36,600 --> 01:04:43,320
and have false refusals, although this only happens 0.05% of the time on the helpfulness data.

508
01:04:45,320 --> 01:04:53,080
The next model I will be discussing is mixed drill of experts. Now mixed drill 8 times 7B

509
01:04:53,080 --> 01:04:57,080
is a sparse mixed drill of experts model that can outperform

510
01:04:57,080 --> 01:05:06,840
mama270B and GPT 3.5 on most benchmarks such as math, math, cold generation, and multilingual tasks,

511
01:05:07,560 --> 01:05:17,320
and the model basically uses a subset of parameters for each token and can change the size of

512
01:05:18,280 --> 01:05:27,560
of the active parameters to make inference faster or have a higher throughput,

513
01:05:28,440 --> 01:05:34,920
and this image illustrates the mixture of experts layer which will be discussed later.

514
01:05:35,880 --> 01:05:49,000
So the Mistral 7B is an earlier version of mixed drill that has a similar architecture to mama,

515
01:05:49,960 --> 01:05:58,520
but also has sliding window attention which limits the amount of tokens each token can attend to

516
01:05:58,520 --> 01:06:07,000
by a window size W and this saves computation and memory, and this is possible with two other

517
01:06:07,000 --> 01:06:15,640
methods used called rolling buffer cache and prefill and chunking, and this is illustrated in the image below.

518
01:06:15,800 --> 01:06:30,200
Now mixed drill also has the same architecture and parameters as Mistral except that the

519
01:06:30,200 --> 01:06:38,920
context length is quadrupled from Mistral to 32K and feed forward blocks are replaced by

520
01:06:38,920 --> 01:06:46,440
eight mixed drill of experts layers and experts are individual feed forward networks

521
01:06:47,960 --> 01:06:56,040
and generally mixed drill of experts output is some of the top products of the expert

522
01:06:56,040 --> 01:06:59,080
and its gating network over each expert eye.

523
01:07:01,800 --> 01:07:07,880
So the gating event network in Mistral takes us off max of top K logits of the linear layer

524
01:07:09,880 --> 01:07:15,800
so then the total sparse parameter count can grow with the number of flares while the active

525
01:07:15,800 --> 01:07:21,960
parameter count which is used for processing a token only grows with K and for mixed drill

526
01:07:21,960 --> 01:07:30,200
the expert used is swiglu activation function and K equals two so this is illustrated by the

527
01:07:31,080 --> 01:07:42,360
image shown here before where the router takes the inputs and it passes it to only two of the eight

528
01:07:42,360 --> 01:07:53,400
experts with the highest logits and so then the active parameters would be from just

529
01:07:53,400 --> 01:07:59,240
these two experts and the output would only depend on those two experts.

530
01:08:00,520 --> 01:08:08,280
Now mixed drill was compared to Lama by evaluating them on several benchmarks similar to

531
01:08:08,280 --> 01:08:16,120
in Lama2's paper and the results are shown in the table below and the columns from

532
01:08:16,120 --> 01:08:26,600
heliswag up to arc challenge represents common sense reasoning and then natural questions

533
01:08:26,600 --> 01:08:35,160
and trivia qa represents world knowledge humanevalent mvpp represent code and finally math and gsmak

534
01:08:35,720 --> 01:08:46,760
represents math and from this table it shows that a mixed drill out performs Lama2 70b on

535
01:08:48,200 --> 01:08:59,320
overall and it outperforms Lama2 even though it has over five times less active parameters

536
01:08:59,960 --> 01:09:09,800
um and also compared to its earlier version mixed drill had mixed drills

537
01:09:10,440 --> 01:09:16,120
multilingual data was significantly up sampled which allowed it to perform well on multilingual

538
01:09:16,120 --> 01:09:24,200
benchmarks particularly on four different languages uh french german spanish and italian

539
01:09:24,280 --> 01:09:33,480
and for each of these languages the um arcs c challenge heliswag and mmlu benchmarks are

540
01:09:33,480 --> 01:09:43,960
reported um and it's shown that it mixed drill does better than Lama2 on all of them

541
01:09:44,440 --> 01:09:46,440
um

542
01:09:48,040 --> 01:09:54,440
and as for long range performance um mixed drill has good long range performance as

543
01:09:55,000 --> 01:10:01,480
as 100 percent retrieval accuracy of the pasky retrieval task which measures the ability of models

544
01:10:01,480 --> 01:10:07,880
to retrieve a pasky randomly inserted in a long prompt and the perplexity uh

545
01:10:08,600 --> 01:10:17,960
the decreases as the context length increases where the perplexity is how um how um is that is

546
01:10:17,960 --> 01:10:31,160
basically how um how um uh how unlikely is to um to be able to predict the uh the models outputs

547
01:10:31,960 --> 01:10:42,760
and um also uh in in terms of the bias benchmarks uh mixed drill l performs Lama2 70b on the uh

548
01:10:42,760 --> 01:10:51,000
bias benchmark for qa and also in the bias in open-ended language generation dataset uh

549
01:10:51,640 --> 01:10:58,440
where it has higher scores overall compared to Lama2 and this represents less social bias

550
01:10:58,760 --> 01:11:05,640
now uh mixed drill instruct is a version of uh mixed drill which with supervised fine-tuning

551
01:11:05,640 --> 01:11:13,240
on an instruction dataset followed by direct performance optimization and and um the and

552
01:11:13,240 --> 01:11:22,840
this was evaluated um using the large model systems chatbot arena uh where users would um

553
01:11:23,080 --> 01:11:33,400
would have a would uh send a prompt to two models and then select which uh which model

554
01:11:33,960 --> 01:11:41,880
they prefer based on the responses and the winning model would get a higher elo while the losing

555
01:11:42,440 --> 01:11:52,600
model would get a lower elo and this um this elo is um is used to measure the performance

556
01:11:53,080 --> 01:12:01,960
models or one way to measure the performance of the models and um as of december 2023

557
01:12:02,600 --> 01:12:09,640
mixed drill instruct L performs other open-weight models on the MT bench which is a set of challenging

558
01:12:09,640 --> 01:12:19,800
multi-turn questions um where uh users would uh have uh conversations um with the uh

559
01:12:22,920 --> 01:12:32,120
with the uh model and um involves multiple prompts and also um mixed drill instruct was

560
01:12:32,120 --> 01:12:41,000
ranked uh sixth place in the arena's elo and this was based on 200 000 human preference votes

561
01:12:43,320 --> 01:12:51,960
now now we analyze the the router um so the distribution of selected experts using the

562
01:12:51,960 --> 01:13:01,000
file datasets was measured and reported for the first middle and last layers um and uh this was

563
01:13:01,080 --> 01:13:11,880
shown in the this is shown in the um graph on the left uh and as we can see uh only dm mathematics

564
01:13:12,520 --> 01:13:19,320
from the file datasets um had significantly different distributions and um this is possible

565
01:13:19,960 --> 01:13:27,000
this is possibly due to the limited coverage of natural language and um the synthetic nature of

566
01:13:27,080 --> 01:13:34,600
the dataset and um also it was found that the distribution at the first and last layer is

567
01:13:34,600 --> 01:13:41,720
very similar to the input and output and beddings respectively which suggests that the router has

568
01:13:41,720 --> 01:13:49,320
structured syntactic behavior and this is also shown in the image on the right where as we can

569
01:13:49,320 --> 01:13:57,320
see the um similar words would have the same colors and also like the indents for the indents

570
01:13:57,320 --> 01:14:07,640
they're generally the same colors and also like groups of symbols uh similar groups of symbols

571
01:14:07,640 --> 01:14:17,800
have the same color um the final model i will be discussing is um POM which stands for Pathways

572
01:14:17,800 --> 01:14:29,560
Language Model uh and um it's a very big um 540 billion parameter model uh that uh using the um

573
01:14:30,280 --> 01:14:41,160
pathway system and um it's uh it's a densely activated transformer language model that um

574
01:14:41,160 --> 01:14:55,720
is trained on um 6144 chips from um google's uh tpu v4 uh chips and um so some features it has is

575
01:14:55,720 --> 01:15:04,840
it's um as efficient scaling as it's scaled very well across thousands of accelerator chips in a

576
01:15:04,920 --> 01:15:13,080
highly efficient manner and also um it has shown continued improvements from scaling as um

577
01:15:14,520 --> 01:15:23,720
it's a uh it's a very big model like um with 540 billion parameters but it's um

578
01:15:24,680 --> 01:15:33,080
it it's performance uh also uh grew with that size um so um

579
01:15:35,720 --> 01:15:45,880
so it can perform reasoning tasks uh via multi-step mathematics and other breakthrough capabilities

580
01:15:45,960 --> 01:15:55,080
and in fact it's actually um it actually uh has a dramatic jump in accuracy when uh come

581
01:15:56,280 --> 01:16:03,320
from the 62 billion uh parameter version of the POM um compared to the um

582
01:16:05,000 --> 01:16:11,080
the jump in accuracy from 8 billion to 62 billion parameters so there's a discontinuous

583
01:16:11,160 --> 01:16:21,800
improvement um the where the um scaling behavior is uh not um where improvements and performance

584
01:16:21,800 --> 01:16:32,760
is non-linear and um so so new capabilities emerge when the model achieves a sufficient scale um and

585
01:16:32,760 --> 01:16:41,880
also um it has multilingual understanding as um uh it has more thorough work than earlier models

586
01:16:43,160 --> 01:16:52,760
such as translation um also it it has improved accuracy on gender and occupation bias however

587
01:16:52,760 --> 01:17:03,240
it still has issues with um other biases um for example it associates Muslims with terrorism

588
01:17:03,240 --> 01:17:14,680
extremism and violence and so this is a work in progress um the the architecture uses a standard

589
01:17:15,240 --> 01:17:23,880
transformer model with a decoder only set up and each time stamp can only attend to itself

590
01:17:23,880 --> 01:17:32,680
and pass time steps um it also uses the swigloo activation function which as discussed earlier

591
01:17:32,680 --> 01:17:43,800
it helps improve performance for um transformers and um it also has um parallel layers where each

592
01:17:43,800 --> 01:17:51,720
transformer block is in parallel and the formulation um the parallel formulation is

593
01:17:52,280 --> 01:17:58,680
similar to the standard formulation but the but the attention term is moved outside of the

594
01:17:58,760 --> 01:18:00,200
multi-layer perceptron

595
01:18:04,600 --> 01:18:15,880
um uh so three different models uh scales were compared uh so 540 billion 62 billion and 8 billion

596
01:18:16,600 --> 01:18:22,040
and the number of flops per token is approximately equal to the number of parameters

597
01:18:22,920 --> 01:18:29,880
uh and um the models are all trained identically except uh with the

598
01:18:31,880 --> 01:18:40,360
hyper parameters shown in the table below which uh shows the number of layers uh the number of

599
01:18:40,360 --> 01:18:47,480
attention heads um and the dimension of the model which refers to the number of neurons in each layer

600
01:18:48,040 --> 01:18:53,400
of the network and number of parameters and batch size um

601
01:18:57,720 --> 01:19:05,320
so the training data set consists of a high quality corpus of 780 billion tokens

602
01:19:05,880 --> 01:19:13,240
representing a wide range of natural language use cases and is based on data sets used to train

603
01:19:13,240 --> 01:19:22,840
the lambda model so the table below shows the proportion of data um for each data source

604
01:19:23,480 --> 01:19:33,480
and um the multilingual corpus contains text from over 100 languages um and also uh five percent of

605
01:19:33,480 --> 01:19:40,760
it comes from github code so if you ever published your code on github the authors of neural networks

606
01:19:40,760 --> 01:19:45,720
will likely to thank you as you have contributed to the training of several models

607
01:19:47,400 --> 01:19:56,200
um now here we show the training infrastructure where um all models are trained on tpu v4 pods

608
01:19:56,920 --> 01:20:04,920
and um the training and evaluation code base is based on jax and t5x and um

609
01:20:04,920 --> 01:20:14,120
um all models are trained on tpu v4 pods where palm 540b is trained over two uh two of those

610
01:20:14,120 --> 01:20:22,040
pods and over a data center network using a combination of model and data parallelism

611
01:20:22,680 --> 01:20:32,360
and um it uses um 3072 tpu v4 chips in each pod attached to 768 hosts

612
01:20:35,480 --> 01:20:45,960
so here are the results um which are obtained by the palm 540b model across um 29 benchmarks

613
01:20:46,760 --> 01:20:55,480
and uh and compared to uh prior state of the art the abbreviated as s sota in the table

614
01:20:56,200 --> 01:21:05,240
and um this was done for um zero shot one shot and few shot where um for the few shots the number

615
01:21:05,240 --> 01:21:12,440
of shots for each task are mentioned in the parentheses so this table shows that um

616
01:21:13,560 --> 01:21:19,080
generally uh the palm performs better than the prior state of the art

617
01:21:19,080 --> 01:21:33,640
um now um for the uh another um benchmark called the big bench um it includes over 150 tasks that

618
01:21:33,640 --> 01:21:39,400
cover a variety of language modeling tasks including logical reasoning translation

619
01:21:40,040 --> 01:21:50,840
question answering mathematics etc and um uh the on the left shows the evaluation of palm

620
01:21:52,120 --> 01:22:03,560
compared to gp3 gopher chinchilla and also um and humans where the average human metric is around

621
01:22:03,560 --> 01:22:15,000
150 while the best humans do around a hundred and um palm five shot um where which shown five

622
01:22:15,000 --> 01:22:22,760
examples in training uh performs better than the average human at of around uh 10 to the 11 model

623
01:22:22,760 --> 01:22:38,120
parameter so for evaluating reasoning um arithmetic reasoning is uh often uses uses grade school level

624
01:22:38,120 --> 01:22:45,320
natural language math problems which require multi-step logical inference while the math itself

625
01:22:45,320 --> 01:22:51,800
is trivial the um difficult part is transforming the natural language into mathematical equations

626
01:22:52,360 --> 01:22:59,160
and for common sense reasoning it involves questions answering tasks which require strong

627
01:22:59,160 --> 01:23:05,000
world knowledge but are not simply factual question answering and they require chaining

628
01:23:05,000 --> 01:23:09,960
multiple logical inferences around the world about about the world

629
01:23:10,120 --> 01:23:19,480
and also for chain of thought prompting um this was um shown to uh

630
01:23:22,520 --> 01:23:30,280
achieve help llm's achieve significantly higher um accuracy um improvements by

631
01:23:30,840 --> 01:23:37,720
generating intermediate reasoning steps before generating the final steps and in a few shot

632
01:23:37,720 --> 01:23:44,120
setting these intermediate reasoning steps are manually written for the first for the few shot

633
01:23:44,120 --> 01:23:51,480
examples um model will then generate its own chain of thoughts for the test examples and

634
01:23:52,200 --> 01:24:00,600
only the final answer and uh while only the final answer is used for evaluation of these generated um

635
01:24:01,160 --> 01:24:09,720
uh the generated chain of thoughts can um be useful for your analysis and model interpretability

636
01:24:10,520 --> 01:24:18,280
which is one one of the major drop backs of llm's and um chain of thought prompting allows language

637
01:24:18,280 --> 01:24:24,840
models to better perform multi-step reasoning tasks such as math work problems as shown in

638
01:24:24,840 --> 01:24:35,320
this image here where the where the second question um it gives a wrong output for standard

639
01:24:35,320 --> 01:24:40,440
prompting but uh correct for the chain of thought prompting um

640
01:24:45,480 --> 01:24:54,600
so uh from chain of thought prompting um achieve strong performance on a range of arithmetic and

641
01:24:55,000 --> 01:24:57,640
common sense reasoning tasks um

642
01:25:01,880 --> 01:25:11,400
and uh as for the code tasks the um uh bomb can be used for two main types of code work the first

643
01:25:11,400 --> 01:25:19,880
one is uh text to code uh which involves generating code from uh natural language description and

644
01:25:19,880 --> 01:25:27,720
the second is um code to code which involves translation work particularly to translate

645
01:25:28,440 --> 01:25:37,880
c or c plus plus programs to python um the um uh the current level of expertise is a

646
01:25:38,600 --> 01:25:47,400
generation of python of a python function from the description but uh but currently it's unable to

647
01:25:47,400 --> 01:25:55,560
generate an entire program uh but this is a very promising research area um and uh

648
01:25:57,640 --> 01:26:04,840
transcoder data is downloaded from github and provides a few shot examples to provide training

649
01:26:04,840 --> 01:26:14,280
to the bomb model uh and examples from the bomb color 540b model is shown in the image here

650
01:26:14,920 --> 01:26:23,560
or in the top left uh it shows the gsm it shows a gsm 8k python question converted from the open

651
01:26:23,560 --> 01:26:31,160
ai gsm 8k math data set in the bottom left it shows a transcoder example translating a simple

652
01:26:31,160 --> 01:26:37,720
function from c plus plus to python and on the right it shows a converted human eval example

653
01:26:38,680 --> 01:26:45,880
bomb included github code in its training set and a total of 39 billion code tokens were used

654
01:26:45,880 --> 01:26:53,000
in the pre-training data set uh bomb supports additional languages such as java html javascript

655
01:26:53,000 --> 01:27:01,960
python c php c sharp and c plus plus now a major risk of using this is that a generated code may

656
01:27:01,960 --> 01:27:12,360
be wrong as there may be presence of several bugs um as for translation uh this task this task

657
01:27:12,360 --> 01:27:18,920
involves rewriting one human language uh one human language into another one while preserving the

658
01:27:18,920 --> 01:27:27,640
content semantics and style of the input so one one of one way is using um um english-centric

659
01:27:27,640 --> 01:27:34,680
language pairs which is a traditional focus of past models and involves using english as the source

660
01:27:34,680 --> 01:27:41,000
or target language so for example translating from english to french or from english to german

661
01:27:41,000 --> 01:27:49,800
or vice versa for both of them and etc um another way is using uh direct language pairs where um

662
01:27:49,800 --> 01:27:55,480
they directly translate between any pair of languages without involving english so for example

663
01:27:56,200 --> 01:28:00,520
translating from french to german directly instead of going from french to english and

664
01:28:00,520 --> 01:28:13,400
then to german and also um uh there are some um there are some extremely low resource

665
01:28:13,400 --> 01:28:20,040
language pairs as in some cases one of the languages have little monolingual data such as

666
01:28:20,040 --> 01:28:26,040
casak so for example french and german have about 24 and 26 billion tokens in training set

667
01:28:26,040 --> 01:28:38,680
while casak only has around 134 million tokens and um this uh this table shows the results uh

668
01:28:38,680 --> 01:28:44,920
the translation blue scores on traditional wmt language pairs and um

669
01:28:45,160 --> 01:28:53,800
um uh use shot evaluation corresponds to five shots for palm and note that zero shot prompt

670
01:28:53,800 --> 01:28:59,240
includes the source and target language names in the prompt while one shot and few shot don't

671
01:28:59,240 --> 01:29:05,880
so languages must be inferred from the exemplars which may explain that the strong zero shot

672
01:29:05,880 --> 01:29:15,720
performance in some language pair translation um in some language pair um and uh translation

673
01:29:15,720 --> 01:29:23,240
quality is better when translating into english rather than out of english um and uh prompts can

674
01:29:23,240 --> 01:29:30,280
deliver even more value than a single example and generalist models relying solely on self

675
01:29:30,280 --> 01:29:38,840
supervision can match specialized models at a smaller scale and um finally uh we will

676
01:29:38,840 --> 01:29:48,600
be discussing the limitations of palm and um so one of the limitations is that it contains and uh

677
01:29:49,160 --> 01:29:56,680
it can contain an amplified biases in underlying data but uh this limitation is not unique to palm

678
01:29:56,680 --> 01:30:07,640
but uh to other lms in general and um there's uh a general occupation bias and um

679
01:30:09,720 --> 01:30:19,240
also toxic toxicity and bias such as um uh having some words like islam being associated with

680
01:30:19,800 --> 01:30:28,760
highly charged terms and zero types such as terrorist violent and radical and um uh the

681
01:30:29,720 --> 01:30:37,320
toxicity probability of the continuation as a function of um the toxicity probability of the

682
01:30:37,320 --> 01:30:48,040
prompt um is um shown in this uh a graph and the human baseline represents the toxicity

683
01:30:48,040 --> 01:31:00,520
probability of the original sentence continuation and um the the um so the toxicity probability

684
01:31:00,520 --> 01:31:07,560
of continuation or tpc is more consistent with the toxicity probability of the prompt

685
01:31:07,560 --> 01:31:16,120
or tpp than that of then the human tpc and this suggests that the model is strongly influenced

686
01:31:16,120 --> 01:31:23,480
by the prompt style and is likely to respond like to like note that the palm 62 billion 540

687
01:31:23,480 --> 01:31:37,480
billion models have very similar toxicity probabilities um and now we will uh now we show

688
01:31:37,560 --> 01:31:51,160
a table that compares the lms of um the all the lms discussed in in this presentation so um these are

689
01:31:53,800 --> 01:32:02,040
each of the lms have multiple versions or settings so only the most typical or the

690
01:32:02,040 --> 01:32:10,600
largest one is picked for each one uh as uh having all of them would not fit in the page um

691
01:32:12,520 --> 01:32:23,880
and finally uh we will be uh having um six questions for the discussion so question one

692
01:32:24,680 --> 01:32:29,560
which is preferable unsupervised learning or reinforcement learning with human feedback

693
01:32:30,200 --> 01:32:37,000
question two what makes a large language model large question three uh can language models be

694
01:32:37,000 --> 01:32:46,760
used maliciously uh and how um question four um how can we reduce societal harm due to misuse of

695
01:32:46,760 --> 01:32:55,560
language models uh question five uh how do we remove societal implicit biases from becoming

696
01:32:55,560 --> 01:33:02,680
part of foundation models and finally question six what led to emergent abilities observed in lms

697
01:33:04,680 --> 01:33:10,280
and here are the references used for the presentation thanks for watching

