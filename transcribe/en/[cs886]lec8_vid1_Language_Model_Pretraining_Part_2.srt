1
00:00:00,000 --> 00:00:03,160
which stands for bi-directional encoder

2
00:00:03,160 --> 00:00:05,700
representations for transformers.

3
00:00:05,700 --> 00:00:09,640
Vert is a language model developed by Google in 2018.

4
00:00:09,640 --> 00:00:12,760
It is considered one of the most significant breakthroughs

5
00:00:12,760 --> 00:00:15,280
in the area of natural language processing.

6
00:00:15,280 --> 00:00:18,080
It caused a paradigm shift by introducing a new approach

7
00:00:18,080 --> 00:00:21,200
to tackling language understanding tasks,

8
00:00:21,200 --> 00:00:23,840
portraying large copies of text data,

9
00:00:23,840 --> 00:00:26,880
and then funtuning for specific tasks,

10
00:00:26,880 --> 00:00:30,520
such as question answering and sediment analysis.

11
00:00:30,520 --> 00:00:33,720
This approach has proven to be highly effective

12
00:00:33,720 --> 00:00:35,000
and has been the foundation

13
00:00:35,000 --> 00:00:39,120
for many subsequent advancements in LP.

14
00:00:39,120 --> 00:00:41,640
Here's an illustration of Bert's pre-training

15
00:00:41,640 --> 00:00:43,280
and funtuning process,

16
00:00:43,280 --> 00:00:46,480
which I will explain in more detail in the following slides.

17
00:00:48,440 --> 00:00:52,280
So Bert is built on the transformer architecture,

18
00:00:52,280 --> 00:00:54,000
which is a type of neural network

19
00:00:54,000 --> 00:00:58,200
that has shown great promise in subsequent sequence tasks.

20
00:00:58,200 --> 00:01:01,440
Since we have already covered the transformer architecture

21
00:01:01,440 --> 00:01:03,120
in our previous lecture,

22
00:01:03,120 --> 00:01:05,880
I brief recap its key components.

23
00:01:05,880 --> 00:01:09,920
So the transformer consists of an encoder and a decoder,

24
00:01:09,920 --> 00:01:13,240
which are composed of multiple layers of self-attention

25
00:01:13,240 --> 00:01:15,400
and the feed-forward neural networks.

26
00:01:15,400 --> 00:01:17,800
The encoder processes the input sequence

27
00:01:17,800 --> 00:01:20,880
and generates a sequence of hidden representations.

28
00:01:20,880 --> 00:01:23,240
While the decoder generates output sequence

29
00:01:23,240 --> 00:01:26,080
based on the encoder's representations,

30
00:01:26,080 --> 00:01:28,840
following the initial success of the transformer

31
00:01:28,840 --> 00:01:32,080
in machine translation tasks,

32
00:01:32,080 --> 00:01:35,640
researchers explored using the decoder part of the transformer

33
00:01:35,640 --> 00:01:38,520
as a standalone language model.

34
00:01:38,520 --> 00:01:41,120
So this led to the development of models

35
00:01:41,120 --> 00:01:46,120
like OpenAI's GBT1, the generative pre-trained transformer,

36
00:01:48,240 --> 00:01:50,240
which demonstrates the effectiveness

37
00:01:50,240 --> 00:01:54,080
of using the transformer decoder for language model.

38
00:01:54,080 --> 00:01:56,840
GBT1, however, was unidirectional,

39
00:01:58,440 --> 00:02:01,160
meaning it could only attend to previous tokens

40
00:02:01,160 --> 00:02:02,360
in the sequence.

41
00:02:02,360 --> 00:02:06,120
This choice is natural for next-world predictions,

42
00:02:06,120 --> 00:02:08,200
but it also has limitations.

43
00:02:08,200 --> 00:02:11,160
So we'll discuss this in more detail in the next slide.

44
00:02:16,240 --> 00:02:17,960
So let's compare unidirectional

45
00:02:17,960 --> 00:02:20,800
and bidirectional language models.

46
00:02:20,800 --> 00:02:23,640
So unidirectional models like GBT1

47
00:02:23,640 --> 00:02:26,520
build representations of words incrementally

48
00:02:26,520 --> 00:02:30,240
based on the words that came before them in the sequence.

49
00:02:30,240 --> 00:02:32,720
This means that the representation of each word

50
00:02:32,720 --> 00:02:36,320
is influenced only by the words that preceded it.

51
00:02:36,320 --> 00:02:38,840
While this approach works well for some tasks,

52
00:02:38,840 --> 00:02:40,480
it doesn't capture the full context

53
00:02:40,480 --> 00:02:42,840
of a word within a sentence.

54
00:02:42,840 --> 00:02:44,320
For example, consider the sentence,

55
00:02:44,320 --> 00:02:47,360
I went to the bank to deposit money.

56
00:02:47,360 --> 00:02:50,240
The ward bank could refer to a financial institution

57
00:02:50,240 --> 00:02:51,600
or the side of a river.

58
00:02:51,600 --> 00:02:53,640
And the money is determined by the context

59
00:02:53,640 --> 00:02:56,400
of the whole sentence for tasks that require

60
00:02:56,400 --> 00:02:57,880
understanding the meaning of a word

61
00:02:57,880 --> 00:02:59,880
in the context of the entire sentence,

62
00:02:59,880 --> 00:03:02,680
such as question answering or natural language inference.

63
00:03:03,600 --> 00:03:06,120
Unidirectional model may struggle.

64
00:03:06,120 --> 00:03:10,040
So this is where bidirectional models like BERT come in.

65
00:03:10,040 --> 00:03:13,040
So for such models, the representations of each word

66
00:03:13,040 --> 00:03:17,080
are built by considering the entire input sequence.

67
00:03:17,080 --> 00:03:21,040
Both the words that precede and follow the target words.

68
00:03:22,480 --> 00:03:25,560
So this allows the model to capture a richer context

69
00:03:25,560 --> 00:03:28,440
for each word, which can be beneficial

70
00:03:28,440 --> 00:03:30,560
for a wide range of NLP tasks.

71
00:03:30,560 --> 00:03:33,960
However, building a bidirectional language model

72
00:03:33,960 --> 00:03:36,200
comes with its own set of challenges,

73
00:03:36,200 --> 00:03:38,120
which we'll discuss in the next slides.

74
00:03:41,440 --> 00:03:44,000
So ELMO, which we discussed before,

75
00:03:44,000 --> 00:03:46,560
was one of the first successful attempts

76
00:03:46,560 --> 00:03:49,200
at building a bidirectional language model.

77
00:03:49,200 --> 00:03:51,800
However, it aggregated the representations

78
00:03:51,800 --> 00:03:54,920
from different directions in an isolated manner,

79
00:03:54,920 --> 00:03:58,080
which limited its ability to capture complex dependency

80
00:03:58,080 --> 00:03:59,200
between words.

81
00:03:59,200 --> 00:04:01,720
BERT addressed these limitations

82
00:04:01,720 --> 00:04:05,360
by replacing LSTMs with the transformer architecture.

83
00:04:05,360 --> 00:04:09,200
In particular, BERT uses the multi-layer transformer encoder

84
00:04:09,200 --> 00:04:11,720
to build a deep bidirectional language model,

85
00:04:11,720 --> 00:04:14,080
which captures the context of each word

86
00:04:14,080 --> 00:04:17,680
by considering every other word in the input sequence.

87
00:04:17,680 --> 00:04:20,240
This architecture allows BERT to generate

88
00:04:20,240 --> 00:04:22,960
rich contextualized representations of words,

89
00:04:22,960 --> 00:04:25,760
which can be used for a wide range of NLP tasks.

90
00:04:28,680 --> 00:04:32,880
The BERT input representation consists of three key components.

91
00:04:32,880 --> 00:04:35,120
Token embeddings, segment embeddings,

92
00:04:35,120 --> 00:04:36,680
and position embeddings.

93
00:04:36,680 --> 00:04:38,480
Token embeddings are the word embeddings

94
00:04:38,480 --> 00:04:40,680
for each token in the input sequence.

95
00:04:40,680 --> 00:04:42,200
BERT uses word piece embedding

96
00:04:42,200 --> 00:04:45,480
with a vocabulary size of 30,000 tokens.

97
00:04:45,480 --> 00:04:47,640
It is a subword tokenization method

98
00:04:47,640 --> 00:04:50,800
that allows the model to handle out of vocabulary words

99
00:04:50,800 --> 00:04:54,880
and capture morphological variation of words.

100
00:04:54,880 --> 00:04:57,680
The input sequence is also augmented with special tokens,

101
00:04:57,680 --> 00:05:00,840
such as CRS and SAP,

102
00:05:00,840 --> 00:05:04,160
which are used to denote the beginning

103
00:05:04,160 --> 00:05:07,480
and the separation of sentences, respectively.

104
00:05:07,480 --> 00:05:11,160
So the CRS token is particularly important,

105
00:05:11,160 --> 00:05:13,960
because it is used to generate a fixed size representation

106
00:05:13,960 --> 00:05:15,760
of the entire input sequence,

107
00:05:15,760 --> 00:05:19,080
which can be used as an input to downstream tasks.

108
00:05:20,440 --> 00:05:23,000
Segment embeddings are used to distinguish

109
00:05:23,000 --> 00:05:26,480
between different sentences in the input sequence.

110
00:05:26,480 --> 00:05:28,960
This allows BERT to handle tasks

111
00:05:28,960 --> 00:05:31,280
that involve multiple sentences,

112
00:05:31,280 --> 00:05:33,160
such as natural language inference.

113
00:05:35,120 --> 00:05:37,960
Finally, position embeddings are added

114
00:05:37,960 --> 00:05:40,200
to the token embeddings to capture the position

115
00:05:40,200 --> 00:05:42,480
of each token in the input sequence.

116
00:05:42,480 --> 00:05:44,560
Different from traditional position embeddings,

117
00:05:44,560 --> 00:05:46,720
BERT uses learned position embeddings,

118
00:05:46,720 --> 00:05:50,280
which are optimized during the pre-training process.

119
00:05:52,520 --> 00:05:56,080
So BERT comes in two sizes, BERT-BASE and BERT-LARGE.

120
00:05:56,080 --> 00:05:58,760
BERT-BASE has 12 transformer layers

121
00:05:58,760 --> 00:06:01,520
and 768 hidden units,

122
00:06:01,520 --> 00:06:04,120
and 12 self-attention heads,

123
00:06:04,120 --> 00:06:09,120
which in total result in 110 million parameters.

124
00:06:10,800 --> 00:06:15,800
BERT-LARGE, on the other hand, has 24 transformer layers.

125
00:06:17,160 --> 00:06:21,680
1024 hidden units and 16 self-attention heads

126
00:06:21,680 --> 00:06:26,280
resulting in 340 million parameters.

127
00:06:26,280 --> 00:06:29,520
The larger model has been shown to achieve

128
00:06:30,560 --> 00:06:33,160
better performance on various NLP tasks,

129
00:06:33,160 --> 00:06:36,400
but it also requires more computational resources

130
00:06:36,400 --> 00:06:38,400
and longer training times.

131
00:06:38,400 --> 00:06:41,280
Back in 2018, BERT-LARGE was considered

132
00:06:41,280 --> 00:06:43,760
one of the largest model in the field.

133
00:06:43,760 --> 00:06:46,800
However, with the continuous scaling of the language models,

134
00:06:46,800 --> 00:06:50,080
we have seen even larger models being developed,

135
00:06:50,080 --> 00:06:55,080
such as GPT-3, which has 175 billion parameters,

136
00:06:55,120 --> 00:06:59,000
about 500 times more than the BERT-LARGE.

137
00:06:59,000 --> 00:07:04,000
Both models are pre-trained on large copies of text data,

138
00:07:04,240 --> 00:07:07,000
such as Wikipedia and the book covers,

139
00:07:07,000 --> 00:07:10,320
using two unsupervised learning objectives,

140
00:07:10,320 --> 00:07:13,880
mass language modeling, and the next sentence prediction.

141
00:07:18,200 --> 00:07:21,200
So the traditional approach to language modeling

142
00:07:21,200 --> 00:07:23,160
is to predict the next wording sequence

143
00:07:23,160 --> 00:07:25,360
given the previous words.

144
00:07:25,360 --> 00:07:27,040
So why this approach works well

145
00:07:27,040 --> 00:07:29,240
for unidirectional language models?

146
00:07:29,240 --> 00:07:32,880
It is not suitable for bi-directional models like BERT.

147
00:07:32,880 --> 00:07:36,200
But because conditioning on the future war

148
00:07:36,600 --> 00:07:39,600
would result in data leakage during pre-training,

149
00:07:39,600 --> 00:07:41,280
to address this issue,

150
00:07:41,280 --> 00:07:43,640
BERT introduces a new pre-training objective

151
00:07:43,640 --> 00:07:46,960
called a mass language modeling, MLM.

152
00:07:46,960 --> 00:07:50,600
So in MLM, a random subset of the input tokens are massed,

153
00:07:50,600 --> 00:07:53,080
and the model is trying to predict the original words

154
00:07:53,080 --> 00:07:54,840
based on their context.

155
00:07:54,840 --> 00:07:58,160
This allows BERT to capture bi-directional context

156
00:07:58,160 --> 00:07:59,480
while pre-training,

157
00:07:59,480 --> 00:08:02,040
but the model has to learn to predict the massed words

158
00:08:02,040 --> 00:08:05,400
based on both the preceding and the following words.

159
00:08:05,440 --> 00:08:07,800
In the original BERT paper,

160
00:08:07,800 --> 00:08:12,160
about 15% of the tokens are massed during pre-training.

161
00:08:12,160 --> 00:08:14,640
Since the mass token is not presented in the input

162
00:08:14,640 --> 00:08:15,480
during fine-tuning,

163
00:08:15,480 --> 00:08:17,640
which may cause a mismatch between pre-training

164
00:08:17,640 --> 00:08:18,640
and fine-tuning,

165
00:08:18,640 --> 00:08:20,960
the BERT uses the following strategy

166
00:08:20,960 --> 00:08:22,840
to handle the mass tokens.

167
00:08:24,480 --> 00:08:29,400
80% of the time, the token is replaced with a mass token.

168
00:08:29,400 --> 00:08:33,000
10% of the time, the token is replaced with a random token.

169
00:08:33,000 --> 00:08:36,120
10% of the time, the token is left unchanged.

170
00:08:36,120 --> 00:08:38,640
So this strategy encouraged the model

171
00:08:38,640 --> 00:08:42,240
to learn robust representations of the mass tokens,

172
00:08:42,240 --> 00:08:44,800
which can be beneficial for downstream tasks.

173
00:08:48,040 --> 00:08:50,280
In addition to mass language modeling,

174
00:08:50,280 --> 00:08:53,720
BERT also uses the next sentence prediction,

175
00:08:53,720 --> 00:08:57,160
NISP objectives during pre-training.

176
00:08:57,160 --> 00:08:59,680
In NISP, the model is trying to predict

177
00:08:59,680 --> 00:09:02,400
whether a pair of sentences are consecutive

178
00:09:02,400 --> 00:09:05,440
in their original document or not.

179
00:09:05,440 --> 00:09:07,080
During pre-training,

180
00:09:07,080 --> 00:09:10,080
50% of the training examples are positive examples,

181
00:09:10,080 --> 00:09:12,280
while the second sentence follows the first sentence

182
00:09:12,280 --> 00:09:13,720
in the original document,

183
00:09:13,720 --> 00:09:16,120
and the 50% are negative examples,

184
00:09:16,120 --> 00:09:18,640
where the second sentence is readily sampled

185
00:09:18,640 --> 00:09:20,320
for the coppers.

186
00:09:20,320 --> 00:09:22,400
This objective encourages the model

187
00:09:22,400 --> 00:09:24,960
to capture relationships between sentences,

188
00:09:24,960 --> 00:09:26,960
which can be beneficial for downstream tasks

189
00:09:26,960 --> 00:09:30,280
such as question altering and a natural language inferences,

190
00:09:30,280 --> 00:09:31,880
where understanding the relationship

191
00:09:31,880 --> 00:09:33,560
between sentences is crucial.

192
00:09:37,200 --> 00:09:40,520
So once pre-trained, BERT can be fine-tuned

193
00:09:40,520 --> 00:09:43,720
for a specific task using supervised learning.

194
00:09:43,720 --> 00:09:46,600
The fine-tuning process involves adding

195
00:09:46,600 --> 00:09:48,440
a task-specific output layer

196
00:09:48,440 --> 00:09:50,600
on top of the pre-trained BERT model,

197
00:09:50,600 --> 00:09:52,760
and fine-tuning the entire model

198
00:09:52,760 --> 00:09:56,080
on the label dataset for target task.

199
00:09:56,080 --> 00:09:59,480
The task-specific output layer can be a single layer

200
00:09:59,480 --> 00:10:00,840
or multiple layers,

201
00:10:00,840 --> 00:10:03,840
depending on the complexity of the task.

202
00:10:03,840 --> 00:10:05,400
In the original BERT paper,

203
00:10:05,400 --> 00:10:07,840
the authors demonstrate the effectiveness

204
00:10:07,840 --> 00:10:10,800
of fine-tuning BERT on a wide range of NLP tasks,

205
00:10:10,800 --> 00:10:13,480
such as sentence parallel classification,

206
00:10:13,480 --> 00:10:15,200
single sentence classification,

207
00:10:15,200 --> 00:10:17,560
question altering, and a sequence tagging.

208
00:10:17,560 --> 00:10:18,720
The token-level task,

209
00:10:18,720 --> 00:10:21,880
such as question altering and a sequence tagging,

210
00:10:21,880 --> 00:10:25,360
uses the token-level representations from BERT.

211
00:10:25,360 --> 00:10:26,960
The wide-sentence-level tasks,

212
00:10:26,960 --> 00:10:28,720
such as sentence parallel classification,

213
00:10:28,720 --> 00:10:31,680
use the CRS token representation

214
00:10:31,680 --> 00:10:34,240
as the input to the task-specific output layer.

215
00:10:36,760 --> 00:10:38,920
So the original BERT paper reported

216
00:10:38,920 --> 00:10:41,000
the state-of-the-art results on a wide range

217
00:10:41,000 --> 00:10:43,760
of NLP benchmarks, including the GRU benchmark,

218
00:10:43,760 --> 00:10:45,480
which consists of nine different tasks,

219
00:10:45,480 --> 00:10:48,600
such as question altering, natural language inference,

220
00:10:48,600 --> 00:10:50,600
and sentiment analysis.

221
00:10:50,600 --> 00:10:52,920
BERT achieved significant improvements

222
00:10:52,920 --> 00:10:55,680
over previous models of most of the tasks.

223
00:10:55,680 --> 00:10:57,560
In particular, BERT large outform

224
00:10:57,560 --> 00:11:00,120
all other models on the GRU benchmark,

225
00:11:00,120 --> 00:11:02,120
demonstrating the effectiveness

226
00:11:02,120 --> 00:11:04,960
of large-scale pre-training for NLP tasks.

227
00:11:04,960 --> 00:11:07,840
The success of BERT fundamentally changed

228
00:11:07,840 --> 00:11:10,640
the landscape of NLP research

229
00:11:10,640 --> 00:11:13,520
and led to the development of many other language models

230
00:11:13,520 --> 00:11:16,000
that built on their BERT architecture.

231
00:11:17,000 --> 00:11:19,760
So following the success of BERT,

232
00:11:19,760 --> 00:11:22,760
several variants have been developed

233
00:11:22,760 --> 00:11:26,160
to address its limitations and improve its performance.

234
00:11:26,160 --> 00:11:29,600
So here's an illustration of the BERT family tree,

235
00:11:29,600 --> 00:11:33,400
which shows the evolution of BERT and its variants.

236
00:11:33,400 --> 00:11:34,720
In the next few slides,

237
00:11:34,720 --> 00:11:38,760
we'll discuss some of the most notable BERT variants,

238
00:11:38,760 --> 00:11:41,360
including Roberta, Albert,

239
00:11:41,360 --> 00:11:43,360
Electra, and Ernie.

240
00:11:44,360 --> 00:11:47,360
Roberta, which stands for a robustly-optimized

241
00:11:47,360 --> 00:11:49,360
BERT pre-training approach,

242
00:11:49,360 --> 00:11:52,360
is a variant of BERT's developed by Facebook AI.

243
00:11:52,360 --> 00:11:55,360
It is one of the most widely used BERT variants

244
00:11:55,360 --> 00:11:57,920
and has achieved state-of-the-art results

245
00:11:57,920 --> 00:11:59,920
on many NLP benchmarks.

246
00:11:59,920 --> 00:12:01,920
Roberta builds on the BERT architecture

247
00:12:01,920 --> 00:12:04,920
and introduces several modifications to the design

248
00:12:04,920 --> 00:12:06,920
and pre-training process of BERT.

249
00:12:06,920 --> 00:12:08,920
In addition, the BERT architecture

250
00:12:09,480 --> 00:12:11,480
introduces several modifications to the design

251
00:12:11,480 --> 00:12:13,480
and the pre-training process of BERT.

252
00:12:13,480 --> 00:12:16,480
In addition, it introduces a new pre-training data set

253
00:12:16,480 --> 00:12:18,480
called CCNEWS,

254
00:12:18,480 --> 00:12:20,480
which consists of 63 million

255
00:12:20,480 --> 00:12:23,480
English news articles proud from the web.

256
00:12:23,480 --> 00:12:27,480
The author of Roberta conducted an extensive study

257
00:12:27,480 --> 00:12:30,480
to identify the impact of different design choices

258
00:12:30,480 --> 00:12:33,480
and the pre-training data on the performance of BERT.

259
00:12:33,480 --> 00:12:37,480
They found that by optimizing the pre-training process

260
00:12:38,040 --> 00:12:41,040
and the training on a larger data set,

261
00:12:41,040 --> 00:12:43,040
they could achieve significant improvements

262
00:12:43,040 --> 00:12:45,040
over the original BERT model.

263
00:12:45,040 --> 00:12:49,040
Here are some of the key modifications introduced in Roberta.

264
00:12:49,040 --> 00:12:52,040
I will discuss a few of them in more detail

265
00:12:52,040 --> 00:12:54,040
in the following slides.

266
00:12:58,040 --> 00:13:01,040
One of the key differences between Roberta and BERT

267
00:13:01,040 --> 00:13:04,040
is the way they handle masked language modeling

268
00:13:04,040 --> 00:13:06,040
during pre-training.

269
00:13:06,600 --> 00:13:08,600
In other words, the masking is performed

270
00:13:08,600 --> 00:13:10,600
during the data pre-processing step,

271
00:13:10,600 --> 00:13:13,600
which results in a fixed set of masked tokens

272
00:13:13,600 --> 00:13:15,600
for each training epoch.

273
00:13:15,600 --> 00:13:17,600
To avoid using the same masked tokens

274
00:13:17,600 --> 00:13:19,600
for each training epoch,

275
00:13:19,600 --> 00:13:24,600
BERT ad hocly duplicates the input data 10 times

276
00:13:24,600 --> 00:13:27,600
and masks different tokens in each duplicate.

277
00:13:27,600 --> 00:13:30,600
In contrast, Roberta uses dynamic masking,

278
00:13:30,600 --> 00:13:34,600
where the masking is generated every time

279
00:13:34,600 --> 00:13:37,600
the input is fed into the model during pre-training.

280
00:13:37,600 --> 00:13:40,600
This allows Roberta to use different masked tokens

281
00:13:40,600 --> 00:13:42,600
for each training epoch,

282
00:13:42,600 --> 00:13:46,600
which can be beneficial for learning robust representations

283
00:13:46,600 --> 00:13:48,600
of the masked tokens.

284
00:13:48,600 --> 00:13:51,600
The authors of Roberta found that dynamic masking

285
00:13:51,600 --> 00:13:55,600
led to better performance compared to static masking.

286
00:13:56,600 --> 00:14:01,600
Multiple studies have questioned the effectiveness

287
00:14:01,600 --> 00:14:04,600
of the next sentence prediction objective in BERT.

288
00:14:04,600 --> 00:14:08,600
To investigate the impact of NSP,

289
00:14:08,600 --> 00:14:10,600
the authors of Roberta conducted experiments

290
00:14:10,600 --> 00:14:13,600
with different input formats and pre-training objectives.

291
00:14:13,600 --> 00:14:16,600
Here are the four settings they considered.

292
00:14:16,600 --> 00:14:18,600
Segment pair plus NSP,

293
00:14:18,600 --> 00:14:21,600
which is the original BERT input format

294
00:14:22,600 --> 00:14:24,600
with NSP objectives.

295
00:14:24,600 --> 00:14:27,600
Each training example consists of a pair of segments,

296
00:14:27,600 --> 00:14:30,600
which can contain multiple natural sentences.

297
00:14:30,600 --> 00:14:33,600
And sentence pair plus NSP.

298
00:14:33,600 --> 00:14:37,600
So each training example consists of a pair of natural sentences,

299
00:14:37,600 --> 00:14:41,600
either sampled consecutively from one document

300
00:14:41,600 --> 00:14:43,600
or from different documents.

301
00:14:43,600 --> 00:14:45,600
And full sentences.

302
00:14:45,600 --> 00:14:48,600
So each training example consists of natural sentences,

303
00:14:48,600 --> 00:14:51,600
sampled consecutively from one or more documents

304
00:14:51,600 --> 00:14:55,600
up to a maximum length of 512 tokens

305
00:14:55,600 --> 00:14:58,600
without the NSP objective.

306
00:14:58,600 --> 00:15:01,600
And the doc sentence,

307
00:15:01,600 --> 00:15:04,600
so similar to full sentences,

308
00:15:04,600 --> 00:15:08,600
but the sampling does not cross document boundaries

309
00:15:08,600 --> 00:15:11,600
and also without the NSP objective.

310
00:15:11,600 --> 00:15:14,600
So the authors found that the NSP objective

311
00:15:14,600 --> 00:15:17,600
did not provide significant improvements

312
00:15:17,600 --> 00:15:19,600
over the other settings.

313
00:15:19,600 --> 00:15:22,600
And in some cases, it even heard the performance.

314
00:15:22,600 --> 00:15:25,600
So they also found that using longer sequence

315
00:15:25,600 --> 00:15:27,600
during pre-training such as full sentences

316
00:15:27,600 --> 00:15:29,600
or document level sentences

317
00:15:29,600 --> 00:15:31,600
led to better performance

318
00:15:31,600 --> 00:15:34,600
compared to using individual sentence pairs.

319
00:15:34,600 --> 00:15:36,600
So based on this findings,

320
00:15:36,600 --> 00:15:38,600
Roberta used the full sentence settings

321
00:15:38,600 --> 00:15:41,600
without the NSP objective for pre-training.

322
00:15:42,600 --> 00:15:45,600
So in addition to the change

323
00:15:45,600 --> 00:15:48,600
in the pre-training objectives and input format,

324
00:15:48,600 --> 00:15:50,600
Roberta also introduces several modifications

325
00:15:50,600 --> 00:15:52,600
to the pre-training process of BERT.

326
00:15:52,600 --> 00:15:56,600
So first, the authors found that using larger batch size

327
00:15:56,600 --> 00:15:58,600
it leads to better pre-prenexity

328
00:15:58,600 --> 00:16:00,600
and downstream test performance.

329
00:16:00,600 --> 00:16:03,600
Second, the authors found that the original BERT

330
00:16:03,600 --> 00:16:05,600
is under-trained

331
00:16:05,600 --> 00:16:09,600
and using more data leads to better performance.

332
00:16:09,600 --> 00:16:11,600
In addition to the original BERT pre-training data,

333
00:16:11,600 --> 00:16:13,600
Roberta used the CC News data set

334
00:16:13,600 --> 00:16:16,600
which consists of 63 million English news articles.

335
00:16:16,600 --> 00:16:18,600
So finally, they also found that

336
00:16:18,600 --> 00:16:21,600
the longer training leads to better performance.

337
00:16:21,600 --> 00:16:24,600
They trained Roberta for 100,000,

338
00:16:24,600 --> 00:16:27,600
so 300,000 and 500,000 steps.

339
00:16:27,600 --> 00:16:29,600
They found that longer training leads to better performance

340
00:16:29,600 --> 00:16:31,600
on downstream tasks.

341
00:16:31,600 --> 00:16:34,600
Roberta did not show signs of overfitting

342
00:16:34,600 --> 00:16:38,600
even after 500,000 steps,

343
00:16:38,600 --> 00:16:40,600
which suggests that longer training could be beneficial

344
00:16:40,600 --> 00:16:42,600
for large-scale language models.

345
00:16:46,600 --> 00:16:48,600
So the modifications introduced in Roberta

346
00:16:48,600 --> 00:16:50,600
led to significant improvements

347
00:16:50,600 --> 00:16:52,600
over the original BERT model.

348
00:16:52,600 --> 00:16:54,600
So Roberta achieved state-of-the-art

349
00:16:54,600 --> 00:16:56,600
on the glue benchmark,

350
00:16:56,600 --> 00:16:58,600
surpassing BERT and other language models

351
00:16:58,600 --> 00:17:00,600
on most of the tests.

352
00:17:02,600 --> 00:17:04,600
Then the ALBERT.

353
00:17:04,600 --> 00:17:07,600
So ALBERT, which stands for a light BERT,

354
00:17:07,600 --> 00:17:09,600
is a variant of BERT

355
00:17:09,600 --> 00:17:11,600
developed by Google Research.

356
00:17:11,600 --> 00:17:13,600
So scaling up language models like BERT

357
00:17:13,600 --> 00:17:15,600
to achieve better performance

358
00:17:15,600 --> 00:17:18,600
often comes with increased computational cost

359
00:17:18,600 --> 00:17:20,600
and memory requirements.

360
00:17:20,600 --> 00:17:23,600
ALBERT addressed this issue by introducing

361
00:17:23,600 --> 00:17:25,600
several parameter reduction techniques

362
00:17:25,600 --> 00:17:27,600
to make the model more efficient

363
00:17:27,600 --> 00:17:29,600
without compromising its effectiveness.

364
00:17:29,600 --> 00:17:32,600
The authors show that a configuration of ALBERT

365
00:17:32,600 --> 00:17:34,600
similar to BERT large

366
00:17:34,600 --> 00:17:38,600
can have up to 18 times fewer parameters

367
00:17:38,600 --> 00:17:41,600
and can be trained up to 1.7 times faster.

368
00:17:41,600 --> 00:17:43,600
So there are three key parameter reduction techniques

369
00:17:43,600 --> 00:17:45,600
introduced in ALBERT,

370
00:17:45,600 --> 00:17:49,600
factorized embedding parameterization,

371
00:17:49,600 --> 00:17:51,600
translator, parameter sharing,

372
00:17:51,600 --> 00:17:53,600
and interest-sentence coherence loss

373
00:17:53,600 --> 00:17:55,600
or sentence order prediction.

374
00:17:55,600 --> 00:17:58,600
So I will discuss each of these techniques

375
00:17:58,600 --> 00:18:00,600
in more detail in the following slides.

376
00:18:01,600 --> 00:18:04,600
So the first parameter reduction techniques

377
00:18:04,600 --> 00:18:06,600
introduced in ALBERT

378
00:18:06,600 --> 00:18:08,600
is factorized embedding parameterization.

379
00:18:08,600 --> 00:18:10,600
In BERT,

380
00:18:10,600 --> 00:18:12,600
the dimensions of the token embeddings

381
00:18:12,600 --> 00:18:14,600
and the hidden layers are the same,

382
00:18:14,600 --> 00:18:16,600
which is a sub-automode choice.

383
00:18:16,600 --> 00:18:18,600
The token embeddings are used to represent

384
00:18:18,600 --> 00:18:20,600
the input tokens and only capture

385
00:18:20,600 --> 00:18:22,600
the context-free information of the tokens.

386
00:18:22,600 --> 00:18:24,600
On the other hand, the hidden layers

387
00:18:24,600 --> 00:18:26,600
are used to capture the context-dependent

388
00:18:26,600 --> 00:18:28,600
information of the tokens.

389
00:18:28,600 --> 00:18:31,600
The power of BERT comes from the hidden layer

390
00:18:31,600 --> 00:18:33,600
and not the token embedding.

391
00:18:33,600 --> 00:18:35,600
So it is natural that the hidden layers

392
00:18:35,600 --> 00:18:37,600
should have a higher dimension than the token embeddings.

393
00:18:37,600 --> 00:18:40,600
However, to increase the dimension of the hidden layers,

394
00:18:40,600 --> 00:18:42,600
the dimension of the token embeddings

395
00:18:42,600 --> 00:18:44,600
must also be increased.

396
00:18:44,600 --> 00:18:46,600
This would result in a significant increase

397
00:18:46,600 --> 00:18:48,600
in the number of parameters

398
00:18:48,600 --> 00:18:50,600
because the vocabulary size is usually very large.

399
00:18:50,600 --> 00:18:52,600
So for example,

400
00:18:52,600 --> 00:18:56,600
BERT has a vocabulary size of 30,000 tokens

401
00:18:56,600 --> 00:19:00,600
and they use 768-dimensional token embeddings,

402
00:19:00,600 --> 00:19:03,600
resulting in 23 million parameters.

403
00:19:03,600 --> 00:19:05,600
So BERT addresses this issue

404
00:19:05,600 --> 00:19:09,600
by decoupling the dimension of the token embeddings

405
00:19:09,600 --> 00:19:11,600
from the dimensions of the hidden layers.

406
00:19:11,600 --> 00:19:13,600
So in BERT,

407
00:19:13,600 --> 00:19:15,600
the token embeddings are first projected

408
00:19:15,600 --> 00:19:17,600
to a lower-dimensional space

409
00:19:17,600 --> 00:19:19,600
and then projected to the higher-dimensional space

410
00:19:19,600 --> 00:19:21,600
of the hidden layers.

411
00:19:21,600 --> 00:19:24,600
This allows BERT to use a higher-dimensional hidden layer

412
00:19:24,600 --> 00:19:27,600
without significantly increasing the number of parameters.

413
00:19:27,600 --> 00:19:29,600
So using this technique,

414
00:19:29,600 --> 00:19:31,600
the number of parameters is reduced from

415
00:19:31,600 --> 00:19:34,600
the big O of the V times H

416
00:19:34,600 --> 00:19:37,600
to the big O of the V times E plus H times E.

417
00:19:37,600 --> 00:19:39,600
So where V is the vocabulary size

418
00:19:39,600 --> 00:19:41,600
and H is the hidden layer dimension

419
00:19:41,600 --> 00:19:43,600
and E is embedding dimension.

420
00:19:46,600 --> 00:19:48,600
The second parameter reduction technique

421
00:19:48,600 --> 00:19:51,600
introduced in BERT is cross-layer parameter sharing.

422
00:19:51,600 --> 00:19:52,600
So in BERT,

423
00:19:52,600 --> 00:19:55,600
each layer has its own set of parameters,

424
00:19:55,600 --> 00:19:58,600
which results in a large number of parameters.

425
00:19:58,600 --> 00:20:00,600
Albert addresses this issue

426
00:20:00,600 --> 00:20:02,600
by sharing parameters across layers.

427
00:20:02,600 --> 00:20:06,600
There are several ways to share parameters across layers,

428
00:20:06,600 --> 00:20:08,600
such as sharing the attention waves,

429
00:20:08,600 --> 00:20:11,600
the feed-over neural network waves, or both.

430
00:20:11,600 --> 00:20:12,600
By default,

431
00:20:12,600 --> 00:20:15,600
Albert shares all parameters across layers.

432
00:20:15,600 --> 00:20:18,600
The authors found that this parameter sharing technique

433
00:20:19,600 --> 00:20:22,600
led to a significant parameter reduction

434
00:20:22,600 --> 00:20:25,600
without compromising the performance of the model

435
00:20:27,600 --> 00:20:29,600
significantly.

436
00:20:30,600 --> 00:20:33,600
And the third parameter reduction technique introduced

437
00:20:33,600 --> 00:20:35,600
is interest sentence coherence,

438
00:20:35,600 --> 00:20:37,600
or sentence order prediction.

439
00:20:37,600 --> 00:20:38,600
So in BERT,

440
00:20:38,600 --> 00:20:40,600
the next sentence prediction objective

441
00:20:40,600 --> 00:20:42,600
is used to encourage the model

442
00:20:42,600 --> 00:20:44,600
to capture relationship between sentences.

443
00:20:44,600 --> 00:20:47,600
However, as we discussed earlier,

444
00:20:47,600 --> 00:20:50,600
the NASP objective has been shown to be less effective

445
00:20:50,600 --> 00:20:52,600
than other pre-training objectives.

446
00:20:52,600 --> 00:20:55,600
So Albert introduced a new pre-training objective

447
00:20:55,600 --> 00:20:57,600
called a sentence order prediction,

448
00:20:57,600 --> 00:20:59,600
which is supposed to be more effective than NASP.

449
00:20:59,600 --> 00:21:01,600
So in sentence order prediction,

450
00:21:01,600 --> 00:21:03,600
the model is trying to predict

451
00:21:03,600 --> 00:21:05,600
whether a pair of sentences

452
00:21:05,600 --> 00:21:07,600
are in the correct order or not.

453
00:21:07,600 --> 00:21:09,600
The authors found that these objectives

454
00:21:09,600 --> 00:21:11,600
lead to better performance compared to NASP.

455
00:21:11,600 --> 00:21:15,600
And actually, this is not a parameter reduction technique,

456
00:21:15,600 --> 00:21:18,600
like to investigate whether the NASP objective

457
00:21:18,600 --> 00:21:20,600
is effective or not,

458
00:21:21,600 --> 00:21:23,600
similar to what the robot had did.

459
00:21:26,600 --> 00:21:29,600
So the parameter reduction techniques introduced

460
00:21:29,600 --> 00:21:32,600
in Albert lead to a significant improvement

461
00:21:32,600 --> 00:21:34,600
in efficiency without compromising

462
00:21:34,600 --> 00:21:36,600
the performance of the model.

463
00:21:36,600 --> 00:21:38,600
So it even achieved the state-of-the-art results

464
00:21:38,600 --> 00:21:40,600
on the group benchmark,

465
00:21:40,600 --> 00:21:43,600
surprising BERT, robot and other language models

466
00:21:43,600 --> 00:21:45,600
for most of the text.

467
00:21:45,600 --> 00:21:47,600
And this performance is quite impressive.

