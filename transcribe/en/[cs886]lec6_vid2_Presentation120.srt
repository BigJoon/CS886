1
00:00:00,000 --> 00:00:11,520
Hello everyone, my name is Hossein and I want to talk about my presentation for our class

2
00:00:11,520 --> 00:00:20,080
which is on state space models for an introduction to Mamba model.

3
00:00:20,080 --> 00:00:26,080
So just to give you a little bit of background and the kind of problem we are talking about

4
00:00:26,080 --> 00:00:33,040
here is basically it's a sequence to sequence modeling so you can imagine that your input

5
00:00:33,040 --> 00:00:38,160
it can be a sequence of tokens, sequence of images or any other modality and then you

6
00:00:38,160 --> 00:00:48,680
want to transform this input into some desired output and in the between you can use different

7
00:00:48,680 --> 00:00:53,680
kind of architecture so for example you can think of RNN to actually perform this task

8
00:00:53,680 --> 00:01:02,800
for you or convolutional network to do this task with you or transformers to perform the

9
00:01:02,800 --> 00:01:14,800
task but for this data space model and Mamba tries to perform this task more efficiently

10
00:01:14,800 --> 00:01:26,840
and they are basically as I said an emerging architecture that tries to be more efficient

11
00:01:26,840 --> 00:01:35,320
in computation so to give you a little bit of background before we go into the details

12
00:01:35,320 --> 00:01:43,080
of the architectures and also some theories on these models, foundation models specifically

13
00:01:43,080 --> 00:01:48,400
large language models are pre-trained on massive data and then they are fine-tuned on

14
00:01:48,400 --> 00:01:56,880
downstream tasks that's what people usually do and the key factor in these foundation

15
00:01:56,880 --> 00:02:04,360
models is that they in almost all of them the predominant architecture used in them

16
00:02:04,360 --> 00:02:10,320
is transformers and the main reason why people use transformers is because of the core attention

17
00:02:10,320 --> 00:02:19,640
layer it has that has the ability to actually recall the previous tokens and also do some

18
00:02:19,640 --> 00:02:29,880
in content learning but it comes at some heavy cost and as you can see for example the number

19
00:02:29,880 --> 00:02:34,880
of tokens that you can actually pass to these large language models are limited for example

20
00:02:34,880 --> 00:02:45,280
to 1024 or even lower and that's because the attention mechanism is very costly it's quadratic so

21
00:02:45,280 --> 00:02:53,200
given your input of sequence length n in order to get your attention matrix you need to do

22
00:02:53,200 --> 00:03:04,640
n squared multiplication so that's a very costly operation and as we are all in computer science

23
00:03:04,640 --> 00:03:10,760
we know that n squared is not a very desirable number when it comes to runtime analysis and

24
00:03:10,760 --> 00:03:20,600
basically structure state space sequence models for intent to address these issues and try to come

25
00:03:20,600 --> 00:03:25,920
up with an alternative architecture that can actually perform the similar task with similar

26
00:03:26,000 --> 00:03:38,880
accuracy but at lower cost as I said many sub there are many sub quadratic time

27
00:03:38,880 --> 00:03:46,000
architectures such as linear attention gated convolution and recurrent models to try to

28
00:03:46,000 --> 00:03:55,440
actually replace transformers computational inefficiency and what can we say about structure

29
00:03:55,920 --> 00:04:03,680
state space model is that they are they can be somehow interpreted as a combination of recurrent

30
00:04:03,680 --> 00:04:10,400
neural networks and convolutional neural networks and we will see throughout this talk how we can

31
00:04:10,400 --> 00:04:17,440
actually derive this kind of interpretation and I will go through the details of their derivations

32
00:04:17,760 --> 00:04:27,920
but as I said the main motivation for structure space models is to try to

33
00:04:30,480 --> 00:04:37,360
do the task for longer range sequences so that we can actually cover for example millions of

34
00:04:37,360 --> 00:04:45,440
tokens as input so the size of the maximum sequence left that we have we see in many of these large

35
00:04:45,440 --> 00:04:53,280
language model can be extended to a million and also it should have the capacity to capture

36
00:04:53,280 --> 00:05:00,000
dependencies over tens of thousands of steps but as we will see there are many struggles that

37
00:05:00,000 --> 00:05:08,480
comes with these s4 model specifically when it comes to meeting the same kind of accuracy and

38
00:05:08,480 --> 00:05:14,720
performance that transformer model has and the next model that I'm going to cover which is

39
00:05:14,720 --> 00:05:26,480
mamba tries to actually address that specific problem to s4 so as I said the goal in a state

40
00:05:26,480 --> 00:05:34,240
space model is to efficient modeling of long sequences and the solution is basically built

41
00:05:34,240 --> 00:05:43,360
a neural network layer based on state space model so actually you can use rnn for

42
00:05:45,600 --> 00:05:54,000
and modeling long sequences because as you know rnn has some internal state and this internal

43
00:05:54,000 --> 00:06:06,800
state is actually constant so the inference time of rnn is actually linear in contrast to

44
00:06:06,800 --> 00:06:13,360
transformers that's that is n squared because of the attention computation but the problem with rnn

45
00:06:13,360 --> 00:06:20,800
is that during the training you cannot make them parallel like the way that you can work with

46
00:06:20,800 --> 00:06:29,600
transformers so if you already know the sequence beforehand which is the case for training you

47
00:06:29,600 --> 00:06:38,400
should actually do it sequentially because of the way that rnn are structured which is a

48
00:06:38,960 --> 00:06:48,400
composition of functions so it can because the training cost is really high for rnn people have

49
00:06:48,400 --> 00:06:59,600
moved to transformers so s4 tries to actually bridge this gap so have a good running time at

50
00:06:59,600 --> 00:07:08,560
the inference time and also have some linear have the capability of being paralyzed during the

51
00:07:08,560 --> 00:07:14,880
training time so we try to answer this question using state space model

52
00:07:16,960 --> 00:07:23,680
so the question is what is the state space model basically you can consider a space space model

53
00:07:24,400 --> 00:07:34,240
is some kind of a mapper that maps or basically a better to say a function that maps a one

54
00:07:34,240 --> 00:07:46,960
dimensional input signal u of t into an n dimensional latent state x of t so x has dimension of n

55
00:07:46,960 --> 00:07:55,680
and u has dimension of one and then it projects x into one dimensional output signal y of t

56
00:07:55,680 --> 00:08:07,760
it's actually coming from the control systems so this idea existed for many years before even

57
00:08:07,840 --> 00:08:11,920
the emergence of deep learning but

58
00:08:15,600 --> 00:08:19,680
as you can see it's it's basically based on the differential equation that

59
00:08:21,440 --> 00:08:32,080
in this particular equation on the right you can see that the gradient of x which is of

60
00:08:32,800 --> 00:08:46,480
dimension the x is of dimension n is basically a function of x and the input u and then the output

61
00:08:48,160 --> 00:08:59,440
y is the projection of x plus some d of u sometimes i think particularly in this paper

62
00:09:00,400 --> 00:09:08,080
they actually neglect d and just talk about a b and c in their formulation

63
00:09:09,280 --> 00:09:15,360
you should also consider that while we have simplified the equations to one dimensional

64
00:09:15,360 --> 00:09:22,800
input and one dimensional output signal you can consider this as you can also generalize

65
00:09:22,880 --> 00:09:27,680
all we are saying here to a multi-dimensional input and output

66
00:09:31,120 --> 00:09:42,400
if you want to see this whole equation from the viewpoint of deep learning you can see a b c and d

67
00:09:42,400 --> 00:09:48,000
as parameters that will be learned during the training by gradient descent and as i said for

68
00:09:48,080 --> 00:09:55,520
simplicity basically we can discard d for the rest of the conversation

69
00:09:58,720 --> 00:10:06,480
so this figure actually shows the relationship and shows the importance of the a matrix

70
00:10:07,040 --> 00:10:16,080
which actually somehow it preserved the history of the previous states basically

71
00:10:16,080 --> 00:10:29,680
and as i said ssm was first introduced in control models in 1960s and as i mentioned x has much

72
00:10:29,680 --> 00:10:38,640
higher dimension than u and y which is our input and output and whatever we are discussing here is on

73
00:10:39,040 --> 00:10:47,360
u y and x are continuous here and our function of time and we will see how we can discretize that

74
00:10:47,360 --> 00:10:53,360
because what we are dealing with sequence modeling in deep learning is that we are not talking about

75
00:10:53,440 --> 00:10:57,360
the continuous functions we have discrete functions

76
00:11:02,240 --> 00:11:12,160
yeah so basically you can use ssm for different kind of tasks like continuous

77
00:11:12,720 --> 00:11:18,800
continuous representation recurrence representation and convolutional representation

78
00:11:18,800 --> 00:11:22,400
and we will go through each of them in details

79
00:11:26,800 --> 00:11:37,120
now let's talk about how we can actually discretize the continuous ssm formula so that we can actually

80
00:11:38,640 --> 00:11:42,240
use that utilize that in deep learning as well

81
00:11:42,800 --> 00:11:50,000
we need to do some additional work after that but this is the very first step

82
00:11:52,960 --> 00:12:00,080
so let's say the problem now is that instead of having some continuous function u of t

83
00:12:00,080 --> 00:12:07,920
now we have u not u1 u2 and so we have some discrete inputs basically

84
00:12:07,920 --> 00:12:17,200
and the way that we do that we should discretize this sequence this continuous function with a

85
00:12:17,200 --> 00:12:25,520
step size delta and basically we can see delta which representing the resolution of the input

86
00:12:25,520 --> 00:12:33,760
for example in the images and u sub i can be viewed as a sample from function u of t

87
00:12:37,600 --> 00:12:47,520
so here basically we can see how we can derive this discretization so given the formulas I just

88
00:12:47,520 --> 00:12:56,480
showed you we can actually use Euler discretization so xt plus delta which is our step size is equal

89
00:12:56,480 --> 00:13:08,320
to xt plus delta times x prime which is the gradient of x so basically you can rewrite that by saying

90
00:13:08,400 --> 00:13:19,840
xt plus delta times a of xt plus b times u of t because x prime t here has this

91
00:13:21,040 --> 00:13:32,400
a particular formula so we just substitute this right hand side with x prime t to get this one

92
00:13:32,400 --> 00:13:41,920
so now the next step is just to simplify the previous equation to get i plus delta times

93
00:13:43,360 --> 00:13:53,280
a times xt plus delta b ut so and you can basically name this

94
00:13:53,680 --> 00:14:05,760
multiplier as a bar and name this one as b bar so this way you can see that starting from xt plus

95
00:14:05,760 --> 00:14:14,640
delta which was the step size we wanted to use to discretize the continuous u of t function

96
00:14:15,280 --> 00:14:26,160
we can actually derive this nice form to actually represent x in a discrete format

97
00:14:32,320 --> 00:14:43,680
so basically we can actually see this discretization as some kind of recurrent

98
00:14:43,680 --> 00:14:48,880
representation that we have in recurrent neural network

99
00:14:52,320 --> 00:15:00,800
so basically instead of what happened here is that because of this discretization instead

100
00:15:00,800 --> 00:15:08,800
of working with continuous function u of t and y of t now we work with u sub k and y sub k

101
00:15:08,800 --> 00:15:17,040
which indicates the k indicates the time steps or each of the tokens in our input sequence

102
00:15:19,360 --> 00:15:26,640
and as you can see now the equation becomes a recurrence on x sub k very similar to the

103
00:15:26,640 --> 00:15:34,800
recurrent neural network and if you want to use the same terminology that we have in RNN

104
00:15:34,800 --> 00:15:42,560
you can see x sub k as basically the hidden state which is carried out in RNN

105
00:15:44,720 --> 00:15:50,240
as I said the main thing about RNN is that there are more efficient and inference time because

106
00:15:50,240 --> 00:15:55,680
they don't have that quadratic multiplication you just need the previous state and as soon as you

107
00:15:55,680 --> 00:16:02,000
have the previous state you can get the output but the problem with them is that you cannot

108
00:16:02,000 --> 00:16:09,760
paralyze them and so in the training you cannot train a model which is sequential because all of

109
00:16:09,760 --> 00:16:16,480
your GPUs should have some kind of parallel computation to get the most out of the GPUs you

110
00:16:16,480 --> 00:16:25,760
have so that's really important thing so just to make clear that we understood this relationship

111
00:16:25,760 --> 00:16:34,880
between SSM and RNN is that if you write down the equation so x sub k is actually

112
00:16:36,880 --> 00:16:45,600
function of the previous state which is multiplied by a bar plus the input

113
00:16:45,920 --> 00:16:59,120
times the b bar and then the output is just a projection of the next state by the matrix c bar

114
00:17:00,400 --> 00:17:08,960
very similar to the recurrent neural network. Now let's give some mechanical example to better

115
00:17:08,960 --> 00:17:16,880
understand how these things actually works so consider mass attached to a wall by a spring

116
00:17:16,880 --> 00:17:27,920
with forward position y t so y t shows the position at time t and we have the force which is our input

117
00:17:27,920 --> 00:17:35,360
u of t applied to the mass and we also have other parameters like spring constant mass

118
00:17:35,920 --> 00:17:42,160
and friction that exists now we can relate everything with the following differential equation

119
00:17:42,880 --> 00:17:51,360
which is m times the acceleration which is the second gradient of second order gradient of

120
00:17:52,800 --> 00:18:02,320
position is equal to the force u of t minus b times velocity which is the gradient of y

121
00:18:02,320 --> 00:18:10,480
minus k y t so this is just a physical formula but the nice thing about that is that you can

122
00:18:10,480 --> 00:18:22,320
actually rewrite this physical formula using a b and c matrix we just saw in SSM and using these

123
00:18:23,280 --> 00:18:35,040
kind of parameters and if you put it if you just multiply these with the y with the u and x you

124
00:18:35,040 --> 00:18:44,800
get to the same equation and basically as you can see a relates all of these terms together so it

125
00:18:44,880 --> 00:18:56,080
has a very important role in actually holding the equation together and so basically here c shows the

126
00:18:56,080 --> 00:19:05,840
first dimension of the hidden state and the second dimension as I said is the velocity

127
00:19:05,840 --> 00:19:11,920
and a relates all of these terms together and the following is just the code of the

128
00:19:12,880 --> 00:19:19,200
of this whole equation using SSM so it's just a simple

129
00:19:22,240 --> 00:19:23,840
code you can see it actually

130
00:19:26,240 --> 00:19:32,880
on the author's page that I have put as a reference I don't want to spend too much on this

131
00:19:34,480 --> 00:19:41,120
and this is just an example of how the SSM actually models this so given the force u of k

132
00:19:41,200 --> 00:19:48,240
you can get the and the position y k you can actually see the

133
00:19:50,880 --> 00:19:58,640
see the place that the object will be based on the prediction done by SSM sorry the position

134
00:19:58,640 --> 00:20:07,120
is actually predicted by the SSM so in this example we just use one SSM with two hidden

135
00:20:07,120 --> 00:20:14,160
states but we can stack many SSM together in deep learning model

136
00:20:16,880 --> 00:20:22,960
as I said the problem with the current formulation we have is that

137
00:20:24,480 --> 00:20:33,280
the training time so basically the main issue here is that okay we have this nice

138
00:20:34,000 --> 00:20:43,040
formula so far but how can we actually train the model if it is sequential so we need to actually

139
00:20:43,040 --> 00:20:52,400
try to dig a little bit deeper here to actually transform RNN into some kind of representation

140
00:20:52,400 --> 00:21:04,640
that can be paralyzed and we use we actually have the opportunity to fully exploit the

141
00:21:04,640 --> 00:21:11,840
capacity of our GPUs one way to do that is actually we can turn the RNN representation

142
00:21:11,840 --> 00:21:20,880
into a CNN representation by actually unrolling it and why CNN representation is better than

143
00:21:20,880 --> 00:21:27,920
RNN representation is because you can actually do training of convolutional neural network

144
00:21:27,920 --> 00:21:36,480
in parallel so you can you don't have the training problem of RNN which is blocked and

145
00:21:39,120 --> 00:21:47,040
recurrent SSM can be actually written as a discrete convolution and let's see the equations

146
00:21:47,040 --> 00:22:00,000
by unrolling them so you start with x sub k a bar plus b bar and so basically this is the

147
00:22:01,440 --> 00:22:08,160
general formula we have and y sub k is c bar k c bar times x sub k

148
00:22:09,040 --> 00:22:19,680
so x naught is actually b bar times u naught and based on the equation on the left you just

149
00:22:20,240 --> 00:22:34,480
multiply a bar by x naught to get x sub one and if you continue like that you actually see that

150
00:22:34,480 --> 00:22:42,800
there is some pattern actually emerging here which actually you can see the impact of the matrix

151
00:22:44,160 --> 00:22:47,680
a bar which actually somehow it is actually

152
00:22:50,400 --> 00:22:57,440
propagating informations from previous states into new states and we will talk about this a

153
00:22:57,440 --> 00:23:02,160
little bit more when it comes to a hippo matrix and how we can actually make this

154
00:23:04,080 --> 00:23:13,360
recall even more efficient and y naught is actually c bar times b bar u naught

155
00:23:14,720 --> 00:23:24,560
and as you can see even for the output again a bar actually emerges so the matrix a bar has a very

156
00:23:24,560 --> 00:23:29,840
important factor in remembering and recalling the previous states

157
00:23:34,160 --> 00:23:42,960
now as we said for training SSM models we need to actually use better architecture

158
00:23:42,960 --> 00:23:50,400
to make sure that the training can be done in parallel and as we saw that we can actually

159
00:23:50,400 --> 00:24:04,000
unroll the whole equation to get to this final output which is as you can see it's actually

160
00:24:04,000 --> 00:24:16,080
kind of a kernel multiplication this is the kernel multiplication of k bar which is defined

161
00:24:16,080 --> 00:24:25,760
like this to the input input tokens y naught up to yk

162
00:24:27,760 --> 00:24:36,800
so k bar is called the SSM convolution kernel or filter and you can basically see k bar as a giant

163
00:24:36,800 --> 00:24:47,920
filter applied to the input u so the major problem here now is that now we have the problem that

164
00:24:47,920 --> 00:24:56,640
k bar is a giant filter and we need to make sure that the computation of k bar is actually

165
00:24:57,280 --> 00:25:06,800
done efficiently and so but still we have solved the problem of not being able to

166
00:25:08,560 --> 00:25:11,840
have the parallel computation in training time

167
00:25:17,200 --> 00:25:24,640
so basically the main takeaway from this is that now output can be computed without computing the

168
00:25:24,640 --> 00:25:33,920
internal state so as you recall in rnn we had the problem that the output if you wanted to

169
00:25:34,720 --> 00:25:41,920
actually calculate the output you need to have the internal state at time step t minus one

170
00:25:42,800 --> 00:25:48,560
so and for t minus one state internal state at t minus one you need to have

171
00:25:48,880 --> 00:25:58,080
internal state at time step t minus two and so on so you had to do this sequential computation

172
00:25:58,080 --> 00:26:02,080
but now we have addressed this problem using convolution

173
00:26:18,560 --> 00:26:20,020
you

174
00:26:48,560 --> 00:26:50,020
you

175
00:27:18,560 --> 00:27:20,020
you

176
00:27:48,880 --> 00:27:49,380
you

177
00:28:05,440 --> 00:28:09,680
but just to give you a little bit of background on convolution theorem

178
00:28:10,640 --> 00:28:17,440
just to give you a broad definition of how it worked it says that the Fourier transform

179
00:28:17,440 --> 00:28:22,400
of a convolution of two signals so here we are talking about continuous function and that's why

180
00:28:22,400 --> 00:28:28,800
you need to do this inverse first Fourier transform to actually get it back to the discrete one

181
00:28:28,800 --> 00:28:34,720
is the product of their Fourier transform so the Fourier transform of convolution of two

182
00:28:34,720 --> 00:28:43,920
signal is the product of their Fourier transform and the convolution of two continuous signal f

183
00:28:43,920 --> 00:28:47,520
and g is actually can be done like this

184
00:28:50,480 --> 00:28:56,560
so the Fourier transform of a product of two signal is the convolution of their

185
00:28:57,440 --> 00:29:02,560
Fourier transform so that that's the main thing that main trick that we use to actually make the

186
00:29:03,440 --> 00:29:05,280
computation more efficient

187
00:29:05,280 --> 00:29:18,080
and this is just the raw code that you can use to actually do the fast Fourier transform so you

188
00:29:20,160 --> 00:29:27,360
so basically no fft is a Boolean flag to choose between direct convolution and fast Fourier

189
00:29:27,360 --> 00:29:34,160
transform based convolution so when it is activated you first do the

190
00:29:36,480 --> 00:29:41,840
fast Fourier transform from the NumPy library

191
00:29:44,240 --> 00:29:54,080
and finally you do the reverse of the fast Fourier transform to get the final result

192
00:29:58,240 --> 00:30:07,520
so far we discussed about a kind of architecture that we can use

193
00:30:09,120 --> 00:30:15,680
to actually make the computation more efficient when it comes to dealing with

194
00:30:17,440 --> 00:30:24,720
long sequences long range sequences but the question is is this new kind of

195
00:30:25,280 --> 00:30:34,640
architecture is capable of addressing long range dependency and actually it has been shown that

196
00:30:34,640 --> 00:30:43,840
the basic SSM performs very poorly in practice why because of the formula we just saw before it

197
00:30:43,840 --> 00:30:49,600
contains lots of multiplication and that's the thing that happens in RNN because you have

198
00:30:50,480 --> 00:30:58,640
nested multiplication as you saw the matrix A bar

199
00:31:00,880 --> 00:31:07,680
is actually multiplied a lot by itself if let's say the sequence length is really

200
00:31:07,680 --> 00:31:13,600
high it's actually multiplied by the order of that by the order of the sequence length

201
00:31:14,560 --> 00:31:19,280
so you can get the problem of vanishing or exploding gradient and

202
00:31:21,520 --> 00:31:25,680
this actually can create the long-term dependency problem

203
00:31:27,600 --> 00:31:33,520
to address this issue we can use hippo matrix to actually solve this problem

204
00:31:36,400 --> 00:31:43,120
so basically I think now we have we have come to the definition of s4 so what is s4

205
00:31:43,920 --> 00:31:51,920
s4 is SSM status-based model plus hippo plus a structured matrices so what hippo does is

206
00:31:51,920 --> 00:31:59,920
very simple so it uses the special formulas for A and B matrices in SSM so this is A

207
00:31:59,920 --> 00:32:07,120
this is B in the formula that we saw for a status-based model and as we saw in the

208
00:32:07,120 --> 00:32:15,760
unrolling of the status-based model computation for the discrete space

209
00:32:16,640 --> 00:32:24,080
A is basically can be seen as the matrix that captured previous states so it actually decides

210
00:32:24,080 --> 00:32:30,560
how the information is passed through different states so that's a very important matrix basically

211
00:32:31,120 --> 00:32:46,240
and so what hippo does is that it specifies a specific class of matrices of A that's actually

212
00:32:46,960 --> 00:32:54,960
allow the state to be preserved so the model retains its ability to recall previous states

213
00:32:55,040 --> 00:33:02,720
and it's defined on the left hand side here so as you can see the hippo matrix is nothing

214
00:33:03,360 --> 00:33:05,920
other than a triangular matrix where the

215
00:33:10,000 --> 00:33:18,400
where this side of the matrix is zero and only this side of the matrix has values

216
00:33:19,280 --> 00:33:29,680
so this way it has been shown that hippo actually can better preserve the information of the previous

217
00:33:29,680 --> 00:33:41,840
steps and so there is one example here that's using this kind of matrix that basically preserved the

218
00:33:41,840 --> 00:33:51,280
previous state you can actually improve the performance on sequential MNIST classification

219
00:33:52,240 --> 00:34:02,400
from 60 percent to 98 percent so up to 38 percent improvement and hippo matrix basically tries to

220
00:34:02,400 --> 00:34:12,400
compress the past history state and basically approximately reconstruct history using this kind

221
00:34:12,400 --> 00:34:25,520
of formula so basically here we want to see an example of hippo in reconstructing a function

222
00:34:26,480 --> 00:34:34,800
so here the function input function is u which is shown in the black line and x here is the blue

223
00:34:34,800 --> 00:34:40,080
line which has higher dimension as we can see it has multiple line and the red line here is the

224
00:34:40,080 --> 00:34:49,840
output y t which is the reconstruction function so we want y t to be the same as x t and here

225
00:34:49,840 --> 00:34:59,840
after 10 000 steps we want to see how perfectly this ssm dynamic using hippo matrix is able to

226
00:35:01,360 --> 00:35:06,160
reconstruct the function so as you can see at each step the

227
00:35:08,640 --> 00:35:15,520
ssm is updated so starting from zero to 10 000 at each step it is updated based on the formulas

228
00:35:15,520 --> 00:35:24,160
for a and other things we saw and so the green line here is somehow some kind of

229
00:35:24,160 --> 00:35:34,480
weight matrix that tells the ssm that basically the more recent history has higher importance

230
00:35:34,480 --> 00:35:43,280
for me than the older one so that's the only thing that's the same and as we can see here specifically

231
00:35:46,480 --> 00:35:53,040
toward the end so at the 10 000 steps so this is the shot from the 10 000 step

232
00:35:55,600 --> 00:36:05,120
and the ssm is actually is able to perfectly reconstruct the function but as we go toward the

233
00:36:05,200 --> 00:36:16,000
beginning of the function the accuracy drops and here I should say that we have only used 64

234
00:36:16,720 --> 00:36:25,200
hidden states here for our ssm which is much lower than 10 000 so that's really remarkable

235
00:36:25,200 --> 00:36:33,360
that the model has this amount of accuracy and basically this test can be used for online

236
00:36:34,240 --> 00:36:45,440
function reconstruction and you can see more information here on this but the question is

237
00:36:45,440 --> 00:36:53,920
how hippo matrix keeps track of history so as I said it produces a hidden states with particular

238
00:36:53,920 --> 00:37:01,840
definition for matrix A that actually memorizes the history and basically it is actually keeping

239
00:37:01,840 --> 00:37:09,600
track of the coefficient of legend polynomial and these coefficients actually let it approximate

240
00:37:09,600 --> 00:37:19,040
all the previous history and legend polynomial are a system of complete and orthogonal polynomials

241
00:37:19,040 --> 00:37:26,160
and here on the right which is from the wikipedia wikipedia you can see the first six legend polynomial

242
00:37:28,000 --> 00:37:33,440
and here is another example so the red line is our

243
00:37:38,320 --> 00:37:48,080
input and this is again the same reconstruction that we want to actually keep track of and these

244
00:37:48,080 --> 00:37:57,120
blue lines are actually our legend coefficient and hippo matrix actually update these coefficient

245
00:37:57,120 --> 00:38:06,000
on each step and if you actually sum up these coefficient you actually can

246
00:38:09,040 --> 00:38:17,040
reconstruct these if you sum up these blue lines you actually can get an approximation of the red

247
00:38:18,560 --> 00:38:22,640
which is the reconstruction of the history

248
00:38:26,480 --> 00:38:35,840
so ssm neural network so all of the the things that we said is actually we can use it to model a

249
00:38:35,840 --> 00:38:44,720
neural network we now have all the ingredients basically needed to build a basic ssm so we assume

250
00:38:44,720 --> 00:38:52,080
that we are going to learn the b matrix c matrix step size delta and the scalar d

251
00:38:53,440 --> 00:39:02,080
and the hippo matrix is used for the transition a which is what we saw in the definition

252
00:39:03,040 --> 00:39:09,040
and the code on the right is basically a pytorch implementation you can see that

253
00:39:10,000 --> 00:39:19,440
um it first defines the um parameters it first defines the shape of these matrices and then

254
00:39:22,320 --> 00:39:33,200
these are uh first discretize um and then based on the step given and then we do the

255
00:39:33,280 --> 00:39:41,040
uh convolution uh to get the final output

256
00:39:48,240 --> 00:39:55,760
and here as you can see we can also add uh dropouts and layer norm i'm not going to

257
00:39:55,760 --> 00:39:58,320
cover this the code is available so

258
00:40:02,800 --> 00:40:09,840
due to the time limit and here is um the emnist experiment i was saying so here

259
00:40:09,840 --> 00:40:18,960
given the initial few pixels which is shown in white we use ssm to actually predict the

260
00:40:18,960 --> 00:40:25,360
remaining pixels so the green lines is actually showing the ground truth and the red line is

261
00:40:25,360 --> 00:40:34,880
the prediction by ssm and as you can see it has i'm sorry the red line is actually the prediction

262
00:40:34,880 --> 00:40:42,240
by s4 and as you can see the red line and green line actually has a good match

263
00:40:42,400 --> 00:40:51,040
but uh again there are some limitations with

264
00:40:52,160 --> 00:41:00,720
ssm model s4 models particularly they have not performed to the same performance that we have

265
00:41:00,720 --> 00:41:08,880
seen in uh transformers and the key weakness is that their inability to do content based reasoning

266
00:41:08,880 --> 00:41:18,000
so they actually given this sequence it actually the model is incapable of uh

267
00:41:18,000 --> 00:41:25,920
understanding the content and the relationship between the tokens because uh because as you saw

268
00:41:25,920 --> 00:41:32,640
the matrices a b and c are time invariant it doesn't change throughout the time it's constant

269
00:41:33,200 --> 00:41:38,400
and mamba is actually tries to address this weakness specifically

270
00:41:41,760 --> 00:41:48,320
um so let's talk about mamba mamba has several improvement in comparison to previous models

271
00:41:48,320 --> 00:41:55,040
it introduces discrete modality allowing model to selectively propagate or forget information

272
00:41:55,040 --> 00:42:02,960
along the sequence length so it's basically selected when it comes to propagating information

273
00:42:04,960 --> 00:42:11,040
and it also introduces some hardware of their parallel algorithm which we will not cover here

274
00:42:11,040 --> 00:42:19,760
due to the time limit and it has a very simplified architecture that we will see by the end

275
00:42:19,920 --> 00:42:26,080
uh the good things about mamba is that it has a very much faster inference

276
00:42:26,800 --> 00:42:33,440
five times higher throughput compared with transformer it has a linear scaling which is

277
00:42:33,440 --> 00:42:42,640
what we are looking for and uh basically these features allow allow mamba to be used for larger

278
00:42:42,640 --> 00:42:51,440
sequence length up to million which is our final uh goal and it actually achieves some

279
00:42:51,440 --> 00:42:58,480
state-of-the-art performance across different modalities and it's basically comparable with

280
00:42:58,480 --> 00:43:11,440
transformer and we will see what's the main design changes here so basically mamba the key

281
00:43:11,440 --> 00:43:17,600
difference is that it has some selection mechanism so it has the ability to efficiently

282
00:43:17,600 --> 00:43:24,000
select data in an input dependent manner so what we saw in s4 design

283
00:43:26,800 --> 00:43:34,240
was that it was time invariant it didn't actually select a specific input or disregard

284
00:43:34,240 --> 00:43:45,040
specific token from the input so mamba introduces a simple selection mechanism that allows the

285
00:43:45,040 --> 00:43:53,440
model to filter out irrelevant information and remember relevant information so this is

286
00:43:54,240 --> 00:44:03,280
very very similar to lstm so as you recall in lstm we have something called gating mechanism

287
00:44:03,280 --> 00:44:12,560
that actually allows the model to modify forget or keep previous state information

288
00:44:13,440 --> 00:44:20,080
so that's the same thing so that's the main punchline of this work as we see these

289
00:44:21,760 --> 00:44:30,400
forgotten models like lstm are are actually coming back to life so this is something very

290
00:44:30,400 --> 00:44:37,360
interesting to see that these kinds of novelties that existed in previous generations of

291
00:44:38,400 --> 00:44:48,480
neural network architectures are actually have some resurgence in mamba and s4 models which was

292
00:44:48,480 --> 00:45:00,720
very interesting to myself another important thing is that mlp blocks of transformers in

293
00:45:00,720 --> 00:45:04,880
mamba is actually compressed into a single block which we will see

294
00:45:05,440 --> 00:45:15,440
so let's talk about the selective status space block that exists in mamba

295
00:45:16,720 --> 00:45:29,440
so basically here ht is our state so i think you can see here that the notation has changed

296
00:45:29,440 --> 00:45:39,200
previously we call these x but here we call input x and the output is still y here and a

297
00:45:39,920 --> 00:45:47,520
is the main matrix that's still here also keep track of the history here

298
00:45:49,920 --> 00:45:58,640
but one important thing is that in contrast to s4 we have some projection layer here

299
00:45:58,640 --> 00:46:10,080
that given the input x it actually projected and then b delta which is our step and c are actually

300
00:46:10,080 --> 00:46:18,720
a function of a projection of our input x so there are no longer time invariant there are actually

301
00:46:18,800 --> 00:46:25,760
time variants but a stays the same

302
00:46:31,520 --> 00:46:38,800
so let's talk about the selective state space motivation why we need this so the fundamental

303
00:46:38,800 --> 00:46:46,320
problem of sequence modeling is compressing the context into a smaller state that's the main

304
00:46:46,320 --> 00:46:53,760
thing because we want to remember and recall the context of previous tokens if let's say the

305
00:46:53,760 --> 00:47:02,640
sequence length is really large so what's the problem with transformer as i said the attention

306
00:47:02,640 --> 00:47:13,360
is very effective but at the same time very inefficient which is n squared and at the same time

307
00:47:13,920 --> 00:47:20,880
because of the autoregressive inference that exists in both attention and

308
00:47:23,120 --> 00:47:29,200
rnn you need to somehow explicitly store the entire context

309
00:47:29,600 --> 00:47:33,360
so

310
00:47:35,680 --> 00:47:44,880
so on the other hand recurrent model are as i said efficient because they have finite state

311
00:47:47,120 --> 00:47:54,400
but at the same time they are very limited by how well they can actually compress the context

312
00:47:54,400 --> 00:48:00,240
that's really important because the context here is

313
00:48:02,560 --> 00:48:12,400
the main thing that makes the model important performance improve so basically what we want is

314
00:48:12,400 --> 00:48:19,280
an efficient model that has small states at the same time the model should be effective

315
00:48:20,000 --> 00:48:26,400
in a way that the its state actually stores all the necessary information from the context

316
00:48:27,440 --> 00:48:32,880
i say all the necessary information because some of the information are useless so you

317
00:48:32,880 --> 00:48:40,080
can't discard some of the information you don't have to have infinite memory to memorize everything

318
00:48:40,080 --> 00:48:50,880
so we as a human also have limited memory so the main novelty of mamba is that achieves this

319
00:48:50,880 --> 00:49:01,120
balance by using selection so it actually selectively recall previous input tokens and as we saw in

320
00:49:01,120 --> 00:49:09,440
the previous slide the matrices b delta and c are function of the projection of input

321
00:49:09,520 --> 00:49:17,760
actually achieve this result so to give you a better understanding of why this is important

322
00:49:17,760 --> 00:49:25,680
let's consider two simple tasks one of them is copying so the model should basically here

323
00:49:26,240 --> 00:49:33,680
given the input tokens which are different colors and there are some white colors in the

324
00:49:34,480 --> 00:49:40,800
middle so the model in the first task should actually shift the input token by some amount

325
00:49:44,080 --> 00:49:51,600
here this can be easily done by s4 but if you tweak this a problem to something else

326
00:49:52,320 --> 00:50:01,360
which is selective copying in selective copying you actually add these irrelevant input token

327
00:50:01,360 --> 00:50:09,920
which is shown by white boxes here so you basically these are just some noise

328
00:50:10,960 --> 00:50:17,760
and the model should be able to actually preserve the same sequence

329
00:50:18,960 --> 00:50:27,680
of input for colored token in the output so if the input is blue orange red green the output should

330
00:50:27,680 --> 00:50:35,840
be blue orange red green and discard all these white tokens white boxes basically

331
00:50:37,040 --> 00:50:42,240
and one of the things that we notice and we will see the graphs for this specific

332
00:50:42,240 --> 00:50:50,240
example is that s4 model all of them fails but mamba has a very astonishing performance

333
00:50:50,880 --> 00:50:57,920
achieving almost 100 percent accuracy and that's mainly because of the selection mechanism we will

334
00:50:57,920 --> 00:51:07,200
also dive into the algorithm of mamba and see how it's achieved so mamba ignores irrelevant input

335
00:51:07,840 --> 00:51:16,080
and because of its selective copying but s4 actually fails because it does it's not content

336
00:51:16,080 --> 00:51:26,400
aber as you saw the a b c and the defining the s4 is actually not able to

337
00:51:29,520 --> 00:51:36,560
change throughout the time it's not actually function of input but that's what i'm saying

338
00:51:36,960 --> 00:51:46,400
another task is also induction head that the model actually requires to recall

339
00:51:47,120 --> 00:51:53,600
information from previous input to build the next input so here for example if so here it

340
00:51:53,600 --> 00:52:02,000
should understand that after each black colored box there should come some blue color box

341
00:52:02,000 --> 00:52:14,640
and we still see even in this particular example if for example in the test set you

342
00:52:14,640 --> 00:52:22,640
actually put some noise white noises basically white box noises the model is incapable of

343
00:52:22,640 --> 00:52:25,520
understanding that given

344
00:52:31,920 --> 00:52:39,360
basically given this particular input the next thing should be blue box it's incapable of for

345
00:52:39,360 --> 00:52:50,880
s4 but mamba is actually able to achieve and as i said the main difference between mamba and s4

346
00:52:50,880 --> 00:52:59,360
is that mamba relaxes the idea of time transition between input sequences or basically independent

347
00:52:59,360 --> 00:53:01,840
it is not in mamba

348
00:53:08,720 --> 00:53:18,640
okay now we dive into the difference between algorithm of s4 and mamba basically

349
00:53:19,600 --> 00:53:28,400
so as you can see let's start with the s4 here we have x which is in the shape of bite size

350
00:53:29,040 --> 00:53:30,880
sequence length and token dimension

351
00:53:33,440 --> 00:53:38,640
and y is actually have the same dimension as the input x

352
00:53:39,600 --> 00:53:49,360
a has the dimension d which is the token dimension or embedding basically we call this

353
00:53:49,360 --> 00:53:58,560
embedding in large language model and n is basically the dimension of our hidden estate here

354
00:53:59,520 --> 00:54:09,120
and as i say a captures a history and as you can see b c and also have the same

355
00:54:10,640 --> 00:54:19,280
dimensions as a and delta has a dimension of d and we discretize delta a and b to get a bar

356
00:54:19,280 --> 00:54:28,640
and b bar and then we can get our y using the ssm architecture so it is time invariant so

357
00:54:29,520 --> 00:54:34,800
the a b c and delta doesn't change over the time it's the same it stays the same

358
00:54:35,760 --> 00:54:45,200
but for selection s6 is actually things are completely different not completely basically

359
00:54:45,200 --> 00:54:53,040
the input output and the a matrix is the same but the matrices b c and delta are not

360
00:54:54,480 --> 00:55:00,800
constant anymore they are actually a function of x as you can see

361
00:55:03,680 --> 00:55:11,920
that's why they have an additional dimension l here which is dependent of the

362
00:55:12,160 --> 00:55:26,720
time step basically in the input sequence and so basically as i said

363
00:55:29,360 --> 00:55:33,280
this allows mamba to be time varying

364
00:55:33,360 --> 00:55:46,880
and yeah as as you can see the value of delta b and c can actually changes based on the input given

365
00:55:46,880 --> 00:55:58,640
input and as i said the punchline of my presentation is that mamba moves s4 a little bit closer to

366
00:55:58,720 --> 00:56:06,000
lstm by introducing time dependent transition from time step to time step in processing input

367
00:56:06,000 --> 00:56:15,120
sequence but at the same time it still preserves the quality of s4 in time efficiency

368
00:56:18,720 --> 00:56:25,600
and let's see the mamba architecture so here you can see ssm which is

369
00:56:26,400 --> 00:56:36,080
a selective space model that we shown is actually inside here in the mamba and you have the

370
00:56:36,080 --> 00:56:43,120
projection layer which is followed by the convolution and then you have this gating

371
00:56:43,280 --> 00:56:48,640
in the between very similar to the lstm

372
00:56:54,080 --> 00:57:00,960
and let's talk about a little bit about the empirical evaluation so we talked about the

373
00:57:00,960 --> 00:57:08,320
selective copying task which is which the input sequence contains some

374
00:57:09,280 --> 00:57:17,360
white noises the model should be able to discard those and you can see that mamba

375
00:57:19,440 --> 00:57:27,920
actually achieves the highest accuracy in this selective copying task here h3 is actually a kind

376
00:57:27,920 --> 00:57:38,160
of s4 design another task which is induction heads that it should understand that for example

377
00:57:38,240 --> 00:57:43,440
after black token there is a blue token that we discussed and you can see that mamba

378
00:57:45,120 --> 00:57:51,440
no matter what the test sequence length is achieves the highest performance and you can

379
00:57:51,440 --> 00:58:00,480
see that all the other s4 as the test length sequence increases and they fail and this is

380
00:58:00,480 --> 00:58:12,800
for downstream evaluation dna you can see that mamba actually achieves the highest scores

381
00:58:12,800 --> 00:58:22,560
on average accuracy scores on average and that's the end of my talk and i hope you enjoyed it

382
00:58:22,560 --> 00:58:33,920
um can definitely contact me if there is any other questions and thank you have a good day

