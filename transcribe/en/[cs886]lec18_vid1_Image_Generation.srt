1
00:00:00,000 --> 00:00:07,120
Hi, this is Max. Today we will be talking about image generation.

2
00:00:07,120 --> 00:00:16,080
So first, we will go through the task definition and the paradigm method of image generation.

3
00:00:16,080 --> 00:00:24,240
And then we will talk about six pivotal papers, which includes latent diffusion model, image

4
00:00:24,240 --> 00:00:32,400
gen, dot e2, party, pixel alpha, and stable diffusion XL turbo. So let's get started.

5
00:00:33,600 --> 00:00:40,880
So what was image generation? In the context of year 2015, it was all about unconditional image

6
00:00:40,880 --> 00:00:49,120
generation, which as you can see here, those generated image of human faces are uncontrollable.

7
00:00:49,760 --> 00:00:58,880
So there's lack of contributing. And nowadays, image generation is often referred to text to

8
00:00:58,880 --> 00:01:08,960
image synthesis, where you can see here, we have a text. And then with the text, we can

9
00:01:08,960 --> 00:01:16,160
synthesize the image that the image will be aligned to the text. So how is it done?

10
00:01:19,920 --> 00:01:25,600
So for state of the art image generation methods, they actually follow a similar paradigm.

11
00:01:26,800 --> 00:01:32,960
In particular, they will have three major components, including a text encoder,

12
00:01:32,960 --> 00:01:41,200
a generation model, and a latency decoder. In detail, you will first have a text form.

13
00:01:42,160 --> 00:01:46,480
By fitting the text form into the text encoder, you get the text vector.

14
00:01:48,880 --> 00:01:54,240
And then with the text vector and the input latency, you fit them into the generation model,

15
00:01:55,200 --> 00:02:01,280
which could be a diffusion model or other kind of models. And it will produce an intermediate

16
00:02:01,280 --> 00:02:10,000
latent, which can be visible or non visible by human. Then you fit this intermediate latent

17
00:02:10,000 --> 00:02:17,120
into a latent decoder. Then you will be able to get back a synthesized image that aligned with the

18
00:02:17,120 --> 00:02:25,280
text form. So the first paper we're going to discuss is the latent diffusion model,

19
00:02:26,080 --> 00:02:33,760
which is also stable diffusion. What it does is that it basically denies in the latent space

20
00:02:33,760 --> 00:02:40,880
instead of the data space compared to the traditional diffusion model. So you can see here,

21
00:02:40,880 --> 00:02:48,160
it has a VAE autoencoder structure that it contains an encoder and decoder. And during the

22
00:02:48,160 --> 00:02:54,400
training process, the latent was compressed. But how compressed is it?

23
00:02:59,840 --> 00:03:04,080
The down-sampling factor 8 on the latents is actually the best configuration,

24
00:03:04,800 --> 00:03:10,640
such that the latents will be 64 by 64. And in the study, they found that

25
00:03:10,640 --> 00:03:21,040
pixel-based LDM requires substantially larger training times, while too much perceptual compression,

26
00:03:21,040 --> 00:03:33,840
such as 32, limits the overall sample quality. Then you will see that the latents now is 64 by 64.

27
00:03:34,480 --> 00:03:41,520
How about the domain-specific encoder? So during the training process, you have this

28
00:03:42,560 --> 00:03:49,920
domain-specific encoder. During the training, it goes through the cross-attention

29
00:03:51,920 --> 00:03:57,520
to control how the image is going to be generated. After the training,

30
00:03:57,520 --> 00:04:05,440
it becomes like this at the inference process. You have a domain-specific encoder,

31
00:04:05,440 --> 00:04:11,600
which is the text encoder. And then you have the generation model. Then finally, you have the decoder,

32
00:04:13,920 --> 00:04:21,440
such that this is following the paradigm that we mentioned at first. So with the text,

33
00:04:22,000 --> 00:04:28,160
you fit the text into the model, and through the cross-attention, it was able to control the unit

34
00:04:28,160 --> 00:04:37,200
to denoise from a Gaussian noise, so that it was able to produce the intermediate latents here,

35
00:04:38,240 --> 00:04:43,360
and finally pass through the decoder, such that it was able to reconstruct the image

36
00:04:43,600 --> 00:04:54,880
that aligns with the text. So what's the advantage of LDM? Obviously, it provides fast training,

37
00:04:54,880 --> 00:05:00,080
right? Because it starts from a lower dimension. And the distribution of the latents and bedding

38
00:05:00,080 --> 00:05:07,040
is closer to normal distribution. And another great advantage is that you can have tailored auto

39
00:05:07,040 --> 00:05:16,480
encoders, because it can be applied to not just text, but also graphs, 3D data, etc.,

40
00:05:16,480 --> 00:05:24,720
depends on the domain-specific encoder. So there are three core components in the LDM,

41
00:05:25,840 --> 00:05:31,680
which we just covered. It's the variational auto encoder to encode and decode the latents.

42
00:05:32,400 --> 00:05:40,720
Then we have a domain-specific encoder, which in the LDM literature, they use a bird tokenizer

43
00:05:40,720 --> 00:05:47,600
or text encoder. And then they have a unit for the diffusion models, which has integrated the

44
00:05:47,600 --> 00:05:55,440
rest block, and also the basic transformer block. Let's talk about the unit in the stable

45
00:05:55,440 --> 00:06:01,520
diffusion, I mean in LDM. So they basically have a rest net, and then they also have a

46
00:06:01,520 --> 00:06:07,760
attention block with the time stack embedding and also with the prompt embedding. So the time

47
00:06:07,760 --> 00:06:14,240
embedding is used to support the denoising process, and the prompt embedding is used to control the

48
00:06:14,240 --> 00:06:21,760
generation process. So how is it done? In the basic transformer block, they have to cross

49
00:06:21,840 --> 00:06:28,480
attention module. So it goes from self-attention, cross-attention, and then speed forward.

50
00:06:29,280 --> 00:06:35,120
So the self-attention was used to compute the spatial relation, while the cross-attention

51
00:06:35,920 --> 00:06:38,560
allows the text control on image generation.

52
00:06:42,000 --> 00:06:49,040
Then what is stable diffusion? Stable diffusion is actually a LDM trained by stability AI

53
00:06:49,040 --> 00:06:59,840
for text-to-image purpose. They trained on a 256-800 GPUs on AWS for a total of 150K GPU hours.

54
00:07:00,560 --> 00:07:08,480
They spent almost 600K USD on it. And stable diffusion is trained on a relatively huge dataset

55
00:07:08,480 --> 00:07:14,160
called Lion 5B, which contains 5B clip filter image text pair.

56
00:07:14,880 --> 00:07:21,440
So it also, for the domain-specific encoder, they also choose to use the clip text encoder

57
00:07:21,440 --> 00:07:28,720
instead of the breadth tokenizer encoder in LDM literature. We will talk about why in the later

58
00:07:28,720 --> 00:07:38,320
sections. For stable diffusion 2 and XL, so in stable diffusion 2, they retrained

59
00:07:39,200 --> 00:07:47,120
stable diffusion from scratch on a filtered Lion 5B data. And they employed a new text encoder,

60
00:07:47,120 --> 00:07:56,400
which they moved from clip to open clip. So then for SDXL, they retrained the auto encoder on a

61
00:07:56,400 --> 00:08:05,200
larger batch size. And also they have a larger 3.5 times larger unit than the original one,

62
00:08:05,200 --> 00:08:09,280
such that it was able to produce a larger resolution of output.

63
00:08:12,160 --> 00:08:22,160
Then let's move to the next paper. Now let's talk about Image Gen. If you look at the Image Gen

64
00:08:22,160 --> 00:08:31,840
architecture, you will see there is also following the paradigm that I first mentioned. You see there

65
00:08:32,000 --> 00:08:41,280
is a text encoder, a generation model, and also a decoder. But this time, it was instead of the

66
00:08:41,280 --> 00:08:48,400
latent model, it's using a cascaded model, which we will talk about in the following slides.

67
00:08:49,920 --> 00:08:57,360
And then it also has the keyword called dynamic thresholding to improve the generated images.

68
00:08:57,760 --> 00:09:06,800
So what's so special about this paper? Image Gen does not involve latent space,

69
00:09:07,600 --> 00:09:14,480
and this is cascaded model. And then through dynamic thresholding, it produced high fidelity and

70
00:09:14,480 --> 00:09:21,680
photorealism outputs. And then another important insight brought by this paper is that they find

71
00:09:21,680 --> 00:09:30,240
that using a large text encoder improves the performance of the Image Generation.

72
00:09:31,040 --> 00:09:37,600
Finally, they present drawbench, which is a challenging set of problems for evaluation.

73
00:09:41,200 --> 00:09:47,680
For cascaded diffusion models, it's actually a paradigm that using pipelines of diffusion

74
00:09:47,680 --> 00:09:55,040
models that can generate images of increasing resolution. In detail, they use the noise

75
00:09:55,040 --> 00:10:01,040
conditioning augmentation, which is a technique that they find critical towards achieving

76
00:10:01,760 --> 00:10:11,280
high sample fidelity. So you can see having this casc conditional bringing it into the

77
00:10:11,280 --> 00:10:18,560
later diffusion models, it was able to perform super resolution while maintaining the sample

78
00:10:18,560 --> 00:10:27,360
fidelity of the image. In Image Gen, they also employed a similar technique, but they found

79
00:10:27,360 --> 00:10:32,640
that noise conditioning augmentation weakens the information from the low-waste models.

80
00:10:34,160 --> 00:10:40,800
Instead, they introduced the text conditioning in the super resolution diffusion models

81
00:10:40,800 --> 00:10:45,680
as the extra information, and they found that the performance is improved.

82
00:10:50,560 --> 00:10:56,000
And other important techniques introduced by Image Gen is about dynamic thresholding.

83
00:10:57,040 --> 00:11:02,880
So what it basically does is that it adjusts the pixel value of the samples at its sampling step

84
00:11:03,600 --> 00:11:07,920
to be within a dynamic range compute over the statistics of the current samples,

85
00:11:08,720 --> 00:11:16,880
such that with dynamic thresholding, you can see the image having more photobelastic and

86
00:11:16,880 --> 00:11:25,360
having higher fidelity. So let's revisit classifier-free guidance. Instead of joining

87
00:11:25,360 --> 00:11:32,160
training an additional classifier, get an implicit classifier by joining training a conditional

88
00:11:32,240 --> 00:11:38,080
and unconditional diffusion model. This is like a very centric combination of the conditional

89
00:11:38,080 --> 00:11:45,040
and unconditional score function, which they used to trade off for sample quality and sample diversity.

90
00:11:46,320 --> 00:11:52,800
So basically, classifier-free guidance is a better method on diffusion models.

91
00:11:52,960 --> 00:12:03,520
So at each sampling step, the output is clipped by the same bounds of the training data.

92
00:12:04,480 --> 00:12:11,520
In the static thresholding, which is the standard practice, they clip it back to negative one-to-one.

93
00:12:12,320 --> 00:12:18,400
This makes the image look saturated. But in the dynamic thresholding proposed by Image Gen,

94
00:12:19,360 --> 00:12:26,080
it was thresholded to first a certain percentile absolute pixel value in the output at each

95
00:12:26,080 --> 00:12:32,080
sampling step, and then divided by the as, which is basically normalizing it back to

96
00:12:32,800 --> 00:12:39,120
negative one-to-one. These pre-framed pixels from saturation and the image would look more realistic.

97
00:12:39,440 --> 00:12:49,440
So another important insight brought by Image Gen is that they find that powerful text encoders

98
00:12:49,440 --> 00:12:56,080
are unnecessary, and having a better text encoder improved the performance of image generation

99
00:12:56,080 --> 00:13:05,120
significantly compared to increasing the unit size. As you can see here, Image Gen was using

100
00:13:05,600 --> 00:13:13,040
T5-XXL, which is like a very large text encoder, and they achieved the best clip score

101
00:13:13,920 --> 00:13:21,280
while achieving the lowest FID score, which I will explain what is FID and clip score.

102
00:13:21,280 --> 00:13:28,720
So just to revisit those basic Image Generation metrics, FID, also called

103
00:13:28,720 --> 00:13:35,120
fresh heights in the inception distance, it measures the image fatality of a model.

104
00:13:36,080 --> 00:13:40,960
So what it does is that it computes some kind of distance between the real and

105
00:13:41,680 --> 00:13:48,400
generated data points distributions, and then the distance will be a range,

106
00:13:48,400 --> 00:13:53,280
it will be a value range from zero to infinity, such that

107
00:13:53,920 --> 00:14:02,400
such that a lot of samples is needed because you compare the distance

108
00:14:02,400 --> 00:14:07,600
between two distributions so that in order to make it accurate, you need a lot of samples.

109
00:14:09,600 --> 00:14:19,920
But I think, so in fact, more than 10K is enough to use, and also a very low

110
00:14:20,640 --> 00:14:26,880
distance actually might represent something because it's only measuring the image fatality,

111
00:14:26,880 --> 00:14:35,360
but it does not measure the diversity of the model. And then we have clip score, which

112
00:14:36,560 --> 00:14:43,600
let's talk about clip first. Clip is constructive language image pre-training. It's a pre-trained

113
00:14:43,600 --> 00:14:53,360
model, trained on 40, 400 million image pairs. And for clip score, it's just to measure the

114
00:14:53,360 --> 00:14:59,680
image text alignment using the knowledge from clip. So what it basically does is that

115
00:15:00,640 --> 00:15:09,280
clip has a text encoder, and clip also has an image encoder. So we employ the frozen text encoder

116
00:15:09,280 --> 00:15:15,760
and the image encoder, and then we fit the text image pair into the encoder respectively

117
00:15:15,760 --> 00:15:26,400
to get the text vector and the image vector such that by calculating the cosine similarity between

118
00:15:26,400 --> 00:15:36,800
the text and image vector, a higher value indicates that the text is actually more relevant to the

119
00:15:36,800 --> 00:15:43,200
image. And while the lower score indicates that the text might not be that relevant to the image.

120
00:15:45,200 --> 00:15:53,440
But in fact, clip score only like the normal range of clip score is like 0.25 to 0.35.

121
00:15:54,560 --> 00:16:01,200
So it's relatively hard to spot whether the text image text alignment is really doing well.

122
00:16:06,800 --> 00:16:21,680
Then we will talk about .e2. So you see here, .e2 actually leverage the clip to perform the image

123
00:16:21,680 --> 00:16:31,280
generation. More in detail, it used the clip latents to do it. So first, it learns a joint

124
00:16:31,280 --> 00:16:42,640
representation space for text and images. And then at the generation process, it also follows

125
00:16:42,640 --> 00:16:49,920
the paradigm of having an encoder and then a generation model and then a decoder.

126
00:16:51,840 --> 00:16:57,920
So in detail, the text vector, I mean the text was fed into the text encoder,

127
00:16:58,560 --> 00:17:06,640
such that the text vector was obtained. Then the text vector is passed into the diffusion model,

128
00:17:07,760 --> 00:17:13,440
which in this case is actually, instead of unit, is more like a transformer decoder structure.

129
00:17:14,880 --> 00:17:21,920
The diffusion model produced the clip image embeddings conditioned on the input caption.

130
00:17:22,480 --> 00:17:30,960
Then it produced an intermediate latent, which we call the image vector, such that

131
00:17:32,720 --> 00:17:39,440
it was passed to the decoder, which is a cascaded super resolution diffusion model

132
00:17:40,320 --> 00:17:46,320
to produce the image condition based on the clip image embedding and text.

133
00:17:46,560 --> 00:17:59,280
So what was the prior model that produced the clip image embedding condition on the input caption?

134
00:18:00,320 --> 00:18:04,400
In the literature, they actually tried two different approaches, which the first approach

135
00:18:04,400 --> 00:18:10,800
is autoregressive. They basically quantized the image embedding into a sequence of discrete codes

136
00:18:11,360 --> 00:18:18,560
and predict them autoregressively, but they find that the diffusion prior actually performed

137
00:18:18,560 --> 00:18:24,240
much better, such that they modeled a continuous image embedding by diffusion models,

138
00:18:25,200 --> 00:18:31,280
which the diffusion model is a decoder only transformer conditioned on the caption.

139
00:18:31,680 --> 00:18:42,720
So about the decoder, the decoder is also conditioned on the clip image embedding and text,

140
00:18:43,520 --> 00:18:47,440
where the clip image embedding captures the high-level semantic meaning,

141
00:18:48,480 --> 00:18:54,720
and the latents in the decoder model take care of the low-level details,

142
00:18:55,600 --> 00:19:04,320
which shares a very similar idea from ImageGen, because their decoder is basically also a

143
00:19:04,320 --> 00:19:10,960
cascaded super resolution diffusion model, which contains one base model to produce the image in

144
00:19:10,960 --> 00:19:19,520
64 by 64, and then two super resolution models to upscale it up to 1000 by 1000.

145
00:19:20,080 --> 00:19:27,040
So this next paper introduces PARTY, which stands for Pathways Autoregressive Text to Image Model.

146
00:19:27,040 --> 00:19:32,640
Unlike the previous models that are mentioned, PARTY is not a diffusion model. PARTY represents a

147
00:19:32,640 --> 00:19:37,280
research effort by Google done in parallel with ImageGen to explore two families of

148
00:19:37,280 --> 00:19:42,480
generative models, which are autoregressive and diffusion models. The autoregressive approach

149
00:19:42,480 --> 00:19:47,440
by PARTY hinges on the principle of using previous predictions as input to generate

150
00:19:47,520 --> 00:19:52,640
subsequent ones, which is a method synonymous with the likes of language translation in NLP.

151
00:19:53,600 --> 00:19:58,560
Specifically in PARTY, it treats text to image generation as a sequence-to-sequence modeling

152
00:19:58,560 --> 00:20:04,240
problem, where image tokens are predicted in the series that corresponds to the sequence of

153
00:20:04,240 --> 00:20:09,600
descriptive text tokens. The motivation for this research is to demonstrate the ability of

154
00:20:09,600 --> 00:20:15,280
autoregressive models to generate high fidelity, photo-realistic images that reflect the content

155
00:20:15,280 --> 00:20:21,920
density and complexity proposed by the textual description. Here are the main contributions

156
00:20:21,920 --> 00:20:26,480
of this paper. First, it highlights a model state-of-the-art performance in autoregressive

157
00:20:26,480 --> 00:20:32,080
modeling for text to image generation, setting a new benchmark in the field. Second, it marks the

158
00:20:32,080 --> 00:20:36,800
consistency of improvements that PARTY has demonstrated across four different scales

159
00:20:36,800 --> 00:20:44,080
of its architecture, from 350 million to 20 billion parameters, emphasizing the scalability of the

160
00:20:44,080 --> 00:20:50,560
system. Third, this research introduces the PARTY PROMPS or P2 benchmark, a development that provides

161
00:20:50,560 --> 00:20:56,400
a comprehensive framework for evaluating text to image models. Last, the paper also acknowledges

162
00:20:56,400 --> 00:21:02,240
the importance of transparency in research by identifying the limitations of the PARTY model

163
00:21:02,240 --> 00:21:10,560
and detailing these alongside observed error types. So PARTY is presented at the two-stage model,

164
00:21:10,560 --> 00:21:16,560
which has an image tokenizer, detokenizer, alongside an autoregressive model. To desec

165
00:21:16,560 --> 00:21:21,840
its structure, let's revisit the text to image schematic. A text to image model begins with the

166
00:21:21,840 --> 00:21:28,000
text prompt, which is PARTY 2.8 text encoder that translates the input into a series of discrete

167
00:21:28,000 --> 00:21:34,000
tokens. These tokens are then fed into the generation model, which interacts with an

168
00:21:34,000 --> 00:21:40,000
input latent space. Then a model generates the intermediate latents. Finally, a latent decoder

169
00:21:40,000 --> 00:21:44,720
interprets these intermediate representations, constructing the output as a detailed for

170
00:21:44,720 --> 00:21:51,840
realistic image that corresponds to the original text prompt. So in the context of the training

171
00:21:51,840 --> 00:21:57,760
processing PARTY, both images and text are converted into tokens. First, an image tokenizer

172
00:21:57,760 --> 00:22:02,640
breaks down an input image into the corresponding image tokens, which are essentially the visual

173
00:22:02,640 --> 00:22:07,600
syllables of the picture. These tokens are then processed by an autoregressive model,

174
00:22:07,600 --> 00:22:12,880
particularly the decoder part of a transformer network. The output is a series of new image

175
00:22:12,880 --> 00:22:19,760
tokens. Finally, an image detokenizer reassembles these tokens into coherent generated image that

176
00:22:19,760 --> 00:22:28,000
visually represents the initial text prompt. So here's a schematic of the structure as

177
00:22:28,000 --> 00:22:33,840
presented in the paper. It follows a two-stage structure. The image tokenizer and detokenizer

178
00:22:33,840 --> 00:22:38,960
stage in green and the autoregressive stage in orange and blue for predicting the next image

179
00:22:38,960 --> 00:22:44,080
token, giving text tokens and the previous tokens. Each of these components is essentially a

180
00:22:44,080 --> 00:22:48,880
transformer. So PARTY is effectively a transformer-based sequence-to-sequence model.

181
00:22:48,880 --> 00:22:52,800
Let's first talk about the image tokenizer and detokenizer stage in green.

182
00:22:56,240 --> 00:23:00,880
This first stage involves a tokenizer that converts an image into a sequence of discrete

183
00:23:00,880 --> 00:23:05,680
visual tokens during training, and training a detokenizer that reconstructs the image from

184
00:23:05,680 --> 00:23:11,200
tokens during inference. However, there's a significant challenge, which is how to linearize

185
00:23:11,200 --> 00:23:17,840
2D images into 1D sequences of patch representations. The problem lies in the complexity of images,

186
00:23:17,840 --> 00:23:24,320
where even a standard 256x256 pixel image with three RGB color channels results in

187
00:23:24,320 --> 00:23:30,240
196,000 rasterized values. Previous methods tackle this by learning quantized representations

188
00:23:30,320 --> 00:23:36,160
of image patches. However, the PARTY model introduces a more refined solution. It uses a

189
00:23:36,160 --> 00:23:41,280
tokenizer to learn patch embeddings, and the map sees embeddings to a visual code book.

190
00:23:41,280 --> 00:23:45,760
Each entry in this code book corresponds to an indexable location in the latent space.

191
00:23:46,320 --> 00:23:51,440
Therefore, it reduces the complexity of the image data. This is achieved using the vision

192
00:23:51,440 --> 00:23:57,360
transformer or VIT structure combined with a vector quantized generative adversarial networks,

193
00:23:57,360 --> 00:24:03,680
or VQGaN, combined as known as a VIT VQGaN. This approach not only streamlines the image data

194
00:24:03,680 --> 00:24:08,160
into a manageable sequence, but also maintains the richness of the image's information.

195
00:24:08,720 --> 00:24:17,200
Let's now talk about VIT VQGaN in detail. To understand VIT VQGaN, let's start with VQVAE.

196
00:24:17,200 --> 00:24:22,400
A VAE typically learns a continuous latent representation, mapping input data into a

197
00:24:22,400 --> 00:24:27,920
continuous distribution. On the other hand, the VQVAE, which stands for vector quantized

198
00:24:27,920 --> 00:24:33,440
variational autoencoder, learns a discrete latent representation. This shift from continuous to

199
00:24:33,440 --> 00:24:39,120
discrete encoding is crucial because it can help address issues like the posterior claps in VAEs,

200
00:24:39,920 --> 00:24:44,240
where the model ignores the latent variables because the decoder is too capable.

201
00:24:44,240 --> 00:24:49,040
Discrete codes, as shown in the encoder part of the slide,

202
00:24:49,920 --> 00:24:53,680
represent the input image as a series of integers, which correspond to

203
00:24:53,680 --> 00:24:59,120
specific vectors in a predefined embedding space. The decoder then translates these discrete codes

204
00:24:59,120 --> 00:25:09,440
back into a reconstructed image. So this is a more detailed description of the VQVAE architecture.

205
00:25:09,440 --> 00:25:15,840
The encoder, represented by a CNN, transforms the input image into a set of latent variables,

206
00:25:15,840 --> 00:25:22,480
ZE for VF, X. These variables are then mapped to a discrete latent space using a code book

207
00:25:22,480 --> 00:25:29,040
that consists of a list of k vectors, each with dimension D. This code book acts as a lookup

208
00:25:29,040 --> 00:25:33,840
table, where each embedding vector EI is associated with an index from 1 to k.

209
00:25:38,560 --> 00:25:43,440
The mapping from the continuous latent space to the discrete code book is performed by a

210
00:25:43,440 --> 00:25:48,480
nearest neighbor search in the shared embedding space, which is the code book, and produces

211
00:25:48,480 --> 00:25:55,920
the discrete latent variables Z. The posterior distribution QZ given X is defined as one hot

212
00:25:55,920 --> 00:26:03,040
where it is equal to 1 if the embedding vector is the nearest neighbor of ZE of X and 0 if otherwise.

213
00:26:06,320 --> 00:26:12,480
The input to the decoder ZQ of X is then the corresponding embedding vector EK,

214
00:26:12,480 --> 00:26:15,840
where k is the index of the nearest neighbor embedding vector.

215
00:26:23,600 --> 00:26:28,160
The training involves copying gradients from the decoder input ZQ of X

216
00:26:28,160 --> 00:26:34,560
to the encoder output ZE of X. This allows for gradient flow despite the non-differentiability

217
00:26:34,560 --> 00:26:40,480
of the discrete latent space. This loss function here that is used to train the network consists

218
00:26:40,480 --> 00:26:45,600
of three parts. The first one is the reconstruction loss, which measures the difference between the

219
00:26:45,600 --> 00:26:51,200
original image and the reconstructed image. The second one is the code book loss, which pulls

220
00:26:51,200 --> 00:26:59,680
the code book vectors E towards the encoder outputs ZE of X, such that E is a better representation

221
00:26:59,680 --> 00:27:05,520
of the encoder outputs, with SG here denoting the stop gradient operation that prevents the

222
00:27:05,520 --> 00:27:13,040
gradients from flowing into the encoder outputs. The third one is the commitment loss, which penalizes

223
00:27:13,040 --> 00:27:19,120
the encoder outputs ZE of X for deviating from the chosen code book embeddings E,

224
00:27:19,760 --> 00:27:28,800
ensuring that the encoder commits to the code book assignment. Beta is a hyperparameter balancing

225
00:27:28,800 --> 00:27:34,240
term. This loss function collectively ensures that the encoder representations are close to the

226
00:27:34,240 --> 00:27:39,120
code book embeddings, while still allowing for efficient reconstruction. And it maintains the

227
00:27:39,120 --> 00:27:43,840
integrity of the code book by having embeddings that are representative of the encoder outputs.

228
00:27:49,680 --> 00:27:55,040
So next we'll look into VQGAN, which builds upon the VQVAE structure with several differences.

229
00:27:55,920 --> 00:28:02,240
In training, VQVAE uses the L2 loss, which is a standard reconstruction loss focusing on pixel

230
00:28:02,240 --> 00:28:09,520
wise accuracy. VQGAN, however, opts for perceptual loss, specifically the learned perceptual image

231
00:28:09,520 --> 00:28:16,400
patch similarity metric. Perceptual loss differs from the L2 loss by measuring the perceptual

232
00:28:16,400 --> 00:28:22,720
similarity between image patches. Additionally, VQGAN incorporates a discriminator, which is

233
00:28:22,720 --> 00:28:28,320
trained to differentiate between real and reconstructed image patches. For sampling,

234
00:28:28,320 --> 00:28:33,760
VQVAE learns a category called prior over the latent codes in the code book from the training

235
00:28:33,760 --> 00:28:40,720
images, often employing an autoregressive model like pixel cnn to model the distribution of the

236
00:28:40,720 --> 00:28:47,200
latent codes. VQGAN enhances this approach by predicting the next code book index using an

237
00:28:47,200 --> 00:28:53,600
autoregressive transformer model trained with latent codes in training images, likely improving

238
00:28:53,600 --> 00:28:58,320
the coherence and quality of the synthesized images by leveraging the transformer's ability to

239
00:28:58,320 --> 00:29:09,680
capture long range dependencies. Here's the schematic of the VQGAN. The first stage involves

240
00:29:09,680 --> 00:29:19,760
training the encoder e, the decoder g, and the discriminator d, using the vector quantize loss

241
00:29:19,760 --> 00:29:31,680
and the GAN loss. The losses are similar to VQVAE, with this addition of the GAN loss and

242
00:29:31,680 --> 00:29:42,320
modification from L2 to perceptual loss. In stage 2, VQGAN trains the transformer to

243
00:29:42,560 --> 00:29:47,360
learn the relationships between code book entries using the cross entropy loss.

244
00:29:50,000 --> 00:29:55,440
Next, VIT VQGAN further improves upon VQGAN with the following modifications.

245
00:29:56,560 --> 00:30:02,880
First, it replaces both the cnn encoder and decoder with VITs. Then it also introduced

246
00:30:02,880 --> 00:30:08,960
code book optimization to improve code book usage. So first, it uses factorized codes,

247
00:30:08,960 --> 00:30:13,840
which applies to linear projection from encoder output to a low dimension latent space

248
00:30:13,840 --> 00:30:20,000
for code index lookup. This decouples code embedding and lookup. Second, it applies L2

249
00:30:20,000 --> 00:30:26,400
normalization on the encoder output latent variables ZE of X and code book latent variables E.

250
00:30:27,040 --> 00:30:32,880
So the resulting Euclidean distance is effectively the cosine similarity of ZE of X and E.

251
00:30:33,120 --> 00:30:41,200
Finally, in addition, it also modified the loss functions to be a combination of several losses

252
00:30:41,200 --> 00:30:49,920
that are introduced in both VQVAE and VQGAN, including the perceptual loss and the Laplace loss.

253
00:30:52,560 --> 00:30:58,480
So this is the schematic of the VIT VQGAN structure as presented in their paper.

254
00:30:59,200 --> 00:31:04,000
The portion on the right here depicts a code book optimization part of VIT VQGAN

255
00:31:04,000 --> 00:31:08,640
that includes a linear projection from a high dimensional space to a low dimensional space.

256
00:31:13,120 --> 00:31:20,800
This is the overall schematic in the VIT VQGAN paper. It is similar to VQGAN, but only replacing

257
00:31:20,880 --> 00:31:24,480
the CNNs with transformer encoder and decoders.

258
00:31:27,440 --> 00:31:33,200
So now we're back to party stage one. This is the overall schematic of the image tokenizer and

259
00:31:33,200 --> 00:31:41,360
detokenizer training with VIT VQGAN. In party, it also uses a super resolution up sampler to

260
00:31:41,360 --> 00:31:47,760
upsample the output image to 1024 by 1024 from 256 by 256.

261
00:31:50,480 --> 00:31:54,800
So now moving on to party stage two, which involves the autoregressive model,

262
00:31:55,520 --> 00:31:58,640
and it's essentially an encoder-decoder transformer.

263
00:32:01,920 --> 00:32:07,280
The encoder is designed to process textual input and convert it into text tokens,

264
00:32:07,360 --> 00:32:13,520
utilizing a sentence piece model with a vocabulary size of 16,000 based on a sample text corpus.

265
00:32:14,240 --> 00:32:20,880
It has a maximum token length capacity of 128. Meanwhile, the decoder focuses on predicting

266
00:32:20,880 --> 00:32:27,280
the next image token in a sequence, taking as input the image tokens generated from stage

267
00:32:27,280 --> 00:32:33,440
one's image tokenizer. The output from the decoder is structured into a fixed lens of 1024 image

268
00:32:33,440 --> 00:32:41,120
tokens, which correspond to a grid of 32 by 32 tokens for a 256 by 256 image.

269
00:32:46,240 --> 00:32:53,680
So four size variants of party were trained with 350 million, 750 million, 3 billion, and 20 billion

270
00:32:53,680 --> 00:33:00,960
parameters. The encoder and decoder are based on standard transformers, and this configuration

271
00:33:00,960 --> 00:33:06,800
prefers larger decoder for generating image tokens so that decoder has more layers.

272
00:33:10,960 --> 00:33:15,520
So for further design considerations, the authors employed text encoder pre-training

273
00:33:15,520 --> 00:33:20,080
with the intention to warm start the model, but found that it very marginally helps.

274
00:33:20,800 --> 00:33:26,000
So it performs comparably to BERT on glue benchmark after pre-training, but it degrades

275
00:33:26,000 --> 00:33:31,840
after full encoder decoder training. They have listed this difference and unification

276
00:33:31,840 --> 00:33:36,960
of generic language understanding and visually grounded language understanding for future research.

277
00:33:41,040 --> 00:33:47,200
Here is the training loss graph with and without text encoder pre-training. As seen,

278
00:33:47,200 --> 00:33:57,040
the difference is small here. So for sampling, the authors apply classifier free guidance in

279
00:33:57,040 --> 00:34:02,640
the context of autoregressive models, which is used to encourage alignment between generic

280
00:34:02,640 --> 00:34:09,840
sample and text prompt. This is similar to the approach made in Make a Scene. They also use

281
00:34:09,840 --> 00:34:15,200
re-ranking to batch sample images per text prompt, which is similar to the approach used in Dali,

282
00:34:15,920 --> 00:34:21,280
and re-rank the outputs based on alignment score of image and text embedding of a

283
00:34:21,280 --> 00:34:28,400
contrastive captioner's model. So here's a summary of PARTY's training pipeline.

284
00:34:28,400 --> 00:34:35,040
Stage 1 focuses on training the VITB Cougar, which consists of a 32 million parameter encoder

285
00:34:35,040 --> 00:34:43,040
to be used at the tokenizer, and a significantly larger 599 million parameter decoder as the

286
00:34:43,040 --> 00:34:49,840
detokenizer. This stage also learns a set of 8192 image token classes, which are then used to

287
00:34:49,840 --> 00:34:57,440
create a codebook. In addition to learning this visual vocabulary, the discriminator and a super

288
00:34:57,440 --> 00:35:04,480
resolution upsampler are trained to enhance image quality. Once the encoder-decoder framework has

289
00:35:04,480 --> 00:35:12,080
been pre-trained, the detokenizer is fine-tuned during the second stage training while keeping

290
00:35:12,080 --> 00:35:17,200
the tokenizer and codebook fixed to ensure consistency in the learned representations.

291
00:35:18,640 --> 00:35:24,000
The losses during this phase are the same as those used in VITB Cougar.

292
00:35:28,400 --> 00:35:34,400
Moving on to stage 2, the training of the encoder-decoder autoregressive model. Here,

293
00:35:34,400 --> 00:35:40,640
the text encoder is pre-trained. Then, the encoder-decoder are trained together using

294
00:35:40,640 --> 00:35:47,200
Softmax cross-entropy loss, which ensures that the model learns to protect the next image token

295
00:35:47,200 --> 00:35:53,200
effectively, based on the sequence of tokens generated by the tokenizer in stage 1.

296
00:35:58,640 --> 00:36:05,120
During inference, a text prompt is passed through a text encoder, which converts the input into text

297
00:36:05,120 --> 00:36:11,680
tokens. These tokens are then used by an autoregressive transformer decoder that generates the corresponding

298
00:36:11,680 --> 00:36:16,560
image tokens, incorporating classifier-free guidance to enhance alignment between the

299
00:36:16,560 --> 00:36:23,680
generated image and prompt. The image tokens are then subsequently converted from token space to

300
00:36:23,680 --> 00:36:31,920
image space by the detokenization process using VITB Cougar's decoder. Following decokenization,

301
00:36:31,920 --> 00:36:38,960
the generated image may be upsampled to achieve higher resolution. To ensure diversity and select

302
00:36:38,960 --> 00:36:45,360
the best outcomes, the model generates 16 images per text prompt. These images are then subjected

303
00:36:45,360 --> 00:36:51,360
to re-ranking, based on alignment score of image and text omitting of the co-cam model.

304
00:36:54,560 --> 00:37:00,960
The datasets used for evaluations are the MS-Coco dataset and the Cocoa subset of the

305
00:37:00,960 --> 00:37:06,960
localized narrative dataset. The localized narrative dataset has much more detailed prompts as seen

306
00:37:07,760 --> 00:37:15,040
here. Then the authors also develop party prompts, which is a set of over 1600 prompts,

307
00:37:15,040 --> 00:37:21,040
which each prompt associated with a broad category from 12 categories, for example,

308
00:37:21,040 --> 00:37:26,880
animals, vehicles, world knowledge, and abstract, and a challenge dimension from 11 divisions.

309
00:37:27,840 --> 00:37:31,120
For example, basic, quantity, words, or symbols.

310
00:37:33,600 --> 00:37:38,880
Here are the FID score results on zero-shot and fine-tuned MS-Coco and localized narrative

311
00:37:39,520 --> 00:37:44,240
Cocoa datasets when compared with other state-of-the-art text-to-image models at the time,

312
00:37:44,240 --> 00:37:50,800
as well as the retrieval baseline. The retrieval baseline simply retrieves images from the training

313
00:37:50,880 --> 00:37:57,280
dataset based on an alignment score between the text prompt embedding and the image embeddings.

314
00:37:58,480 --> 00:38:06,800
As seen, party outperforms other state-of-the-art models in terms of both zero-shot and fine-tuned FID

315
00:38:06,800 --> 00:38:14,720
scores. Here are some results based on human evaluations on the MS-Coco dataset when compared

316
00:38:14,720 --> 00:38:23,120
with the XMCGAN model and the retrieval baseline, showing that party outperforms XMCGAN in terms of

317
00:38:23,120 --> 00:38:29,600
image realism and image text match, and outperforming the retrieval baseline based on image text match.

318
00:38:31,920 --> 00:38:37,280
On the new party prompts, results show that the larger 20 billion party model outperforms

319
00:38:38,240 --> 00:38:41,520
the 3 billion model as well as the retrieval baseline.

320
00:38:43,040 --> 00:38:50,240
The next paper is Pixar Alpha, fast training of diffusion transformer for photo-realistic text-to-image

321
00:38:50,240 --> 00:38:56,080
synthesis. This paper aims to harness the capabilities of the diffusion transformer

322
00:38:56,080 --> 00:39:00,000
to train a text-to-image model comparable to the state-of-the-art models.

323
00:39:00,880 --> 00:39:05,520
The method is motivated by designing a training strategy to independently optimize

324
00:39:05,520 --> 00:39:12,400
three critical components, pixel dependency, text-image alignment, and image aesthetic quality.

325
00:39:13,040 --> 00:39:18,880
Pixar Alpha also tries to support high-resolution image synthesis up to 1024 pixels,

326
00:39:19,440 --> 00:39:23,520
achieving this with notably reduced training costs and improved efficiency.

327
00:39:25,520 --> 00:39:30,720
The core contributions of the paper are as follows. Firstly, the model's training time

328
00:39:30,720 --> 00:39:37,440
is remarkably short, requiring only 10.8% of the time needed for a stable diffusion version 1.5,

329
00:39:37,440 --> 00:39:41,120
while also using less than 1.25% of its training data.

330
00:39:42,000 --> 00:39:47,840
Second, it introduces this three-stage training strategy which enhances the process by concentrating

331
00:39:47,840 --> 00:39:54,560
on model initialization followed by a targeted pre-training and a fine-tuning stage, with each

332
00:39:54,560 --> 00:40:00,880
stage designed to streamline specific subtasks. Third, the introduction of cross-attention modules

333
00:40:00,880 --> 00:40:08,160
within the diffusion transformer DIT architecture improves text-condition injection while also

334
00:40:08,160 --> 00:40:15,600
refining the entire computation process, particularly in the computation-intensive

335
00:40:15,600 --> 00:40:21,840
class-condition branch. And finally, it leverages a large vision language model,

336
00:40:21,920 --> 00:40:24,640
a lava, to auto-label dense pseudo-captions.

337
00:40:27,280 --> 00:40:31,200
The first stage of the training strategy is to learn pixel dependency.

338
00:40:31,760 --> 00:40:36,800
This stage is dedicated to understanding the pixel distribution of natural images without

339
00:40:36,800 --> 00:40:42,880
the interference of text prompts, essentially teaching the model to the inherent structure

340
00:40:42,880 --> 00:40:48,640
and coherence found within the visual data. It does this by employing a class-condition

341
00:40:48,640 --> 00:40:56,480
diffusion transformer, DIT Excel 2, with 28 transformer blocks that have been pre-trained

342
00:40:56,480 --> 00:41:04,160
on the ImageNet dataset. This methodology allows Pixar Alpha to capture the essence of

343
00:41:04,160 --> 00:41:12,960
visual information. The model is trained on 1 million images over 300,000 training steps.

344
00:41:13,200 --> 00:41:19,360
So this paper leverages the diffusion transformer architecture,

345
00:41:20,560 --> 00:41:25,440
which pivots an architectural change from the traditional unit backbone of latent diffusion

346
00:41:25,440 --> 00:41:31,600
models to a transformer-based model. The diffusion transformer operates first by converting noise

347
00:41:31,600 --> 00:41:37,680
through linear layers and reshaping, followed by normalization and processing through multiple

348
00:41:37,680 --> 00:41:46,320
DIT blocks. These blocks are structured with multi-head self-attention and point-wise feed-forward

349
00:41:46,320 --> 00:41:53,520
layers interspersed with scaling and normalization. The conditional inputs such as the diffusion

350
00:41:53,520 --> 00:42:02,480
time steps and class labels are incorporated into the transformer via AMLP. The adaptation of the

351
00:42:02,560 --> 00:42:08,960
VIT blocks or vision transformer blocks equipped with the adaptive layer norm layers

352
00:42:09,600 --> 00:42:16,400
ensures flexible normalization. So furthermore, the integration of Pixar Alpha with this

353
00:42:16,400 --> 00:42:23,840
architecture utilizes a 250-60 frequency embedding followed by a two-layer MLP.

354
00:42:26,400 --> 00:42:32,080
So Stage 2 is centered on enhancing text image alignment learning. The shift from pre-trained

355
00:42:32,080 --> 00:42:40,640
class-guided image generation to this text-to-image generation calls for accurate alignment between

356
00:42:40,640 --> 00:42:47,840
detailed text descriptions and generated images. So a significant challenge in this stage arises

357
00:42:47,840 --> 00:42:53,280
from the text captions in the datasets, which often lack informative content. Are misaligned

358
00:42:53,280 --> 00:42:57,840
with their corresponding images or present a long-tailed distribution that makes learning from

359
00:42:57,840 --> 00:43:03,760
them difficult? So to counter these issues, Pixar Alpha implements a solution that involves

360
00:43:03,760 --> 00:43:09,360
creating highly informative text captions with the help of Lava, a large vision language model.

361
00:43:09,920 --> 00:43:17,040
By leveraging Lava in auto-labeling pipeline, Pixar Alpha aims to improve the informativeness

362
00:43:17,040 --> 00:43:23,600
of the text data, so thereby enhancing the training efficiency and alignment accuracy.

363
00:43:24,320 --> 00:43:31,920
So here are some examples of raw captions from the Lyon dataset and Lava refined captions.

364
00:43:32,640 --> 00:43:39,600
As seen, refinements can deal with problems like text image misalignment, deficient descriptions,

365
00:43:39,600 --> 00:43:43,520
and infrequent vocabulary, causing this long-tailed characteristic.

366
00:43:44,320 --> 00:43:54,400
So here is the detailed dataset refinement process. So using Lava, the dataset undergoes this

367
00:43:54,400 --> 00:44:02,880
auto-labeling process with prompt that encourage informative descriptions. So particularly the

368
00:44:02,880 --> 00:44:10,000
prompt asks it to describe this image and its style in a very detailed manner. So SAM is used

369
00:44:10,000 --> 00:44:17,120
instead of Lyon, since the segment anything dataset tends to lead to a more robust vocabulary

370
00:44:17,120 --> 00:44:23,680
demonstrated to a vocabulary analysis. So this analysis shows that Lava refined SAM and Lyon

371
00:44:23,680 --> 00:44:31,840
datasets increase in the number of total nouns and the ratio of valid distinct noun to total

372
00:44:31,840 --> 00:44:41,120
distinct noun. So valid distinct noun means nouns that occur more frequently. And SAM Lava is able

373
00:44:41,120 --> 00:44:50,560
to outperform Lyon Lava, so that's why it is used. In this stage, 10 million SAM captions are refined.

374
00:44:51,280 --> 00:45:00,720
So Stage 3 of the Pixar Lava training focuses on generating high-resolution images with a refined

375
00:45:00,720 --> 00:45:08,800
of set of quality. So this stage leverages fine-tuning on data with superior quality,

376
00:45:08,800 --> 00:45:17,040
specifically combining a 4 million image dataset from JourneyDB and a 10 million internal image dataset.

377
00:45:18,000 --> 00:45:26,720
The training in this stage benefits from the foundational work established in the previous

378
00:45:26,720 --> 00:45:36,800
stages, which results in a faster convergence. So the table shows that there are three scales

379
00:45:36,800 --> 00:45:46,000
of resolution used for training in this stage. And as the resolution increases from 256x256

380
00:45:46,000 --> 00:45:54,720
to 1024x1024, fewer training steps are needed in this stage and with a decrease in batch size.

381
00:45:55,920 --> 00:46:01,760
And this reflects an optimization of the learning process due to the high quality of the input data

382
00:46:01,760 --> 00:46:08,240
and the cumulative knowledge from earlier training steps. So here are some design considerations

383
00:46:08,240 --> 00:46:16,640
presented in this paper. So it uses the DIT XL2 as the base architecture and a 4.3 billion

384
00:46:16,640 --> 00:46:25,200
Flan T5 as the text encoder for conditional feature extraction and also a pre-train and

385
00:46:25,200 --> 00:46:30,000
frozen VAE encoder from latent diffusion model for latent feature extraction.

386
00:46:30,960 --> 00:46:37,760
And here within the DIT block, multi-hat cross-attention is used to facilitate interactions

387
00:46:37,760 --> 00:46:42,640
between the model and text embeddings extracted from the language model.

388
00:46:45,920 --> 00:46:51,120
So the authors of Pixar and Alpha modified the original adaptive layer norm layers in the

389
00:46:51,120 --> 00:46:57,280
diffusion transformer to make it more efficient. They found that linear projections in the adaptive

390
00:46:58,240 --> 00:47:06,080
layer norm layer module of the DIT account for substantial proportion, which is 27% of the

391
00:47:06,080 --> 00:47:12,560
parameters. And this large number of parameters is not useful since the class condition is not

392
00:47:12,560 --> 00:47:20,080
employed in text to image generation model. So they proposed the adaptive layer norm single

393
00:47:21,040 --> 00:47:28,560
where all in blocks share the same parameters for time conditions. Global MLP with layer-wise

394
00:47:28,560 --> 00:47:36,560
embeddings are used instead of layer-specific MLPs. So to adapt to this new structure,

395
00:47:36,560 --> 00:47:42,320
layer-specific trainable embeddings are introduced to adaptively adjust the shifts and scales

396
00:47:43,280 --> 00:47:48,800
in different blocks without the class condition. So this preserves the compatibility with

397
00:47:49,760 --> 00:47:54,720
when initialized to yield the same scales and shifts at the pre-trained diffusion transformer.

398
00:47:55,920 --> 00:48:03,120
So here are some experimental results on the MsCoCo dataset comparing the FID scores with

399
00:48:03,120 --> 00:48:09,360
other state-of-the-art models. Although it doesn't achieve the lowest FID score,

400
00:48:10,400 --> 00:48:16,560
it does have the lowest number of parameters, training images, as well as number of GPU days.

401
00:48:19,520 --> 00:48:26,080
So here are some results on the T2ICOM bench, a benchmark for assessing alignment

402
00:48:26,080 --> 00:48:32,400
between text conditions and generated images. And this shows that Pixar Alpha

403
00:48:33,680 --> 00:48:39,840
is very competitive in the categories of attribute binding and object relationship,

404
00:48:40,480 --> 00:48:47,760
as well as complexity. So the authors also conducted experiments on human evaluations

405
00:48:47,760 --> 00:48:56,560
based on generated images from 300 prompts. For this, 50 participants ranked models based on

406
00:48:56,560 --> 00:49:04,720
perceptual quality and text image alignment. As seen, the results show that Pixar Alpha

407
00:49:04,720 --> 00:49:10,480
outperforms other models in terms of both quality and alignment.

408
00:49:10,480 --> 00:49:21,120
Before moving to the last paper, we want to think about what makes a good generation model.

409
00:49:22,640 --> 00:49:28,800
So as we see here, the diffusion models cannot do fast sampling.

410
00:49:30,240 --> 00:49:39,360
Although it provides a good model coverage and diversity and produce high quality

411
00:49:39,920 --> 00:49:48,240
high fatality on the image, we wonder, are there any state-of-the-art methods that

412
00:49:48,880 --> 00:49:56,800
have diffusion models with faster sampling? This brings us to the final paper that we are

413
00:49:56,800 --> 00:50:06,080
presenting today, which is SDXL Turbo. It showcases a technique called ADD,

414
00:50:06,800 --> 00:50:14,160
which is an Adverse Serial Diffusion Distillation, a new distillation technique for text-to-image

415
00:50:14,160 --> 00:50:23,440
models. And there are some special properties of SDXL Turbo that the classifier-free guidance

416
00:50:23,440 --> 00:50:31,040
were not used here. And the main highlights of the paper is that it can do a single-step

417
00:50:31,040 --> 00:50:38,400
image generation, which is extremely fast. It can be even done in real-time, such that

418
00:50:39,120 --> 00:50:46,320
originally SDXL, it has to do like 50 steps, but in SDXL Turbo, you basically do it in one step.

419
00:50:48,320 --> 00:50:55,600
So intuitively, it's actually combining the superior sample quality from diffusion models,

420
00:50:56,320 --> 00:51:03,520
but also the inherent speed of GANs. So let's delve into this paper.

421
00:51:05,360 --> 00:51:12,480
But first, let's revisit the Adverse Serial Models, the Generative Adverse Serial Networks, GANs.

422
00:51:13,440 --> 00:51:21,520
It has a generator which optimizes to synthesize samples that capture the distribution of training

423
00:51:21,520 --> 00:51:28,720
data. Then we also have a discriminator which optimizes to classify the real and fake data.

424
00:51:29,840 --> 00:51:35,920
So during the training process, a generator will generate a bunch of fake images,

425
00:51:37,360 --> 00:51:43,520
while the training set, the actual training data, and the fake data are both fed to the

426
00:51:43,520 --> 00:51:48,400
discriminator so that the discriminator has to tell whether which one is fake or real.

427
00:51:48,480 --> 00:51:57,440
The both generators and discriminators are trained simultaneously in the case that both

428
00:51:57,440 --> 00:52:02,880
have to get better in each epoch during the training just to make sure that they are in

429
00:52:02,880 --> 00:52:11,280
fair fights so that they can learn from each other. The objective is to learn at a point

430
00:52:12,000 --> 00:52:19,440
that discriminator was unable to tell whether the generator's fake image is real or fake.

431
00:52:19,440 --> 00:52:24,080
So at that period of time, it means the generator has been successful chained,

432
00:52:24,720 --> 00:52:30,960
and after that, the discriminator can be dumped into a garbage bin and we only care about the

433
00:52:30,960 --> 00:52:41,360
generator. In SDXL Turbo, it actually has a student-teacher model so it's like a

434
00:52:41,360 --> 00:52:48,800
distillation process where the student is having like a relatively short time step

435
00:52:49,760 --> 00:52:54,480
while the teacher model is using a long time step for scheduling.

436
00:52:55,040 --> 00:53:01,680
So what is that in the training process is that first you have the latent

437
00:53:03,840 --> 00:53:08,240
which is basically compute by the forward diffusion process by adding

438
00:53:09,280 --> 00:53:18,640
noise progressively on it, then you get the XS here, then it got passed into the generator

439
00:53:19,280 --> 00:53:27,440
which is the student model. We're going to train this and it will generate an image

440
00:53:28,080 --> 00:53:36,880
that looks to be what we wanted here. Then it got fed into the discriminator to tell whether it's

441
00:53:37,600 --> 00:53:45,520
fake or not. This is very similar to the GAN methodology but also at the same time

442
00:53:46,240 --> 00:53:58,320
the synthesized output was added noise progressively again to fed into a teacher model

443
00:53:58,320 --> 00:54:04,480
which is a frozen, which is a freezing diffusion model to try to generate again.

444
00:54:05,120 --> 00:54:11,200
But this time it doesn't compute the gradient and then it computes the distillation laws

445
00:54:11,200 --> 00:54:17,840
to see which is like some kind of distant metric between the generated image from generator

446
00:54:18,400 --> 00:54:21,760
and also the generated image from the teacher diffusion model.

447
00:54:23,520 --> 00:54:29,840
So that it penalized the the gradients in the student model such that it was able to learn

448
00:54:30,480 --> 00:54:34,880
the teacher, I mean the student was able to learn how to become more like the teacher model.

449
00:54:35,200 --> 00:54:43,120
In that case, it was expected the student could eventually perform better than teacher

450
00:54:43,920 --> 00:54:48,800
while also fooling the discriminator so that the discriminator was not able to tell

451
00:54:49,520 --> 00:54:56,080
it's true or fake. That means the student has been successfully become a good synthesizer.

452
00:54:57,920 --> 00:55:04,320
And then a generation process, this becomes like a one pass model, not necessary one pass but

453
00:55:04,960 --> 00:55:10,800
it can inference at a very short time step such that you just simply fit a noise

454
00:55:11,440 --> 00:55:17,840
into it and then with some kind of tech conditioning you can generate it with

455
00:55:18,640 --> 00:55:23,360
you can generate it with very low time step such that it was able to compute in real time.

456
00:55:24,320 --> 00:55:36,320
So in detail, the generator was used in network like in unit again,

457
00:55:36,320 --> 00:55:45,440
a pixel space unit as a generator. While the discriminator is a vision transformer with

458
00:55:45,440 --> 00:55:54,560
the dyno objective and the teacher model, the paper used SDXL and also SDs 2.1 as a teacher model.

459
00:55:55,520 --> 00:56:04,240
For the training objective, they employ the traditional gain loss which is the adversarial

460
00:56:04,240 --> 00:56:10,960
loss and then they use the hinge loss with the gradient penalty which divides from the GP gain

461
00:56:10,960 --> 00:56:18,880
paper. And for the distillation loss, it's simply a weighted L2 loss which the CT is

462
00:56:19,760 --> 00:56:24,800
the alpha t according to the time step defined by them.

463
00:56:29,280 --> 00:56:36,800
So in summary, we cover six pivotal texture image models in this presentation including

464
00:56:37,440 --> 00:56:44,160
latent division model which is the first work to use the latent like the compressed latent

465
00:56:44,160 --> 00:56:51,040
and also the cross-attention. Then we have image gen paper which is a cascaded division model

466
00:56:51,040 --> 00:57:00,000
to do text to image generation. Then next we have Thor E2 which utilizes the clip embeddings

467
00:57:00,000 --> 00:57:07,920
and the latents to synthesize high-quality image. Then we have party, a transformer-based

468
00:57:07,920 --> 00:57:14,000
auto-regressive model and then we have pixel alpha which is a diffusion transformer.

469
00:57:14,960 --> 00:57:23,440
Finally, we have an SDXL turbo which utilized the adversarial distillation from SDXL.

470
00:57:24,320 --> 00:57:29,840
We also did cover some common metrics for t2i I mean text-to-image models including

471
00:57:30,720 --> 00:57:38,160
fasciitis inception distance which measures the image fidelity and also clip score to measure

472
00:57:38,160 --> 00:57:48,480
the image text alignment. In fact, there are more t2i models to explore so we can see here there are

473
00:57:49,280 --> 00:57:58,960
more than six latitudes that we just covered and we hope people who listen to our presentation

474
00:57:59,600 --> 00:58:03,840
will rise more interest into this area. Thank you.

