1
00:00:00,000 --> 00:00:10,780
Okay, cool. So in my part of the lecture, I'll cover three topics. I'll be first going

2
00:00:10,780 --> 00:00:16,480
over positional encoding, which basically discusses how we can incorporate positional

3
00:00:16,480 --> 00:00:23,120
information into our word embeddings. And then I'll be going over attention inference

4
00:00:23,120 --> 00:00:30,520
acceleration, which is which are methods that combine attention matrices to accelerate

5
00:00:30,520 --> 00:00:38,520
attention at inference. And third, I will wear based attention acceleration methods.

6
00:00:38,520 --> 00:00:46,720
So let's get started with positional encoding. So before we talk about the mechanism, the

7
00:00:46,720 --> 00:00:52,720
first mechanism rope, let's let me talk about why we need positional encoding.

8
00:00:52,720 --> 00:00:58,960
So suppose you have a transformer, you're feeding a sentence into transformers, and

9
00:00:58,960 --> 00:01:10,520
you have a lot of words. In particular, you have w1 to let's say wn. And the first thing

10
00:01:10,520 --> 00:01:15,720
you do is you feed them through embedding layer. It's called that like, e or something.

11
00:01:16,720 --> 00:01:26,520
And then you get vectors x1 to xn. These are the outputs of the embedding layer and

12
00:01:26,520 --> 00:01:37,600
w1 to wn are just raw words. And you feed x1, xn, you combine them into this big matrix

13
00:01:37,600 --> 00:01:55,200
x. And you say, okay, for every xi, I want to get, I want to get matrix matrices q, k,

14
00:01:55,200 --> 00:02:04,400
m, v. So the way that we do that is we have three matrices pq, pk, and pv. And then we

15
00:02:04,400 --> 00:02:17,280
do xpq equal to q. And then the same for k and v. And you can see what happens here

16
00:02:17,360 --> 00:02:27,640
is that x1 will be, this is effectively doing, multiplying, this is effectively, this here

17
00:02:27,640 --> 00:02:52,200
is effectively saying x1 pq, xn pq. Because we first multiply x1 by the first column pq,

18
00:02:52,200 --> 00:03:02,720
and then the second entry will be also x1 multiplied by the second entry of pq, which

19
00:03:02,720 --> 00:03:11,000
is exactly what we're going to get here. But the point is throughout this entire process,

20
00:03:11,000 --> 00:03:17,480
the positional information is not encoded. Here in the embedding, the embedding function,

21
00:03:17,480 --> 00:03:26,000
it doesn't take the index wi, it doesn't take this i into any consideration. And here when

22
00:03:26,000 --> 00:03:37,320
we, when we map it to q, the pq, when we multiply the matrix pq with just the matrix multiplication,

23
00:03:37,320 --> 00:03:43,120
so it also doesn't take the information, the positional information of the particular word

24
00:03:43,120 --> 00:03:53,760
into account. And when we later on do q, q, k, t, and take the softmax of it, and multiply

25
00:03:53,760 --> 00:04:01,320
v, you see, there's still nowhere do we consider the positional information. So, so in a way

26
00:04:01,320 --> 00:04:08,480
the positional information is lost, if we just do raw transformers and don't look at

27
00:04:08,480 --> 00:04:15,000
the positional encoding at all. So that's why we need positional information. And originally

28
00:04:15,000 --> 00:04:20,440
in the attention is all you need paper, there were two, two ways to combat this problem.

29
00:04:20,440 --> 00:04:32,000
So one way is to use sine cosine functions. And then we basically what we do is we add,

30
00:04:32,000 --> 00:04:40,520
we add to each of these x i's, some sine cosine values, I'll be going with this later,

31
00:04:40,520 --> 00:04:44,760
just to just to give you a brief on the other approach that was covered in tension is all

32
00:04:44,760 --> 00:04:54,000
you need. There were learned positional embeddings that we could apply to each of these words.

33
00:04:54,000 --> 00:05:08,620
So to talk about the sine cosine, sine cosine functions first, basically what we do is before

34
00:05:08,620 --> 00:05:18,960
we apply, so here is basically, we just talked about except they wrote it in w multiply x

35
00:05:18,960 --> 00:05:32,880
i, I wrote w x times p, you know, q k v. So we do here is for each of the x i's, we multiply

36
00:05:32,880 --> 00:05:50,280
we add a positional vector to it. And basically this positional vector i is going to be heavily

37
00:05:50,280 --> 00:06:06,920
dependent on what this i is. And see, this is the p i here. And at each position of x

38
00:06:06,920 --> 00:06:29,760
i. So suppose that x i is a small vector. At even indices, we add sine, okay, sine of

39
00:06:30,760 --> 00:06:52,480
i over one 10,000 to T over D. And at odd positions, we add sine of i over 10,000 to T, sorry,

40
00:06:52,800 --> 00:07:02,440
cosine over D. And i is the positional vector at i. So as you can see, this is p i will be heavily

41
00:07:02,440 --> 00:07:14,200
dependent on what I is. And it's going to be unique to the specific i. And T here is the index

42
00:07:14,520 --> 00:07:24,440
within the, within p i, because p i is not a single value, it's an entire vector. So, you know,

43
00:07:24,440 --> 00:07:34,440
here T will be equal to one, here T will be two, and it's going to increase as we go along. So it's

44
00:07:34,440 --> 00:07:41,080
basically p i is going to be a unique combination of these sine and cosine values. And altogether,

45
00:07:41,080 --> 00:07:46,440
they're supposed to transmit a unique signal. This vector is supposed to transmit a unique

46
00:07:46,440 --> 00:07:58,120
signal, depending on what this i is. So by adding this positional vector to x i, well, our goal is

47
00:07:58,120 --> 00:08:04,120
to retain some information. Now, before we feed it into the attention part, we want to add some

48
00:08:04,120 --> 00:08:14,200
signal so that the attention part can know that, hey, this is maybe the i-th position, because

49
00:08:14,200 --> 00:08:23,800
we've detected some, detected some signal here with the addition of p i. And there's been an

50
00:08:23,800 --> 00:08:33,560
ablation study that has shown that we can actually learn, based on the positional encoding,

51
00:08:34,520 --> 00:08:46,360
the absolute position of the, of the, of the basically, basically we can, we know that given

52
00:08:47,160 --> 00:08:55,080
x i plus p i, we can learn i with a linear, linear regression.

53
00:08:57,560 --> 00:09:03,640
So basically, this is why we kind of need, we need a positional encoding in the first place.

54
00:09:07,560 --> 00:09:14,360
Okay, so how does rope encode the positional encoding? So I'm just going to bring you back

55
00:09:14,360 --> 00:09:22,280
to here. So we don't necessarily have to encode the positional information at the

56
00:09:22,840 --> 00:09:33,400
signing soil stage. Consider the following setup. So we have w i, passes goes through an encoding,

57
00:09:34,440 --> 00:09:39,960
becomes x, and then it goes through these projection matrices, and it becomes q k and v.

58
00:09:39,960 --> 00:09:50,920
And what we can do is that we're going to say we believe that for, for q and k,

59
00:09:53,320 --> 00:10:01,640
here, here, like each entry, the i-jth entry of this matrix is q i t k j. We're going to say

60
00:10:01,640 --> 00:10:07,560
for for i and j that are far apart, then this signal doesn't matter so much.

61
00:10:08,520 --> 00:10:15,240
Um, before i and j that are close, this signal is going to matter a lot. And then we want to,

62
00:10:15,240 --> 00:10:22,440
we want to kind of convey this relative position information into, into the transformers.

63
00:10:24,280 --> 00:10:29,960
So, yeah, so that's the idea. So, so, so the way that rope does it is that

64
00:10:32,040 --> 00:10:36,760
basically the idea comes from in the complex plane. In the complex plane, if you want to rotate

65
00:10:36,840 --> 00:10:45,320
a vector x by theta, you multiply it by e to the i theta.

66
00:10:47,640 --> 00:10:56,440
So here we say, in ropey say, if we want to rotate it, for x m, we rotate x m by m theta.

67
00:10:56,920 --> 00:11:14,440
And we rotate x n by m theta. So as you can see, if, if x m and x n, which are assumed to be in the,

68
00:11:15,000 --> 00:11:24,920
roughly the same area that they're being mapped to by the, by the, by the, by the word embedding

69
00:11:24,920 --> 00:11:31,160
here, suppose that maps to like a particular area, that's, that's rather a small part of the whole

70
00:11:31,160 --> 00:11:39,000
in higher dimension. Um, if m and n are close, then they're rotated to roughly the same area.

71
00:11:39,000 --> 00:11:46,120
But if m and n are far apart, then, then they're also going to be far apart. And what that really

72
00:11:46,120 --> 00:11:53,880
means is that, uh, here, here we're really not rotating x m anymore. We're rotating like q m and

73
00:11:53,880 --> 00:12:02,920
you know, q n and then k m and then k n. What, what this means is that when we take the dot product

74
00:12:02,920 --> 00:12:15,320
here, the dot product is basically going to be small. Um, because we call that the dot product,

75
00:12:15,320 --> 00:12:21,720
it's going to be basically, it's going to be, okay, maybe this is not the right symbol, but

76
00:12:21,720 --> 00:12:35,080
basically it has a term of, it's equal to q i k j multiplied by the, the cos of the angle between

77
00:12:35,080 --> 00:12:43,240
them. So if this is rotated by m theta, um, and this is rotated by n theta, then some of this,

78
00:12:43,240 --> 00:12:49,560
the cos, the theta here is going to be something like m theta minus n theta, plus, plus some,

79
00:12:49,720 --> 00:12:59,480
some like smaller value. Um, so if m minus n is large, that means the theta is going to be large.

80
00:13:00,280 --> 00:13:05,320
So then the cosine value is going to be small. Um, so then the signal won't, won't be as big.

81
00:13:05,320 --> 00:13:11,640
But if m and n are close, then the value, this dot product is going to be large. So that's,

82
00:13:11,640 --> 00:13:16,440
that's a way of telling the transformers to, to keep the product product large, if i and j are

83
00:13:16,440 --> 00:13:25,960
close. Um, but keep them small when they're far, far further apart. Uh, and the way we do that in

84
00:13:25,960 --> 00:13:33,080
the complex plane is multiplying by e to the i of m theta. But if we consider, if we consider, uh,

85
00:13:34,760 --> 00:13:41,320
if we consider this, the complex plane to be a bijection to the, to the r2 plane, um,

86
00:13:42,200 --> 00:13:49,800
okay, first of all, e to the i, e to the i m theta is equal to cos of

87
00:13:50,920 --> 00:14:00,040
m theta plus i sine of m theta. And then if we take this to be one coordinate, and we take this

88
00:14:01,000 --> 00:14:06,040
to be the other coordinate, this is the imaginary coordinate, and this is the real coordinate.

89
00:14:06,040 --> 00:14:12,280
Um, and we multiply it to some, some, some arbitrary number.

90
00:14:16,280 --> 00:14:27,880
We'll basically get that multiplying by this, by this number to this is multiplying by, uh,

91
00:14:28,200 --> 00:14:40,600
by a matrix to this. And that matrix is exactly this here. So basically we successfully math this

92
00:14:40,600 --> 00:14:47,320
from, uh, from, from c to r, r2. And the way that we rotate a vector in r2 is basically just

93
00:14:47,320 --> 00:14:52,920
multiplying by this matrix here. And, um, I won't go into more details about this. And if we want

94
00:14:52,920 --> 00:15:00,920
to do this in higher dimensional, say, d dimensional space, um, what these guys propose is very

95
00:15:00,920 --> 00:15:10,520
simple. So suppose that we have, uh, this vector here, this is basically either q m or k m.

96
00:15:11,720 --> 00:15:16,680
And it looks something like this. We say that we, we take every pair of coordinates and we

97
00:15:16,680 --> 00:15:23,960
rotate the first pair by m theta one, rotate the second pair by m theta two, and so on and so forth.

98
00:15:26,200 --> 00:15:31,800
And, and by the way, we keep, we keep the theta i's to be this, which are relatively smaller

99
00:15:31,800 --> 00:15:41,160
numbers. Um, they're smaller. So, so to ensure that we don't rotate a full cycle. So basically if m and

100
00:15:41,160 --> 00:15:48,520
n are, if m and n are actually far enough, m minus n times theta i, you know, it's equal to like,

101
00:15:49,160 --> 00:15:53,880
like two pi or something. So then it actually means that m and n are like kind of close. So we don't,

102
00:15:53,880 --> 00:16:01,720
we don't want that to happen. So to keep, so we keep it small. Um, and yeah, so this is what I

103
00:16:01,720 --> 00:16:07,400
talked about, um, keeping the, basically want to apply the positional encoding at, during the,

104
00:16:08,360 --> 00:16:13,720
uh, during the matrix multiplication between q and k. And this is q. This is k.

105
00:16:15,080 --> 00:16:23,880
This is basically q m. This is k n. And we rotated this by n theta and this m theta. And,

106
00:16:25,320 --> 00:16:34,120
you know, that's basically equivalent, uh, to multiplying by this matrix,

107
00:16:34,840 --> 00:16:44,840
um, here. If we take the transpose of everything, basically what we can do is multiply these two

108
00:16:44,840 --> 00:16:53,640
first, uh, which look like this. And when you multiply this by the transpose of the other one,

109
00:16:53,640 --> 00:17:00,680
which is going to be the negative theta, you're going to get m minus m theta.

110
00:17:01,080 --> 00:17:08,760
And we can actually do the, the m minus m theta very fast. So we don't actually have to multiply,

111
00:17:09,560 --> 00:17:17,400
this huge matrix right here by another huge one. Um, we can instead do this quick operation here.

112
00:17:18,280 --> 00:17:23,880
And we can continue doing this for linear attention because common choices of them,

113
00:17:23,880 --> 00:17:29,560
of the map here, um, basically, it's basically just like a normalization scheme.

114
00:17:30,280 --> 00:17:39,160
So, like, they're still around. So this is like q and k. And then this is like v of

115
00:17:39,880 --> 00:17:49,880
q and v of k. Like, they're still around the same area, um, after a normalization scheme.

116
00:17:49,880 --> 00:17:55,080
So rotating them, I guess the idea is that rotating, um, would still work.

117
00:17:59,640 --> 00:18:06,280
Okay. So another, another thing to note is that

118
00:18:07,320 --> 00:18:14,360
sinusoidal encoding is applied at the very beginning. So applied

119
00:18:15,800 --> 00:18:23,720
right here, when the word is mapped to embedding, we add, uh, here we add like a positional encoding

120
00:18:23,720 --> 00:18:30,520
PI. Yeah, like here. And it's only applied once throughout the entire transformers and

121
00:18:30,520 --> 00:18:39,880
architecture. However, for rope, it is applied at every q and k. Every time you multiply,

122
00:18:39,880 --> 00:18:52,120
you do this, you apply rope. Um, and yeah, this is like a bound on basically saying that, uh,

123
00:18:52,120 --> 00:19:00,040
if, if you have two things that are far apart, um, this is the relative distance. So this is

124
00:19:00,040 --> 00:19:11,560
basically m minus m. This is the, the upper bound, um, and, uh, the upper bound, upper bound of

125
00:19:11,880 --> 00:19:20,040
how, um, how large their dot products between the rotated, rotated q and m and

126
00:19:21,400 --> 00:19:31,640
are, uh, yeah, this multiplied by q and m and then multiplied by, uh, the rotated k and how far,

127
00:19:31,960 --> 00:19:44,200
okay, and there's nothing here. So basically how, how far these could be.

128
00:19:46,120 --> 00:19:52,440
And it's saying basically when, when, when n minus m is large and they're far apart,

129
00:19:53,400 --> 00:19:59,880
this value, this bound goes down, which is kind of what we want. Um,

130
00:20:01,720 --> 00:20:09,160
so here is how rope performs. So as you can see here on the, on this task over here,

131
00:20:09,160 --> 00:20:16,200
it does a bit better. Um, and on, on, on these tasks, on these downstream tasks,

132
00:20:17,320 --> 00:20:18,680
it performs better for some of them.

133
00:20:19,240 --> 00:20:33,560
Um, and for the mask lm scores, um, basically, uh, roll former, which the blue line is lower.

134
00:20:35,720 --> 00:20:41,480
Same for lm here. This is masked. This is just normal lm and it's used by gb3.

135
00:20:41,960 --> 00:20:49,400
Okay. So let's, that was a very fancy mechanism. And, um, let's look like, look at the next

136
00:20:49,400 --> 00:20:55,480
mechanism, which, which is called alibi. And it's much simpler compared to the previous one.

137
00:20:56,600 --> 00:21:07,400
And, um, the, the motivation for alibi is that, uh, models can take arbitrary long

138
00:21:07,400 --> 00:21:12,440
input lengths. Transformers models can take arbitrary long input lengths compared to what

139
00:21:12,440 --> 00:21:19,320
it has seen during your training. Um, so again, if you recall, what transformers does is basically

140
00:21:20,280 --> 00:21:26,680
it multiplies this x, which is your, uh, your embeddings that came from words.

141
00:21:27,480 --> 00:21:38,360
Um, and there's, suppose there's n words and suppose the dimension of each of them is d,

142
00:21:39,160 --> 00:21:47,480
and during, when, when you projected to matrix q, it multiplies x with the matrix q, which is,

143
00:21:48,280 --> 00:21:52,040
um, which is of dimension

144
00:21:54,600 --> 00:21:58,200
here, d times dq.

145
00:22:06,600 --> 00:22:13,720
So, so therefore how, how, however large n is, we can always multiply these two matrices together

146
00:22:13,720 --> 00:22:19,480
and transformers will also always work. The thing is when we train on shorter lengths,

147
00:22:19,480 --> 00:22:26,840
length x's, it's usually unable to generalize well to longer length x's, x's, um, at inference time.

148
00:22:28,520 --> 00:22:35,400
So alibi overcomes this and it's able to extrapolate to longer sequence lengths.

149
00:22:37,160 --> 00:22:41,400
And again, we've considered sinusoidal, which is intentionally all you need, rope, we just covered

150
00:22:41,400 --> 00:22:51,320
this and, uh, and there's also another learned embedding called t5 bias. Um, and the idea is that,

151
00:22:52,920 --> 00:22:59,240
you know, sinusoidal is the worst and then rope and it's t5 bias, but alibi is still better. Um,

152
00:23:01,880 --> 00:23:08,680
and another idea is that we might want to compute how, we don't want to compute the exact contact

153
00:23:08,680 --> 00:23:13,720
length that was fed in. Like another argument is that this might not be a fair comparison.

154
00:23:13,720 --> 00:23:19,640
And what is actually fair is if we consider the amount of compute it takes, because sinusoidal

155
00:23:19,640 --> 00:23:28,280
is much lighter. So for example, um, for sinusoidal, maybe training like the length of 1024 is same

156
00:23:28,280 --> 00:23:38,040
as training at the same transformers with the t5 bias, uh, with, with like context length 512.

157
00:23:38,760 --> 00:23:45,720
Um, because t5 bias takes a more compute, so training this and training this actually takes

158
00:23:45,720 --> 00:23:55,320
around the same compute. So what alibi does is it basically adds, so for, so for query i,

159
00:23:56,280 --> 00:24:03,400
it adds, um, query i dotted k is like you have one q here.

160
00:24:06,040 --> 00:24:12,200
And you have like a huge k matrix. You do this, multiply by this, you get like this value here

161
00:24:12,200 --> 00:24:17,960
and then you get this multiply by this and you get this value here. Um, and basically at this

162
00:24:17,960 --> 00:24:32,520
position here is the ith out of like many. Um, here we add minus i minus one. Here we add minus

163
00:24:32,520 --> 00:24:44,760
i minus two all the way to zero. And then zero is after that. Um, yeah, the reason that we, we,

164
00:24:44,760 --> 00:24:50,440
we add this is, um, for the same reason as before. For q, for q, for i and j,

165
00:24:51,720 --> 00:25:01,080
j is the same like the k index. For i and j that are far apart, we want the qi, qi dot k, we want qi

166
00:25:01,640 --> 00:25:07,880
kjt. We want this value to be relatively smaller signal to our transformers model.

167
00:25:08,840 --> 00:25:15,640
Um, it's in a way it doesn't matter as much. Um, and the, the reason that we have zeros all after

168
00:25:15,640 --> 00:25:22,760
that is because, um, we want to add a mask so that the transformer doesn't cheat during training.

169
00:25:22,760 --> 00:25:32,520
So basically, um, we have like, we have like, uh, what do we have? We have like my name

170
00:25:32,680 --> 00:25:41,400
is Matt. And during training, the q is going to, the first q is going to be like on my,

171
00:25:42,680 --> 00:25:47,480
and if we're trying to do language modeling, k is also going to be on my, and then, um,

172
00:25:48,120 --> 00:25:55,960
so that's going to be like the first one. And then, and then, and then q is going to have,

173
00:25:56,680 --> 00:26:09,000
um, basically my, this is basically, uh, wq times x1 equal to q1.

174
00:26:11,480 --> 00:26:15,480
And then we're going to have, and then we're going to have, sorry, this is not,

175
00:26:21,240 --> 00:26:22,120
and then we're going to have,

176
00:26:26,440 --> 00:26:35,480
name is equal to like x2. So we're going to have, we're going to get q2 here. And

177
00:26:38,040 --> 00:26:48,840
the idea is that when we're doing q1 times kt, um, we don't want to be able to get k2,

178
00:26:49,800 --> 00:26:54,840
which tells us about name, because this is equal to

179
00:26:57,240 --> 00:27:07,960
wq, wk times x2, and x2 is based on name. So, so we basically, if we're trying to complete,

180
00:27:12,280 --> 00:27:18,120
we're trying to complete the sentence qi, like if we're trying to complete the sentence my,

181
00:27:18,200 --> 00:27:23,400
we don't want k, k2, it already tell you what the next word name is.

182
00:27:25,320 --> 00:27:29,880
So, so yeah, that's kind of the reason we have zeros all after that, because we add a mask anyways.

183
00:27:34,360 --> 00:27:40,920
So yeah, uh, this is the same idea and m is a head specific constant specified before training.

184
00:27:41,720 --> 00:27:47,400
And this is the, with n heads, this will be set, set m to b.

185
00:27:49,720 --> 00:27:58,200
And that's just, I guess the number that worked well for them. And these, they observed that

186
00:27:58,920 --> 00:28:02,280
trainable m's didn't work well, and slopes between zero and one

187
00:28:02,920 --> 00:28:05,480
that had a higher density around zero worked well.

188
00:28:05,880 --> 00:28:13,480
Uh, so yeah, so, so this is, each head is going to have a different m.

189
00:28:15,320 --> 00:28:17,640
And therefore, it's going to take

190
00:28:21,880 --> 00:28:26,600
l times l times n space, where l is basically,

191
00:28:28,600 --> 00:28:32,840
we kind of, kind of like mixed up the terminology here, l is the length of the,

192
00:28:32,840 --> 00:28:38,680
the context and it's l times l, because that's the size of q, k, t.

193
00:28:39,960 --> 00:28:46,680
And you have, you kind of have to have a different, um, a different n for every head.

194
00:28:49,240 --> 00:28:54,840
So, so I did a different m for every head. So, so you keep a q, k, t for every head.

195
00:28:54,840 --> 00:28:59,080
So that's why it takes that much space. So it's also applied to every layer, just like sinusoidal.

196
00:29:03,800 --> 00:29:09,560
Um, we can also talk about perplexity here. So this is just basically explaining what

197
00:29:09,560 --> 00:29:16,200
perplexity is. So we want the LM to be able to assign real, real sentences, high probabilities.

198
00:29:16,200 --> 00:29:22,040
That's the idea. Um, and perplexity is some function of this probability function.

199
00:29:22,040 --> 00:29:32,040
So basically, lower perplexity is better. And just to comparison between alibi versus sinusoidal,

200
00:29:33,480 --> 00:29:44,040
um, with, with alibi trained on half the tokens compared to the sinusoidal one, um,

201
00:29:45,960 --> 00:29:51,640
alibi performs around the same as the sinusoidal one when tested on the longer sequence that

202
00:29:51,640 --> 00:29:56,200
sinusoidal was trained on, even though it uses, it's faster and uses less memory.

203
00:29:57,080 --> 00:30:06,520
And here are some results. Um, they're all Trino 512 tokens here. As you can see, alibi stays,

204
00:30:06,520 --> 00:30:15,960
stays pretty good. While the others just diverge. The same here for 10,000, 10,024 tokens.

205
00:30:15,960 --> 00:30:28,680
And here you can see that alibi extrapolates pretty well. Trino 512 where sinusoidal, um,

206
00:30:28,680 --> 00:30:34,600
when trained on 512, it doesn't, basically performs here. And you don't really see the next one

207
00:30:34,600 --> 00:30:45,800
because it explodes. And, uh, that's the same phenomenon here. Just explodes. So the performance

208
00:30:45,800 --> 00:30:53,160
just, um, it's very poor as, as soon as you increase the input length by a little bit,

209
00:30:53,160 --> 00:31:02,760
but alibi does, does fine. And, um, here's the validation perplexity. And it shows that alibi,

210
00:31:03,720 --> 00:31:09,320
um, trained on half the tokens, does just as well on, on the tokens, basically that

211
00:31:10,040 --> 00:31:16,840
playing, playing sinusoidal's game here, basically on 10,024 tokens. And here is basically 2048 tokens.

212
00:31:21,560 --> 00:31:26,920
Okay. So now I'll be going over multi-query attention. Multi-query attention is basically

213
00:31:26,920 --> 00:31:40,120
reusing the same, um, the same WQK, okay, V, okay, KQKV matrices across different heads.

214
00:31:40,120 --> 00:31:44,920
So usually we would have one of these for every single head. So it looks something like this.

215
00:31:48,200 --> 00:31:54,280
But now, uh, we all, we use this, if we use the same one, then it looks something like this. And

216
00:31:54,280 --> 00:32:01,800
that's supposed to give us speed up because we only have one matrix. Um, attention basics,

217
00:32:01,800 --> 00:32:08,600
I won't be going over it again. And yeah, let me just, uh, I won't go through the in-depth analysis

218
00:32:08,600 --> 00:32:15,080
here, but let me tell you what, what, at least with, with each of these variables mean. So

219
00:32:15,960 --> 00:32:25,640
basically X is your, um, is your input matrix and it has context length N.

220
00:32:29,800 --> 00:32:32,040
And this is the input to your encoder.

221
00:32:36,520 --> 00:32:42,760
And this is the M is the input to your decoder, basically. And then this has length M.

222
00:32:45,560 --> 00:33:01,400
So D is the, is the size of every word embedding.

223
00:33:02,920 --> 00:33:07,320
Um, H is the number of heads. B is the batch that you're taking this over.

224
00:33:08,280 --> 00:33:16,440
Um, and yeah, K, K and V are the sizes of W, K and WV.

225
00:33:20,680 --> 00:33:26,360
And yeah, I think that should be all. And basically what we want is we want the total

226
00:33:26,360 --> 00:33:33,160
memory access of multi head attention to be, and not to confuse multi head attention with

227
00:33:33,160 --> 00:33:38,200
multi query intention, which is the, which is what we're covering here. Multi head attention is

228
00:33:38,200 --> 00:33:45,640
the original attention mechanism suggested and attention is all you need. So, um, here we have,

229
00:33:47,240 --> 00:33:53,400
we have memories to operation ratio for training. You know, it's, it's a lot less than one. It's

230
00:33:54,120 --> 00:34:05,240
one over K, um, K is about here. And plus one over BN, which should be a lot less than one.

231
00:34:07,240 --> 00:34:14,840
But for inference, it becomes N over D plus one over B. And one over B, it's easy to deal with.

232
00:34:14,840 --> 00:34:19,960
We just increase the batch size to be larger. But N over D is something that we might need to worry

233
00:34:19,960 --> 00:34:27,640
about. Um, when, when our context length is roughly, uh, for example, if our complex length

234
00:34:27,640 --> 00:34:34,920
is like a thousand words or something, and, and our date, the value of D, um, which is the dimension

235
00:34:34,920 --> 00:34:42,280
of, of each, each embedding, uh, word embedding, it's also like around the thousand, then we might

236
00:34:42,280 --> 00:34:46,440
need to worry because it's not, this is not a lot less than one anymore.

237
00:34:50,840 --> 00:35:00,280
And so yeah, it's, it's very, it's a very simple idea. We just basically use one. We can see here,

238
00:35:00,280 --> 00:35:09,000
we had the PK, we had H of them for each head and the D times K. Um, but as soon as we use

239
00:35:09,000 --> 00:35:15,640
multi-query attention, it's now just DK and DV. Uh, and I guess here we don't actually need to,

240
00:35:16,600 --> 00:35:22,120
we don't actually need to worry about, we don't actually need to worry about PQ here because,

241
00:35:22,920 --> 00:35:31,880
um, because we're dealing with an encoder, decoder task. So the bottleneck comes from

242
00:35:32,680 --> 00:35:38,680
repeatedly reloading K and V. So if you think about it, if I want to like do inference again

243
00:35:38,680 --> 00:35:53,400
with like my name is Matthew, and if we wanted to like translate it into like French or something,

244
00:35:53,400 --> 00:36:04,600
it would be like, je m'appelle. Okay, my French is not good enough for this, uh, Mathieu or something.

245
00:36:04,600 --> 00:36:12,120
So, um, so suppose that we're doing, we're doing, we're doing this task and we start with

246
00:36:13,800 --> 00:36:24,600
my, the word my, um, at inference, what we need to do is, uh, at inference, we have this entire

247
00:36:24,600 --> 00:36:29,640
query. My name is Matthew, um, and we want to translate it. And the first time we're gonna,

248
00:36:29,640 --> 00:36:36,600
we're gonna get, and then, um, the Q, the PQ, or like the Q matrix is going to remain the same.

249
00:36:37,480 --> 00:36:48,520
Q equal to PQ and X, but, um, the K and V are going to equal to P, K, V, multiplied by M.

250
00:36:48,520 --> 00:36:52,360
This M is going to keep on changing. The input to our decoder is going to keep on changing because

251
00:36:52,920 --> 00:36:58,120
after the first word, we have a, and then we have, uh, my, and then we have my,

252
00:36:59,080 --> 00:37:03,320
so we're gonna have to keep on reloading this K and V, and that's where the bottleneck comes from.

253
00:37:04,040 --> 00:37:09,720
Um, so yeah, during training, you know, it still does pretty well. No surprises here,

254
00:37:09,720 --> 00:37:13,720
but for during inferences is where this multi-query attention release shines.

255
00:37:13,720 --> 00:37:20,680
So before when, where we had, uh, N over D, um, which is, which is what we wanted to reduce.

256
00:37:20,680 --> 00:37:28,840
Now we have N over DH. Uh, we basically, because we, we had, we reduced the number of,

257
00:37:28,840 --> 00:37:34,600
number of Ks and Vs that we wanted to reload by a factor of H, it shows through here. Um,

258
00:37:35,560 --> 00:37:42,520
so, so, so then the memory to operation ratio is getting to Ketflow, and that's why multi-query

259
00:37:42,520 --> 00:37:48,680
attention works pretty well. Uh, okay, so, so let's look at some results. Um,

260
00:37:50,680 --> 00:37:55,000
so here, here are basically how, uh, how they perform.

261
00:38:00,440 --> 00:38:07,560
Um, you can see that multi-head still performs the best, uh, because it does have, have more heads,

262
00:38:07,560 --> 00:38:15,640
but multi-query is comparable to it. And, uh, and same, the same for here. Multi-head

263
00:38:15,640 --> 00:38:22,520
that's the best, uh, but multi-query, it's, you know, it's, it's comfortable. Um, and,

264
00:38:23,320 --> 00:38:30,840
and to look at the time that we saved, uh, here, again, if you look at the encoder time,

265
00:38:30,840 --> 00:38:36,360
it's all around the same because Q remains the same. This is the, this is the translation task.

266
00:38:36,360 --> 00:38:43,800
So Q remains the same. It's always going to be, my name is Matthew, but, um, so, so, so, you know,

267
00:38:43,800 --> 00:38:48,280
multi-query attention really didn't speed anything up there, but for the decoder part,

268
00:38:48,280 --> 00:39:00,360
we have to constantly reload my, my, my, so by doing that, um, we waste a lot of time if we reload

269
00:39:00,360 --> 00:39:07,080
H as, H times as many more heads and multi-head versus multi-query. So, yeah, so, so that's,

270
00:39:07,080 --> 00:39:15,800
that's a lot faster here. Um, yeah. So, so, so there's still some problems with multi-query

271
00:39:15,800 --> 00:39:23,000
attention. Uh, they're basically just two, two, two big problems. The first problem is that

272
00:39:23,880 --> 00:39:29,480
if we already have, you know, we were like the open AI trained GPT for spend like $2 billion

273
00:39:29,480 --> 00:39:35,160
on it or something. And then now you say, Oh, you know, what you really should do to speed up your

274
00:39:35,160 --> 00:39:42,200
inference is use multi-query attention and, but you, when you started training the model, you

275
00:39:42,200 --> 00:39:46,040
used multi-head attention. So, you know, what are you going to do now? Are you going to train,

276
00:39:46,040 --> 00:39:51,320
train the model from scratch and spend like another like a billion dollars on the training?

277
00:39:51,320 --> 00:39:57,240
Or is there a faster way to adapt your existing model, um, to multi-query attention? So it's

278
00:39:57,240 --> 00:40:02,760
faster during inference. Um, so the answer is affirmative. There is, it's called up training.

279
00:40:03,320 --> 00:40:08,600
And another thing is that we want to find an intermediate stage between having, you know,

280
00:40:09,400 --> 00:40:18,680
like in this example, having, having, uh, eight, eight, like here, there's eight heads. I'll

281
00:40:18,680 --> 00:40:31,320
sharing this one, I'll sharing one, uh, k, kv matrix and having eight separate, uh, kv matrices.

282
00:40:31,320 --> 00:40:35,000
We want to find some, some, somewhere in between. So what it would have felt like two of them,

283
00:40:35,000 --> 00:40:39,960
for example. Um, so that's called group query attention. That's how that's we're going to cover

284
00:40:39,960 --> 00:40:49,080
next. Um, so we're first going to cover up, up training and idea is very simple. Uh, so we basically,

285
00:40:49,800 --> 00:40:58,760
we basically have our original, um, for example, we already had our original, you know, k, k, uh,

286
00:40:58,760 --> 00:41:06,120
k different projection matrices and we take the mean and we say, okay, now we have a new,

287
00:41:06,200 --> 00:41:12,120
new projection matrix. Uh, um, and this might not work well at first, but we're, we're going to say,

288
00:41:12,120 --> 00:41:17,880
okay, we're going to train it with multi-query attention and they're, and then it turns out,

289
00:41:17,880 --> 00:41:22,760
if we just do 5% of the original training steps, this, this matrix turns out to be good enough.

290
00:41:22,760 --> 00:41:29,400
We can also randomly initialize it. Uh, it doesn't do a, it's not, it doesn't, it doesn't work as well.

291
00:41:29,400 --> 00:41:34,600
And we can also initialize it from like, uh, a particular key projection matrix also doesn't

292
00:41:34,600 --> 00:41:42,120
work as well. Uh, the meaningful works the best. Um, and for the other thing I was talking about,

293
00:41:42,120 --> 00:41:46,200
instead of having, you know, only one, we can maybe have like four and separate these two,

294
00:41:46,200 --> 00:41:52,040
these heads into four groups. So two of them share a single one. So, you know, this is sort of in

295
00:41:52,040 --> 00:42:02,360
between and, um, uh, up training can also be applied to group query. Uh, namely we just,

296
00:42:02,360 --> 00:42:06,200
we just pulled these two heads, me and pulled these two heads into this one and train.

297
00:42:07,960 --> 00:42:13,240
Okay. So the benefits of group query attention. So one is that it might be like a natural trade

298
00:42:13,240 --> 00:42:19,960
off between MQA and MHA between performance and time. Um, you know, it's going to have,

299
00:42:19,960 --> 00:42:22,920
in terms of speed, it's going to be somewhere in the middle, but in terms of,

300
00:42:24,200 --> 00:42:27,720
in terms of performance, it's always, it's also going to be somewhere in the middle.

301
00:42:27,720 --> 00:42:33,480
And it might be just be the best middle ground. Um, and moreover, if you consider MQA,

302
00:42:34,200 --> 00:42:37,560
where, you know, eight of these, eight of these heads,

303
00:42:39,960 --> 00:42:45,400
I'll share a single, uh, I'll share a single projection matrix. Um,

304
00:42:47,480 --> 00:42:53,560
in standard charting, this matrix might be, you know, these four might be grouped into one

305
00:42:53,560 --> 00:42:59,640
chart and these four might need to grouped into another chart. And this, this, uh, this,

306
00:42:59,640 --> 00:43:05,320
this projection matrix here will have to, would originally have to been copied twice anyways.

307
00:43:05,320 --> 00:43:09,880
So, you know, that's kind of saying, you know, if, if, if it was going to cost this much memory

308
00:43:09,880 --> 00:43:18,760
anyways, why not just do GQA? And now in terms of GQA results, as you can see, um, in terms of time,

309
00:43:19,480 --> 00:43:27,480
it's comparable to MQA, but it beats MQA performance. And in terms of performance,

310
00:43:27,480 --> 00:43:33,000
it's comparable to MHA, but it beats it in time. Um, and here's a, here's a graph showing the trade

311
00:43:33,000 --> 00:43:43,240
off between GQA, uh, as the number of groups increase. So at one, basically, um, GQA is basically

312
00:43:43,880 --> 00:43:51,480
equivalent to MQA. Um, but as you scale it up, it's going to take, uh, I mean, it's going to take

313
00:43:51,480 --> 00:43:58,360
more time to sample. Uh, and that's basically shows the trade off. And here, uh, it shows the trade

314
00:43:58,360 --> 00:44:07,800
off between again, time and the performance. So as you can see, MHA here has the highest time,

315
00:44:08,440 --> 00:44:16,680
but also has the performance. And MQA has the, um, lowest time, but you know, also a decent

316
00:44:16,680 --> 00:44:21,560
performance. I mean, let's ignore this one. And out of the three has the lowest performance. And

317
00:44:22,440 --> 00:44:29,960
for GQA with, with eight groups, it has, you know, relatively good time and relatively good performance.

318
00:44:31,720 --> 00:44:36,200
And here are the results for up training. As you can see, when we take the mean, um,

319
00:44:37,160 --> 00:44:44,200
that's, that achieved the best performance with alpha equal to 0.05. And, uh,

320
00:44:46,600 --> 00:44:53,400
you can see here with alpha equal to 0.05, we can kind of achieve comparable performance

321
00:44:53,400 --> 00:45:01,000
with GQA and slightly worth within QA. Then with 1%, uh, basically identical performance

322
00:45:01,000 --> 00:45:13,640
with GQA and worth with MQA. Okay. And now, uh, I'm done my section on, on my, on group query

323
00:45:13,640 --> 00:45:18,920
and multi query retention. And I'll move to IO aware based methods, uh, namely flash attention.

324
00:45:19,720 --> 00:45:28,680
Um, so basically previous attention speed ups, um, many of them, you know, for example,

325
00:45:28,680 --> 00:45:35,880
they have sparser attention or low rank approximation. Um, the, the, they, what they

326
00:45:35,880 --> 00:45:44,280
don't focus on is input out being input and output aware. And what I mean by that is being,

327
00:45:44,280 --> 00:45:49,720
being using the SRAM, the in chip SRAM in a smart way. They don't, they don't take that into

328
00:45:49,720 --> 00:45:58,280
consideration. Um, and flash attention thinks about this and uses SRAM in a very smart way.

329
00:45:58,760 --> 00:46:06,120
So that, uh, there are fewer access to, to HBM, um, which is basically a larger, but slower memory.

330
00:46:07,960 --> 00:46:17,080
Yeah. So to give you some basics about this HBM and SRAM, there's like 40 to 80 gigabytes of HBM.

331
00:46:18,200 --> 00:46:23,960
You know, it's, it's not that fast and there's a much smaller SRAM unit, but it is much faster.

332
00:46:24,040 --> 00:46:31,720
So the idea is that instead of, um, instead of doing many trips of HBM, reason rights, like,

333
00:46:31,720 --> 00:46:42,600
suppose that this is the HBM, um, and this is your SRAM, this is your, um, this is your CPU,

334
00:46:42,600 --> 00:46:54,600
or sorry, GPU. This is where you like, this is where you, this is, this is your unit, this is

335
00:46:54,600 --> 00:47:04,040
the processing unit. Basically, you can think of it as retrieving information. If you keep on,

336
00:47:04,040 --> 00:47:09,720
you know, sending stuff from HBM and then sending it back, going forward, sending it back,

337
00:47:09,720 --> 00:47:16,280
it's going to take a long time. But if instead what you do is you, you say, okay, first I'm going to

338
00:47:16,280 --> 00:47:21,960
read from HBM and then I'm going to take, put some intermediate back, not all the way back to HBM,

339
00:47:21,960 --> 00:47:25,720
because that's going to take way too long, but I'm going to put some intermediate back into SRAM

340
00:47:25,720 --> 00:47:30,280
and then process again and then do some of this and then go all the way back to HBM.

341
00:47:30,280 --> 00:47:34,520
You know, this, that's going to be a loss, a lot more cost effective and that's the idea behind

342
00:47:34,520 --> 00:47:44,600
flash attention. And, um, you know, just, just for some reminder of the matrix dimensions and,

343
00:47:45,800 --> 00:47:55,800
you know, storing takes a one squared memory because, because of this. And, and, uh, yeah,

344
00:47:55,800 --> 00:48:01,560
so, so, so let's get into flash attention. And before that, let's just go over standard

345
00:48:01,640 --> 00:48:09,080
attention again. And this is the thing I was talking about, um, about going back and forth between

346
00:48:09,080 --> 00:48:16,760
HBM and the processing unit. So, you know, here you load from HBM, you write it back, you load,

347
00:48:16,760 --> 00:48:21,560
you know, you load, you load it, you write it back, you load it again, you write it back again,

348
00:48:21,560 --> 00:48:25,880
you load it again, you write it back again, and finally you return it.

349
00:48:26,760 --> 00:48:31,400
Um, you know, that's, that's, that's not the smartest way to do things and it doesn't really

350
00:48:31,400 --> 00:48:39,160
use SRAM to speed this process up. Um, so the idea, the idea is that we can use SRAM. Well,

351
00:48:39,160 --> 00:48:44,120
how can we use it? So first of all, we can use this method called tiling, which basically says,

352
00:48:44,120 --> 00:48:54,120
okay, so this is your SRAM here. You break it up into, um, a few different slots.

353
00:48:55,080 --> 00:49:03,160
We dedicate one slot for the Q, one slot for K, one slot for V, and one slot for O.

354
00:49:04,040 --> 00:49:11,480
And then here we dedicate two, you know, smaller linear slots for L, and then another for M,

355
00:49:11,480 --> 00:49:21,880
and I'll explain what they mean after. Um, so the final goal of what we want to achieve,

356
00:49:21,880 --> 00:49:31,800
so suppose that we originally, we started with your Xs. What we want to do is we want to compute

357
00:49:31,800 --> 00:49:45,480
your, um, your, your Q, K, T, and then we want to take the softmax of it.

358
00:49:49,400 --> 00:50:00,520
And the reason, okay, so, so, okay, so what does the softmax function look like? Okay,

359
00:50:00,520 --> 00:50:05,000
so suppose that this matrix, okay,

360
00:50:12,520 --> 00:50:18,200
suppose that this matrix is only a single vector, um, we want to do is take the exp,

361
00:50:19,160 --> 00:50:24,600
we take it to, to, to, to, to the exponential and then we subtract by the maximum.

362
00:50:25,400 --> 00:50:32,920
And then we divide it by this L, which is the sum across all of them.

363
00:50:36,120 --> 00:50:41,800
Um, this is L. And, uh, that's basically our end goal.

364
00:50:41,960 --> 00:50:55,160
Now, because the SRM is so small, each time it's impossible for us to load an entire Q

365
00:50:55,720 --> 00:51:02,200
or entire K or entire V. So we can only, we can only compute a very smart part of it,

366
00:51:02,760 --> 00:51:09,560
a small part of it, uh, at once, and we'll have to concatenate them together. The problem is that

367
00:51:09,640 --> 00:51:22,440
there's a softmax outside of Q, K, T. So, um, if our final goal is, this is, okay, this is without

368
00:51:22,440 --> 00:51:31,080
the softmax Q, K, T, V, uh, just for illustration, FR, and basically this is, uh, this is our O

369
00:51:31,080 --> 00:51:35,560
without the softmax. If our final goal is to compute something like this

370
00:51:35,560 --> 00:51:46,680
and each time we only have like Q, Q, like a small component of Q. So, so suppose that this is an

371
00:51:46,680 --> 00:51:54,120
entire Q, uh, this is Q. And this, each time we only load a little bit of Q and we load, uh, this

372
00:51:54,120 --> 00:52:02,040
is K, we only load a lit, this is KT, we only load a little bit of K. Um, so that means out of this

373
00:52:02,040 --> 00:52:10,760
entire output, we only have like Q from one to like, maybe like somewhere here. And then K,

374
00:52:12,280 --> 00:52:18,280
uh, sorry, Q goes this way, Q goes this way. So Q, one to like somewhere here. And then K

375
00:52:18,280 --> 00:52:25,320
from here to over here. So we only have like this part. And then what we have to compute is,

376
00:52:26,040 --> 00:52:31,400
what we have, we do is we compute this and then we compute this, and we compute this,

377
00:52:31,400 --> 00:52:37,000
we compute this, and then we compute this and an added to this and then we compute this and added

378
00:52:37,000 --> 00:52:49,640
to this. Um, and then etc, so let me just draw that more clearly come first compute this little

379
00:52:49,640 --> 00:52:55,400
part, and then this, and then this, and then compute this, and then add it to this, compute

380
00:52:55,400 --> 00:53:00,200
this, add it to this, compute this, add it to this. Now the problem is that if we're

381
00:53:00,200 --> 00:53:06,840
taught, we have to take the softmax. So we can't just compute this and take its softmax

382
00:53:06,840 --> 00:53:10,960
and then compute this and take its softmax and add it to this. You know, that doesn't

383
00:53:10,960 --> 00:53:16,720
work like that. You're going to get the wrong softmax values. So that's why we need to keep

384
00:53:16,800 --> 00:53:26,680
an array of m, which is the maximum of this row here, and then another array of l, which

385
00:53:26,680 --> 00:53:36,800
is basically the sum here. And we're going to use this to finally compute the correct

386
00:53:36,800 --> 00:53:46,120
softmax at the very end. Okay, so how do we do it? So high level idea. What we do is

387
00:53:46,160 --> 00:53:57,200
we start out with this. Yeah, maybe let me just go through the pseudo code or, yeah.

388
00:53:57,200 --> 00:54:03,320
So the high level idea is that we do this, and then we do this and do this and do this.

389
00:54:03,320 --> 00:54:12,160
And then when we get to here, basically we normalize this by some factor. And when we

390
00:54:12,200 --> 00:54:18,160
get to here, we multiply it by that factor again, and then divide this by the softmax

391
00:54:18,160 --> 00:54:24,280
for the entire thing. And then we repeat. So then when we get to the very end, we basically

392
00:54:24,280 --> 00:54:34,880
are multiplying, we have the entire thing divided by the alpha factor for everything.

393
00:54:34,880 --> 00:54:46,240
So yeah, so let's take a look at the pseudo code. So we first loop on K and V. That means

394
00:54:46,240 --> 00:55:00,400
the inner loop is on Q and O. Yeah, maybe we should have this Q, O, K, V, L, M. This

395
00:55:00,400 --> 00:55:14,560
is our SRAM chip. We initially load K, K, I, KJ, VJ, which is basically like a subset

396
00:55:14,560 --> 00:55:25,360
of K and V. You can think of the entire matrix of K like this, KT. And then KJ is like some

397
00:55:25,360 --> 00:55:38,080
number of columns of K. And then V is some number of VJ. This is KJT. This is like VJT.

398
00:55:38,400 --> 00:55:49,520
This is V. And the same for Q and O. So we load KJ, VJ from HBM onto SRAM. And if you

399
00:55:49,520 --> 00:55:56,560
look at the final outputs of Q, KT, and V, that means we're trying to narrow down for,

400
00:55:57,760 --> 00:56:04,640
you know, for example, if J only includes two rows, then we're only looking at this group here.

401
00:56:05,440 --> 00:56:17,600
And then we loop over Q, which means we go down this way. And then we load Q and O.

402
00:56:17,600 --> 00:56:25,920
So we load the first one. And on chip, we compute Q, Q, I, J, Q, I, K, J, T. And so we first compute

403
00:56:25,920 --> 00:56:32,080
this. And then we compute this. And then we compute this. And the question is, what do we do after

404
00:56:32,080 --> 00:56:41,920
every single one of them? Okay, so as an example, we first compute this here. And we compute the

405
00:56:42,640 --> 00:56:57,040
max of this row. And then we normalize it with, we take this row, and then we take its value,

406
00:56:57,040 --> 00:57:11,760
and we take it to the exponent. So we do exponent of all of these values to it. Oh,

407
00:57:11,760 --> 00:57:17,120
by the way, this is sorry, this is not the, this is not the intermediate value. This is the,

408
00:57:17,920 --> 00:57:27,760
what I should really be doing is towards this, this is Q, K, T. So we take a small component of it,

409
00:57:29,200 --> 00:57:38,160
like somewhere here first. And then we look at each row. So for the first row, we look at the max,

410
00:57:38,160 --> 00:57:45,120
and then we do the exponent to this value minus the row max. And then the next value minus the row

411
00:57:45,120 --> 00:57:51,520
max next value minus row max. And then we look at the next row and we say this row, this value

412
00:57:51,520 --> 00:58:02,320
e to the this minus the max of this, etc. And then we divide by this L. And then we say,

413
00:58:04,400 --> 00:58:10,320
okay, so let's let's look at if this, this maximum value. So suppose that we were like

414
00:58:10,320 --> 00:58:17,040
somewhere in the middle here, if this maximum value were the max across all of these previous values. So

415
00:58:21,440 --> 00:58:26,400
if we already did this part, already did this part, and then we're like looking at this partition.

416
00:58:27,520 --> 00:58:33,520
So maybe like the third, the third K, K three or something. And then we're looking at Q one.

417
00:58:34,480 --> 00:58:41,680
And we look at the maximum here, and we say, Oh, is this maximum, you know, is it larger than the previous two?

418
00:58:45,920 --> 00:58:51,440
So and then and then and then we take the maximum here, we take the true maximum here. And then for

419
00:58:51,440 --> 00:58:56,240
the previous two, we have the previously computed maximum here. And then we basically

420
00:58:57,760 --> 00:59:02,480
multiply it by that maximum to get rid of it in the previous two terms that was already saved

421
00:59:02,480 --> 00:59:08,800
in the previous O. And then we subtract by the true maximum, because we need to divide everything

422
00:59:08,800 --> 00:59:14,800
by the by basically e to the negative mi prime. And then we do the same thing for our current

423
00:59:14,800 --> 00:59:22,880
current one. We want to multiply it by whatever it currently is divided, divided by the global

424
00:59:22,880 --> 00:59:36,400
maximum. And then what we want to write to to the O I is basically we want to have our original O I.

425
00:59:36,400 --> 00:59:39,440
So we already computed this, we already computed this. Now we're somewhere here.

426
00:59:40,640 --> 00:59:48,080
We want to normalize it, we want to multiply by L L I, because we divided by that a lot before.

427
00:59:48,720 --> 00:59:57,840
And we divided by the global softmax softmax true softmax. And, and yeah, we do the same thing

428
00:59:57,840 --> 01:00:04,000
on our third one here. That's that's this term. And this is basically these terms that was already

429
01:00:04,000 --> 01:00:11,520
written. And and we rinse and repeat and then we compute the entire thing. So yeah, hopefully

430
01:00:11,520 --> 01:00:17,520
that was clear enough. So flash attention guarantees. So it requires of an additional memory,

431
01:00:17,600 --> 01:00:28,400
which comes from L and M. So, but of course, it requires way fewer HBM accesses, because basically

432
01:00:28,400 --> 01:00:33,200
we already do all the intermediate steps, we're basically do on the chip, and then we basically

433
01:00:33,200 --> 01:00:42,960
do one final write at the end. Yeah, these are on the chip, right? So, so yeah, and they prove

434
01:00:42,960 --> 01:00:48,080
some theorem that says in terms of memory access complexity, there's nothing better under some

435
01:00:48,080 --> 01:00:57,200
mild assumptions. And the next thing is blocks bars flash attention, it's basically an addition to

436
01:00:57,200 --> 01:01:09,520
this. So in blocks bars attention, basically, in this, this matrix here, basically, we say, oh, for

437
01:01:09,520 --> 01:01:14,960
these, we only care about like, for example, like the center ones. And for these, like, we don't

438
01:01:14,960 --> 01:01:20,880
really care about them, because they're too far apart. And that's very easy to do, you know, just

439
01:01:20,880 --> 01:01:33,840
chop this QKT into into blocks. And set this to zero, set this to zero, set this to zero. And

440
01:01:33,840 --> 01:01:39,200
during the for loop, you know, just skip it. Just do a continue statement in that for loop.

441
01:01:39,280 --> 01:01:51,440
And you won't assign anything to it. So, yeah, then in terms of its results, yeah, you know,

442
01:01:51,440 --> 01:01:59,280
it's a it's it's much faster than before. And yeah, again, the performance is should be the

443
01:01:59,280 --> 01:02:04,640
exact same, because we're not really changing anything. Anything inherent to the model or

444
01:02:04,640 --> 01:02:09,280
model training, where all we're doing is changing the way that we read them right on numbers.

445
01:02:13,360 --> 01:02:19,920
And again, here shows the speed up. Here's the training time.

446
01:02:24,800 --> 01:02:29,840
And as you can see, does a lot faster. It's, you know, it's a lot faster than before.

447
01:02:30,480 --> 01:02:34,320
Ford only covered the Ford pass, there's actually additional backward pass.

448
01:02:35,200 --> 01:02:38,240
You can you can check it out in the appendix of the flash attention paper.

449
01:02:39,840 --> 01:02:43,200
So, yeah, so flash attention is the black one.

450
01:02:45,360 --> 01:02:52,160
Block sparse flash attention, of course, the fastest. And then, and then it's the green one,

451
01:02:52,160 --> 01:02:57,280
and then it's flash attention. Actually, Lin former beats it sometimes. But it's kind of cheating,

452
01:02:57,280 --> 01:03:01,760
because, you know, flash attention is still of n squared, you're still storing, you're still

453
01:03:01,760 --> 01:03:06,720
storing the entire QKT matrix, whereas Lin former, I think, is only storing something linear,

454
01:03:08,240 --> 01:03:10,800
or the or the computation is is linear.

455
01:03:12,800 --> 01:03:20,160
Okay, so flash attention actually also achieves in particular some some some pretty amazing results

456
01:03:20,160 --> 01:03:27,760
on path x. So what it does is it achieves non random performance on path x,

457
01:03:30,320 --> 01:03:38,960
where where it is the first transformer to achieve. She achieved this non random. Sorry.

458
01:03:44,080 --> 01:03:49,360
So flash attention is also the first to achieve non random performance. It is the first transformer

459
01:03:49,360 --> 01:03:54,880
to achieve non random performance on path x with 16 k context. And flash attention with

460
01:03:54,880 --> 01:04:01,840
block sparse attention is the first sequence to sequence model to achieve non random performance

461
01:04:01,840 --> 01:04:08,000
on 16 64 k context. Now let me talk about flash attention version two.

462
01:04:10,480 --> 01:04:15,840
So there's a few improvements over the existing one. So if you recall from before, instead of

463
01:04:16,400 --> 01:04:20,800
instead of dividing,

464
01:04:23,920 --> 01:04:29,120
remember how we basically loaded this and loaded this and then loaded this computed it

465
01:04:29,120 --> 01:04:34,960
computed it. And then we computed this. And then, but back here we divided by some l.

466
01:04:37,280 --> 01:04:42,320
And then, and then here we multiplied that l back and divided by l prime, which is the sum of

467
01:04:42,320 --> 01:04:48,240
everything. Basically, it's saying that we don't need to do that. We just don't divide by l in

468
01:04:48,240 --> 01:04:58,480
the first place and wait till the end to divide by this big L. And so yeah, so that's that's the idea

469
01:04:58,480 --> 01:05:06,960
here. So that reduces some non matrix multiplication flops, which are usually slower to compute.

470
01:05:07,360 --> 01:05:13,120
And also, another thing is that when we're doing causal masking language modeling,

471
01:05:14,320 --> 01:05:22,480
if we're going to mask, this is this is the q, this is the q k t matrix. And if we're going to

472
01:05:22,480 --> 01:05:28,000
mask this part anyways, you know, we never had to compute in the first place and we just escape

473
01:05:28,000 --> 01:05:35,040
it during our for loop. And another thing that flash attention v2 does for us is parallelism.

474
01:05:35,760 --> 01:05:41,760
So in v2, rather than we recall that this was in the k and v were in the inner loop in flash

475
01:05:41,760 --> 01:05:49,200
attention. In v2, we keep it on the outer loop. We keep it on the outer loop because

476
01:05:50,480 --> 01:05:56,640
so considered like let's look at this diagram here, consider this basically the square to be

477
01:05:57,520 --> 01:06:11,200
q k t. And if we parallelize across q, what does that mean? That means we have one worker

478
01:06:11,200 --> 01:06:17,040
starting here, and then going this way. Recall that before was like, if we was like going this

479
01:06:17,120 --> 01:06:30,960
way and this way, where we like, we like computed. Okay, it's fun, where we compute it like this

480
01:06:30,960 --> 01:06:38,560
first and then this and this and this and this and this, etc. Now, now what we do is we we

481
01:06:38,560 --> 01:06:43,680
parallelize this way. So when workers, one is on this, it can immediately go to the backward

482
01:06:43,760 --> 01:06:48,720
pass on this. And by the time it gets to the second backward pass, worker two has also done this.

483
01:06:49,360 --> 01:06:54,960
So then these five workers here, you can see I'll end up doing six tasks, and they're all able to

484
01:06:56,400 --> 01:07:02,160
do it. It's possible to do it in a sequential manner because the way we arrange it. And so

485
01:07:02,160 --> 01:07:09,760
so that's the way we parallelize it. Another feature that flash attention v2 offers is warp

486
01:07:09,760 --> 01:07:15,520
partitioning. So within each thread block, there are different threads. And what we want to do is

487
01:07:15,520 --> 01:07:27,760
we want to split across these threads across q rather than across k. And the reason we want to do

488
01:07:27,760 --> 01:07:37,200
that is because when we split across q with each thread splits across q, then you know, completing

489
01:07:37,200 --> 01:07:44,240
this means that the cross the threads don't have to come communicate with each other. But if we

490
01:07:44,880 --> 01:07:49,120
split across k, you know, if we come to this and compute this, then we still have to add them

491
01:07:49,120 --> 01:07:53,360
together. Whereas this because you know, everything's already added together for you. And it's like

492
01:07:53,360 --> 01:08:00,560
row wise, we don't have to do the additional communication cost. And in terms of results,

493
01:08:00,560 --> 01:08:05,200
on the left hand side, we have the forward speed and the right hand hand side, we have the backward

494
01:08:05,200 --> 01:08:12,080
speed. And as you can see, purple is the flash attention to and it's much faster than any other

495
01:08:12,080 --> 01:08:21,120
existing method. And so yeah, that's here's the references. And thank you.

